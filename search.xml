<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Kafka</title>
      <link href="/2020/09/27/BigData/Kafka/"/>
      <url>/2020/09/27/BigData/Kafka/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Kafka概述">Kafka概述</a></li><li><a href="#Kafka快速入门">Kafka快速入门</a></li><li><a href="#Kafka架构深入">Kafka架构深入</a></li></ul><a id="more"></a><h2 id="Kafka概述"><a href="#Kafka概述" class="headerlink" title="Kafka概述"></a>Kafka概述</h2><ul><li>消息队列(Message Queue)<ul><li>传统消息队列的应用场景<br><img src="%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%BC%A0%E7%BB%9F%E5%BC%82%E6%AD%A5%E5%A4%84%E7%90%86%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.png" alt="消息队列传统异步处理应用场景"></li><li>消息队列的两种模式<ol><li>点对点模式(1对1,消费者主动拉取数据,消息收到后消息清除)<br>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费<br><img src="%E7%82%B9%E5%AF%B9%E7%82%B9%E6%A8%A1%E5%BC%8F%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="点对点模式示意图"></li><li>发布/订阅模式(一对多,消费者消费数据之后不会清除消息)<br>消息生产者(发布)将消息发布到topic中，同时有多个消息消费者(订阅)消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费<br><img src="%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E6%A8%A1%E5%BC%8F%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="发布订阅模式示意图"></li></ol></li></ul></li><li>Kafka定义：Kafka是一个分布式的基于发布/订阅模式的<strong>消息队列</strong>，主要应用于大数据实时处理领域</li><li>Kafka基础架构：<br><img src="Kafka%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Kafka基础架构示意图"><ol><li><strong>Producer</strong>：消息生产者，就是向kafka broker发消息的客户端</li><li><strong>Consumer</strong>：消息消费者，向kafka broker取消息的客户端</li><li><strong>Consumer Group(CG)</strong>：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者</li><li><strong>Broker</strong>：一台kafka服务器就是一个broker，一个集群由多个broker组成，一个broker可以容纳多个topic</li><li><strong>Topic</strong>：可以理解为一个队列，<strong>生产者和消费者面向的都是一个topic</strong></li><li><strong>Partition</strong>：为了实现扩展性，一个非常大的topic可以分布到多个broker(即服务器)上，<strong>一个topic可以分为多个partition</strong>，每个partition是一个有序的队列</li><li><strong>Replica</strong>：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失且kafka仍然能够继续工作，<strong>kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower</strong></li><li><strong>leader</strong>：每个分区多个副本的”主”，生产者发送数据的对象以及消费者消费数据的对象都是leader</li><li><strong>follower</strong>：每个分区多个副本中的”从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader</li></ol></li></ul><h2 id="Kafka快速入门"><a href="#Kafka快速入门" class="headerlink" title="Kafka快速入门"></a>Kafka快速入门</h2><ul><li><p>安装部署</p><ul><li><p>集群规划：Hadoop101、hadoop102、hadoop103各自都运行zookeeper和kafka</p></li><li><p>安装包下载：<a href="http://kafka.apache.org/downloads" target="_blank" rel="noopener">http://kafka.apache.org/downloads</a></p></li><li><p>集群部署</p><ul><li><p>解压安装包：tar -zxvf kafka_2.12-2.6.0.tgz -C /opt/module</p></li><li><p>修改配置文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 需要修改的地方</span><br><span class="line"># broker(主机)的全局唯一编号,不能重复</span><br><span class="line">broker.id=x</span><br><span class="line"># 设置允许删除topic功能</span><br><span class="line">delete.topic.enable=true</span><br><span class="line"># 设置kafka运行日志存放地址</span><br><span class="line">log.dirs=/opt/module/kafka_2.12-2.6.0/logs</span><br><span class="line"># 配置链接zookeeper集群地址</span><br><span class="line">zookeeper.connect=hadoop101:2181,hadoop102:2181,hadoop103:2181</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量(vim + source)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># KAFKA_HOME</span><br><span class="line">export KAFKA_HOME=/opt/module/kafka_2.12-2.6.0</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure></li><li><p>分发安装包(修改环境变量和配置文件的broker.id)：xsync kafka_2.12-2.6.0</p></li><li><p>启动集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 依次启动hadoop101、hadoop102、hadoop103的zookeeper</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 该命令hadoop101、hadoop102、hadoop103均需使用</span></span><br><span class="line">kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure></li><li><p>kafka群起脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for i in `cat $HADOOP_HOME/etc/hadoop/workers`</span><br><span class="line">do</span><br><span class="line">echo "========== $i =========="</span><br><span class="line">ssh $i 'kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties'</span><br><span class="line">echo $?</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>Kafka命令行操作(直接使用,不加参数可以查看用法)</p><ul><li><p>查看当前服务器中的所有topic：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --zookeeper hadoop101:2181 --list</span><br></pre></td></tr></table></figure></li><li><p>创建topic：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --zookeeper hadoop101:2181 --create --replication-factor 3 --partitions 1 --topic first</span><br></pre></td></tr></table></figure><p>选项说明：</p><ol><li>–topic：定义topic名</li><li>–replication-factor：定义副本数</li><li>–partitions：定义分区数</li></ol></li><li><p>删除topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 需要server.properties中设置delete.topic.enable=<span class="literal">true</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 否则只是标记删除</span></span><br><span class="line">kafka-topics.sh --zookeeper hadoop101:2181 --delete --topic first</span><br></pre></td></tr></table></figure></li><li><p>发送消息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 出现 &gt; ,可输入消息字符</span></span><br><span class="line">kafka-console-producer.sh --broker-list hadoop101:9092 --topic first</span><br></pre></td></tr></table></figure></li><li><p>消费消息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> --from-beginning会把topic以往所有的消息数据都取出来</span></span><br><span class="line">kafka-console-consumer.sh --bootstrap-server hadoop101:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure></li><li><p>修改分区数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --zookeeper hadoop101:2181 --alter --topic first --partitions 6</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Kafka架构深入"><a href="#Kafka架构深入" class="headerlink" title="Kafka架构深入"></a>Kafka架构深入</h2><ul><li><p>Kafka工作流程及文件存储机制<br><img src="Kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" alt="Kafka工作流程示意图"></p><ol><li><p>topic<br>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的<br>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时从上次的位置继续消费</p></li><li><p>Kafka文件存储机制<br><img src="Kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6.png" alt="Kafka文件存储机制示意图"><br>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——<strong>.index文件和.log文件</strong>。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称 + 分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。index和log文件以当前segment的第一条消息的offset命名</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure></li><li><p>.index和.log文件详解<br><img src="index%E5%92%8Clog%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3.png" alt="index和log文件详解示意图"><br>.index文件存储大量的索引信息，.log文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址</p></li></ol></li><li><p>Kafka生产者</p><ul><li>分区策略<ul><li>分区原因<ol><li><strong>方便在集群中扩展</strong>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以由多个Partition组成，因此整个集群就可以适应任意大小的数据了</li><li><strong>可以提高并发能力</strong>，因为可以以Partition为单位读写了</li></ol></li><li>分区原则：我们需要将producer发送的数据封装成一个ProducerRecord对象<br><img src="ProducerRecord%E5%AF%B9%E8%B1%A1%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0.png" alt="ProducerRecord对象构造函数"><ol><li>指明partition的情况下，直接将指明的值直接作为partiton值</li><li>没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值</li><li>既没有partition值又没有key值的情况下，第一次调用时随机生成一个整数(后面每次调用在这个整数上自增)，将这个值与topic可用的partition总数取余得到partition值，也就是常说的 round-robin(轮询)算法</li></ol></li></ul></li><li>数据可靠性保证<br>为保证producer发送的数据能可靠地发送到指定的topic，topic的每个partition收到producer发送的数据后都需要向producer发送ack(acknowledgement确认收到)，如果producer收到ack，就会进行下一轮的发送，否则重新发送数据</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM</title>
      <link href="/2020/09/26/Language/Java/JVM/"/>
      <url>/2020/09/26/Language/Java/JVM/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#JVM体系结构概述">JVM体系结构概述</a></li><li><a href="#堆体系结构概述">堆体系结构概述</a></li><li><a href="#堆参数调优">堆参数调优</a></li></ul><a id="more"></a><h2 id="JVM体系结构概述"><a href="#JVM体系结构概述" class="headerlink" title="JVM体系结构概述"></a>JVM体系结构概述</h2><ul><li><p>JVM位置：运行与操作系统之上(可以认为是一种中间件)，与硬件没有直接的交互<br><img src="JVM%E4%BD%8D%E7%BD%AE.png" alt="JVM位置"></p></li><li><p>JVM结构<br><img src="JVM%E7%BB%93%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="JVM结构示意图"></p><ol><li><p>类装载器ClassLoader</p><ul><li><p>作用：负责加载class文件，<strong>class文件在文件开头有特定的文件标示</strong>，将class文件字节码内容加载到内存中，并将这些内容转换成方法区中的运行时数据结构。ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定<br><img src="%E7%B1%BB%E8%A3%85%E8%BD%BD%E5%99%A8%E5%B7%A5%E4%BD%9C%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="类装载器工作示意图"></p></li><li><p>种类：</p><ol><li>虚拟机自带的加载器<ol><li>启动类加载器(Bootstrap) C++($JAVA_HOME/jre/lib/rt.jar)</li><li>扩展类加载器(Extension) Java($JAVA_HOME/jre/lib/ext/*.jar)</li><li>应用程序类加载器(AppClassLoader)，Java中也叫系统类加载器(System Class Loader)，加载当前应用的classpath的所有类</li></ol></li><li>用户自定义加载器：java.lang.ClassLoader的子类，用户可以定制类的加载方式<br><img src="%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt="类加载器继承图"></li></ol></li><li><p>种类案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Object obj = <span class="keyword">new</span> Object();</span><br><span class="line"><span class="comment">// null</span></span><br><span class="line"><span class="comment">// Object定义在rt.jar中,采用C++的Bootstrap加载器</span></span><br><span class="line">System.out.println(obj.getClass().getClassLoader());</span><br><span class="line"></span><br><span class="line">MyObject mObj = <span class="keyword">new</span> MyObject();</span><br><span class="line"><span class="comment">// sun.misc.Launcher$AppClassLoader@18b4aac2</span></span><br><span class="line"><span class="comment">// MyObject是用户自定义类,采用系统类加载器AppClassLoader</span></span><br><span class="line">System.out.println(mObj.getClass().getClassLoader());</span><br><span class="line"><span class="comment">// sun.misc.Launcher$ExtClassLoader@61bbe9ba</span></span><br><span class="line"><span class="comment">// 系统类加载器的父类即为扩展类加载器ExtClassLoader</span></span><br><span class="line">System.out.println(mObj.getClass().getClassLoader().getParent());</span><br><span class="line"><span class="comment">// null</span></span><br><span class="line"><span class="comment">// 扩展类加载器ExtClassLoader的父类即为C++的Bootstrap加载器</span></span><br><span class="line">System.out.println(mObj.getClass().getClassLoader().getParent().getParent());</span><br></pre></td></tr></table></figure></li><li><p>类加载机制(双亲委派)：一个类收到了类加载请求，它首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成，每一个层次类加载器都是如此，因此所有的加载请求都应该传送到启动类加载器中，只有当父类加载器反馈自己无法完成这个请求的时候(在它的加载路径下没有找到所需加载的Class)，子类加载器才会尝试自己去加载。采用双亲委派的一个好处是比如加载位于rt.jar包中的类java.lang.Object，不管是哪个加载器加载这个类，最终都是委托给顶层的启动类加载器进行加载，这样就保证了使用不同的类加载器最终得到的都是同样一个Object对象(也防止用户自定义了系统预先定义的类[包名和类名完全相同]造成的类加载冲突——编译器不报错,运行时报错)</p></li></ul></li><li><p>Execution Engine：执行引擎负责解释命令，提交操作系统执行</p></li><li><p>Native Interface(本地接口)：<br> 本地接口的作用是融合不同的编程语言为Java所用，它的初衷是融合C/C++程序(Java诞生的时候是C/C++横行的时候,要想立足,必须能够调用C/C++程序)，于是就在内存中专门开辟了一块区域处理标记为native的代码，它的具体做法是Native Method Stack中登记native方法，在Execution Engine执行时加载native libraies<br> 目前该方法使用得越来越少了，除非是与硬件有关的应用，在企业级应用中已经比较少见。因为现在的异构领域间的通信很发达，比如可以使用Socket通信、Web Service等</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Test.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  Thread t1 = <span class="keyword">new</span> Thread();</span><br><span class="line">  t1.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 同一个thread不能start()两次</span></span><br><span class="line">  <span class="keyword">if</span> (threadStatus != <span class="number">0</span>) <span class="keyword">throw</span> <span class="keyword">new</span> IllegalThreadStateException();</span><br><span class="line">  group.add(<span class="keyword">this</span>);</span><br><span class="line">  <span class="keyword">boolean</span> started = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    start0();</span><br><span class="line">    started = <span class="keyword">true</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (!started) &#123;</span><br><span class="line">        group.threadStartFailed(<span class="keyword">this</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable ignore) &#123; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">start0</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure></li><li><p>Native Method Stack：登记native方法，在Execution Engine执行时加载本地方法库</p></li><li><p>PC寄存器(类似汇编)：<br> 每个线程都有一个程序计数器，是线程私有的，就是一个指针，指向方法区中的方法字节码(用来存储指向下一条指令的地址,也即将要执行的指令代码)，由执行引擎读取下一条指令<br> 这块内存区域(空间)很小，几乎可以忽略不记，它是当前线程所执行的字节码的行号指示器，字节码解释器通过改变这个计数器的值来选取下一条需要执行的字节码指令<br> 如果执行的是一个Native方法，那这个计数器是空的<br> 用以完成分支、循环、跳转、异常处理、线程恢复等基础功能。不会发生内存溢出(OOM：Out Of Memory)错误</p></li><li><p>Method Area(方法区)：<br> 是供各线程共享的运行时内存区域。它存储了每一个类的结构信息，例如运行时常量池(Runtime Constant Pool)、字段和方法数据、构造函数和普通方法的字节码内容<br> 上面讲的是规范，在不同虚拟机中实现是不一样的，最典型的就是Java7的永久代(PermGen space)和Java8的元空间(Metaspace)<br> 实例变量存在堆内存中，和方法区无关</p></li><li><p>Stack(栈)：</p><ul><li>介绍：栈也叫栈内存，主管Java程序的运行。在线程创建时创建，它的生命期是跟随线程的生命期，线程结束栈内存也就释放，<strong>对于栈来说不存在垃圾回收问题，只要线程一结束该栈就Over，生命周期和线程一致，是线程私有的</strong>。<strong>8种基本类型的变量、对象的引用变量、实例方法都是在函数的栈内存中分配</strong></li><li>栈存储什么(主要保存3类数据)：<ol><li>本地变量(Local Variables)：输入参数、输出参数以及方法内的变量</li><li>栈操作(Operand Stack)：记录出栈、入栈的操作</li><li>栈帧数据(Frame Data)：包括类文件、方法等</li></ol></li><li>栈运行原理：<br>栈中的数据都是以栈帧(Stack Frame)格式存在，栈帧是一个内存区块、一个数据集、一个有关方法(Method)和运行期数据的数据集<br>当一个方法A被调用时就产生了一个栈帧F1，并被压入到栈中；A方法又调用了B方法，于是产生的栈帧F2也被压入栈；B方法又调用了C方法，于是产生的栈帧F3也被压入栈…..方法相继执行完毕后，先弹出F3栈帧，再弹出F2栈帧，再弹出F1栈帧……遵循“先进后出”/“后进先出”原则<br><strong>每个方法执行的同时都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息</strong>。每一个方法从调用直至执行完毕的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程。<strong>栈的大小和具体JVM的实现有关，通常在256K~756K之间，约等于1Mb左右</strong><br><img src="Java%E6%A0%88%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Java栈示意图"></li></ul></li></ol></li><li><p>栈/堆/方法区的交互关系<br>HotSpot(Java8)是使用指针的方式来访问对；Java堆中会存在访问<strong>类元数据</strong>的地址；reference存储的就是对象的地址<br><img src="%E6%A0%88%E5%A0%86%E6%96%B9%E6%B3%95%E5%8C%BA%E7%9A%84%E4%BA%A4%E4%BA%92%E5%85%B3%E7%B3%BB.png" alt="栈/堆/方法区的交互关系"></p></li></ul><h2 id="堆体系结构概述"><a href="#堆体系结构概述" class="headerlink" title="堆体系结构概述"></a>堆体系结构概述</h2><ul><li>Heap堆<ul><li>介绍：一个JVM实例只存在一个堆内存，堆内存的大小是可以调节的。类加载器读取了类文件后，需要把类、方法、常变量放到堆内存中，保存所有引用类型的真实信息，以方便执行器执行</li><li>组成部分(<strong>逻辑上划分</strong>)：<ol><li>Young Generation Space(新生区 Young/New)</li><li>Tenure Generation Space(老年区 Old/Tenure)</li><li>Permanent Space(永久区 Perm)<br><img src="%E5%A0%86%E5%86%85%E5%AD%98%E9%80%BB%E8%BE%91%E5%88%92%E5%88%86.png" alt="堆内存逻辑划分"></li></ol></li></ul></li><li>GC过程：<ol><li>新生区是类的诞生、成长和消亡的区域，一个类在这里产生、应用、最后被垃圾回收器收集结束生命。新生区又分为两部分：伊甸区(Eden Space)和幸存者区(Survivor pace)，所有的类都是在伊甸区被new创建出来的。幸存区有两个：0区(Survivor 0 Space)和1区(Survivor 1 Space)</li><li>当伊甸区的空间用完时，程序又需要创建对象，JVM的垃圾回收器将对伊甸区进行垃圾回收(Minor GC)，将伊甸区中的不再被其他对象所引用的对象进行销毁。然后将伊甸区中的剩余对象移动到幸存0区。若幸存0区也满了，再对该区进行垃圾回收，然后移动到1区，如果1区也满了再移动到老年区</li><li>若老年区也满了，那么这时候将发生Major GC(Full GC)进行老年区的内存清理。若老年区执行了Full GC之后发现依然无法进行对象的保存，就会产生OOM异常(OutOfMemoryError)</li></ol></li><li>Minor GC过程(复制 -&gt; 清空 -&gt; 互换)：<br><img src="GC%E8%A7%92%E5%BA%A6%E5%A0%86%E7%9A%84%E5%88%86%E7%B1%BB%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="GC角度堆的分类示意图"><ol><li><strong>Eden、Survivor From区复制到Survivor To区，年龄+1</strong>：首先，当Eden区满的时候会触发首次GC——把还活着的对象拷贝到Survivor From区。当Eden区再次触发GC时会扫描Eden区和From区，对这两个区域进行垃圾回收，经过这次回收后还存活的对象，则直接复制到To区域(如果有对象的年龄已经达到了老年的标准则赋复制老年区)，同时把这些对象的年龄+1</li><li><strong>清空Eden、Survivor From区</strong>：然后，清空Eden和Survivor From区中的对象</li><li><strong>Survivor To区和Survivor From区互换</strong>：最后，Survivor To和Survivor From互换，原Survivor To区成为下一次GC时的Survivor From区。部分对象会在From和To区域中复制来复制去，如此交换15次(由JVM参数MaxTenuringThreshold决定,默认为15)最终如果还是存活，就存入到老年区</li></ol></li><li>方法区(Method Area)<ul><li>介绍：实际而言，方法区(Method Area)和堆一样是各个线程共享的内存区域，它用于存储虚拟机加载的类信息、普通常量、静态常量和编译器编译后的代码等。<strong>虽然JVM规范将方法区描述为堆的一个逻辑部分，但它却还有一个别名叫做Non-Heap(非堆)，目的就是要和堆分开</strong></li><li>方法区的实现：对于HotSpot虚拟机，很多开发者习惯将方法区称之为永久代(Parmanent Gen)，但严格本质上说两者不同，永久代是方法区的一个实现。jdk1.7的版本中，已经将原本放在永久代的字符串常量池移走(永久带是1.7版本的叫法,1.8则为元空间Metaspace)<br><img src="Java7%E5%A0%86%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Java7堆体系结构示意图"></li><li>Java7永久区(7及之前)：永久存储区是一个常驻内存区域，用于存放JDK自身所携带的Class、Interface的元数据，也就是说它存储的是运行环境必须的类信息。被装载进此区域的数据是不会被垃圾回收器回收掉的，关闭JVM才会释放此区域所占用的内存</li></ul></li></ul><h2 id="堆参数调优"><a href="#堆参数调优" class="headerlink" title="堆参数调优"></a>堆参数调优</h2><ul><li><p>Java堆：<br><img src="Java7%E5%A0%86%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Java7堆示意图"><br><img src="Java8%E5%A0%86%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="Java8堆示意图"></p><ol><li>在Java8中永久代已经被移除，被一个称为元空间的区域所取代。元空间的本质和永久代类似</li><li>元空间与永久代之间最大的区别：<strong>永久带使用的JVM的堆内存；Java8以后的元空间并不在虚拟机中，而是使用本机物理内存</strong></li><li>默认情况下，元空间的大小仅受本地内存限制。类的元数据放入Native Memory，字符串池和类的静态变量放入Java堆中，这样可以加载多少类的元数据就不再由MaxPermSize控制而由系统的实际可用空间来控制</li></ol></li><li><p>堆参数(在VM options中指定)</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>-Xms</td><td>设置JVM初始内存大小,默认为物理内存的1/64</td></tr><tr><td>-Xmx</td><td>设置JVM最大分配内存,默认为物理内存的1/4</td></tr><tr><td>-XX:+PrintGCDetails</td><td>输出详细的GC处理日志</td></tr></tbody></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 返回Java虚拟机试图使用的最大内存量</span></span><br><span class="line"><span class="keyword">long</span> maxMemory = Runtime.getRuntime().maxMemory();</span><br><span class="line"><span class="comment">// 返回Java虚拟机中的内存总量</span></span><br><span class="line"><span class="keyword">long</span> totalMemory = Runtime.getRuntime().totalMemory();</span><br><span class="line">System.out.println(<span class="string">"-Xmx:maxMemory = "</span> + maxMemory + <span class="string">"Byte , "</span> + (maxMemory / (<span class="keyword">double</span>) <span class="number">1024</span> / <span class="number">1024</span>) + <span class="string">"MB"</span>);</span><br><span class="line">System.out.println(<span class="string">"-Xms:totalMemory = "</span> + totalMemory + <span class="string">"Byte , "</span> + (totalMemory / (<span class="keyword">double</span>) <span class="number">1024</span> / <span class="number">1024</span>) + <span class="string">"MB"</span>);</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// java.lang.OutOfMemoryError: Java heap space</span></span><br><span class="line"><span class="comment">// byte[] bytes = new byte[40 * 1024 * 1024];</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// VM Options: -Xms8m -Xmx8m -XX:+PrintGCDetails</span></span><br><span class="line">String str = <span class="string">"www.sobxiong.com"</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  str += str + <span class="keyword">new</span> Random().nextInt(<span class="number">88888888</span>) + <span class="keyword">new</span> Random().nextInt(<span class="number">99999999</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">[GC (Allocation Failure) [PSYoungGen: 1508K-&gt;496K(2048K)] 1508K-&gt;535K(7680K), 0.0017728 secs] [Times: user=0.01 sys=0.00, real=0.01 secs]</span></span><br><span class="line"><span class="comment">[GC (Allocation Failure) [PSYoungGen: 1882K-&gt;505K(2048K)] 1921K-&gt;797K(7680K), 0.0053175 secs] [Times: user=0.01 sys=0.00, real=0.00 secs]</span></span><br><span class="line"><span class="comment">[GC (Allocation Failure) [PSYoungGen: 2041K-&gt;352K(2048K)] 3331K-&gt;1891K(7680K), 0.0014598 secs] [Times: user=0.00 sys=0.00, real=0.01 secs]</span></span><br><span class="line"><span class="comment">[Full GC (Ergonomics) [PSYoungGen: 1488K-&gt;0K(2048K)] [ParOldGen: 5531K-&gt;1366K(5632K)] 7019K-&gt;1366K(7680K), [Metaspace: 3036K-&gt;3036K(1056768K)], 0.0054889 secs] [Times: user=0.01 sys=0.00, real=0.00 secs]</span></span><br><span class="line"><span class="comment">[GC (Allocation Failure) [PSYoungGen: 1074K-&gt;96K(2048K)] 4436K-&gt;3458K(7680K), 0.0015233 secs] [Times: user=0.01 sys=0.00, real=0.01 secs]</span></span><br><span class="line"><span class="comment">[GC (Allocation Failure) [PSYoungGen: 96K-&gt;96K(2048K)] 3458K-&gt;3458K(7680K), 0.0017007 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]</span></span><br><span class="line"><span class="comment">[Full GC (Allocation Failure) [PSYoungGen: 96K-&gt;0K(2048K)] [ParOldGen: 3362K-&gt;3363K(5632K)] 3458K-&gt;3363K(7680K), [Metaspace: 3055K-&gt;3055K(1056768K)], 0.0047922 secs] [Times: user=0.02 sys=0.00, real=0.00 secs]</span></span><br><span class="line"><span class="comment">[GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2048K)] 3363K-&gt;3363K(7680K), 0.0009790 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]</span></span><br><span class="line"><span class="comment">[Full GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2048K)] [ParOldGen: 3363K-&gt;3344K(5632K)] 3363K-&gt;3344K(7680K), [Metaspace: 3055K-&gt;3055K(1056768K)], 0.0045256 secs] [Times: user=0.02 sys=0.00, real=0.01 secs]</span></span><br><span class="line"><span class="comment">Heap</span></span><br><span class="line"><span class="comment">PSYoungGen      total 2048K, used 66K [0x00000007bfd80000, 0x00000007c0000000, 0x00000007c0000000)</span></span><br><span class="line"><span class="comment">  eden space 1536K, 4% used [0x00000007bfd80000,0x00000007bfd90978,0x00000007bff00000)</span></span><br><span class="line"><span class="comment">  from space 512K, 0% used [0x00000007bff80000,0x00000007bff80000,0x00000007c0000000)</span></span><br><span class="line"><span class="comment">  to   space 512K, 0% used [0x00000007bff00000,0x00000007bff00000,0x00000007bff80000)</span></span><br><span class="line"><span class="comment">ParOldGen       total 5632K, used 3344K [0x00000007bf800000, 0x00000007bfd80000, 0x00000007bfd80000)</span></span><br><span class="line"><span class="comment">  object space 5632K, 59% used [0x00000007bf800000,0x00000007bfb44040,0x00000007bfd80000)</span></span><br><span class="line"><span class="comment">Metaspace       used 3109K, capacity 4496K, committed 4864K, reserved 1056768K</span></span><br><span class="line"><span class="comment">  class space    used 338K, capacity 388K, committed 512K, reserved 1048576K</span></span><br><span class="line"><span class="comment">Exception in thread "main" java.lang.OutOfMemoryError: Java heap space</span></span><br><span class="line"><span class="comment">  at java.util.Arrays.copyOf(Arrays.java:3332)</span></span><br><span class="line"><span class="comment">  at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)</span></span><br><span class="line"><span class="comment">  at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:674)</span></span><br><span class="line"><span class="comment">  at java.lang.StringBuilder.append(StringBuilder.java:208)</span></span><br><span class="line"><span class="comment">  at com.xiong.jvm.Test2.main(Test2.java:12)</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 不是立刻执行,禁止使用</span></span><br><span class="line"><span class="comment">// System.gc();</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>回溯法</title>
      <link href="/2020/09/26/Algorithm/LeetCode/%E5%9B%9E%E6%BA%AF%E6%B3%95/"/>
      <url>/2020/09/26/Algorithm/LeetCode/%E5%9B%9E%E6%BA%AF%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#滑动窗口介绍">回溯法介绍</a></li><li><a href="#方法总结">方法总结</a></li><li><a href="#LeetCode题集">LeetCode题集</a></li></ul><a id="more"></a><h2 id="回溯法介绍"><a href="#回溯法介绍" class="headerlink" title="回溯法介绍"></a>回溯法介绍</h2><h2 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h2><h2 id="LeetCode题集"><a href="#LeetCode题集" class="headerlink" title="LeetCode题集"></a>LeetCode题集</h2><ol><li>22.括号生成<br><img src="Question22.png" alt="22.括号生成"></li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针</title>
      <link href="/2020/09/24/Algorithm/LeetCode/%E5%8F%8C%E6%8C%87%E9%92%88/"/>
      <url>/2020/09/24/Algorithm/LeetCode/%E5%8F%8C%E6%8C%87%E9%92%88/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#双指针介绍">双指针介绍</a></li><li><a href="#方法总结">方法总结</a></li><li><a href="#LeetCode题集">LeetCode题集</a></li></ul><a id="more"></a><h2 id="双指针介绍"><a href="#双指针介绍" class="headerlink" title="双指针介绍"></a>双指针介绍</h2><h2 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h2><h2 id="LeetCode题集"><a href="#LeetCode题集" class="headerlink" title="LeetCode题集"></a>LeetCode题集</h2><ol><li><p>11.盛最多水的容器<br><img src="Question11.png" alt="11.盛最多水的容器"></p></li><li><p>15.三数之和<br><img src="Question15.png" alt="15.三数之和"></p></li><li><p>18.四数之和<br><img src="Question18.png" alt="18.四数之和"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划</title>
      <link href="/2020/09/21/Algorithm/LeetCode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
      <url>/2020/09/21/Algorithm/LeetCode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#动态规划介绍">动态规划介绍</a></li><li><a href="#方法总结">方法总结</a></li><li><a href="#LeetCode题集">LeetCode题集</a></li></ul><a id="more"></a><h2 id="动态规划介绍"><a href="#动态规划介绍" class="headerlink" title="动态规划介绍"></a>动态规划介绍</h2><h2 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h2><h2 id="LeetCode题集"><a href="#LeetCode题集" class="headerlink" title="LeetCode题集"></a>LeetCode题集</h2><ol><li>10.正则表达式匹配<br><img src="Question10.png" alt="10.正则表达式匹配"></li><li>32.最长有效括号<br><img src="Question32.png" alt="32.最长有效括号"></li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>滑动窗口</title>
      <link href="/2020/09/20/Algorithm/LeetCode/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"/>
      <url>/2020/09/20/Algorithm/LeetCode/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#滑动窗口介绍">滑动窗口介绍</a></li><li><a href="#方法总结">方法总结</a></li><li><a href="#LeetCode题集">LeetCode题集</a></li></ul><a id="more"></a><h2 id="滑动窗口介绍"><a href="#滑动窗口介绍" class="headerlink" title="滑动窗口介绍"></a>滑动窗口介绍</h2><h2 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h2><h2 id="LeetCode题集"><a href="#LeetCode题集" class="headerlink" title="LeetCode题集"></a>LeetCode题集</h2><ol><li><p>3.无重复字符的最长字串<br><img src="Question3.png" alt="3.无重复字符的最长字串"></p></li><li><p>30.串联所有单词的子串<br><img src="Question30.png" alt="30.串联所有单词的子串"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JUC</title>
      <link href="/2020/09/18/Language/Java/JUC/"/>
      <url>/2020/09/18/Language/Java/JUC/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#JUC是什么">JUC是什么</a></li><li><a href="#Lock接口">Lock接口</a></li><li><a href="#线程间通信">线程间通信</a></li><li><a href="#线程间定制化调用通信">线程间定制化调用通信</a></li><li><a href="#线程八锁">线程八锁</a></li><li><a href="#线程不安全集合">线程不安全集合</a></li><li><a href="#Callable接口">Callable接口</a></li><li><a href="#JUC辅助类">JUC辅助类</a></li><li><a href="#ReentrantReadWriteLock读写锁">ReentrantReadWriteLock读写锁</a></li><li><a href="#BlockingQueue阻塞队列">BlockingQueue阻塞队列</a></li><li><a href="#ThreadPool线程池">ThreadPool线程池</a></li><li><a href="#Java8流式计算">Java8流式计算</a></li><li><a href="#Java8分支合并">Java8分支合并</a></li><li><a href="#异步回调">异步回调</a></li></ul><a id="more"></a><h2 id="JUC是什么"><a href="#JUC是什么" class="headerlink" title="JUC是什么"></a>JUC是什么</h2><ul><li><p>JUC介绍：JDK1.5时Java引入的并发编程工具包——java.util.concurrent</p></li><li><p>基础知识回顾：</p><ul><li><p>进程/线程是什么：</p><ul><li>进程：进程是一个具有一定独立功能的程序关于某个数据集合的一次运行活动。它是操作系统动态执行的基本单元，在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元</li><li>线程：通常在一个进程中可以包含若干个线程，当然一个进程中至少有一个线程，不然没有存在的意义。线程可以利用进程所拥有的资源，在引入线程的操作系统中，通常都是把进程作为分配资源的基本单位，而把线程作为独立运行和独立调度的基本单位，由于线程比进程更小，基本上不拥有系统资源，故对它的调度所付出的开销就会小得多，能更高效的提高系统多个程序间并发执行的程度</li></ul></li><li><p>进程/线程例子：</p><ul><li>进程：QQ.ext、word.exe</li><li>线程：word检查拼写、word容灾备份</li></ul></li><li><p>线程的状态</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Thread.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> State &#123;</span><br><span class="line">  NEW, <span class="comment">// 创建</span></span><br><span class="line">  RUNNABLE, <span class="comment">// 准备就绪(还需等待OS),Thread实例start()后并不是马上运行,只是进入就绪状态,等待OS</span></span><br><span class="line">  BLOCKED, <span class="comment">// 阻塞</span></span><br><span class="line">  WAITING, <span class="comment">// 等待(一直等下去——不见不散)</span></span><br><span class="line">  TIMED_WAITING, <span class="comment">// 等待(有时限的等待——过时不候)</span></span><br><span class="line">  TERMINATED; <span class="comment">// 终止</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>wait/sleep的区别：</p><ul><li>wait/sleep都可以使当前线程暂停</li><li>wait放开手睡眠，放开手里的锁</li><li>sleep握紧手睡眠，唤醒后手里还有锁</li></ul></li><li><p>并发/并行各自都是什么：</p><ul><li>并发：同一时刻多个线程在访问同一个资源(例子：抢车票)</li><li>并行：多项工作同时执行，之后在汇合(例子：泡脚玩手机)</li></ul></li></ul></li></ul><h2 id="Lock接口"><a href="#Lock接口" class="headerlink" title="Lock接口"></a>Lock接口</h2><ul><li><p>复习Synchronized：</p><ul><li><p><strong>多线程口诀1、2</strong>：</p><ul><li>高内聚低耦合</li><li>线程、操作、资源类</li></ul></li><li><p>实现步骤：</p><ol><li><p>创建资源类</p></li><li><p>资源类里创建同步方法(代码块)</p></li><li><p>创建线程，访问资源</p></li></ol></li><li><p>卖票实例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 题目：三个售票员,卖100张票</span></span><br><span class="line"><span class="comment">* 多线程编程的企业级套路 + 模版</span></span><br><span class="line"><span class="comment">* 1、高内聚低耦合</span></span><br><span class="line"><span class="comment">* 2、线程 操作(对外暴露的调用方法) 资源类</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SaleTicket</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Ticket ticket = <span class="keyword">new</span> Ticket();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建线程不能直接继承Thread类,因为Java是单继承,资源宝贵,要使用接口方式</span></span><br><span class="line">    <span class="comment">// 如果方法体简单,可以不用继承Runnable接口,而直接采用匿名内部类/lambda表达式</span></span><br><span class="line">    <span class="comment">// 创建线程要使用两个参数Thread(runnable, name)的方式</span></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">120</span>; i++) ticket.saleTicket();</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">120</span>; i++) ticket.saleTicket();</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">120</span>; i++) ticket.saleTicket();</span><br><span class="line">    &#125;, <span class="string">"C"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 资源类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ticket</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">saleTicket</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (number &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      System.out.println(Thread.currentThread().getName() + <span class="string">"\t卖出第"</span> + (number--) + <span class="string">"张票\t,还剩下"</span> + number + <span class="string">"张票"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Lock接口：</p><ul><li><p>Lock介绍(摘录自JDK1.8)：Lock implementations provide more extensive locking operations than can be obtained using synchronized methods and statements. They allow more flexible structuring, may have quite different properties, and may support multiple associated Condition objects —— 锁实现提供了比使用同步方法和语句可以获得的更广泛的锁操作。它们允许更灵活的结构，可能具有非常不同的属性，并且可能支持多个关联的条件对象</p></li><li><p>Lock的常用实现类ReentrantLock(可重入锁)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Lock使用模版</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SharedResource</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * synchronized与Lock的区别</span></span><br><span class="line"><span class="comment">  * 1、首先synchronized是java内置关键字,在jvm层面;Lock是个java类</span></span><br><span class="line"><span class="comment">  * 2、synchronized无法判断是否获取锁的状态,Lock可以判断是否获取到锁</span></span><br><span class="line"><span class="comment">  * 3、synchronized会自动释放锁(a:线程执行完同步代码会释放锁;b:线程执行过程中发生异常会释放锁);Lock需在finally中手动释放锁(unlock()方法释放锁),否则容易造成线程死锁</span></span><br><span class="line"><span class="comment">  * 4、用synchronized关键字的两个线程1和线程2,如果当前线程1获得锁,线程2线程等待。如果线程1阻塞,线程2则会一直等待下去;而Lock锁就不一定会等待下去,如果尝试获取不到锁,线程可以不用一直等待就结束了</span></span><br><span class="line"><span class="comment">  * 5.synchronized的锁可重入、不可中断、非公平,而Lock锁可重入、可判断、可公平(默认非公平,二者皆可)</span></span><br><span class="line"><span class="comment">  * 6.Lock锁适合大量同步的代码的同步问题,synchronized锁适合代码少量的同步问题</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">competitionMethod</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// block until condition holds</span></span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// ... method body</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Lock方式卖票实例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SaleTicket</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Ticket ticket = <span class="keyword">new</span> Ticket();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">120</span>; i++) ticket.saleTicket();</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">120</span>; i++) ticket.saleTicket();</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">120</span>; i++) ticket.saleTicket();</span><br><span class="line">    &#125;, <span class="string">"C"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 资源类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ticket</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">100</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Lock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saleTicket</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (number &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">"\t卖出第"</span> + (number--) + <span class="string">"张票\t,还剩下"</span> + number + <span class="string">"张票"</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="线程间通信"><a href="#线程间通信" class="headerlink" title="线程间通信"></a>线程间通信</h2><ul><li><p>题目：两个线程来操作初始值为零的一个变量，实现一个线程对该变量加1,另一个线程对该变量减1。实现交替10个轮次,变量初始值为0</p></li><li><p>线程间通信：</p><ol><li>生产者/消费者模型</li><li>通知等待唤醒机制</li></ol></li><li><p><strong>多线程口诀3</strong>：</p><ol><li>判断</li><li>干活</li><li>通知</li></ol></li><li><p>老版本synchronized实现：</p><ul><li><p>示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cake</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">increment</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、判断</span></span><br><span class="line">    <span class="keyword">if</span> (number != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">this</span>.wait();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、干活</span></span><br><span class="line">    number++;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"\t生产,剩余"</span> + number);</span><br><span class="line">    <span class="comment">// 3、通知</span></span><br><span class="line">    <span class="keyword">this</span>.notifyAll();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">decrement</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、判断</span></span><br><span class="line">    <span class="keyword">if</span> (number == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">this</span>.wait();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、干活</span></span><br><span class="line">    number--;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"\t消费,剩余"</span> + number);</span><br><span class="line">    <span class="comment">// 3、通知</span></span><br><span class="line">    <span class="keyword">this</span>.notifyAll();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 题目：两个线程来操作初始值为零的一个变量,实现一个线程对该变量加1,另一个线程对该变量减1;实现交替10个轮次,变量初始值为0</span></span><br><span class="line"><span class="comment">* 1、高聚合低耦合前提下,线程操作资源类</span></span><br><span class="line"><span class="comment">* 2、判断/干活/通知</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadWaitNotify</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Cake cake = <span class="keyword">new</span> Cake();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          cake.increment();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"ProducerA"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          cake.decrement();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"ConsumerA"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>结果：符合要求</p></li><li><p>如果换成4个线程(2消费者,2生产者)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 只改变main</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  Cake cake = <span class="keyword">new</span> Cake();</span><br><span class="line">  <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        cake.increment();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, <span class="string">"ProducerA"</span>).start();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        cake.decrement();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, <span class="string">"ConsumerA"</span>).start();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        cake.increment();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, <span class="string">"ProducerB"</span>).start();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        cake.decrement();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, <span class="string">"ConsumerB"</span>).start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>结果：出现错误，有可能生产出大于1的cake来</p></li><li><p>原因：换成4个线程会导致错误——虚假唤醒，因为在Java多线程判断时，不能用if。错误出在了判断上面：如果突然有一个增加cake的线程进入到if里面了，但突然中断了并交出控制权。等到唤醒后由于是if，不需要再次进行验证，而是直接走下去了，所以进行了错误的增加<br><img src="%E8%99%9A%E5%81%87%E5%94%A4%E9%86%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="虚假唤醒示意图"></p></li><li><p>解决方法：把所有的资源类的increment()和decrement()方法中的if判断变为while判断</p></li></ul></li><li><p><strong>多线程口诀4</strong>：注意多线程之间的虚假唤醒</p></li><li><p>新版本Lock实现：</p><ul><li><p>新老版本对标：</p><ol><li>synchronized - Lock</li><li>wait - await</li><li>notify - signal</li></ol></li><li><p>Lock示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cake1</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Lock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Condition condition = lock.newCondition();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">increment</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、判断</span></span><br><span class="line">      <span class="keyword">while</span> (number != <span class="number">0</span>) &#123;</span><br><span class="line">        condition.await();</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">"\t生产,剩余"</span> + number);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、干活</span></span><br><span class="line">      number++;</span><br><span class="line">      <span class="comment">// 3、通知</span></span><br><span class="line">      condition.signalAll();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">decrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、判断</span></span><br><span class="line">      <span class="keyword">while</span> (number == <span class="number">0</span>) &#123;</span><br><span class="line">        condition.await();</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">"\t消费,剩余"</span> + number);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、干活</span></span><br><span class="line">      number--;</span><br><span class="line">      <span class="comment">// 3、通知</span></span><br><span class="line">      condition.signalAll();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 题目：两个线程来操作初始值为零的一个变量,实现一个线程对该变量加1,另一个线程对该变量减1;实现交替10个轮次,变量初始值为0</span></span><br><span class="line"><span class="comment">* 1、高聚合低耦合前提下,线程操作资源类</span></span><br><span class="line"><span class="comment">* 2、判断/干活/通知</span></span><br><span class="line"><span class="comment">* 3、多线程交互中,必须要防止多线程的虚假唤醒,也即(判断只能用while,不能用if)</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadAwaitSignal</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Cake1 cake = <span class="keyword">new</span> Cake1();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        cake.increment();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"ProducerA"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        cake.decrement();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"ConsumerA"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        cake.increment();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"ProducerB"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        cake.decrement();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"ConsumerB"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="线程间定制化调用通信"><a href="#线程间定制化调用通信" class="headerlink" title="线程间定制化调用通信"></a>线程间定制化调用通信</h2><ul><li><p>题目：多线程之间按顺序调用，实现A -&gt; B -&gt; C，三个线程启动,要求如下：AAAAA打印5次，BBBBB打印10次，CCCCC打印15次，以上操作进行10轮</p></li><li><p><strong>多线程口诀5</strong>：标志位</p></li><li><p>实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShareResource</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> number = <span class="number">1</span>; <span class="comment">// 1 -&gt; A, 2 -&gt; B, 3 -&gt; C</span></span><br><span class="line">  <span class="comment">// 一把锁lock</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Lock lock = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">  <span class="comment">// 三把钥匙condition</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Condition conditionA = lock.newCondition();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Condition conditionB = lock.newCondition();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Condition conditionC = lock.newCondition();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printFromA</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、判断</span></span><br><span class="line">      <span class="keyword">while</span> (number != <span class="number">1</span>) &#123;</span><br><span class="line">        conditionA.await();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、干活</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        System.out.println(<span class="string">"AAAAA ~~~"</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 3、通知(修改标志位,通知下一个)</span></span><br><span class="line">      number = <span class="number">2</span>;</span><br><span class="line">      conditionB.signal();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printFromB</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、判断</span></span><br><span class="line">      <span class="keyword">while</span> (number != <span class="number">2</span>) &#123;</span><br><span class="line">        conditionB.await();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、干活</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        System.out.println(<span class="string">"BBBBB ~~~"</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 3、通知(修改标志位,通知下一个)</span></span><br><span class="line">      number = <span class="number">3</span>;</span><br><span class="line">      conditionC.signal();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printFromC</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、判断</span></span><br><span class="line">      <span class="keyword">while</span> (number != <span class="number">3</span>) &#123;</span><br><span class="line">        conditionC.await();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、干活</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">15</span>; i++) &#123;</span><br><span class="line">        System.out.println(<span class="string">"CCCCC ~~~"</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 3、通知(修改标志位,通知下一个)</span></span><br><span class="line">      number = <span class="number">1</span>;</span><br><span class="line">      conditionA.signal();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 题目：多线程之间按顺序调用,实现A -&gt; B -&gt; C,三个线程启动,要求如下：AAAAA打印5次,BBBBB打印10次,CCCCC打印15次,以上操作进行10轮</span></span><br><span class="line"><span class="comment">* 1、高聚合低耦合前提下,线程操作资源类</span></span><br><span class="line"><span class="comment">* 2、判断/干活/通知</span></span><br><span class="line"><span class="comment">* 3、多线程交互中,必须要防止多线程的虚假唤醒,也即(判断只能用while,不能用if)</span></span><br><span class="line"><span class="comment">* 4、标志位</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadOrderAccess</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    ShareResource shareResource = <span class="keyword">new</span> ShareResource();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) shareResource.printFromA();</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) shareResource.printFromB();</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) shareResource.printFromC();</span><br><span class="line">    &#125;, <span class="string">"C"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="线程八锁"><a href="#线程八锁" class="headerlink" title="线程八锁"></a>线程八锁</h2><ul><li><p>八锁示例：</p><ul><li><p>情况1(标准访问)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1、标准访问,请问先打印邮件还是短信? 邮件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Lock8</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendMessage();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>情况2(其一线程sleep)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、邮件方法暂停4秒,请问先打印邮件还是短信? 邮件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendMessage();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>情况3(新增一个普通方法)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Hello ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3、在2的基础上,新增并使用一个普通方法hello(),请问先打印邮件还是hello? hello</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.hello();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>情况4(两个对象)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4、两部手机,请问先打印邮件还是短信? 短信</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    Phone phone1 = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone1.sendMessage();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>情况5(改为静态同步方法)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5、两个静态同步方法,同一部手机,请问先打印邮件还是短信? 邮件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendMessage();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>情况6(两个对象调用静态同步方法)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6、两个静态同步方法,两部手机,请问先打印邮件还是短信? 邮件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    Phone phone1 = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone1.sendMessage();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>情况7(一个静态同步方法,一个普通同步方法)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 7、一个普通同步方法,一个静态同步方法,一部手机,请问先打印邮件还是短信? 短信</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendMessage();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>情况8(一个静态同步方法,一个普通同步方法,两个对)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Phone</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendEmail</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"Send Email ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Send Message ~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 8、一个普通同步方法,一个静态同步方法,两部手机,请问先打印邮件还是短信? 短信</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Phone phone = <span class="keyword">new</span> Phone();</span><br><span class="line">    Phone phone1 = <span class="keyword">new</span> Phone();</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone.sendEmail();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"A"</span>).start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        phone1.sendMessage();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="string">"B"</span>).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>八锁分析：</p><ul><li>情况1、2：如果一个对象有多个synchronized方法，某个时刻内只要有一个线程去调用当前对象的一个synchronized方法，那么其它线程只能等待。换句话说，某个时刻内只能有唯一一个线程去访问这些synchronized方法。锁作用的是当前对象this，this被锁定后其它的线程都不能进入到当前对象的其它的synchronized方法(情况1、2都进入了sendEmail()方法,因此不响应sendMessage()方法)</li><li>情况3、4：普通方法和同步锁无关，一个线程调用了synchronized方法，另一个线程可以同时调用普通方法；当换成两个对象后，synchronized锁的是对象实例，而当前有两个实例，锁的就不是同一把锁了，因此sendMessage先打印</li><li>情况5、6：对于静态同步方法，锁是当前类的Class对象，对于同一个Phone类锁是相同的(Phone.class)，因此进入sendEmail()方法后不会响应sendMessage()方法，而是等待sendEmail()方法执行完毕</li><li>情况7、8：对于普通同步方法和静态同步方法，他们锁的对象不同，普通同步方法锁的是当前对象实例(Phone的一个实例对象)，而静态同步方法锁的是当前类的Class对象(Phone.class)。他们锁的对象不同，不会相互影响，因此先打印sendMessage</li></ul></li><li><p><strong>线程八锁总结</strong>：</p><ul><li>synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现为以下3种形式：<ul><li>对于普通同步方法，锁是当前实例对象</li><li>对于静态同步方法，锁是当前类的Class对象</li><li>对于同步方法块，锁是Synchonized括号里配置的对象</li></ul></li><li>当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。也就是说如果一个实例对象的普通同步方法获取锁后，该实例对象的其他普通同步方法必须等待已获取锁的方法释放锁后才能获取锁</li><li>其他实例对象的普通同步方法跟当前实例对象的普通同步方法用的是不同的锁(不同的实例对象)，所以无须等待当前实例对象已获取锁的普通同步方法释放锁就可以获取他们自己的锁</li><li>所有静态同步方法用的是同一把锁——类对象本身，普通同步方法的锁和静态同步方法的锁是两个不同的对象，所以静态同步方法与普通同步方法之间是不会有竞态条件的。但一旦一个静态同步方法获取锁后，其他静态同步方法都必须等待该方法释放锁后才能获取锁(而不管是同一个实例对象的静态同步方法之间，还是不同实例对象的静态同步方法之间，只要它们是同一个类的实例对象)</li></ul></li></ul><h2 id="线程不安全集合"><a href="#线程不安全集合" class="headerlink" title="线程不安全集合"></a>线程不安全集合</h2><ul><li><p>请举例说明集合类是不安全的</p></li><li><p>示例：</p><ul><li><p>List集合</p><ul><li><p>情况1：3个线程同时读写ArrayList(结果：运行基本不报错,但是会出现List中有时内容为null或者集合元素个数不等于3的情况)</p></li><li><p>情况2：30个线程同时读写ArrayList(结果：运行报错——java.util.ConcurrentModificationException并发修改异常)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span> or <span class="number">30</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      list.add(UUID.randomUUID().toString().substring(<span class="number">0</span>, <span class="number">8</span>));</span><br><span class="line">      System.out.println(list);</span><br><span class="line">    &#125;, <span class="string">"list"</span> + i).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>出错原因：ArrayList本身就是线程不安全的(为了性能考虑,不加锁性能提升但会出错误)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ArrayList.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">  ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// Increments modCount!!</span></span><br><span class="line">  elementData[size++] = e;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>解决方案：</p><ul><li><p>Vector(线程安全,加了synchronized,加锁数据一致但性能下降;性能较差,不要使用)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Vector.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">  modCount++;</span><br><span class="line">  ensureCapacityHelper(elementCount + <span class="number">1</span>);</span><br><span class="line">  elementData[elementCount++] = e;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Collections工具类(少量数据可以使用)：Collections.synchronizedList(new ArrayList&lt;&gt;())</p></li><li><p>CopyOnWriteArrayList(推荐使用)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CopyOnWriteArrayList.java</span></span><br><span class="line"><span class="comment">// CopyOnWrite容器即写时复制的容器。往一个容器添加元素的时候,不直接往当前容器Object[]添加,</span></span><br><span class="line"><span class="comment">// 而是先将当前容器Object[]进行Copy,复制出一个新的容器Object[] newElements,</span></span><br><span class="line"><span class="comment">// 然后向新的容器Object[] newElements里添加元素</span></span><br><span class="line"><span class="comment">// 添加元素后,再将原容器的引用指向新的容器setArray(newElements)。</span></span><br><span class="line"><span class="comment">// 这样做的好处是可以对CopyOnWrite容器进行并发的读,而不需要加锁,因为当前容器不会添加任何元素。</span></span><br><span class="line"><span class="comment">// 所以CopyOnWrite容器是一种读写分离的思想,读和写不同的容器  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">  lock.lock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    Object[] elements = getArray();</span><br><span class="line">    <span class="keyword">int</span> len = elements.length;</span><br><span class="line">    Object[] newElements = Arrays.copyOf(elements, len + <span class="number">1</span>);</span><br><span class="line">    newElements[len] = e;</span><br><span class="line">    setArray(newElements);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    lock.unlock();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>Set集合：</p><ul><li><p>示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// new HashSet&lt;&gt;()</span></span><br><span class="line">  <span class="comment">// Collections.synchronizedSet(new HashSet&lt;&gt;())</span></span><br><span class="line">  <span class="comment">// new CopyOnWriteArraySet&lt;&gt;()</span></span><br><span class="line">  Set&lt;String&gt; set = <span class="keyword">new</span> CopyOnWriteArraySet&lt;&gt;();</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">30</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      set.add(UUID.randomUUID().toString().substring(<span class="number">0</span>, <span class="number">8</span>));</span><br><span class="line">      System.out.println(set);</span><br><span class="line">    &#125;, <span class="string">"set"</span> + i).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>解决方案：</p><ul><li>Collections工具类：Collections.synchronizedSet(new HashSet&lt;&gt;())</li><li>CopyOnWriteArraySet</li></ul></li></ul></li><li><p>Map集合：</p><ul><li><p>示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// new HashMap&lt;&gt;()</span></span><br><span class="line">  <span class="comment">// Collections.synchronizedMap(new HashMap&lt;&gt;())</span></span><br><span class="line">  <span class="comment">// new ConcurrentHashMap&lt;&gt;()</span></span><br><span class="line">  Map&lt;String, String&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">300</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">      map.put(Thread.currentThread().getName(), UUID.randomUUID().toString().substring(<span class="number">0</span>, <span class="number">8</span>));</span><br><span class="line">      System.out.println(map);</span><br><span class="line">    &#125;, <span class="string">"map"</span> + i).start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>解决方案：</p><ul><li>Collections工具类：Collections.synchronizedMap(new HashMap&lt;&gt;())</li><li>ConcurrentHashMap</li></ul></li></ul></li></ul></li></ul><h2 id="Callable接口"><a href="#Callable接口" class="headerlink" title="Callable接口"></a>Callable接口</h2><ul><li><p>获得多线程的方法有几种?</p><ol><li>继承Thread类(不建议使用)</li><li>实现Runnale接口</li><li>实现Callable接口</li><li>从线程池获取</li></ol></li><li><p>Callable是什么：是一个JDK1.5推出的线程接口，比Runnable更强大。是一个函数式接口，可用作lambda表达式</p></li><li><p>与Runnable的区别：</p><ol><li>是否有返回值</li><li>是否会抛出异常</li><li>落地方法不同(run()/call())</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread2</span> <span class="keyword">implements</span> <span class="title">Callable</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123; <span class="keyword">return</span> <span class="keyword">null</span>; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>怎么使用：</p><ol><li><p><del>直接替换runnable：不可行，Thread的构造方法传参都是Runnable接口，没有Callable接口</del></p></li><li><p>找中间人FutureTask：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// FutureTask.class</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FutureTask</span>&lt;<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">RunnableFuture</span>&lt;<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FutureTask</span><span class="params">(Callable&lt;V&gt; callable)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (callable == <span class="keyword">null</span>)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="keyword">this</span>.callable = callable;</span><br><span class="line">    <span class="keyword">this</span>.state = NEW;       <span class="comment">// ensure visibility of callable</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread2</span> <span class="keyword">implements</span> <span class="title">Callable</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">4</span>);</span><br><span class="line">    System.out.println(<span class="string">"~~~"</span> + Thread.currentThread().getName() + <span class="string">" Come in call() ~~~"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"1024"</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 多线程第3种创建多线程的方式</span></span><br><span class="line"><span class="comment">* get()方法一般请放在最后一行,get()方法会阻塞线程</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CallableDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">    FutureTask&lt;String&gt; futureTask = <span class="keyword">new</span> FutureTask&lt;&gt;(<span class="keyword">new</span> MyThread2());</span><br><span class="line">    <span class="keyword">new</span> Thread(futureTask, <span class="string">"A"</span>).start();</span><br><span class="line">    <span class="keyword">new</span> Thread(futureTask, <span class="string">"B"</span>).start();</span><br><span class="line">    <span class="comment">// System.out.println(Thread.currentThread().getName() + "计算中~~~");</span></span><br><span class="line">    <span class="keyword">while</span> (!futureTask.isDone())&#123;</span><br><span class="line">        TimeUnit.MILLISECONDS.sleep(<span class="number">500</span>);</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">"计算中~~~"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(futureTask.get());</span><br><span class="line">    System.out.println(futureTask.get());</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"计算完成~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li><li><p>FutureTask/Callable应用场景：在主线程中需要执行比较耗时的操作但又不想阻塞主线程时，可把这些操作交给FutureTask对象在后台完成。当主线程将来需要操作结果时可以通过FutureTask对象获得后台作业的计算结果或者执行状态。一般FutureTask多用于耗时的计算任务，主线程可在完成自己的任务后再去获取结果。仅在计算完成时才能检索结果；如果计算尚未完成，则会阻塞get()方法。get()方法获取结果只有在计算完成时获取，否则会阻塞直到任务转入完成状态，再会返回结果或者抛出异常。一旦计算完成，就不会再重新开始或取消计算，如果再次调用结果方法，会将缓存的结果直接返回。</p></li></ul><h2 id="JUC辅助类"><a href="#JUC辅助类" class="headerlink" title="JUC辅助类"></a>JUC辅助类</h2><ul><li><p>CountDownLatch(减少计数)</p><ul><li><p>例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要求：所有子线程完成后(所有同学离开教室),主线程退出(班长离开教室)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountDownLatchDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">6</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">int</span> finalI = i;</span><br><span class="line">      <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          TimeUnit.SECONDS.sleep(finalI);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">"离开教室~~~"</span>);</span><br><span class="line">        countDownLatch.countDown();</span><br><span class="line">      &#125;, UUID.randomUUID().toString().substring(<span class="number">0</span>, <span class="number">8</span>)).start();</span><br><span class="line">    &#125;</span><br><span class="line">    countDownLatch.await();</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"班长关门走人~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 常规方法无法完成,会有乱序</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeDoor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">new</span> Thread(() -&gt; System.out.println(Thread.currentThread().getName() + <span class="string">"离开教室~~~"</span>), i + <span class="string">""</span>).start();</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"班长关门走人~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>原理：CountDownLatch主要有两个方法(countDown()以及await())。当一个或多个线程调用await()方法时，这些线程会阻塞。其它线程调用countDown()方法会将计数器减1(调用countDown()方法的线程不会阻塞)，当计数器的值变为0时，因await()方法阻塞的线程会被唤醒，继续执行</p></li></ul></li><li><p>CyclicBarrier(循环栅栏)</p><ul><li><p>例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要求：子线程全部完成后再运行指定方法(集齐七棵龙珠召唤神龙)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CyclicBarrierDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    CyclicBarrier cyclicBarrier = <span class="keyword">new</span> CyclicBarrier(<span class="number">7</span>, () -&gt; System.out.println(Thread.currentThread().getName() + <span class="string">"召唤神龙~~~"</span>));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">7</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">int</span> finalI = i;</span><br><span class="line">      <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          TimeUnit.SECONDS.sleep(finalI);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">"收集到第"</span> + finalI + <span class="string">"颗龙珠~~~"</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          cyclicBarrier.await();</span><br><span class="line">          <span class="comment">// 在7个线程中最后一个线程到达await()屏障，之后下面的语句和cyclicBarrier中设定的动作才会被调度执行</span></span><br><span class="line">          System.out.println(Thread.currentThread().getName() + <span class="string">"收集第"</span> + finalI + <span class="string">"颗龙珠完毕~~~"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException | BrokenBarrierException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, i + <span class="string">""</span>).start();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>原理：CyclicBarrier的字面意思是可循环(Cyclic)使用的屏障(Barrier)。它要做的事情是让一组线程到达一个屏障(也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。线程进入屏障通过CyclicBarrier的await()方法</p></li></ul></li><li><p>SemaphoreDemo</p><ul><li><p>例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 要求：只有三个线程，但是希望六个线程都能够运行(有3个空闲车位,共有6辆车,一开始3辆车抢到,之后开走1辆另外的车占1个车位)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SemaphoreDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 模拟资源类,有3个空车位</span></span><br><span class="line">    Semaphore semaphore = <span class="keyword">new</span> Semaphore(<span class="number">3</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">int</span> finalI = i;</span><br><span class="line">      <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          semaphore.acquire();</span><br><span class="line">          System.out.println(Thread.currentThread().getName() + <span class="string">" 抢到了车位 ~~~"</span>);</span><br><span class="line">          TimeUnit.SECONDS.sleep(finalI);</span><br><span class="line">          System.out.println(Thread.currentThread().getName() + <span class="string">" 离开了车位 ~~~"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          semaphore.release();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, i + <span class="string">"号车"</span>).start();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>原理：在信号量上我们定义两种操作：acquire()——获取，当一个线程调用acquire()操作时，它要么成功并获取信号量(信号量减1)，要么一直等下去直到有线程释放信号量或超时；release()——释放，实际上会将信号量的值加1，然后唤醒等待的线程。信号量主要用于两个目的，一个是用于多个共享资源的互斥使用，另一个用于并发线程数的控制</p></li></ul></li></ul><h2 id="ReentrantReadWriteLock读写锁"><a href="#ReentrantReadWriteLock读写锁" class="headerlink" title="ReentrantReadWriteLock读写锁"></a>ReentrantReadWriteLock读写锁</h2><ul><li><p>类似案例：缓存(允许多个线程读,但读写和写写互斥)</p></li><li><p>案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCache</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">volatile</span> Map&lt;String, Object&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ReadWriteLock readWriteLock = <span class="keyword">new</span> ReentrantReadWriteLock();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(String key, Object value)</span> </span>&#123;</span><br><span class="line">    readWriteLock.writeLock().lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      System.out.println(Thread.currentThread().getName() + <span class="string">"写入数据: [key = "</span> + key + <span class="string">']'</span>);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        TimeUnit.MILLISECONDS.sleep(<span class="number">300</span>);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">      map.put(key, value);</span><br><span class="line">      System.out.println((Thread.currentThread().getName() + <span class="string">"写入完成"</span>));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      readWriteLock.writeLock().unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">(String key)</span> </span>&#123;</span><br><span class="line">    readWriteLock.readLock().lock();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      System.out.println(Thread.currentThread().getName() + <span class="string">"读取数据: [key = "</span> + key + <span class="string">']'</span>);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        TimeUnit.MILLISECONDS.sleep(<span class="number">300</span>);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">      Object value = map.get(key);</span><br><span class="line">      System.out.println((Thread.currentThread().getName() + <span class="string">"读取完成: [value = "</span> + value + <span class="string">']'</span>));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      readWriteLock.readLock().unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 多个线程同时读一个资源类没有任何问题,所以为了满足并发量,读取共享资源应该可以同时进行</span></span><br><span class="line"><span class="comment">* 但是如果有一个线程想去写共享资源,就不应该再有其他线程可以对该资源进行读或写</span></span><br><span class="line"><span class="comment">* 小总结：</span></span><br><span class="line"><span class="comment">* 读 - 读能共存</span></span><br><span class="line"><span class="comment">* 读 - 写不能共存</span></span><br><span class="line"><span class="comment">* 写 - 写不能共存</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadWriteLockDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    MyCache myCache = <span class="keyword">new</span> MyCache();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">int</span> finalI = i;</span><br><span class="line">      <span class="keyword">new</span> Thread(() -&gt; myCache.put(finalI + <span class="string">""</span>, finalI + <span class="string">""</span>), <span class="string">"Thread "</span> + i).start();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">int</span> finalI = i;</span><br><span class="line">      <span class="keyword">new</span> Thread(() -&gt; myCache.get(finalI + <span class="string">""</span>), <span class="string">"Thread "</span> + (i + <span class="number">5</span>)).start();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="BlockingQueue阻塞队列"><a href="#BlockingQueue阻塞队列" class="headerlink" title="BlockingQueue阻塞队列"></a>BlockingQueue阻塞队列</h2><ul><li><p>阻塞队列介绍：<br><img src="%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="阻塞队列示意图"></p><ol><li>线程1往阻塞队列里添加元素，线程2从阻塞队列里移除元素</li><li>当队列是空的，从队列中获取元素的操作将会被阻塞</li><li>当队列是满的，从队列中添加元素的操作将会被阻塞</li><li>试图从空的队列中获取元素的线程将会被阻塞，直到其他线程往空的队列插入新的元素</li><li>试图向已满的队列中添加新元素的线程将会被阻塞，直到其他线程从队列中移除一个或多个元素或者完全清空使队列变得空闲起来再进行后续新增</li></ol></li><li><p>阻塞队列作用：</p><ol><li>在多线程领域所谓的阻塞就是指在某些情况下线程会被挂起，但一旦条件满足，被挂起的线程又会自动被唤起</li><li>阻塞队列的好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，为这一切BlockingQueue都给你一手包办了</li><li>在concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度</li></ol></li><li><p>阻塞队列种类：</p><ul><li>继承图介绍：<br><img src="%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt="阻塞队列类继承图"></li><li>各实现类介绍：<ol><li><strong>ArrayBlockingQueue</strong>：由数组结构组成的有界阻塞队列</li><li><strong>LinkedBlockingQueue</strong>：由链表结构组成的有界(但大小默认值为integer.MAX_VALUE)阻塞队列</li><li>PriorityBlockingQueue：支持优先级排序的无界阻塞队列</li><li>DelayQueue：使用优先级队列实现的延迟无界阻塞队列</li><li><strong>SynchronousQueue</strong>：不存储元素的阻塞队列，也即单个元素的队列</li><li>LinkedTransferQueue：由链表组成的无界阻塞队列</li><li>LinkedBlockingDeque：由链表组成的双向阻塞队列</li></ol></li></ul></li><li><p>阻塞队列核心方法：<br><img src="%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95.png" alt="阻塞队列核心方法"></p><ol><li>抛出异常：当阻塞队列满时，再往队列里add插入元素会抛出异常IllegalStateException:Queue full；当阻塞队列空时，再往队列里remove移除元素会抛出异常NoSuchElementException</li><li>特殊值：插入方法——成功ture、失败false；移除方法——成功返回出队列的元素、队列里没有就返回null</li><li>一直阻塞：当阻塞队列满时，生产者线程继续往队列里put元素，队列会一直阻塞生产者线程直到put数据成功(元素被消费)或响应中断退出；当阻塞队列空时，消费者线程试图从队列里take元素，队列会一直阻塞消费者线程直到队列可用(元素被生产)</li><li>超时退出：当阻塞队列满时，队列会阻塞生产者线程一定时间，超过限时后生产者线程会退出</li></ol></li><li><p>案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">BlockingQueue&lt;String&gt; blockingQueue = <span class="keyword">new</span> ArrayBlockingQueue&lt;&gt;(<span class="number">3</span>);</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.add("a"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.add("b"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.add("c"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.add("x"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.remove());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.remove());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.remove());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.remove());</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.offer("a"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.offer("b"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.offer("c"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.offer("d"));</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.poll());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.poll());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.poll());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.poll());</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">blockingQueue.put("a");</span></span><br><span class="line"><span class="comment">blockingQueue.put("b");</span></span><br><span class="line"><span class="comment">blockingQueue.put("c");</span></span><br><span class="line"><span class="comment">blockingQueue.put("d");</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.take());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.take());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.take());</span></span><br><span class="line"><span class="comment">System.out.println(blockingQueue.take());</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"></span><br><span class="line">System.out.println(blockingQueue.offer(<span class="string">"a"</span>, <span class="number">3</span>, TimeUnit.SECONDS));</span><br><span class="line">System.out.println(blockingQueue.offer(<span class="string">"b"</span>, <span class="number">3</span>, TimeUnit.SECONDS));</span><br><span class="line">System.out.println(blockingQueue.offer(<span class="string">"c"</span>, <span class="number">3</span>, TimeUnit.SECONDS));</span><br><span class="line">System.out.println(blockingQueue.offer(<span class="string">"e"</span>, <span class="number">3</span>, TimeUnit.SECONDS));</span><br></pre></td></tr></table></figure></li></ul><h2 id="ThreadPool线程池"><a href="#ThreadPool线程池" class="headerlink" title="ThreadPool线程池"></a>ThreadPool线程池</h2><ul><li><p>为什么使用线程池：主要特点有线程复用、控制最大并发数、管理线程</p><ol><li>降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的销耗</li><li>提高响应速度。当任务到达时，任务可以不需要等待线程创建就能立即执行</li><li>提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会销耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控</li></ol></li><li><p>线程池如何使用：</p><ul><li>线程池类继承图：<br><img src="%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt="线程池类继承图"></li><li>获取线程池的几种方式：<ol><li>Executors.newFixedThreadPool(int)：执行长期任务性能好，创建一个线程池(有N个固定的线程)</li><li>Executors.newSingleThreadExecutor()：一个任务一个任务的执行，一池一线程</li><li>Executors.newCachedThreadPool()：执行很多短期异步任务，线程池根据需要创建新线程，但在先前构建的线程可用时将重用它们。可扩容，遇强则强</li></ol></li></ul></li><li><p>Executors创建线程原理：<br><img src="Executors%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B%E5%8E%9F%E7%90%86.png" alt="Executors创建线程原理"></p></li><li><p>线程池参数解释：</p><ol><li>corePoolSize：线程池中的常驻核心线程数</li><li>maximumPoolSize：线程池中能够容纳同时执行的最大线程数，此值必须大于等于1</li><li>keepAliveTime：多余的空闲线程的存活时间，若当前池中线程数量超过corePoolSize时且当空闲时间达到keepAliveTime时，多余线程会被销毁直到只剩下corePoolSize个线程为止</li><li>unit：keepAliveTime的单位</li><li>workQueue：任务队列，用于存储被提交但尚未被执行的任务(也即上面介绍的阻塞队列)</li><li>threadFactory：表示生成线程池中工作线程的线程工厂，用于创建线程，<strong>一般默认的即可</strong></li><li>handler：拒绝策略，表示当队列满了，并且工作线程大于等于线程池的最大线程数(maximumPoolSize)时如何来拒绝请求执行的runnable的策略</li></ol></li><li><p>线程池底层工作原理：<br><img src="%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%BA%95%E5%B1%82%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png" alt="线程池底层工作原理"></p><ol><li>在创建了线程池后，开始等待请求</li><li>当调用execute()方法添加一个请求任务时，线程池会做出如下判断：<ol><li>如果正在运行的线程数量小于corePoolSize，那么马上创建线程运行这个任务</li><li>如果正在运行的线程数量大于或等于corePoolSize，那么将这个任务<strong>放入队列</strong></li><li>如果这个时候队列满了且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务</li><li>如果队列满了且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会<strong>启动饱和拒绝策略来执行</strong></li></ol></li><li>当一个线程完成任务时，它会从队列中取下一个任务来执行</li><li>当一个线程无事可做超过一定的时间(keepAliveTime)时，线程会判断：如果当前运行的线程数大于corePoolSize，那么多余的线程就被停掉。当线程池的所有任务完成后，<strong>线程池线程数目最终会收缩到corePoolSize</strong></li></ol></li><li><p>线程池拒绝策略</p><ul><li>拒绝策略是什么：等待队列已经排满了，再也塞不下新任务了。同时，线程池中也达到了最大线程数，无法继续为新任务服务。这时就需要一种拒绝策略机制合理地处理这个问题</li><li>JDK内置的拒绝策略(内置拒绝策略均实现了RejectedExecutionHandle接口)<ol><li>AbortPolicy(默认)：直接抛出RejectedExecutionException异常阻止系统正常运行</li><li>CallerRunsPolicy：”调用者运行”调节机制，该策略既不会抛弃任务也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量</li><li>DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务</li><li>DiscardPolicy：该策略默默地丢弃无法处理的任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的一种策略</li></ol></li></ul></li><li><p>线程池的选用</p><ul><li><p>在工作中单一的/固定数的/可变的三种创建线程池的方法哪个用的多：一个都不用<br><img src="%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%98%BF%E9%87%8C%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C%E8%A7%84%E5%AE%9A.png" alt="线程池阿里开发手册规定"></p></li><li><p>自定义线程池示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyThreadPoolDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    System.out.println(Runtime.getRuntime().availableProcessors());</span><br><span class="line">    <span class="comment">// 最大承载线程数(理论)：max + Queue.capacity</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// RejectedExecutionException AbortPolicy</span></span><br><span class="line">    <span class="comment">// CallerRunsPolicy()</span></span><br><span class="line">    <span class="comment">// DiscardPolicy()</span></span><br><span class="line">    <span class="comment">// DiscardOldestPolicy()</span></span><br><span class="line">    ExecutorService threadPool = <span class="keyword">new</span> ThreadPoolExecutor(</span><br><span class="line">        <span class="number">2</span>,</span><br><span class="line">        <span class="number">5</span>,</span><br><span class="line">        <span class="number">2L</span>,</span><br><span class="line">        TimeUnit.SECONDS,</span><br><span class="line">        <span class="keyword">new</span> LinkedBlockingQueue&lt;&gt;(<span class="number">3</span>),</span><br><span class="line">        Executors.defaultThreadFactory(),</span><br><span class="line">        <span class="keyword">new</span> ThreadPoolExecutor.DiscardOldestPolicy());</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 模拟有10个顾客来银行办理业务</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">        threadPool.execute(() -&gt; &#123;</span><br><span class="line">          System.out.println(Thread.currentThread().getName() + <span class="string">"办理业务~~~"</span>);</span><br><span class="line">        &#125;);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      threadPool.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">useSystemPool</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1池5个工作线程</span></span><br><span class="line">    <span class="comment">// ExecutorService threadPool = Executors.newFixedThreadPool(5);</span></span><br><span class="line">    <span class="comment">// 1池1个工作线程</span></span><br><span class="line">    <span class="comment">// ExecutorService threadPool = Executors.newSingleThreadExecutor();</span></span><br><span class="line">    <span class="comment">// 1池N个工作线程</span></span><br><span class="line">    ExecutorService threadPool = Executors.newCachedThreadPool();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 模拟有10个顾客来银行办理业务</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">        threadPool.execute(() -&gt; &#123;</span><br><span class="line">          System.out.println(Thread.currentThread().getName() + <span class="string">"办理业务~~~"</span>);</span><br><span class="line">        &#125;);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      threadPool.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Java8流式计算"><a href="#Java8流式计算" class="headerlink" title="Java8流式计算"></a>Java8流式计算</h2><ul><li><p>函数式接口(只有一个方法(除default和static方法)的接口)：</p><ul><li><p>Lambda表达式口诀：<strong>拷贝小括号，写死右括号，落地大括号</strong></p></li><li><p>Lambda案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">sayHello</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">Foo1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">default</span> <span class="keyword">double</span> <span class="title">div</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">double</span> y)</span> </span>&#123; <span class="keyword">return</span> x / y; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">multiply</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123; <span class="keyword">return</span> x * y; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Lambda表达式</span></span><br><span class="line"><span class="comment">* 口诀：拷贝小括号(参数),写死右括号,落地大括号</span></span><br><span class="line"><span class="comment">* 注释<span class="doctag">@FunctionalInterface</span>(只有一个方法的接口,函数式接口)</span></span><br><span class="line"><span class="comment">* default方法允许接口有默认实现(jdk1.8),可以有多个</span></span><br><span class="line"><span class="comment">* 静态方法实现</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LambdaExpress</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Foo foo = () -&gt; System.out.println(<span class="string">"Hello LambdaExpress"</span>);</span><br><span class="line">    foo.sayHello();</span><br><span class="line"></span><br><span class="line">    Foo1 foo1 = (x, y) -&gt; <span class="number">2</span> * x + y;</span><br><span class="line">    System.out.println(foo1.add(<span class="number">1</span>, <span class="number">2</span>));</span><br><span class="line">    System.out.println(foo1.div(<span class="number">1.0</span>, <span class="number">0.3333</span>));</span><br><span class="line">    System.out.println(Foo1.multiply(<span class="number">2</span>, <span class="number">5</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Java内置核心四大函数式接口<br><img src="Java%E5%86%85%E7%BD%AE%E6%A0%B8%E5%BF%83%E5%9B%9B%E5%A4%A7%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3.png" alt="Java内置核心四大函数式接口"></p></li></ul></li><li><p>Stream流</p><ul><li><p>是什么：流(Stream)是数据渠道，用于操作数据源(集合、数组等)所生成的元素序列。集合讲的是数据，流讲的是计算</p></li><li><p>特点：</p><ol><li>Stream自己不会存储元素</li><li>Stream不会改变源对象。相反，他们会返回一个持有结果的新Stream</li><li>Stream操作是延迟执行的。这意味着他们会等到需要结果的时候才执行(懒加载)</li></ol></li><li><p>怎么用：</p><ol><li>创建一个Stream：一个数据源(数组、集合)</li><li>中间操作：一个中间操作，处理数据源数据</li><li>终止操作：一个终止操作，执行中间操作链，产生结果</li></ol></li><li><p>案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Integer id;</span><br><span class="line">  <span class="keyword">private</span> String  userName;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span>     age;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 题目：请按照给出数据，找出同时满足</span></span><br><span class="line"><span class="comment">*      偶数ID且年龄大于24且用户名转为大写且用户名字母倒排序</span></span><br><span class="line"><span class="comment">*      最后只输出一个用户名字</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    User u1 = <span class="keyword">new</span> User(<span class="number">11</span>,<span class="string">"a"</span>,<span class="number">23</span>);</span><br><span class="line">    User u2 = <span class="keyword">new</span> User(<span class="number">12</span>,<span class="string">"b"</span>,<span class="number">24</span>);</span><br><span class="line">    User u3 = <span class="keyword">new</span> User(<span class="number">13</span>,<span class="string">"c"</span>,<span class="number">22</span>);</span><br><span class="line">    User u4 = <span class="keyword">new</span> User(<span class="number">14</span>,<span class="string">"d"</span>,<span class="number">28</span>);</span><br><span class="line">    User u5 = <span class="keyword">new</span> User(<span class="number">16</span>,<span class="string">"e"</span>,<span class="number">26</span>);</span><br><span class="line"></span><br><span class="line">    List list = Arrays.asList(u1,u2,u3,u4,u5);</span><br><span class="line">    list.stream().filter(p -&gt; p.getId() % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">      .filter(p -&gt; p.getAge() &gt; <span class="number">24</span>)</span><br><span class="line">      .map(f -&gt; f.getUserName().toUpperCase())</span><br><span class="line">      .sorted((o1, o2) -&gt; o2.compareTo(o1))</span><br><span class="line">      .limit(<span class="number">1</span>)</span><br><span class="line">      .forEach(System.out::println);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Java8分支合并"><a href="#Java8分支合并" class="headerlink" title="Java8分支合并"></a>Java8分支合并</h2><ul><li><p>原理(类同MapReduce)：<br>Fork：把一个复杂任务进行分拆，大事化小<br>Join：把分拆任务的结果进行合并</p></li><li><p>相关类：</p><ol><li>ForkJoinPool(分支合并线程池)<br><img src="ForkJoinPool%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt="ForkJoinPool继承图"></li><li>ForkJoinTask(类似FutureTask)<br><img src="ForkJoinTask%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt="ForkJoinTask继承图"></li><li>RecursiveTask(递归任务,继承自ForkJoinTask,递归自己)<br><img src="RecursiveTask%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt="RecursiveTask继承图"></li></ol></li><li><p>案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTask</span> <span class="keyword">extends</span> <span class="title">RecursiveTask</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Integer ADJUST_VALUE = <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> begin;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> end;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> result = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyTask</span><span class="params">(<span class="keyword">long</span> begin, <span class="keyword">long</span> end)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.begin = begin;</span><br><span class="line">      <span class="keyword">this</span>.end = end;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Long <span class="title">compute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ((end - begin) &lt;= ADJUST_VALUE) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">long</span> i = begin; i &lt;= end; i++) &#123;</span><br><span class="line">        result += i;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">long</span> middle = (end + begin) / <span class="number">2</span>;</span><br><span class="line">      MyTask leftTask = <span class="keyword">new</span> MyTask(begin, middle);</span><br><span class="line">      MyTask rightTask = <span class="keyword">new</span> MyTask(middle + <span class="number">1</span>, end);</span><br><span class="line">      leftTask.fork();</span><br><span class="line">      rightTask.fork();</span><br><span class="line">      result = leftTask.join() + rightTask.join();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForkJoinDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">    MyTask myTask = <span class="keyword">new</span> MyTask(<span class="number">0</span>, <span class="number">1023423423L</span>);</span><br><span class="line">    ForkJoinPool threadPool = <span class="keyword">new</span> ForkJoinPool();</span><br><span class="line">    ForkJoinTask&lt;Long&gt; forkJoinTask = threadPool.submit(myTask);</span><br><span class="line">    System.out.println(forkJoinTask.get());</span><br><span class="line">    threadPool.shutdown();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="异步回调"><a href="#异步回调" class="headerlink" title="异步回调"></a>异步回调</h2><ul><li><p>案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 同步</span></span><br><span class="line">  CompletableFuture&lt;Void&gt; completableFuture = CompletableFuture.runAsync(() -&gt; System.out.println(Thread.currentThread().getName() + <span class="string">"无返回!"</span>));</span><br><span class="line">  completableFuture.get();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 异步</span></span><br><span class="line">  CompletableFuture&lt;Integer&gt; completableFuture2 = CompletableFuture.supplyAsync(() -&gt; &#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"有返回!"</span>);</span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">1</span> / <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1024</span>;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 异步回调</span></span><br><span class="line">  System.out.println(completableFuture2.whenComplete((t, u) -&gt; &#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"：t = "</span> + t);</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"：u = "</span> + u);</span><br><span class="line">  &#125;).exceptionally(f -&gt; &#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName() + <span class="string">"：f = "</span> + f.getMessage());</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4444</span>;</span><br><span class="line">  &#125;).get());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java高级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ Primer</title>
      <link href="/2020/09/15/Language/C_C++/C++_Primer/"/>
      <url>/2020/09/15/Language/C_C++/C++_Primer/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#开始">开始</a></li><li><a href="#变量和基本类型">变量和基本类型</a></li></ul><a id="more"></a><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><ul><li><p>标准输入输出对象：</p><ul><li>标准库定义了另外两个ostream对象：cerr和clog，通常使用cerr来输出警告和错误消息，而使用clog来输出程序运行的一般性信息</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"test"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><ul><li>::是作用域运算符</li><li>输出(输入)运算符(&lt;&lt;或&gt;&gt;)接受两个运算对象：左侧的运算对象必须是一个ostream(istream)对象，右侧的运算对象是要打印(读入)的值。该运算符将给定的值写到给定的ostream(istream)对象中，计算结果就是其左侧运算对象(每次都返回std::cout/std::cin)</li><li>运算符打印endl是一个被称为<strong>操纵值</strong>的特殊值。写入endl的效果是结束当前行，并将与设备关联的<strong>缓冲区(buffer)</strong>中的内容刷到设备中。缓冲刷新操作可以保证到目前为止程序所产生的所有输出都真正写入输入流中，而不是仅停留在内存中等待写入流(我们常在调试时打印语句,这类语句应该保证”一直”刷新流。否则,如果程序崩溃,输出可能还留在缓冲区中,从而导致关于程序崩溃位置的错误推断)</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; value)</span><br></pre></td></tr></table></figure><ul><li>当使用一个istream对象作为条件时，其效果是检测流的状态。如果流是有效的(流未遇到错误)，那么检测成功。当遇到<em>文件结束符</em>，或遇到一个无效输入时(与value定义的类型不符)，istream对象的状态会变为无效(此时会使条件变为假)</li></ul></li><li><p>包含来自标准库的头文件应该使用(&lt;&gt;)包围文件名。对于不属于标准库的头文件则应该使用(“”)包围</p></li></ul><h2 id="变量和基本类型"><a href="#变量和基本类型" class="headerlink" title="变量和基本类型"></a>变量和基本类型</h2><ul><li><p>类型转换：</p><ul><li>当把一个非布尔类型的算术值赋给布尔类型时，初始值为0则结果为false；否则结果为true</li><li>当把一个布尔值赋值给非布尔类型时，初始值为false则结果为0；否则结果为1</li><li>当把一个浮点数赋给整数类型时进行了近似处理，结果值将仅保留整数部分</li><li>当把一个整数值赋给浮点类型时，小数部分记为0。如果该整数所占控件超过了浮点类型的容量，精度有可能丢失</li><li>当赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数去模后的余数</li><li>当赋给带符号类型一个超出它表示范围的值时，结果是<strong>未定义</strong>的</li><li>当无符号和带符号相互赋值时，可能会导致数值的偏差(符号位)</li></ul></li><li><p>字面值</p><ul><li>默认情况下，十进制字面值是带符号数，八进制和十六进制字面值既可能时带符号也可能时无符号</li><li>如果一个字面值连与之关联的最大的数据类型都放不下，将产生错误</li></ul></li><li><p>初始化</p><ul><li>C++11定义了许多初始化的方式，C++11后花括号得到全面应用</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> test = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> test = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> test&#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><ul><li>默认初始化<ul><li>内置类型：如果是内置类型的变量未被显式初始化，它的值由定义的位置决定。定义于任何函数体之外的变量被初始化为0。一种例外情况是，定义在函数体内部的内置类型变量将不被初始化。一个未被初始化的内置类型变量的值是未定义的，如果试图拷贝或以其他方式访问此类值将引发错误</li><li>类类型：每个类各自决定其初始化对象的方式。而且是否允许不经初始化就定义对象也由类自己决定。如果类允许这种行为，它将决定对象的初始值到底是什么</li></ul></li></ul></li><li><p>变量声明和定义</p><ul><li>为了允许把程序拆分成多个逻辑部分来编写，C++语言支持<em>分离式编译</em>机制，该机制允许将程序分割为若干个文件，每个文件可被独立编译。如果将程序分为多个文件，则需要有在文件间共享代码的方法(例如std::cout定义于标准库,却能被我们使用)。为了支持分离式编译，C++将声明和定义区分开。<strong>声明</strong>使得名字为程序所致，一个文件如果想使用别处定义的名字则必须包含对那个名字的声明。<strong>定义</strong>负责创建与名字关联的实体</li><li>变量声明规定了变量的类型和名字，这一点与定义相同。此外，定义还申请存储空间，也可能会为变量赋一个初始值</li><li>如果想声明一个变量而非定义它，就需要在变量名前添加关键字extern，并且不要显式地初始化变量</li><li>任何包含了显式初始化的声明即成为定义。我们能给由extern关键字标记的变量赋一个初始值，但这么做抵消了extern的作用，就变成定义了</li><li>在函数体内部，如果试图初始化一个由extern关键字标记的变量，将引发错误</li><li>变量能且只能被定义一次，但可以被多次声明</li></ul></li><li><p>引用</p><ul><li>引用为对象起了另外一个名字，引用类型引用另外一种类型</li><li>引用并非对象，相反的，它只是为一个已经存在的对象所起的另外一个名字</li><li>定义引用时，程序把引用和它的初始值<strong>绑定</strong>在一起，而不是将初始值拷贝给引用。一旦初始化完成，引用将和它的初始值对象一直绑定在一起。因为无法令引用重新绑定到另一个对象，因此引用必须初始化</li><li>引用本身不是对象，因此不能定义引用的引用</li><li>引用只能绑定在对象上，而不能与字面值或某个表达式的计算结果绑定在一起</li></ul></li><li><p>指针</p><ul><li>指针是”指向”另外一种类型的符合类型。与引用类似，指针也实现了对其他对象的间接访问</li><li>指针本身就是一个对象，允许对指针赋值和拷贝，而且在指针的生命周期内它可以先后指向几个不同的对象</li><li>指针无须在定义时赋初值。和其他内置类型一样，在块作用域内定义的指针如果没有被初始化，也将拥有一个不确定的值(野指针)</li><li>指针存放某个对象的地址。引用不是对象，没有实际地址，因此不能定义指向引用的指针</li><li>绝大部分情况下指针的类型都要和它所指向的对象严格匹配(void*传参、多态等除外)</li><li>‘*’为解引用符，对指针解引用会得到所指的对象。如果给解引用的结果赋值，实际上就是给指针所指的对象赋值</li><li>void*是一种特殊的指针类型，可用于存放任意对象的地址</li><li>利用void*指针能做的事比较有限：拿它和其他指针比较、作为函数的输入或输出、赋值给另外一个void*指针。不能直接操作void*所指的对象，因为并不知道这个对象的类型，也就无法确定能在这个对象上能做哪些操作(如果能够确认该对象的类型,可以进行强制转换——cast)</li></ul></li><li><p>复合类型</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// p1是指向int的指针,p2是一个int型变量</span></span><br><span class="line"><span class="keyword">int</span>* p1, p2;</span><br></pre></td></tr></table></figure><ul><li>这种写法可能会产生误导：int*放在一起好像是这条赋值语句中所有变量共同的类型一样。恰恰相反，基本数据类型是int而非int*。*仅仅是修饰了p而且，对该声明语句中的其他变量并不产生任何作用(因此最好把&amp;和*这俩类型修饰符写在变量名前,与数据类型以空格间隔)</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> *p; <span class="comment">// p是一个int型指针</span></span><br><span class="line"><span class="keyword">int</span> *&amp;r = p; <span class="comment">// p是一个对指针p的引用</span></span><br></pre></td></tr></table></figure><ul><li>引用本身不是一个对象，因此不能定义指向引用的指针。但指针是对象，所以存在对指针的引用</li><li>要理解r的类型的简单方法是从右向左阅读r的定义。离变量名最近的符号(&amp;)对变量类型有最直接的影响，因此r是一个引用。声明符的其余部分用以确定r引用的类型是什么，*说明r引用的是一个指针。最后由声明的基本类型int指出r引用的是一个int指针</li></ul></li><li><p>const限定符</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C/C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx</title>
      <link href="/2020/09/06/Middleware/Nginx/"/>
      <url>/2020/09/06/Middleware/Nginx/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Nginx简介">Nginx简介</a></li><li><a href="#Nginx安装">Nginx安装</a></li><li><a href="#Nginx常用的命令和配置文件">Nginx常用的命令和配置文件</a></li><li><a href="#Nginx配置实例-反向代理">Nginx配置实例-反向代理</a></li><li><a href="#Nginx配置实例-负载均衡">Nginx配置实例-负载均衡</a></li><li><a href="#Nginx配置实例-动静分离">Nginx配置实例-动静分离</a></li><li><a href="#Nginx搭建高可用集群">Nginx搭建高可用集群</a></li><li><a href="#Nginx原理简述">Nginx原理简述</a></li></ul><a id="more"></a><h2 id="Nginx简介"><a href="#Nginx简介" class="headerlink" title="Nginx简介"></a>Nginx简介</h2><ul><li>Nginx概述：是一个高性能的HTTP和反向代理服务器,特点是占有内存少，并发能力强，事实上Nginx的并发能力确实在同类型的网页服务器中表现较好</li><li>Nginx的作用：<ul><li>作为Web服务器：Nginx可以作为静态页面的web服务器，同时还支持CGI协议的动态语言，比如perl、php等。但是不支持java。Java程序只能通过与tomcat配合完成</li><li>正向代理：如果把局域网外的Internet想象成一个巨大的资源库，则局域网中的客户端要访问Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理(需要设置代理地址)</li><li>反向代理：客户端对代理是无感知的，因为客户端不需要任何配置就可以访问。我们只需要将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后再返回给客户端。此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器地址，隐藏了真实服务器IP地址</li><li>负载均衡：单个服务器解决不了并发需求，可以增加服务器的数量，然后将请求分发到各个服务器上，将原先请求集中到单个服务器上的情况变为将请求分发到多个服务器上，将负载分发到不同的服务器。这就是所说的负载均衡</li><li>动静分离：为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速度。降低原来单个服务器的压力</li></ul></li></ul><h2 id="Nginx安装"><a href="#Nginx安装" class="headerlink" title="Nginx安装"></a>Nginx安装</h2><ul><li><p>进入Nginx官网<a href="http://nginx.org/" target="_blank" rel="noopener">http://nginx.org/</a>的download板块下载</p></li><li><p>具体安装Nginx</p><ul><li><p>安装所需的第三方库：pcre、openssl、zlib</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel</span><br></pre></td></tr></table></figure></li><li><p>安装nginx</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 解压缩nginx的tar.gz包</span></span><br><span class="line">tar -zxvf nginx-1.18.0.tar.gz -C /opt/module</span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入目录,执行./configure</span></span><br><span class="line">cd /opt/module/nginx-1.18.0</span><br><span class="line">sudo ./configure</span><br><span class="line"><span class="meta">#</span><span class="bash"> make安装</span></span><br><span class="line">sudo make &amp;&amp; make install</span><br></pre></td></tr></table></figure></li><li><p>测试(注意Linux的防火墙设置)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入/usr/<span class="built_in">local</span>/nginx/sbin目录</span></span><br><span class="line">cd /usr/local/nginx/sbin</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动nginx</span></span><br><span class="line">sudo ./nginx</span><br><span class="line"><span class="meta">#</span><span class="bash"> 输入当前虚拟机的地址的80端口,查看是否能看到Welcome to nginx</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> http://sobxiong.com</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Nginx常用的命令和配置文件"><a href="#Nginx常用的命令和配置文件" class="headerlink" title="Nginx常用的命令和配置文件"></a>Nginx常用的命令和配置文件</h2><ul><li><p>Nginx常用的命令(当前路径:/usr/local/nginx/sbin)</p><ul><li>查看nginx版本号：./nginx -v</li><li>启动命令：./nginx</li><li>关闭命令：./nginx -s stop</li><li>重新加载命令(重新加载配置文件)：./nginx - s reload</li></ul></li><li><p>nginx.conf配置文件</p><ul><li><p>介绍：Nginx默认的配置文件都放在主目录下的conf目录，主配置文件nginx.conf也在其中，后续对nginx的使用基本上都是对此配置文件进行相应的修改</p></li><li><p>配置文件示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">worker_processes  1;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application&#x2F;octet-stream;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost;</span><br><span class="line"></span><br><span class="line">        location &#x2F; &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">        location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>配置文件解释：根据上述文件，可以明显地讲nginx.conf配置文件分为三部分</p><ul><li><p><strong>全局块</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Nginx服务器并发处理服务的关键配置,worker_processes值越大,可以支持的并发处理量也越多</span><br><span class="line"># 但是会受到硬件、软件等设备的制约,一般设置为当前计算机的CPU核心数</span><br><span class="line">worker_processes  1;</span><br></pre></td></tr></table></figure><p>从配置文件开始到events块之间的内容，主要会设置一些影响nginx服务器整体运行的配置指令，主要包括配置运行Nginx服务器的用户(组)、允许生成的worker process数、进程pid存放路径、日志存放路径和类型以及配置文件的引入等</p></li><li><p><strong>events块</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">events &#123;</span><br><span class="line">    # 每个work process支持的最大连接数为1024</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>events块涉及的指令主要影响Nginx服务器与用户的网络连接。常用的设置包括是否开启对多work process下的网络连接进行序列化、是否允许同时接收多个网络连接、选取哪种事件驱动模型来处理连接请求、每个work process可以同时支持的最大连接数等。这部分的配置对Nginx的性能影响较大，在实际中应该灵活配置</p></li><li><p><strong>http块</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application&#x2F;octet-stream;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost;</span><br><span class="line"></span><br><span class="line">        location &#x2F; &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">            location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这算是Nginx服务器配置中修改最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里<br><strong>http块还可以包括http全局块、server块</strong></p><ul><li>http全局块：配置的指令包括文件引入、MIME-TYPE定义、日志自定义、连接超时时间、单链接请求数上限等</li><li>server块：<br>server块和虚拟主机有密切关系。虚拟主机从用户角度看和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器硬件成本<br>每个http块可以包括多个server块，而每个server块就相当于一个虚拟主机<br>而每个server块也分为全局server块以及可以同时包含多个locaton块<ul><li>全局server块：最常见的配置是本虚拟机主机的监听配置和本虚拟主机的名称或IP配置</li><li>location块：<br>一个server块可以配置多个location块<br>该块的主要作用是基于Nginx服务器接收到的请求字符串(例如server_name/uri-string)，对虚拟主机名称(也可以是IP别名)之外的字符串(例如前面的/uri-string)进行匹配，对特定的请求进行处理。地址定向、数据缓存和应答控制等功能，还有许多第三方模块的配置也在这里进行</li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="Nginx配置实例-反向代理"><a href="#Nginx配置实例-反向代理" class="headerlink" title="Nginx配置实例-反向代理"></a>Nginx配置实例-反向代理</h2><ul><li><p>反向代理实例1</p><ul><li><p>最终需求：使用nginx反向代理，访问 <a href="http://www.123.com" target="_blank" rel="noopener">www.123.com</a> 跳转到虚拟机的8080端口</p></li><li><p>实现步骤：</p><ul><li><p>测试端口8080准备：</p><ul><li>虚拟机安装Tomcat并启动(启动命令bin/startup.sh)</li><li>在mac中通过浏览器访问虚拟机Tomcat主页：<a href="http://172.16.85.201:8080" target="_blank" rel="noopener">http://172.16.85.201:8080</a></li></ul></li><li><p>修改host文件：在mac中修改hosts文件，将 <a href="http://www.123.com" target="_blank" rel="noopener">www.123.com</a> 映射到虚拟机地址172.16.85.201。此时可以通过 <a href="http://www.123.com:8080" target="_blank" rel="noopener">www.123.com:8080</a> 访问到测试端口</p></li><li><p>修改Nginx配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen  80;</span><br><span class="line">    server_name 172.16.85.201;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;127.0.0.1:8080;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>最终测试</li></ul></li></ul></li></ul></li><li><p>反向代理实例2</p><ul><li><p>最终需求：使用Nginx反向代理，根据访问的路径跳转到不同端口的服务中。其中Nginx监听端口为 9001，要求访问<a href="http://172.16.85.201:9001/edu/" target="_blank" rel="noopener">http://172.16.85.201:9001/edu/</a>直接跳转到172.16.85.201:8080，访问<a href="http://172.16.85.201:9001/vod/" target="_blank" rel="noopener">http://172.16.85.201:9001/vod/</a>直接跳转到172.16.85.201:8081</p></li><li><p>实现步骤：</p><ul><li><p>准备两个Tomcat服务器，一个8080端口，一个8081端口；创建文件夹和测试页面(在8080的Tomcat目录下创建edu目录，其内创建一个a.html测试页面;同理在8081的Tomcat目录下创建vod目录，其内创建一个b.html测试页面)</p></li><li><p>修改Nginx配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen  9001;</span><br><span class="line">    server_name 172.16.85.201;</span><br><span class="line"></span><br><span class="line">    location ~ &#x2F;edu&#x2F; &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;127.0.0.1:8080;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ &#x2F;vod&#x2F; &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;127.0.0.1:8081;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>分别测试网址&lt;172.16.85.201:9001/edu/a.html&gt;和&lt;172.16.85.201:9001/vod/b.html&gt;，验证结果</p></li></ul></li></ul></li><li><p>Location指令说明</p><ul><li><p>用途：用于匹配URL</p></li><li><p>语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location [ &#x3D; | ~ | ~* | ^~] uri &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>参数说明：</p><ul><li>= ：用于不含正则表达式的uri前，要求请求字符串与uri严格匹配，如果匹配成功，就停止继续向下搜索并立即处理该请求</li><li>~：用于表示uri包含正则表达式，并且区分大小写</li><li>~*：用于表示uri包含正则表达式，并且不区分大小写</li><li>^~：用于不含正则表达式的uri前，要求Nginx服务器找到标识uri和请求字符串匹配度最高的 location后，立即使用此location处理请求，而不再使用location块中的正则uri和请求字符串做匹配</li><li><strong>如果uri包含正则表达式，则必须要有<del>或者</del>*标识</strong></li></ul></li></ul></li></ul><h2 id="Nginx配置实例-负载均衡"><a href="#Nginx配置实例-负载均衡" class="headerlink" title="Nginx配置实例-负载均衡"></a>Nginx配置实例-负载均衡</h2><ul><li><p>负载均衡示例</p><ul><li><p>最终效果：浏览器访问<a href="http://172.16.85.201/edu.a.html" target="_blank" rel="noopener">http://172.16.85.201/edu.a.html</a>，能够将请求负载均衡到8080端口和8081端口</p></li><li><p>准备工作：</p><ul><li>准备一台虚拟机，装上两个Tomcat服务器，端口为8080和8081</li><li>在两个Tomcat服务器的webapps目录中，创建edu文件夹和其中的a.html用于测试</li></ul></li><li><p>在Nginx的配置文件中进行负载均衡的配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">upstream myserver&#123;</span><br><span class="line">  server 172.16.85.201:8080;</span><br><span class="line">  server 172.16.85.201:8081;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">  listen       80;</span><br><span class="line">  server_name  172.16.85.201;</span><br><span class="line"></span><br><span class="line">  location / &#123;</span><br><span class="line">    proxy_pass  http://myserver;</span><br><span class="line">    root   html;</span><br><span class="line">    index  index.html index.htm;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Nginx分配服务器策略</p><ul><li><p>轮询(默认)：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除</p></li><li><p>weight(权重)：weight代表权重，默认为1，权重越高被分配的客户端越多</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">upstream myserver&#123;</span><br><span class="line">  server 172.16.85.201:8080 weight=10;</span><br><span class="line">  server 172.16.85.201:8081 weight=20;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>ip_hash(适用于session)：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">upstream myserver&#123;</span><br><span class="line">  ip_hash;</span><br><span class="line">  server 172.16.85.201:8080;</span><br><span class="line">  server 172.16.85.201:8081;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>fair(第三方,公平)：按后端服务器的响应时间来分配请求，响应时间短的优先分配</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">upstream myserver&#123;</span><br><span class="line">  server 172.16.85.201:8080;</span><br><span class="line">  server 172.16.85.201:8081;</span><br><span class="line">  fair;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Nginx配置实例-动静分离"><a href="#Nginx配置实例-动静分离" class="headerlink" title="Nginx配置实例-动静分离"></a>Nginx配置实例-动静分离</h2><ul><li><p>基本介绍：Nginx动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和<br>静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用Nginx处理静态页面，Tomcat处理动态页面。动静分离从目前实现角度来讲大致分为两种：一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案；另外一种方法就是动态跟静态文件混合在一起发布，通过nginx来分开</p></li><li><p>动静分离示例</p><ul><li><p>准备工作：在虚拟机linux系统的本地文件系统中准备静态资源，用于进行访问</p></li><li><p>Nginx进行动静分离的配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  listen       80;</span><br><span class="line">  server_name  172.16.85.201;</span><br><span class="line"></span><br><span class="line">  location /text/ &#123;</span><br><span class="line">    root    /opt/data/;</span><br><span class="line">    index   index.html index.htm;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  location /image/ &#123;</span><br><span class="line">    root    /opt/data/;</span><br><span class="line">    # 开启该配置后,访问/image/页面会展示当前目录下的文件基本信息列表</span><br><span class="line">    autoindex   on;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Nginx搭建高可用集群"><a href="#Nginx搭建高可用集群" class="headerlink" title="Nginx搭建高可用集群"></a>Nginx搭建高可用集群</h2><ul><li><p>准备工作：</p><ul><li><p>准备两台Linux虚拟机，各自都装上Nginx</p></li><li><p>在两台服务器安装keepalived：yum install keepalived -y</p></li><li><p>修改/etc/keepalived下的keepalived.conf配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">global_defs &#123;</span><br><span class="line">  notification_email &#123;</span><br><span class="line">    acassen@firewall.loc</span><br><span class="line">    failover@firewall.loc</span><br><span class="line">    sysadmin@firewall.loc</span><br><span class="line">  &#125;</span><br><span class="line">  notification_email_from Alexandre.Cassen@firewall.loc</span><br><span class="line">  smtp_server 172.16.85.201</span><br><span class="line">  smtp_connect_timeout 30</span><br><span class="line">  router_id LVS_DEVEL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script chk_http_port &#123;</span><br><span class="line">  script "/opt/data/nginx_check.sh"</span><br><span class="line">  #(检测脚本执行的间隔)</span><br><span class="line">  interval 2</span><br><span class="line">  weight 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">  # 备份服务器上将MASTER改为BACKUP</span><br><span class="line">  state MASTER</span><br><span class="line">  # 网卡</span><br><span class="line">  interface ens33</span><br><span class="line">  # 主、备机的virtual_router_id必须相同</span><br><span class="line">  virtual_router_id 51</span><br><span class="line">  # 主、备机取不同的优先级,主机值较大,备份机值较小</span><br><span class="line">  priority 100</span><br><span class="line">  advert_int 1</span><br><span class="line">  authentication &#123;</span><br><span class="line">    auth_type PASS</span><br><span class="line">    auth_pass 1111</span><br><span class="line">  &#125;</span><br><span class="line">  virtual_ipaddress &#123;</span><br><span class="line">    # VRRP H 虚拟地址</span><br><span class="line">    172.16.85.250</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在/opt/data下添加检测脚本文件nginx_check.sh：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">A=`ps -C nginx –no-header |wc -l`</span><br><span class="line">if [ $A -eq 0 ];then</span><br><span class="line">    /usr/local/nginx/sbin/nginx</span><br><span class="line">    sleep 2</span><br><span class="line">    if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then</span><br><span class="line">        killall keepalived</span><br><span class="line">    fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></li><li><p>启动两台Linux虚拟机上的nginx和keepalived</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动nginx(/usr/<span class="built_in">local</span>/nginx/sbin)</span></span><br><span class="line">./nginx</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动keepalived服务</span></span><br><span class="line">systemctl start keepalived.service</span><br></pre></td></tr></table></figure></li><li><p>输入ifconfig可以查看到虚拟ip——172.16.85.250</p></li></ul></li><li><p>测试</p><ul><li>在主机上访问172.16.85.250，nginx主页显示正常</li><li>把主服务器(172.16.85.250)的nginx和keepalived停止</li><li>再次访问172.16.85.250，主页依旧正常</li></ul></li></ul><h2 id="Nginx原理简述"><a href="#Nginx原理简述" class="headerlink" title="Nginx原理简述"></a>Nginx原理简述</h2><ul><li>Nginx主要采用master-worker模式</li><li>一个master和多个worker的好处：<ul><li>可以使用nginx –s reload热部署</li><li>每个worker是独立的进程，如果有其中的一个worker出现问题，其他worker可继续进行争抢，实现请求过程，不会造成服务中断</li></ul></li><li>worker个数：和服务器cpu数相等</li><li>发送请求，占用了多少worker的连接数：2/4</li><li>Nginx有一个master，有四个worker，每个worker支持最大的连接数1024，那么支持的最大并发数是多少?<br>普通的静态访问最大并发数是worker_connections * worker_processes / 2；如果作为反向代理，最大并发数量应该是worker_connections * worker_processes / 4</li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud第一季</title>
      <link href="/2020/09/06/Spring/SpringCloud/SpringCloud%E7%AC%AC%E4%B8%80%E5%AD%A3/"/>
      <url>/2020/09/06/Spring/SpringCloud/SpringCloud%E7%AC%AC%E4%B8%80%E5%AD%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#微服务概述">微服务概述</a></li></ul><a id="more"></a><h2 id="微服务概述"><a href="#微服务概述" class="headerlink" title="微服务概述"></a>微服务概述</h2><ul><li>微服务是什么：微服务的核心就是将传统的一站式应用根据业务拆分成一个一个的服务，彻底地去耦合。每一个微服务提供单个业务功能的服务，一个服务做一件事。从技术角度看就是一种小而独立的处理过程，类似进程概念，能够自行单独启动或销毁，还可以拥有自己独立的数据库</li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark</title>
      <link href="/2020/09/04/BigData/Spark/"/>
      <url>/2020/09/04/BigData/Spark/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Spark概述">Spark概述</a></li><li><a href="#Spark快速上手">Spark快速上手</a></li><li><a href="#Spark运行环境">Spark运行环境</a></li><li><a href="#Spark核心编程">Spark核心编程</a></li></ul><a id="more"></a><h2 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h2><ul><li>Spark是什么：一种基于内存的快速、通用、可扩展的大数据分析计算引擎(unified analytics engine for large-scale data processing)</li><li>Spark And Hadoop<ul><li>从时间节点上看：<ul><li>Hadoop<ul><li>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</li><li>2008年1月，Hadoop成为Apache顶级项目</li><li>2011年1.0正式发布</li><li>2012年3月稳定版发布</li><li>2013年10月发布2.X(Yarn)版本</li></ul></li><li>Spark<ul><li>2009年，Spark诞生于伯克利大学的AMPLab实验室</li><li>2010年，伯克利大学正式开源了Spark项目</li><li>2013年6月，Spark成为了Apache基金会下的项目</li><li>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</li><li>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark</li></ul></li></ul></li><li>从功能上看：<ul><li>Hadoop<ul><li>Hadoop是由Java编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架</li><li>作为Hadoop分布式文件系统，HDFS处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的The Google File System这篇论文，是GFS的开源实现</li><li>MapReduce是一种编程模型，Hadoop根据Google的MapReduce论文将其实现。作为Hadoop的分布式计算模型，MapReduce是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易</li><li>HBase是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop中非常重要的组件</li></ul></li><li>Spark<ul><li>Spark是一种由Scala开发的快速、通用、可扩展的大数据分析引擎</li><li>Spark Core中提供了Spark最基础与最核心的功能</li><li>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据</li><li>Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API</li></ul></li></ul></li><li>综合以上，Spark出现的时间相对较晚，并且主要功能主要是用于数据计算。因此Spark一直被认为是Hadoop MapReduce框架的升级版</li></ul></li><li>Spark Or Hadoop<ul><li>Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景(如：机器学习、图挖掘算法、交互式数据挖掘算法)中存在诸多计算效率等问题。因此Spark应运而生，Spark就是在传统的MapReduce计算框架的基础上，优化其计算过程，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型</li><li>机器学习中ALS、凸优化梯度下降等都需要基于数据集或者数据集的衍生数据反复查询、反复操作。MR模式不太合适，即使多MR串行处理，性能和时间也是一个问题，而且数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的Scala语言恰恰擅长函数的处理</li><li>Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集(Resilient Distributed Datasets)，它提供了比MapReduce更丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法</li><li><strong>Spark和Hadoop的根本差异是多个作业之间的数据通信问题：Spark多个作业之间的数据通信是基于内存的，而Hadoop是基于磁盘的</strong></li><li>Spark Task的启动时间快。Spark采用fork线程的方式，而Hadoop采用创建新的进程的方式</li><li>Spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都要依赖于磁盘交互</li><li>Spark的缓存机制比HDFS的缓存机制高效</li><li>综上所述，在绝大多数的数据计算场景中，Spark确实会比MapReduce更有优势。但Spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够而导致Job执行失败，此时MapReduce是一个更好的选择，所以Spark并不能完全替代MR</li></ul></li><li>Spark核心模块<br><img src="Spark%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97.png" alt="Spark核心模块"><ul><li>Spark Core：Spark Core中提供了Spark最基础与最核心的功能。Spark其他的功能如Spark SQL、Spark Streaming、GraphX以及MLlib都是在Spark Core的基础上进行扩展的</li><li>Spark SQL：Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据</li><li>Spark Streaming：Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API</li><li>Spark MLlib：MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语</li><li>Spark GraphX：GraphX是Spark面向图计算提供的框架与算法库</li></ul></li></ul><h2 id="Spark快速上手"><a href="#Spark快速上手" class="headerlink" title="Spark快速上手"></a>Spark快速上手</h2><ul><li><p>在IDEA上初体验Spark API</p><ul><li><p>创建Maven项目(IDEA中最简单的maven项目,不采用任何模版项目)</p></li><li><p>IDEA安装Scala插件</p></li><li><p>pom添加依赖关系</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 该插件用于将Scala代码编译成class文件 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">          <span class="comment">&lt;!-- 声明绑定到maven的compile阶段 --&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>WordCount案例</p><ul><li><p>配置log4j日志输出(过滤Spark框架的执行日志)——在resource目录下创建log4j.properties文件</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootCategory</span>=<span class="string">ERROR, console</span></span><br><span class="line"><span class="meta">log4j.appender.console</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.console.target</span>=<span class="string">System.err</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout.ConversionPattern</span>=<span class="string">%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n</span></span><br><span class="line"><span class="comment"># Set the default spark-shell log level to ERROR. When running the spark-shell, the</span></span><br><span class="line"><span class="comment"># log level for this class is used to overwrite the root logger's log level, so that</span></span><br><span class="line"><span class="comment"># the user can have different defaults for the shell and regular Spark apps.</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.Main</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="comment"># Settings to quiet third party logs that are too verbose</span></span><br><span class="line"><span class="meta">log4j.logger.org.spark_project.jetty</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.parquet</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.parquet</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="comment"># SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler</span>=<span class="string">FATAL</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry</span>=<span class="string">ERROR</span></span><br></pre></td></tr></table></figure></li><li><p>案例代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Spark是一个计算框架</span></span><br><span class="line">    <span class="comment">// 开发人员使用Spark框架的Api实现计算</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、准备Spark环境</span></span><br><span class="line">    <span class="comment">// setMaster：设定Spark环境的位置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConfig = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">"local"</span>)</span><br><span class="line">      .setAppName(<span class="string">"wordCount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、建立和Spark的连接</span></span><br><span class="line">    <span class="comment">// jdbc：connection</span></span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConfig)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、实现业务操作</span></span><br><span class="line">    <span class="comment">// 3.1、读取指定目录下的数据文件(多个)</span></span><br><span class="line">    <span class="comment">// 参数path可以指向单一的文件/文件目录</span></span><br><span class="line">    <span class="comment">// RDD: 更适合并行计算的数据模型</span></span><br><span class="line">    <span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(path = <span class="string">"./src/main/resources/input"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.2、将读取的内容进行扁平化操作,切分单词</span></span><br><span class="line">    <span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.3、将分词后的数据进行结构的转换</span></span><br><span class="line">    <span class="comment">// word -&gt; (word,1)</span></span><br><span class="line">    <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.4、将转换结构后的数据根据单词进行分组聚合</span></span><br><span class="line">    <span class="comment">// reduceByKey: 根据数据key进行分组,然后对value进行统计聚合</span></span><br><span class="line">    <span class="keyword">val</span> wordSumRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    <span class="keyword">val</span> wordCountArray: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordSumRDD.collect()</span><br><span class="line">    println(wordCountArray.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、释放连接</span></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h2 id="Spark运行环境"><a href="#Spark运行环境" class="headerlink" title="Spark运行环境"></a>Spark运行环境</h2><ul><li><p>基本介绍：Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行，在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来<br><img src="Spark%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.png" alt="Spark运行环境"></p></li><li><p>Local本地模式</p><ul><li><p>介绍：所谓的Local模式就是不需要其他任何节点资源就可以在本地执行Spark代码的环境，一般用于教学、调试、演示等。而之前在IDEA中运行代码的环境我们称之为开发环境</p></li><li><p>环境准备：</p><ul><li>解压缩spark文件</li><li>引入hadoop等Jar包</li></ul></li><li><p>启动Local环境</p><ul><li><p>进入解压缩的目录，执行：bin/spark-shell –master local[*]</p></li><li><p>启动后，可以使用当前主机的4040端口进行Web UI监控</p></li><li><p>命令行工具</p><ul><li><p>准备：在spark根目录下的data目录中，添加word.txt文件，准备一些英文单词</p></li><li><p>在Local环境中输入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"data/word.txt"</span>).flatMap(_.split(<span class="string">""</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure></li><li><p>回车后会实时输出结果，sc是Spark Context的简写，该变量由命令行工具提供</p></li><li><p>退出：:quit(Scala)或者Ctrl + C</p></li></ul></li><li><p>提交应用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master local[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure></li><li><p>提交应用参数解释</p><ul><li>–class：表示要执行程序的主类</li><li>–master local[2]：部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量</li><li>spark-examples_2.12-2.4.5.jar：运行的应用类所在的jar包</li><li>10：表示程序的入口参数，用于设定当前应用的任务数量</li></ul></li></ul></li></ul></li><li><p>Standalone模式</p><ul><li><p>介绍：local本地模式只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行。Spark自身节点运行的集群模式叫做独立部署(Standalone)模式。Spark的Standalone模式体现了经典的master-slave模式</p></li><li><p>集群规划：</p><table><thead><tr><th>hadoop101</th><th>hadoop102</th><th>hadoop103</th></tr></thead><tbody><tr><td>Worker Master</td><td>Worker</td><td>Worker</td></tr></tbody></table></li><li><p>环境准备</p><ul><li><p>解压缩spark文件</p></li><li><p>引入hadoop等Jar包</p></li><li><p>修改配置(conf目录)</p><ul><li><p>修改slaves.template文件名改为slaves</p></li><li><p>修改slaves文件，添加work节点：hadoop101、hadoop102、hadoop103(用回车分割,不能其他空格空行)</p></li><li><p>修改spark-env.sh.template文件名为spark-env.sh</p></li><li><p>修改spark-env.sh文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line">SPARK_MASTER_HOST=hadoop101</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure></li><li><p>分发配置文件：xsync spark-env.sh</p></li></ul></li></ul></li><li><p>启动集群</p><ul><li>执行脚本命令：sbin/start-all.sh</li><li>查看Master资源监控Web UI界面：<a href="http://hadoop101:8080" target="_blank" rel="noopener">http://hadoop101:8080</a></li></ul></li><li><p>提交应用(–master spark://hadoop101:7077：独立部署模式,连接到Spark集群)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop101:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure></li><li><p>提交参数说明</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \</span><br><span class="line">[application-arguments]</span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>解释</th><th>可选值举例</th></tr></thead><tbody><tr><td>–class</td><td>Spark程序中包含主函数的类</td><td>/</td></tr><tr><td>–master</td><td>Spark程序运行的模式</td><td>local模式(local[*])、standalone模式(spark://hadoop101:7077)、Yarn模式(Yarn)</td></tr><tr><td>–executor-memory 1G</td><td>指定每个executor可用内存为1G</td><td>符合集群内存配置即可，具体情况具体分析</td></tr><tr><td>–total-executor-cores 2</td><td>指定所有executor使用的cpu核数为2个</td><td>同上</td></tr><tr><td>–executor-cores</td><td>指定每个executor使用的cpu核数</td><td>同上</td></tr><tr><td>application-jar</td><td>打包好的应用jar(包含依赖)。该URL在集群中全局可见。比如hdfs://共享存储系统；如果是file://path，那么所有的节点的path都要包含同样的jar</td><td>同上</td></tr><tr><td>application-arguments</td><td>传给main()方法的参数</td><td>同上</td></tr></tbody></table></li><li><p>配置历史服务器</p><ul><li><p>介绍：由于spark-shell停止或spark任务结束后，集群监控的4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况</p></li><li><p>具体配置步骤：</p><ul><li><p>修改spark-defaults.conf.template文件名为spark-defaults.conf</p></li><li><p>修改spark-default.conf文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 设置开启日志记录</span><br><span class="line">spark.eventLog.enabled  true</span><br><span class="line"># 设置日志存储路径</span><br><span class="line">spark.eventLog.dir  hdfs://hadoop101:9000/spark_log</span><br></pre></td></tr></table></figure></li><li><p>启动hadoop集群，hdfs上的spark_log目录需要提前存在</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br><span class="line">hadoop dfs -mkdir /spark_log</span><br></pre></td></tr></table></figure></li><li><p>修改spark-env.sh文件，添加日志配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 设置历史日志选项</span><br><span class="line"># 参数1：Web UI访问端口号</span><br><span class="line"># 参数2：指定历史服务器日志存储路径</span><br><span class="line"># 参数3: 指定保存Application历史记录的个数,如果超过这个值,旧的应用程序信息将被删除(是内存中的应用数,而不是页面上显示的应用数)</span><br><span class="line">export SPARK_HISTORY_OPTS="</span><br><span class="line">-Dspark.history.ui.port=18080</span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/spark_log</span><br><span class="line">-Dspark.history.retainedApplications=30"</span><br></pre></td></tr></table></figure></li><li><p>分发配置文件：xsync conf</p></li><li><p>重启集群和历史服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure></li><li><p>重新执行任务</p></li><li><p>查看历史服务情况：<a href="http://hadoop101:18080" target="_blank" rel="noopener">http://hadoop101:18080</a></p></li></ul></li></ul></li><li><p>配置高可用(HA)</p><ul><li><p>介绍：所谓的高可用是因为当前集群中的Master节点只有一个，因此会存在单点故障问题。为了解决单点故障问题，需要在集群中配置多个Master节点，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置</p></li><li><p>集群规划：</p><table><thead><tr><th>hadoop101</th><th>hadoop102</th><th>hadoop103</th></tr></thead><tbody><tr><td>Master、Zookeeper、Worker</td><td>Master、Zookeeper、Worker</td><td>Zookeeper、Worker</td></tr></tbody></table></li><li><p>停止集群：sbin/stop-all.sh</p></li><li><p>启动Zookeeper：bin/zkServer.sh start</p></li><li><p>修改spark-env.sh配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 注释master的host和port,不能把master固定</span><br><span class="line"># SPARK_MASTER_HOST=hadoop101</span><br><span class="line"># SPARK_MASTER_PORT=7077</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"></span><br><span class="line"># 设置zookeeper配置</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS="</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url=hadoop101,hadoop102,hadoop103</span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark"</span><br></pre></td></tr></table></figure></li><li><p>分发配置文件：xsync spark-env.sh</p></li><li><p>重启集群：sbin/start-all.sh</p></li><li><p>启动hadoop102的单独master节点(使hadoop102节点的master状态处于备用状态)：sbin/start-master.sh</p></li><li><p>提交应用到高可用集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop101:7077,hadoop102:7077 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure></li><li><p>停止hadoop101的master进程：kill -9 xxx(Master的进程号)</p></li><li><p>查看hadoop102的Master资源监控Web UI(8989端口)，经过一段时间，hadoop102节点master状态提升为活动状态</p></li></ul></li></ul></li><li><p>Yarn模式</p><ul><li><p>基本介绍：独立部署(Standalone)模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是Spark主要是计算框架，而不是资源调度框架，所以资源调度并不是它的强项，因此还是和其他专业的资源调度框架集成会更靠谱一些。其中，在国内工作中，Yarn使用的非常多</p></li><li><p>环境准备</p><ul><li><p>解压缩spark文件</p></li><li><p>引入hadoop等Jar包</p></li><li><p>修改配置文件</p><ul><li><p><strong>hadoop的配置文件yarn-site.xml</strong>，并分发</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  是否启动一个线程检查每个任务正使用的物理内存量,如果任务超出分配值,</span></span><br><span class="line"><span class="comment">  则直接将其杀掉,默认是true</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  是否启动一个线程检查每个任务正使用的虚拟内存量,</span></span><br><span class="line"><span class="comment">  如果任务超出分配值,则直接将其杀掉,默认是true</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>spark的配置文件spark-env.sh，并分发</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line"># 设置yarn配置目录</span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>启动HDFS和YARN集群</p></li><li><p>提交应用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure></li><li><p>之后便可以在hadoop102节点的8088的Web UI上查看到跑的Spark应用</p></li><li><p>配置历史服务器</p><ul><li><p>参照Standalone模式的spark-env.sh配置</p></li><li><p>修改spark-defaults.conf配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.address=hadoop101:18080</span><br><span class="line">spark.history.ui.port=18080</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>k8s以及Mesos模式</p><ul><li><p>Mesos介绍：Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核，在Twitter得到广泛使用，管理着Twitter超过300000台服务器上的应用部署。但国内依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但原理其实都差不多<br><img src="Mesos%E6%A1%86%E6%9E%B6%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Mesos框架架构图"></p></li><li><p>k8s模式：容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes(k8s)，Spark也在最近的版本中支持了k8s部署模式。具体介绍网址如下：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p></li></ul></li><li><p>部署模式对比</p><table><thead><tr><th>模式</th><th>机器数</th><th>需启动的进程</th><th>应用场景</th></tr></thead><tbody><tr><td>Local</td><td>1</td><td>无</td><td>Spark</td></tr><tr><td>Standalone</td><td>3</td><td>Master及Worker</td><td>单独部署</td></tr><tr><td>Yarn</td><td>1</td><td>Yarn以及HDFS</td><td>混合部署</td></tr></tbody></table></li><li><p>端口号总结</p><ul><li>Spark查看当前Spark-shell运行任务情况端口号：4040(计算)</li><li>Spark Master内部通信服务端口号：7077</li><li>Standalone模式下，Spark Master Web端口号：8080(资源)</li><li>Spark历史服务器端口号：18080</li><li>Hadoop YARN任务运行情况查看端口号：8088</li></ul></li></ul><h2 id="Spark核心编程"><a href="#Spark核心编程" class="headerlink" title="Spark核心编程"></a>Spark核心编程</h2><ul><li><p>基本介绍：Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p><ul><li>RDD : 弹性分布式数据集</li><li>累加器：分布式共享<strong>只写</strong>变量</li><li>广播变量：分布式共享<strong>只读</strong>变量</li></ul></li><li><p>RDD</p><ul><li><p>基本介绍：RDD(Resilient Distributed Dataset)弹性分布式数据集，是Spark中最基本的数据处理模型。Scala代码中是一个抽象类，它代表一个弹性的、不可变、可分区并且其中元素可并行计算的集合</p></li><li><p>重点：</p><ul><li>弹性：<ul><li>存储的弹性：内存与磁盘的自动切换</li><li>容错的弹性：数据丢失可以自动恢复</li><li>计算的弹性：计算出错重试机制</li><li>分片的弹性：可根据需要重新分片</li></ul></li><li>分布式：数据存储在大数据集群(hadoop的HDFS集群)不同节点上</li><li>数据集：RDD封装了计算逻辑，并不保存数据</li><li>数据抽象：RDD是一个抽象类，需要子类具体实现</li><li>不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</li><li>可分区、并行计算</li></ul></li><li><p>基础编程</p><ul><li><p>RDD创建</p><ul><li><p>从集合(内存)中创建RDD：两个方法parallelize()和makeRDD()，其中后者只是包装了前者</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="comment">// 从内存中创建RDD</span></span><br><span class="line"><span class="comment">// 1、parallelize:并行</span></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.parallelize(list)</span><br><span class="line">println(rdd.collect().mkString(<span class="string">","</span>))</span><br><span class="line"><span class="comment">// makeRDD底层代码就是调用了parallelize,只是为了方便理解</span></span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(list)</span><br><span class="line">println(rdd1.collect().mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li><li><p>从外部存储(文件)创建RDD：包括本地文件系统、所有Hadoop支持的数据集(HDFS、HBase等)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="comment">// 从磁盘File中创建RDD</span></span><br><span class="line"><span class="comment">// path：读取文件(目录)的路径</span></span><br><span class="line"><span class="comment">// 相对路径,如果是IDEA,那么是从项目根开始查找</span></span><br><span class="line"><span class="comment">// path路径根据环境的不同自动发生改变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark读取文件时,默认采用Hadoop读取文件的规则——一行一行读取</span></span><br><span class="line"><span class="comment">// 指向文件目录,目录的文本文件都会被读取</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取目录</span></span><br><span class="line"><span class="comment">// val fileRDD: RDD[String] = sparkContext.textFile(path = "input")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取指定文件</span></span><br><span class="line"><span class="comment">// val fileRDD: RDD[String] = sparkContext.textFile(path = "input/w.txt")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取通配符文件</span></span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(path = <span class="string">"input/word*.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 文件路径还可以指向第三方存储系统：HDFS</span></span><br><span class="line"><span class="comment">// val fileRDD: RDD[String] = sparkContext.textFile(path = "hdfs://input/word*.txt")</span></span><br><span class="line">println(fileRDD.collect().mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li><li><p>从其他RDD创建：通过一个RDD运算完后，再产生新的RDD</p></li><li><p>直接创建RDD(new)：使用new的方式直接构造RDD，一般由Spark框架自身使用</p></li></ul></li><li><p>RDD并行度与分区：</p><ul><li><p>基本介绍：默认情况下，Spark可以将一个作业切分成多个任务(Task)后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。该数量可以在构建RDD时指定。这里的并行执行的任务数量并不是指的切分任务的数量，不要混淆了</p></li><li><p>案例：</p><ul><li><p>内存分区案例1：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从内存中创建RDD</span></span><br><span class="line"><span class="comment">// makeRDD</span></span><br><span class="line"><span class="comment">// 参数1：seq: Seq[T] 数据源</span></span><br><span class="line"><span class="comment">// 参数2：numSlices: Int = defaultParallelism(默认并行度——分区的数量)</span></span><br><span class="line"><span class="comment">// 简单总结：RDD中分区的数量就是并行度,设定并行度就是在设定分区数量</span></span><br><span class="line"><span class="comment">// scheduler.conf.getInt("spark.default.parallelism", totalCores)</span></span><br><span class="line"><span class="comment">// 并行度默认会从Spark配置信息中获取spark.default.parallelism的值</span></span><br><span class="line"><span class="comment">// 如果获取不到指定参数,会采用默认值totalCores——机器的总核数</span></span><br><span class="line"><span class="comment">// 机器总核数= 当前环境中可用核数</span></span><br><span class="line"><span class="comment">// local -&gt; 单核(单线程) -&gt; 1</span></span><br><span class="line"><span class="comment">// local[4] -&gt; 4核(4个线程) -&gt; 4</span></span><br><span class="line"><span class="comment">// local[*] -&gt; 当前最大核数 -&gt; 8</span></span><br><span class="line"><span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment">// println(rdd.collect().mkString(","))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将RDD的处理后的数据保存到分区文件中</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li><li><p>内存分区案例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 12,34</span></span><br><span class="line"><span class="comment">// 内存中的集合按照平均分的方式进行分区处理</span></span><br><span class="line"><span class="comment">// val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)</span></span><br><span class="line"><span class="comment">// rdd.saveAsTextFile("output1")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1234</span></span><br><span class="line"><span class="comment">// 1,2,34</span></span><br><span class="line"><span class="comment">// 12345</span></span><br><span class="line"><span class="comment">// 1,23,45</span></span><br><span class="line"><span class="comment">// saveAsTextFile方法如果文件已存在,会发生错误</span></span><br><span class="line"><span class="comment">// 内存中数据的分区基本上就是平均分,如果不能整除,会采用一个基本的算法实现分配</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>,<span class="number">5</span>), <span class="number">3</span>)</span><br><span class="line">rdd1.saveAsTextFile(<span class="string">"output2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1,2,3,4</span></span><br><span class="line"><span class="comment">// val rdd2 = sparkContext.makeRDD(List(1, 2, 3, 4), 4)</span></span><br><span class="line"><span class="comment">// rdd2.saveAsTextFile("output3")</span></span><br><span class="line"></span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li><li><p>文件分区案例1：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// textFile</span></span><br><span class="line"><span class="comment">// 参数1 path：读取文件的路径</span></span><br><span class="line"><span class="comment">// 参数2 minPartitions：最小分区数量</span></span><br><span class="line"><span class="comment">// minPartitions默认值为math.min(defaultParallelism, 2)</span></span><br><span class="line"><span class="comment">// 其中defaultParallelism是totalCores</span></span><br><span class="line"><span class="comment">//    val fileRDD1 = sparkContext.textFile("input/w.txt")</span></span><br><span class="line"><span class="comment">//    fileRDD1.saveAsTextFile("output")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    val fileRDD2 = sparkContext.textFile("input/w.txt", 1)</span></span><br><span class="line"><span class="comment">//    fileRDD2.saveAsTextFile("output2")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1、Spark读取文件采用的是Hadoop的读取规则</span></span><br><span class="line"><span class="comment">// 文件切片规则：以字节方式来切片</span></span><br><span class="line"><span class="comment">// 数据读取规则：以行为单位来读取</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、问题：</span></span><br><span class="line"><span class="comment">//     文件到底切成几片(分区的数量)</span></span><br><span class="line"><span class="comment">//     文件字节数,预计切片数量(2)</span></span><br><span class="line"><span class="comment">// 所谓的最小分区数,取决于总的字节数是否能整除分区数并且剩余的字节小于一定比率(10%,hadoop方式)</span></span><br><span class="line"><span class="comment">// 实际产生的分区数量可能大于最小分区数</span></span><br><span class="line"><span class="keyword">val</span> fileRDD1 = sparkContext.textFile(<span class="string">"input/w.txt"</span>, <span class="number">2</span>)</span><br><span class="line">fileRDD1.saveAsTextFile(<span class="string">"output3"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区的数据如何存储?</span></span><br><span class="line"><span class="comment">// 分区数据是以行为单位读取的,不是以字节</span></span><br><span class="line"><span class="comment">// 数据是以行的方式读取,但是会考虑偏移量(数据的offset)的设置</span></span><br><span class="line"><span class="comment">// 1@@ =&gt; 012</span></span><br><span class="line"><span class="comment">// 2@@ =&gt; 345</span></span><br><span class="line"><span class="comment">// 3@@ =&gt; 678</span></span><br><span class="line"><span class="comment">// 4   =&gt; 9</span></span><br><span class="line"><span class="comment">// 10 byte / 4 = 2 .... 2 =&gt; 5</span></span><br><span class="line"><span class="comment">// 以行为单位...</span></span><br><span class="line"><span class="comment">// 以下左右都是闭区间(取得到)</span></span><br><span class="line"><span class="comment">// 0 =&gt; (0, 2) =&gt; 1</span></span><br><span class="line"><span class="comment">// 1 =&gt; (2, 4) =&gt; 2</span></span><br><span class="line"><span class="comment">// 2 =&gt; (4, 6) =&gt; 3</span></span><br><span class="line"><span class="comment">// 3 =&gt; (6, 8) =&gt;</span></span><br><span class="line"><span class="comment">// 4 =&gt; (8,10) =&gt; 4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    val fileRDD3 = sparkContext.textFile("input/w.txt", 4)</span></span><br><span class="line"><span class="comment">//    fileRDD3.saveAsTextFile("output3")</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    val fileRDD4 = sparkContext.textFile("input/w.txt", 3)</span></span><br><span class="line"><span class="comment">//    fileRDD4.saveAsTextFile("output4")</span></span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>文件分区案例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 / 2 = 3</span></span><br><span class="line"><span class="comment">// (0 , 0 + 3)</span></span><br><span class="line"><span class="comment">// (3 , 3 + 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1@@ =&gt; 012</span></span><br><span class="line"><span class="comment">// 234 =&gt; 345</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// hadoop分区是以文件为单位进行划分的</span></span><br><span class="line"><span class="comment">// 读取数据不能跨越文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 10 / 3 = 3 ... 1 =&gt; 4</span></span><br><span class="line"><span class="comment">// (0,3) (3,6)</span></span><br><span class="line"><span class="keyword">val</span> fileRDD1 = sparkContext.textFile(<span class="string">"input"</span>, <span class="number">3</span>)</span><br><span class="line">fileRDD1.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>分区原理</p><ul><li><p>读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">  (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">    <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">    <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">    (start, end)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> InputSplit[] getSplits(JobConf job, <span class="keyword">int</span> numSplits)</span><br><span class="line">  <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">  <span class="keyword">for</span> (FileStatus file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">    <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Not a file: "</span>+ file.getPath());</span><br><span class="line">    &#125;</span><br><span class="line">    totalSize += file.getLen();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">  <span class="keyword">long</span> minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">    FileInputFormat.SPLIT_MINSIZE, <span class="number">1</span>), minSplitSize);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (FileStatus file: files) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">      <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">      <span class="keyword">long</span> splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">computeSplitSize</span><span class="params">(<span class="keyword">long</span> goalSize, <span class="keyword">long</span> minSize, <span class="keyword">long</span> blockSize)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>RDD转换算子：RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型</p><ul><li><p>Value类型</p><ul><li><p>map</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将处理的数据逐条进行映射转换(可以是类型的转换,也可以是值的转换)</p></li><li><p>案例1：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark - RDD - 算子(方法)</span></span><br><span class="line"><span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 旧RDD -&gt; 转换算子 -&gt; 新RDD</span></span><br><span class="line"><span class="comment">// 转换算子能将旧的RDD通过方法转换为新的RDD,但是不会触发作业的执行</span></span><br><span class="line"><span class="comment">// 分区问题</span></span><br><span class="line"><span class="comment">// RDD中有分区列表</span></span><br><span class="line"><span class="comment">// 默认分区数量不变,数据会转换后输出</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.map(_ * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line"><span class="comment">// collect方法不会转换RDD,会触发作业的执行</span></span><br><span class="line"><span class="comment">// 所以将collect这样的方法称之为行动(action)算子</span></span><br><span class="line"><span class="comment">//    println(rdd1.collect.mkString(","))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    rdd1.saveAsTextFile("output")</span></span><br><span class="line">println(rdd1.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>案例2：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.map(x =&gt; &#123;</span><br><span class="line">  println(<span class="string">s"Map 1st : <span class="subst">$x</span>"</span>)</span><br><span class="line">  x</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.map(x =&gt; &#123;</span><br><span class="line">  println(<span class="string">s"Map 2nd : <span class="subst">$x</span>"</span>)</span><br><span class="line">  x</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// (1, 2)           1(1)           1(2) 2(1) 2(2)</span></span><br><span class="line"><span class="comment">// (3, 4) 3(1) 3(2)      4(1) 4(2)</span></span><br><span class="line"><span class="comment">// 分区内数据按照顺序依次执行,每一条数据的所有逻辑全部执行完毕后才会执行下一条数据</span></span><br><span class="line"><span class="comment">// 分区间数据执行没有顺序,而且无需等待</span></span><br><span class="line">println(rdd2.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：从服务器日志数据apache.log中获取用户请求URL资源路径</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从服务器日志数据apache.log中获取用户请求URL资源路径</span></span><br><span class="line"><span class="keyword">val</span> fileRDD = sparkContext.textFile(<span class="string">"input/apache.log"</span>)</span><br><span class="line"><span class="keyword">val</span> urlRDD = fileRDD.map(</span><br><span class="line">  line =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> datas = line.split(<span class="string">" "</span>)</span><br><span class="line">    datas(<span class="number">6</span>)</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">urlRDD.collect.foreach(println)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>mapPartitions</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">  f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">  preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据)</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// mapPartitions</span></span><br><span class="line"><span class="comment">// 以分区为单位进行计算,和map算子很相似</span></span><br><span class="line"><span class="comment">// 区别就在于map算子是一个一个执行,mapPartitions是一个分区一个分区执行</span></span><br><span class="line"><span class="comment">// 类似于批处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// map方法是全量数据操作,不能丢失数据</span></span><br><span class="line"><span class="comment">// mapPartitions一次性获取分区的所有数据,那么可以执行迭代器集合的所有操作(filter、max、sum)</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"><span class="comment">//    val rdd = dataRDD.mapPartitions(iter =&gt; &#123;</span></span><br><span class="line"><span class="comment">//      iter.map(_ * 2)</span></span><br><span class="line"><span class="comment">//    &#125;)</span></span><br><span class="line"><span class="comment">//    println(rdd.collect.mkString(","))</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitions(iter =&gt; &#123;</span><br><span class="line">  iter.filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">&#125;)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：获取每个数据分区的最大值</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取每个数据分区的最大值</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitions(iter =&gt; <span class="type">List</span>(iter.max).iterator)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>mapPartitionsWithIndex</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">  f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">  preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据)，在处理时同时可以获取当前分区索引</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取每个分区最大值以及分区号</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>), <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitionsWithIndex(</span><br><span class="line">  (index, iter) =&gt; &#123;</span><br><span class="line">    <span class="type">List</span>((index, iter.max)).iterator</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：获取第二个数据分区的数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取第二个数据分区的数据</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取的分区索引是从0开始的</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitionsWithIndex(</span><br><span class="line">  (index, iter) =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (index == <span class="number">1</span>) iter</span><br><span class="line">    <span class="keyword">else</span> <span class="type">Nil</span>.iterator</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>flatMap</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将处理的数据进行扁平化后再进行映射处理，也称之为扁平映射</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(</span><br><span class="line">  <span class="type">List</span>(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.flatMap(list =&gt; list)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：将List(List(1, 2), 3, List(4, 5))进行扁平化操作</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(</span><br><span class="line">  <span class="type">List</span>(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="number">3</span>, <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.flatMap &#123;</span><br><span class="line">  <span class="keyword">case</span> list: <span class="type">List</span>[_] =&gt; list</span><br><span class="line">  <span class="keyword">case</span> d =&gt; <span class="type">List</span>(d)</span><br><span class="line">&#125;</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>glom</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// glom =&gt; 将每个分组的数据在转换为数组</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> valueRDD = dataRDD.glom</span><br><span class="line">valueRDD.foreach(array =&gt; println(array.mkString(<span class="string">","</span>)))</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：计算所有分区最大值求和(分区内取最大值,分区间最大值求和)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// glom</span></span><br><span class="line"><span class="comment">// 计算所有分区最大值求和(分区内取最大值,分区间最大值求和)</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将每个分区的数据转换为数组</span></span><br><span class="line"><span class="keyword">val</span> glomRDD = dataRDD.glom</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数组中的最大值取出</span></span><br><span class="line"><span class="comment">// Array -&gt; max</span></span><br><span class="line"><span class="keyword">val</span> maxRDD = glomRDD.map(array =&gt; array.max)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将取出的最大值求和</span></span><br><span class="line"><span class="keyword">val</span> sum = maxRDD.collect.sum</span><br><span class="line">println(sum)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>groupBy</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将数据根据指定的规则进行分组，分区默认不变，但数据会被打乱重新组合(将这样的操作称之为shuffle)。极限情况下，数据可能被分在同一个分区中。一个组的数据在一个分区中，但并不是说一个分区中只有一个组</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分组</span></span><br><span class="line"><span class="comment">// groupBy方法可以根据指定的规则进行分组,指定的规则的返回值就是分组的key</span></span><br><span class="line"><span class="comment">// groupBy方法的返回值为元组</span></span><br><span class="line"><span class="comment">//     元组的第一个元素：表示分组的key</span></span><br><span class="line"><span class="comment">//     元组的第二个元素：表示相同key的数据形成的可迭代的集合</span></span><br><span class="line"><span class="comment">// groupBy方法执行完毕后,会将数据进行分组操作,但是分区不会改变</span></span><br><span class="line"><span class="comment">//     不同组的数据会打乱分散到不同的分区中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果将上游的分区数据打乱重新组合到下游的分区中,那么这个操作称之为shuffle</span></span><br><span class="line"><span class="comment">// 如果数据被打乱重新组合,那么数据就可能出现不均匀的情况,可以改变下游RDD的数据分区</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// groupBy方法可能会导致数据不均匀,如果想改变分区,可以传递参数</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.groupBy(_ % <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"><span class="comment">//    println(s"Group Num = $&#123;rdd.glom.collect.length&#125;")</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    rdd.collect.foreach &#123;</span></span><br><span class="line"><span class="comment">//      case (key, list) =&gt; println(s"Key = $key , list = &#123; $&#123;list.mkString(",")&#125; &#125;")</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：将List(“Hello”, “hive”, “hbase”, “Hadoop”)根据单词首写字母进行分组</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据单词首写字母进行分组</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="string">"Hello"</span>, <span class="string">"hive"</span>, <span class="string">"hbase"</span>, <span class="string">"Hadoop"</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// StringOps =&gt; String(0),隐式转换,取首个字符串Char</span></span><br><span class="line"><span class="keyword">val</span> valueRDD = dataRDD.groupBy(word =&gt; word(<span class="number">0</span>))</span><br><span class="line">valueRDD.foreach(println)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：从服务器日志数据apache.log中获取每个时间段访问量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> fileRDD = sparkContext.textFile(<span class="string">"input/apache.log"</span>)</span><br><span class="line"><span class="keyword">val</span> timeRDD = fileRDD.map(log =&gt; log.split(<span class="string">" "</span>)(<span class="number">3</span>))</span><br><span class="line"><span class="keyword">val</span> hourRDD = timeRDD.groupBy(time =&gt; time.substring(<span class="number">11</span>, <span class="number">13</span>))</span><br><span class="line">hourRDD.map(it =&gt; (it._1, it._2.size)).foreach(println)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>filter</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将数据根据指定的规则进行过滤筛选，符合规则的数据保留，不符合规则的数据丢弃。当数据进行筛选过滤后分区不变，但分区内的数据可能不均衡。生产环境下，可能会出现<strong>数据倾斜</strong></p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// filter过滤</span></span><br><span class="line"><span class="comment">// 根据指定的规则对数据进行筛选过滤,满足条件的数据保留,不满足的数据丢弃</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">rdd.collect.foreach(println)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>sample</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">  withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">  fraction: <span class="type">Double</span>,</span><br><span class="line">  seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：根据指定的规则从数据集中抽取数据</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// sample用于从数据集中抽取数据</span></span><br><span class="line"><span class="comment">// 参数1：withReplacement(Boolean)表示数据抽取后是否放回,可以重复抽取</span></span><br><span class="line"><span class="comment">// 参数2：fraction(Double)表示数据抽取的几率(每个数据被抽取的几率,不是数据总量的比率)——不放回的场合可重复抽取</span></span><br><span class="line"><span class="comment">// 参数3：seed(Long,默认Utils.random.nextLong)表示随机数种子,可以确定数据的抽取</span></span><br><span class="line"><span class="comment">//  随机数不随机,所谓的随机数依靠随机算法实现</span></span><br><span class="line"><span class="keyword">val</span> valueDRR = dataRDD.sample(withReplacement = <span class="literal">false</span>, <span class="number">0.5</span>, <span class="number">1</span>)</span><br><span class="line">println(valueDRR.collect.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> valueDRR2 = dataRDD.sample(withReplacement = <span class="literal">true</span>, <span class="number">0.5</span>)</span><br><span class="line">println(valueDRR2.collect.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>distinct</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将数据集中重复数据去重</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// distinct 去重</span></span><br><span class="line"><span class="comment">// distinct 可以改变分区的数量</span></span><br><span class="line">dataRDD.distinct.foreach(println)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>coalesce</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">            partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">            (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：根据数据量<strong>缩减分区</strong>，用于大数据集过滤后提高小数据集的执行效率。当spark程序中存在过多小任务时，可以通过coalesce()方法收缩合并分区、减少分区的个数、减小任务调度成本</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), <span class="number">2</span>)</span><br><span class="line"><span class="comment">//    val filterRDD = dataRDD.filter(num =&gt; num % 2 == 0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    filterRDD.saveAsTextFile("output")</span></span><br><span class="line"><span class="comment">// 多 -&gt; 少</span></span><br><span class="line"><span class="comment">// 当数据过滤后,发现数据不够均匀,那么可以缩减分区</span></span><br><span class="line"><span class="comment">//    filterRDD.coalesce(1).saveAsTextFile("output")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果发现数据分区不合理,也可以缩减分区</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// coalesce主要目的是缩减分区,扩大分区时没有效果</span></span><br><span class="line"><span class="comment">// 为什么不能扩大分区? 因为在分区缩减时,数据不会打乱重新组合,没有shuffle的过程</span></span><br><span class="line"><span class="comment">//    dataRDD.coalesce(2).saveAsTextFile("output")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果非得要将数据扩大分区,那么必须打乱数据后重新组合,必须使用shuffle</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// coalesce()</span></span><br><span class="line"><span class="comment">// 参数1：numPartitions: Int——表示缩减分区后的分区数量</span></span><br><span class="line"><span class="comment">// 参数2：shuffle: Boolean = false——表示分区改变时是否会打乱重新组合数据</span></span><br><span class="line">dataRDD.coalesce(<span class="number">6</span>, shuffle = <span class="literal">true</span>).saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>repartition</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 缩减分区 -&gt; coalesce()</span></span><br><span class="line"><span class="comment">// 扩大分区 -&gt; repartition()——底层就是coalesce(..., true)</span></span><br><span class="line">dataRDD.repartition(<span class="number">5</span>).saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>sortBy</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">  f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">  ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">  numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">  (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：该操作用于排序数据。在排序之前可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为正序排列。排序后新产生的RDD的分区数与原RDD的分区数一致</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">4</span>, <span class="number">-2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// sortBy</span></span><br><span class="line"><span class="comment">// 默认排序规则为升序</span></span><br><span class="line"><span class="comment">// sortBy可以通过传递第二个参数改变排序的方式(false逆序)</span></span><br><span class="line"><span class="comment">// sortBy可以设定第三个参数,用于改变分区</span></span><br><span class="line">dataRDD.sortBy(num =&gt; num, ascending = <span class="literal">false</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>pipe</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pipe</span></span>(command: <span class="type">String</span>): <span class="type">RDD</span>[<span class="type">String</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：管道——针对每个分区，都调用一次shell脚本，返回输出的RDD(注意：shell脚本需要放在计算节点可以访问到的位置)</p></li><li><p>案例：</p><ul><li><p>编写一个脚本，并增加执行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim pipe.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo "Start"</span><br><span class="line">while read LINE; do</span><br><span class="line">  echo "&gt;&gt;&gt;"$&#123;LINE&#125;</span><br><span class="line">done</span><br><span class="line"><span class="meta">#</span><span class="bash"> chmod 777 pipe.sh</span></span><br></pre></td></tr></table></figure></li><li><p>命令行工具中创建一个只有一个分区的RDD：val rdd = sc.makeRDD(List(“hi”,”Hello”,”how”,”are”,”you”), 1)</p></li><li><p>将脚本作用该RDD并打印：rdd.pipe(“/opt/data/pipe.sh”).collect</p></li></ul></li></ul></li></ul></li><li><p>双Value类型</p><ul><li><p>intersection</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：对源RDD和参数RDD求交集后返回一个新的RDD</p></li></ul></li><li><p>union</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：对源RDD和参数RDD求并集后返回一个新的RDD</p></li></ul></li><li><p>subtract</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集</p></li></ul></li><li><p>zip</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的元素</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 并集：数据合并,分区也会合并</span></span><br><span class="line"><span class="comment">//    println(dataRDD1.union(dataRDD2).collect.mkString(","))</span></span><br><span class="line">dataRDD1.union(dataRDD2).saveAsTextFile(<span class="string">"output11"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 交集：分区数不变,数据被打乱重组(shuffle),保留最大分区数量</span></span><br><span class="line"><span class="comment">//    println(dataRDD1.intersection(dataRDD2).collect.mkString(","))</span></span><br><span class="line">dataRDD1.intersection(dataRDD2).saveAsTextFile(<span class="string">"output12"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 差集：数据被打乱重组(shuffle)</span></span><br><span class="line"><span class="comment">// 当调用rdd的subtract()方法时,以当前rdd的分区为主,所以分区数量等于当前rdd的分区数量</span></span><br><span class="line"><span class="comment">//    println(dataRDD1.subtract(dataRDD2).collect.mkString(","))</span></span><br><span class="line">dataRDD1.subtract(dataRDD2).saveAsTextFile(<span class="string">"output13"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 拉链：分区数不变</span></span><br><span class="line"><span class="comment">// 两个RDD的分区一致,但是数据量不相同的场合：</span></span><br><span class="line"><span class="comment">// =&gt; Exception：Can only zip RDDs with the same number of elements in each partition</span></span><br><span class="line"><span class="comment">// 两个RDD的分区不一致,数据量也不相同,但是每个分区数据量一致/分区数不一致</span></span><br><span class="line"><span class="comment">// =&gt; Exception：Can't zip RDDs with unequal numbers of partitions</span></span><br><span class="line"><span class="comment">//    println(dataRDD1.zip(dataRDD2).collect.mkString(","))</span></span><br><span class="line">dataRDD1.zip(dataRDD2).saveAsTextFile(<span class="string">"output14"</span>)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>KV类型</p><ul><li><p>partitionBy</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionBy</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// K-V类型的数据操作</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(</span><br><span class="line">  <span class="type">List</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"c"</span>, <span class="number">3</span>), (<span class="string">"d"</span>, <span class="number">4</span>)), <span class="number">1</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">// Spark中很多方法是基于Key进行操作,所以数据格式应该为键值对(对偶元组)</span></span><br><span class="line"><span class="comment">// 如果数据类型为K-V类型,那么Spark会自动给RDD补充很多新的方法(扩展)</span></span><br><span class="line"><span class="comment">// 隐式转换(A =&gt; B)</span></span><br><span class="line"><span class="comment">// partitionBy()方法来自于PairRDDFunctions.class</span></span><br><span class="line"><span class="comment">// RDD的伴生对象中提供了隐式函数可以将RDD[K,V]转换为PairRDDFunctions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// partitionBy：根据指定的规则对数据进行分区</span></span><br><span class="line"><span class="comment">// 其他影响分区的方法：groupBy、filter -&gt; coalesce、repartition(shuffle)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// partitionBy(Partitioner)方法,参数为partitioner——分区器对象</span></span><br><span class="line"><span class="comment">// Partitioner为抽象类,其实现类有：HashPartitioner、RangePartitioner</span></span><br><span class="line"><span class="comment">// HashPartitioner分区规则：将当前数据key的hashCode进行取余操作</span></span><br><span class="line"><span class="comment">// HashPartitioner是spark默认的分区器</span></span><br><span class="line"><span class="comment">// sortBy()使用了RangePartitioner</span></span><br><span class="line">dataRDD.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>)).saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li><li><p>小功能：自定义分区器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line">  <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义分区器——自己决定数据放置在哪个分区做处理</span></span><br><span class="line">  <span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    (<span class="string">"cba"</span>, <span class="string">"Message 1"</span>), (<span class="string">"cba"</span>, <span class="string">"Message 2"</span>), (<span class="string">"cba"</span>, <span class="string">"Message 3"</span>),</span><br><span class="line">    (<span class="string">"nba"</span>, <span class="string">"Message 1"</span>), (<span class="string">"nba"</span>, <span class="string">"Message 2"</span>), (<span class="string">"wnba"</span>, <span class="string">"Message 1"</span>)</span><br><span class="line">  ), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rdd = dataRDD.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(<span class="number">3</span>))</span><br><span class="line">  rdd.mapPartitionsWithIndex((index, datas) =&gt; &#123;</span><br><span class="line">    datas.map(data =&gt; (index, data))</span><br><span class="line">  &#125;).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">  sparkContext.stop</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义分区器</span></span><br><span class="line"><span class="comment">// 1、继承Partitioner</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">num: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 获取分区的数量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = num</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 根据数据的Key来决定数据在哪个分区中进行处理</span></span><br><span class="line">  <span class="comment">// 方法的返回值表示分区编号(索引)</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    key <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"nba"</span> =&gt; <span class="number">0</span></span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>reduceByKey</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：可以将数据按照相同的Key对Value进行聚合</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduceByKey：根据数据的Key进行分组,然后对value进行聚合</span></span><br><span class="line"><span class="comment">// 参数1——func: (V, V) =&gt; V表示相同key的value的聚合方式</span></span><br><span class="line"><span class="comment">// 参数2——numPartitions: Int表示聚合后的分区数量</span></span><br><span class="line">println(sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"hello"</span>, <span class="number">1</span>), (<span class="string">"scala"</span>, <span class="number">1</span>), (<span class="string">"Hello"</span>, <span class="number">2</span>), (<span class="string">"scala"</span>, <span class="number">2</span>)</span><br><span class="line">)).reduceByKey(_ + _).collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>groupByKey</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将分区的数据直接转换为相同类型的内存数组进行后续处理</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// groupByKey：根据固定的规则(数据的Key)进行分组</span></span><br><span class="line"><span class="comment">// groupBy：根据指定的规则对数据进行分组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// groupByKey()方法返回的数据类型为元组</span></span><br><span class="line"><span class="comment">// 元组的元素1——表示为用于分组的Key</span></span><br><span class="line"><span class="comment">// 元素的元素2——表示为分组后的相同Key的value的集合</span></span><br><span class="line"></span><br><span class="line">sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"hello"</span>, <span class="number">1</span>), (<span class="string">"scala"</span>, <span class="number">1</span>), (<span class="string">"Hello"</span>, <span class="number">2</span>), (<span class="string">"scala"</span>, <span class="number">2</span>)</span><br><span class="line">)).groupByKey.map &#123;</span><br><span class="line">  <span class="keyword">case</span> (word, iter) =&gt; (word, iter.sum)</span><br><span class="line">&#125;.collect.foreach(println)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>aggregateByKey</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span><br><span class="line">  combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：将数据根据不同的规则进行分区内计算和分区间计算</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将分区内相同的Key取最大值,分区间相同的Key求和</span></span><br><span class="line"><span class="comment">// 分区内和分区间的计算规则不同</span></span><br><span class="line"><span class="comment">// reduceByKey：分区内和分区间计算规则相同</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 0 -&gt; [(a, 2), (c, 3)]</span></span><br><span class="line"><span class="comment">// 1 -&gt; [(b, 4), (c, 6)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// aggregateByKey：根据Key进行数据聚合</span></span><br><span class="line"><span class="comment">// Scala语法：函数柯里化</span></span><br><span class="line"><span class="comment">// 参数1——zeroValue: U表示计算的初始值,用于在分区内进行计算时当作初始值使用(首次遇到Key)</span></span><br><span class="line"><span class="comment">// 参数2——seqOp: (U, V) =&gt; U表示分区内的计算规则,相同key的value计算</span></span><br><span class="line"><span class="comment">// 参数3——combOp: (U, U) =&gt; U)表示分区间的计算规则,相同key的value的计算</span></span><br><span class="line">println(sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">2</span>), (<span class="string">"c"</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="string">"b"</span>, <span class="number">4</span>), (<span class="string">"c"</span>, <span class="number">5</span>), (<span class="string">"c"</span>, <span class="number">6</span>)</span><br><span class="line">), <span class="number">2</span>).aggregateByKey(zeroValue = <span class="number">0</span>)(</span><br><span class="line">  (x, y) =&gt; math.max(x, y),</span><br><span class="line">  (x, y) =&gt; x + y</span><br><span class="line">).collect.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>foldByKey</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldByKey</span></span>(zeroValue: <span class="type">V</span>)(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">2</span>), (<span class="string">"c"</span>, <span class="number">3</span>),</span><br><span class="line">  (<span class="string">"b"</span>, <span class="number">4</span>), (<span class="string">"c"</span>, <span class="number">5</span>), (<span class="string">"c"</span>, <span class="number">6</span>)</span><br><span class="line">), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算规则相同,都是求和,可以实现wordCount</span></span><br><span class="line"><span class="comment">//    dataRDD.aggregateByKey(0)(</span></span><br><span class="line"><span class="comment">//      (x, y) =&gt; x + y,</span></span><br><span class="line"><span class="comment">//      (x, y) =&gt; x + y</span></span><br><span class="line"><span class="comment">//    )</span></span><br><span class="line">println(dataRDD.aggregateByKey(<span class="number">0</span>)(_ + _, _ + _).collect.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果分区内计算规则和分区间计算规则相同,可以将aggregateByKey简化为foldByKey</span></span><br><span class="line">println(dataRDD.foldByKey(<span class="number">0</span>)(_ + _).collect.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>combineByKey</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">  createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">  mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">  mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：最通用的对kv型rdd进行聚集操作的聚集函数(aggregation function)。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// combineByKey</span></span><br><span class="line"><span class="comment">// 每个Key的平均值：相同key的数据的总和 / 相同key的数量</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"a"</span>, <span class="number">88</span>), (<span class="string">"b"</span>, <span class="number">95</span>), (<span class="string">"a"</span>, <span class="number">91</span>), (<span class="string">"b"</span>, <span class="number">93</span>), (<span class="string">"a"</span>, <span class="number">95</span>), (<span class="string">"b"</span>, <span class="number">98</span>)</span><br><span class="line">), <span class="number">2</span>)</span><br><span class="line"><span class="comment">// 计算时需要将value的格式发生改变,只需要第一个value改变结构即可</span></span><br><span class="line"><span class="comment">// 如果计算时发现相同key的value不符合计算规则的格式时,那么选择combineByKey</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// combineByKey()</span></span><br><span class="line"><span class="comment">// 参数1——createCombiner: V =&gt; C表示将计算的第一个值结构进行转换</span></span><br><span class="line"><span class="comment">// mergeValue: (C, V) =&gt; C表示分区内的计算规则</span></span><br><span class="line"><span class="comment">// mergeCombiners: (C, C) =&gt; C表示分区间的计算规则</span></span><br><span class="line">dataRDD.combineByKey(</span><br><span class="line">  value =&gt; (value, <span class="number">1</span>),</span><br><span class="line">  (tuple: (<span class="type">Int</span>, <span class="type">Int</span>), value) =&gt; (tuple._1 + value, tuple._2),</span><br><span class="line">  (tuple1: (<span class="type">Int</span>, <span class="type">Int</span>), tuple2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (tuple1._1 + tuple2._1, tuple1._2 + tuple2._2)</span><br><span class="line">).map &#123; <span class="keyword">case</span> (key, (total, cnt)) =&gt; (key, total * <span class="number">1.0</span> / cnt) &#125;</span><br><span class="line">  .collect.foreach(println)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>sortByKey</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line">  <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    (<span class="keyword">new</span> <span class="type">User</span>, <span class="number">1</span>), (<span class="keyword">new</span> <span class="type">User</span>, <span class="number">2</span>), (<span class="keyword">new</span> <span class="type">User</span>, <span class="number">3</span>)</span><br><span class="line">  ))</span><br><span class="line"></span><br><span class="line">  println(dataRDD.sortByKey(ascending = <span class="literal">true</span>).collect.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">  sparkContext.stop</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义Key进行排序需要将Key混入特质Ordered</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">User</span>] <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">User</span>): <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>join</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"rddMemory"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"c"</span>, <span class="number">3</span>), (<span class="string">"d"</span>, <span class="number">5</span>), (<span class="string">"a"</span>, <span class="number">10</span>)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"a"</span>, <span class="number">5</span>), (<span class="string">"c"</span>, <span class="number">9</span>), (<span class="string">"c"</span>, <span class="number">2</span>)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="comment">// join可以将两个rdd中相同的key的value连接在一起</span></span><br><span class="line"><span class="comment">// join性能不太高,会形成笛卡尔积,不建议用</span></span><br><span class="line">println(dataRDD1.join(dataRDD2).collect.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li><li><p>leftOuterJoin</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：类似于SQL语句的左外连接</p></li></ul></li><li><p>cogroup</p><ul><li><p>函数签名：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure></li><li><p>函数说明：在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD</p></li><li><p>案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"rddMemory"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"c"</span>, <span class="number">3</span>), (<span class="string">"d"</span>, <span class="number">5</span>), (<span class="string">"a"</span>, <span class="number">10</span>)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">  (<span class="string">"b"</span>, <span class="number">2</span>), (<span class="string">"a"</span>, <span class="number">5</span>), (<span class="string">"c"</span>, <span class="number">9</span>), (<span class="string">"c"</span>, <span class="number">2</span>), (<span class="string">"e"</span>, <span class="number">5</span>)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="comment">//    dataRDD1.leftOuterJoin(dataRDD2).collect.foreach(println)</span></span><br><span class="line"><span class="comment">//    println("-----------")</span></span><br><span class="line"><span class="comment">//    dataRDD1.rightOuterJoin(dataRDD2).collect.foreach(println)</span></span><br><span class="line">dataRDD1.cogroup(dataRDD2).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li></ul></li><li><p>核心属性</p></li><li><p>执行原理</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala</title>
      <link href="/2020/07/24/Language/Scala/Scala/"/>
      <url>/2020/07/24/Language/Scala/Scala/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Scala概述">Scala概述</a></li><li><a href="#变量">变量</a></li><li><a href="#运算符">运算符</a></li><li><a href="#程序流程控制">程序流程控制</a></li><li><a href="#函数式编程基础">函数式编程基础</a></li><li><a href="#面向对象编程-基础">面向对象编程-基础</a></li><li><a href="#面向对象编程-中级">面向对象编程-中级</a></li></ul><a id="more"></a><h2 id="Scala概述"><a href="#Scala概述" class="headerlink" title="Scala概述"></a>Scala概述</h2><ul><li><p>学习Scala的原因</p><ul><li>Spark是新一代内存级大数据计算框架，是大数据的重要内容</li><li>Spark就是使用Scala编写的。因此为了更好的学习Spark, 需要掌握Scala这门语言</li><li>Scala是Scalable Language的简写，是一门多范式(范式/编程方式[面向对象/函数式编程])的编程语言</li><li>联邦理工学院洛桑(EPFL)的Martin Odersky于2001年开始设计Scala(2003年推出)</li><li>Spark的兴起，带动Scala语言的发展</li></ul></li><li><p>Scala由来<br>创始人马丁·奥德斯基(Martin Odersky)是编译器及编程的狂热爱好者，长时间的编程之后，希望发明一种语言，能够让写程序这样的基础工作变得高效，简单。所以当接触到JAVA语言后，对JAVA这门便携式，运行在网络，且存在垃圾回收的语言产生了极大的兴趣，所以决定将函数式编程语言的特点融合到JAVA中，由此发明了两种语言(Pizza &amp; Scala)<br>Pizza和Scala极大地推动了Java编程语言的发展(jdk5.0的泛型，for循环增强, 自动类型转换等，都是从Pizza引入的新特性;jdk8.0的类型推断，Lambda表达式就是从scala引入的特性)<br>现在主流JVM的javac编译(jdk5.0、8.0)就是马丁·奥德斯基编写出来的</p></li><li><p>Scala和Java以及JVM的关系分析图<br><img src="Scala%E5%92%8CJava%E4%BB%A5%E5%8F%8AJVM%E7%9A%84%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90%E5%9B%BE.png" alt="Scala和Java以及JVM的关系分析图"></p></li><li><p>Scala语言的特点<br>Scala是一门以java虚拟机(JVM)为运行环境并将面向对象和函数式编程的最佳特性结合在一起的<strong>静态类型</strong>编程语言</p><ul><li>Scala是一门多范式(multi-paradigm)的编程语言，Scala支持面向对象和函数式编程</li><li>Scala源代码(.scala)会被编译成Java字节码(.class)，然后运行于JVM之上，并可以调用现有的Java类库，实现两种语言的无缝对接</li><li>Scala简洁高效</li><li>Scala设计时参考了Java的设计思想，源于Java</li></ul></li><li><p>Mac上搭建Scala开发环境(Window/Linux类似)</p><ul><li>Scala需要Java运行时库，首先先安装JDK环境</li><li>在<a href="http://www.scala-lang.org/" target="_blank" rel="noopener">http://www.scala-lang.org/</a>下载mac版本tar.gz包</li><li>解压tar.gz包(不配置环境变量)</li><li>在命令行下cd进入解压包的bin目录下</li><li>输入scala进入Scala Cli，会打印版本信息</li></ul></li><li><p>搭建IDEA的Scala开发环境</p><ul><li>在Plugin面板中安装(如果下载太慢,去官网下载对应版本的插件到本地,在安装)</li><li>新建空的maven项目</li><li>当前默认不支持scala的框架，需要引入scala框架，右键项目点击add framework support</li><li>选中scala，在use library中设定解压的目录</li><li>右键main目录创建一个diretory，名为scala，右键scala目录，mark directory，选择source root</li></ul></li><li><p>Scala执行流程</p><ul><li>.scala源文件通过scalac编译成.class字节码，再通过scala运行</li><li>.scala源文件直接通过scala运行(运行慢)</li></ul></li><li><p>Scala程序特点</p><ul><li>以.scala为扩展名</li><li>执行入口为main()函数</li><li>严格区分大小写</li><li>方法由一条条语句构成，每个语句后不需要添加分号</li><li>如果在一行有多条语句，除了最后一行语句不要分号，其他语句都需要分号</li></ul></li><li><p>Scala输出的三种方式</p><ul><li>字符串通过’+’方式(类似Java)</li><li>printf方式进行格式化(%,类似C)</li><li>字符串通过$引用(类似Kotlin)</li></ul></li><li><p>Scala在IDEA下进行源码关联</p><ul><li>在官网下载source源码包</li><li>解压到本地</li><li>在IDEA中打开一个源码文件，在右上角上点击Attach Sources</li><li>选中解压后的本地目录</li></ul></li><li><p>Scala注释</p><ul><li>单行/多行注释(同Java)</li><li>文档注释：scaladoc -d 源码.scala</li></ul></li></ul><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><ul><li><p>Scala变量声明</p><ul><li>基础语法：var | val 变量名 [:变量类型] = 变量值</li><li>声明变量时，类型可以省略(编译器自动推导,即类型推导)</li><li>类型确定后，就不能修改，说明Scala是强数据类型语言</li><li>在声明/定义一个变量时，可以使用var或者val来修饰，var修饰的变量可改变，val修饰的变量不可改(同Kotlin)</li><li>val修饰的变量在编译后，等同于加上final(同Kotlin)</li><li>var修饰的对象引用可以改变，val修饰的则不可改变，但对象的状态(值,属性)却是可以改变的(比如自定义对象、数组、集合等等)</li><li>变量声明时，需要初始值</li></ul></li><li><p>数据类型</p><ul><li><p>Scala与Java有着相同的数据类型，在Scala中数据类型都是对象(包装了基础数据类型)，也就是说scala没有java中的原生类型(同Kotlin)</p></li><li><p>Scala数据类型分为两大类AnyVal(值类型)和AnyRef(引用类型)——注意：不管是AnyVal还是AnyRef都是对象<br><img src="Scala%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BD%93%E7%B3%BB%E5%9B%BE.png" alt="Scala数据类型体系图"></p><ul><li>在Scala中有一个根类型Any，他是所有类的父类</li><li>scala中一切皆为对象，分为两类AnyVal和AnyRef，它们都是Any子类</li><li>Null类型是scala的特殊类型，只有一个值null，是bottom class，也是所有AnyRef类型的子类</li><li>Nothing类型也是bottom class，是所有类的子类，开发中通常可以将Nothing类型的值返回给任意变量或函数(抛异常使用很多)</li></ul></li><li><p>数据类型列表</p><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>Byte[1]</td><td>8位有符号补码整数。数值区间为-128～127</td></tr><tr><td>Short[2]</td><td>16位有符号补码整数。数值区间为-32768～32767</td></tr><tr><td>Int[3]</td><td>32位有符号补码整数。数值区间为-2^31～2^31-1</td></tr><tr><td>Long[4]</td><td>64位有符号补码整数。数值区间为-2^63～2^63-1</td></tr><tr><td>Float[4]</td><td>32位，IEEE754标准的单精度浮点数</td></tr><tr><td>Double[8]</td><td>64位，IEEE754标准的双精度浮点数</td></tr><tr><td>Char[2]</td><td>16位无符号Unicode字符, 区间值为U+0000～U+FFFF</td></tr><tr><td>String</td><td>字符序列</td></tr><tr><td>Boolean[1]</td><td>true或false</td></tr><tr><td>Unit</td><td>表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()</td></tr><tr><td>Null</td><td>null</td></tr><tr><td>Nothing</td><td>Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型</td></tr><tr><td>Any</td><td>Any是所有其他类的超类</td></tr><tr><td>AnyRef</td><td>AnyRef类是Scala里所有引用类(reference class)的基类</td></tr></tbody></table><ul><li>整数类型使用细节<ul><li>Scala各整数类型有固定的表数范围和字段长度，不受具体OS的影响，以保证Scala程序的可移植性</li><li>Scala的整型常量/字面量默认为Int型，声明Long型常量/字面量需后加’l’或’L’</li><li>表示特大整数：BigInt类</li></ul></li><li>浮点类型使用细节<ul><li>与整数类型类似，Scala浮点类型也有固定的表数范围和字段长度，不受具体OS的影响</li><li>Scala的浮点型常量默认为Double型，声明Float型常量，须后加’f’或’F’</li><li>两种表示方式<ul><li>十进制数形式：如5.12、512.0f、.512(必须有小数点)</li><li>科学计数法形式：如5.12e2</li></ul></li><li>通常情况下应使用Double类型，比Float类型更精准(小数点后大致7位)</li><li>表示更为精确的小数：BigDecimal</li></ul></li><li>字符类型使用细节<ul><li>字符常量是用单引号’’括起来的单个字符</li><li>Scala也允许使用转义字符’&#39;来将其后的字符转变为特殊字符型常量(同Java)</li><li>Char相当于一个整数，可以进行运算</li><li>字符类型存取本质<ul><li>存储：字符 -&gt; 码值 -&gt; 二进制 -&gt; 存储</li><li>读取：二进制 -&gt; 码值 -&gt; 字符 -&gt; 读取</li></ul></li></ul></li><li>Unit、Null和Nothing类型使用细节<ul><li>Null类只有一个实例对象null，类似于Java中的null引用。null可以赋值给任意引用类型(AnyRef)，但是不能赋值给值类型</li><li>Unit类型用来标识过程，也就是没有明确返回值的函数，类似于Java里的void。Unit只有一个实例()</li><li>Nothing可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于Nothing是其他任意类型的子类，它还能跟要求返回值的方法兼容</li></ul></li></ul></li></ul></li><li><p>值类型转换</p><ul><li><p>隐式转换：当Scala程序在进行赋值或者运算时，精度小的类型自动转换为精度大的数据类型<br><img src="%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%B2%BE%E5%BA%A6%E5%A4%A7%E5%B0%8F%E6%8E%92%E5%BA%8F%E5%9B%BE.png" alt="数据类型精度大小排序图"><br>细节说明：</p><ul><li>有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算</li><li>当我们把精度(容量)大的数据类型赋值给精度(容量)小的数据类型时会报错</li><li>byte、short和char之间不会相互自动转换，三者计算时首先都转换为int类型</li><li>自动提升原则：表达式结果的类型自动提升为操作数中(容量、精度)最大的类型</li></ul></li><li><p>强制类型转换：自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转函数，<strong>但可能造成精度降低或溢出</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// java</span></span><br><span class="line">int num = (int)<span class="number">2.5</span></span><br><span class="line"><span class="comment">// scala</span></span><br><span class="line"><span class="keyword">var</span> num : <span class="type">Int</span> = <span class="number">2.7</span>.toInt</span><br></pre></td></tr></table></figure><p>细节说明：</p><ul><li>强转符号只针对于最近的操作数有效，往往会使用小括号提升优先级</li><li>Char类型可以保存Int的常量值，但不能保存Int的变量值，需要强转</li></ul></li><li><p>值类型和String类型的转换</p><ul><li>基本类型转String类型：将基本类型的值 + “”(同Java)</li><li>String类型转基本数据类型：调用String.toXxx方法</li><li>小数字符串转Int会抛出异常，不会进行截取</li></ul></li></ul></li><li><p>标识符命名规则</p><ul><li>基本和Java一致</li><li>首字符为字母，后续字符任意字母和数字，美元符号，可后接下划线_</li><li>数字不可以开头</li><li>首字符为操作符(比如+ - * /)，后续字符也需跟至少一个操作符(反编译后scala将其转译)</li><li>操作符(比如+-*/)不能在标识符中间和最后</li><li>用反引号``包括的任意字符串，即使是关键字也可以</li><li>预定义标识符可以用，如Int，但不推荐</li></ul></li><li><p>Scala的39个关键字</p><ul><li>package, import, class, object, trait, extends, with, type, forSome</li><li>private, protected, abstract, sealed, final, implicit, lazy, override</li><li>try, catch, finally, throw</li><li>if, else, match, case, do, while, for, return, yield</li><li>def, val, var</li><li>this, super</li><li>new</li><li>true, false, null</li></ul></li></ul><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><ul><li><p>运算符分类</p><ul><li>算术运算符<ul><li>%的运算原则：a % b = a - a / b * b(同Java)</li><li>在scala中没有++和–，使用+=1和-=1代替</li><li>/的整数除和小数除是有区别的；整数除只保留整数部分而舍弃小数部分</li></ul></li><li>关系运算符(==、!=、&gt;、&lt;、&lt;=、&gt;=)<ul><li>关系运算的结果都是Boolean类型(true/false)</li><li>如果两个浮点数进行比较，应当保证数据类型一致</li></ul></li><li>逻辑运算符(&amp;&amp;、||、!)</li><li>赋值运算符(=、+=、-=、*=、/=、%=、&lt;&lt;=、&gt;&gt;=、&amp;=、^=、|=)<ul><li>运算顺序从右往左</li><li>赋值运算符的左边只能是变量，右边可以是变量、表达式、常量值/字面量</li></ul></li><li>位运算符(&amp;、|、^、~、&lt;&lt;、&gt;&gt;、&gt;&gt;&gt;)</li><li>Scala不支持三目运算符(x ? x : x)，使用if-else代替(类似kotlin)</li><li>运算符优先级(同Java)<ul><li>()[]</li><li>单目运算</li><li>算术运算</li><li>移位运算</li><li>比较运算</li><li>位运算</li><li>关系运算</li><li>赋值运算</li><li>,<br><img src="%E8%BF%90%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E7%BA%A7.png" alt="运算符优先级"></li></ul></li></ul></li><li><p>键盘输入语句</p><ul><li>输入String：StdIn.readLine()</li><li>输入Int：StdIn.readInt</li><li>…</li></ul></li></ul><h2 id="程序流程控制"><a href="#程序流程控制" class="headerlink" title="程序流程控制"></a>程序流程控制</h2><ul><li><p>三大流程控制</p><ul><li>顺序控制</li><li>分支控制<ul><li>Scala中任意表达式都是有返回值的，也就意味着if else表达式其实是有返回结果的，具体返回结果的值取决于满足条件的代码体的最后一行内容</li><li>Scala中没有switch，使用<strong>模式匹配(match-case)来处理</strong></li></ul></li><li>循环控制<ul><li>for循环<ul><li>基本语法：<ul><li>for(i &lt;- start to end)：其中i表示循环的变量，i将会从start～end循环，前后闭合</li><li>for(item &lt;- list)：集合遍历</li><li>for(i &lt;- start until end)：与to不同的是前闭后开</li><li>for(i &lt;- start to end if i % 2 != 0)：循环守卫，即循环保护式(也称条件判断式,守卫)。保护式为true则进入循环体内部，为false则跳过，类似于continue</li><li>for(i &lt;- start to end ; j = f(i))：引入变量，’;’不可少；其次i和j均为val不可变类型变量</li><li>for(i &lt;- start1 to end1 ; j &lt;- start2 to end2)：嵌套循环，’;’不可少；上面代码不常用，基本用单层for的嵌套</li><li>val res = for(i &lt;- start to end) yield i：循环返回值，将遍历过程中的每个结果i返回到一个新的Vector集合中；yield后可以是一个代码块，在最后一行返回</li><li>for(i &lt;- Range(start,end,step))：控制for循环的步长</li></ul></li><li>补充<ul><li>{}和()对于for表达式都可以</li><li>有一个不成文的约定：当for推导式仅包含单一表达式时使用圆括号，当其包含多个表达式时使用大括号</li><li>当使用{}来换行写表达式时，分号就不用写了</li></ul></li></ul></li><li>while/do-while循环(同Java)<ul><li>与if语句不同，while语句本身没有返回值，即结果是Unit类型</li><li>while没有返回值，所以用while语句来计算并返回结果时，不可避免地使用声明在while外部的变量，那么就等同于循环的内部对外部的变量造成了影响，所以不推荐使用，而是推荐使用for循环</li></ul></li></ul></li></ul></li><li><p>while循环的中断</p><ul><li><p>说明：Scala内置控制结构特地去掉了break和continue，是为了更好的适应函数化编程，推荐使用函数式的风格解决break和contine的功能，而不是一个关键字</p></li><li><p>if-else和循环守卫也可以实现continue效果</p></li><li><p>举例说明(break的使用)：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 导入break相关函数</span></span><br><span class="line"><span class="keyword">import</span> util.control.<span class="type">Breaks</span>._</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">breakable()函数</span></span><br><span class="line"><span class="comment">  1、是一个高阶函数：可以接受函数的函数</span></span><br><span class="line"><span class="comment">  2、源码实现：</span></span><br><span class="line"><span class="comment">    def breakable(op: =&gt; Unit) &#123;</span></span><br><span class="line"><span class="comment">      try &#123;</span></span><br><span class="line"><span class="comment">        op</span></span><br><span class="line"><span class="comment">      &#125; catch &#123;</span></span><br><span class="line"><span class="comment">        case ex: BreakControl =&gt;</span></span><br><span class="line"><span class="comment">          if (ex ne breakException) throw ex</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    op：=&gt; Unit表示接受的参数是一个没有参数和返回值的函数，可简单理解为一段代码块</span></span><br><span class="line"><span class="comment">  3、breakable对break()抛出的异常做了处理，代码就继续执行</span></span><br><span class="line"><span class="comment">  4、传入代码块时，一般将()改为&#123;&#125;(类似Kotlin)</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">breakable&#123;</span><br><span class="line">  <span class="keyword">while</span>(n &lt;= <span class="number">20</span>)&#123;</span><br><span class="line">    n+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">18</span>)&#123;</span><br><span class="line">      <span class="comment">// 在scala中使用函数break()中断循环</span></span><br><span class="line">      <span class="comment">// def break(): Nothing = &#123; throw breakException &#125;</span></span><br><span class="line">      <span class="keyword">break</span>()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="函数式编程基础"><a href="#函数式编程基础" class="headerlink" title="函数式编程基础"></a>函数式编程基础</h2><ul><li><p>函数式编程介绍</p><ul><li>概念说明<ul><li>函数式编程<ul><li>是一种”编程范式”</li><li>属于”结构化编程”的一种，主要思想是把运算过程尽量写成一系列嵌套的函数调用</li><li>函数式编程中，将函数也当做数据类型，因此可以接受函数当作输入(参数)和输出(返回值)</li></ul></li><li>在scala中，<strong>方法</strong>和<strong>函数</strong>几乎可以等同(比如他们的定义、使用、运行机制都一样的)，只是函数的使用方式更加的灵活多样</li><li>函数式编程是从编程方式(范式)的角度来谈的，可以这样理解：函数式编程把函数当做一等公民，充分利用函数、支持的函数的多种使用方式。<br>比如：在Scala当中，函数是一等公民，像变量一样，既可以作为函数的参数使用，也可以将函数赋值给一个变量。函数的创建不用依赖于类或者对象；而在Java当中，函数的创建则要依赖于类、抽象类或者接口</li><li>Scala中函数式编程和面向对象编程(是以对象为基础的编程方式)融合在一起</li></ul></li><li>函数式编程、面向对象编程关系分析<br><img src="%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%85%B3%E7%B3%BB%E5%88%86%E6%9E%90%E5%9B%BE.png" alt="函数式编程、面向对象编程关系分析图"></li></ul></li><li><p>函数的定义(为完成某一功能的程序指令(语句)的集合)</p><ul><li><p>基本语法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">函数名</span> </span>([参数名: 参数类型], ...) [[: 返回值类型] =] &#123;</span><br><span class="line">  语句...</span><br><span class="line">  <span class="keyword">return</span> 返回值</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>语法介绍</p><ul><li>函数声明关键字为def(definition)</li><li>[参数名: 参数类型], …：表示函数的输入(参数列表)，可以没有。如果有，多个参数使用逗号间隔</li><li>函数返回值<ul><li>(: 返回值类型 =) 明确返回值的类型</li><li>(=) 表示返回值类型不确定，使用类型推导完成</li><li>(空)，表示没有返回值，return不生效</li><li>如果没有return，默认以执行到最后一行的结果作为返回值</li></ul></li></ul></li></ul></li><li><p>函数注意事项</p><ul><li>递归调用的规则<ul><li>程序执行一个函数时，就创建一个新的受保护的独立空间(新函数栈)</li><li>函数的局部变量是独立的，不会相互影响</li><li>递归调用必须有递归出口，否则就是无限递归</li></ul></li><li>函数的形参列表可以是多个，如果函数没有形参，调用时可以不带()</li><li>形参列表和返回值列表的数据类型可以是值类型和引用类型</li><li>Scala中的函数可以根据函数体最后一行代码自行推断函数返回值类型。那么在这种情况下，return关键字可以省略</li><li>因为Scala可以自行推断，所以在省略return关键字的场合，返回值类型也可以省略</li><li>如果函数明确使用return关键字，那么函数返回就不能使用自行推断了，这时要明确写成(: 返回类型 =  )，当然如果你什么都不写，即使有return，返回值也为()</li><li>如果函数明确声明无返回值(声明Unit)，那么函数体中即使使用return关键字也不会有返回值</li><li>如果明确函数无返回值或不确定返回值类型，那么返回值类型可以省略(或声明为Any)</li><li>Scala语法中任何的语法结构都可以嵌套其他语法结构(灵活)，即：函数中可以再声明/定义函数，类中可以再声明类，方法中可以再声明/定义方法</li><li>Scala函数的形参，在声明参数时，直接赋初始值(默认值)，这时调用函数时，如果没有指定实参，则会使用默认值。如果指定了实参，则实参会覆盖默认值</li><li>如果函数存在多个参数，每一个参数都可以设定默认值，那么这个时候，传递的参数到底是覆盖默认值，还是赋值给没有默认值的参数，就不确定了(默认按照声明顺序[从左到右])。在这种情况下，可以采用带名参数(类似Kotlin)</li><li>Scala函数的形参默认是val的，因此不能在函数中进行修改</li><li>递归函数未执行之前是无法推断出来结果类型，在使用时必须有明确的返回值类型</li><li>Scala函数支持可变参数，可变参数必须放在最后，如args :Int*</li></ul></li><li><p>过程</p><ul><li>基本介绍：将函数的返回类型为Unit的函数称之为过程(procedure)，如果明确函数没有返回值，那么等号可以省略</li><li>注意区分：如果函数声明时没有返回值类型，但是有等号，可以进行类型推断(最后一行代码)；这时这个函数实际是有返回值的，该函数并不是过程</li></ul></li><li><p>惰性函数</p><ul><li><p>一种应用场景<br>惰性计算(<strong>尽可能延迟表达式求值</strong>)是许多函数式编程语言的特性。惰性集合在需要时提供其元素，无需预先计算它们，这带来了一些好处：首先，可以将耗时的计算推迟到绝对需要的时候；其次，可以创造无限个集合，只要它们继续收到请求，就会继续提供元素。函数的惰性使用能够得到更高效的代码。Java并没有为惰性提供原生支持，Scala提供了</p></li><li><p>Java实现懒加载(单例模式——懒汉式)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LazyDemo</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> LazyDemo instance = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="title">LazyDemo</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> LazyDemo <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 如果没有初始化过,那么进行初始化</span></span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">      instance = <span class="keyword">new</span> LazyDemo();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> instance;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>介绍<br>当函数返回值被声明为lazy时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称之为惰性函数，在Java的某些框架代码中称之为懒加载(延迟加载)</p></li><li><p>案例</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> res = sum(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">  println(<span class="string">"-----------------"</span>)</span><br><span class="line">  println(<span class="string">"res = "</span> + res)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(n1 : <span class="type">Int</span>, n2 : <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  println(<span class="string">"sum() ~~~"</span>)</span><br><span class="line">  n1 + n2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">结果：</span></span><br><span class="line"><span class="comment">-----------------</span></span><br><span class="line"><span class="comment">sum() ~~~</span></span><br><span class="line"><span class="comment">res = 30</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li><li><p>注意事项和细节</p><ul><li>lazy不能修饰var类型的变量</li><li>不但在调用函数时，加了lazy会导致函数的执行被推迟；<strong>在声明一个变量时，如果声明了lazy，那么变量值得分配也会推迟。比如lazy val i = 10</strong></li></ul></li></ul></li><li><p>异常</p><ul><li><p>介绍</p><ul><li>Scala提供try和catch块来处理异常：try块用于包含可能出错的代码；catch块用于处理try块中发生的异常。可以根据需要在程序中有任意数量的try…catch块</li><li>语法处理上和Java类似，但是又不尽相同(许多异常包装了Java中的Exception——类似Kotlin)</li></ul></li><li><p>Java异常回顾</p><ul><li><p>示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> b = <span class="number">10</span>;</span><br><span class="line">  <span class="comment">// ArithmeticException,除0异常</span></span><br><span class="line">  <span class="keyword">int</span> c = b / i;</span><br><span class="line">&#125; <span class="keyword">catch</span>(Exception e)  &#123;</span><br><span class="line">  e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="comment">// 最终要执行的代码</span></span><br><span class="line">  System.out.println(<span class="string">"java finally"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Java异常处理的注意点：</p><ul><li>java语言按照try—catch-catch-…—finally的方式来处理异常</li><li>不管有没有异常捕获，都会执行finally，因此通常可以在finally代码块中释放资源</li><li>可以有多个catch，分别捕获对应的异常，这时需要把范围小的异常类写在前面，把范围大的异常类写在后面，否则编译错误。会提示”Exception ‘java.lang.xxxxxx’ has already been caught”</li></ul></li></ul></li><li><p>Scala异常处理</p><ul><li><p>示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> r = <span class="number">10</span> / <span class="number">0</span></span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> ex: <span class="type">ArithmeticException</span>=&gt; println(<span class="string">"ArithmeticException!"</span>)</span><br><span class="line">  <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; println(<span class="string">"Normal Exception!"</span>)</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="comment">// 最终要执行的代码</span></span><br><span class="line">  println(<span class="string">"scala finally"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Scala异常处理的注意点：</p><ul><li><p>在scala异常处理中只有一个catch；在catch中有多个case，每个case可以匹配一种异常；”=&gt;”是一个关键符号，表示后面是对该异常的处理代码块</p></li><li><p>我们将可疑代码封装在try块中。在try块之后使用了一个catch处理程序来捕获异常。如果发生任何异常，catch处理程序将处理它，程序将不会异常终止</p></li><li><p>Scala的异常的工作机制和Java一样，但是Scala没有”checked(编译期)”异常，即Scala没有编译异常这个概念，异常都是在运行的时候捕获处理</p></li><li><p>可使用throw关键字抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing(因为Nothing是所有类型的子类型，所以throw表达式可以用在任何需要类型的地方)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> res = test()</span><br><span class="line">  println(res)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span></span>(): <span class="type">Nothing</span> = &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">"My Exception!"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在Scala里，借用了模式匹配的思想来做异常的匹配。因此可以在catch的代码里，使用一系列case子句来匹配异常。”=&gt;”可以接着多条语句(换行)，类似java的switch-case语句</p></li><li><p>异常捕捉的机制与其他语言中一样，如果有异常发生，catch子句是按次序捕捉的。因此在catch子句中，越具体的异常越要靠前，越普遍的异常越靠后。如果把越普遍的异常写在前，把具体的异常写在后，scala不会报错，但这样是非常不好的编程风格</p></li><li><p>finally子句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作，这点和Java一样</p></li><li><p>Scala提供了throws关键字来声明异常。可以使用方法定义声明异常。它向调用者函数提供了此方法可能引发此异常的信息。它有助于调用函数处理并将该代码包含在try-catch块中，以避免程序异常终止。在scala中，可以使用throws注释来声明异常</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  f11()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等同于NumberFormatException.class</span></span><br><span class="line"><span class="meta">@throws</span>(classOf[<span class="type">NumberFormatException</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f11</span></span>()  = &#123;</span><br><span class="line">  <span class="string">"abc"</span>.toInt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li></ul><h2 id="面向对象编程-基础"><a href="#面向对象编程-基础" class="headerlink" title="面向对象编程-基础"></a>面向对象编程-基础</h2><ul><li><p>Scala是面向对象的</p><ul><li>Java是面向对象的编程语言。但由于历史原因，Java中还存在着非面向对象的内容：基本类型、null、静态方法等</li><li>Scala来源于Java，所以天生就是面向对象的语言，而且Scala是纯粹的面向对象的语言(即在Scala中，一切皆为对象)</li></ul></li><li><p>Scala定义类</p><ul><li><p>基本语法(基本与Java一致)：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[修饰符] <span class="class"><span class="keyword">class</span> <span class="title">类名</span> </span>&#123;</span><br><span class="line">  类体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>注意事项</p><ul><li>在scala语法中类并不声明为public，所有这些类都具有公有可见性(即默认就是public)</li><li>一个Scala源文件可以包含多个类(同Java)，而且默认都是public</li></ul></li><li><p>属性/成员变量注意事项</p><ul><li><p>属性的定义语法同变量：[访问修饰符] var/val 属性名称 [：类型] = 属性值</p></li><li><p>属性的定义类型可以为任意类型，包含值类型或引用类型</p></li><li><p>Scala中声明一个属性必须显式初始化，Scala可根据初始化数据的类型自动推断，此时属性类型可以省略(这与Java不同)</p></li><li><p>如果赋值为null，则一定要加类型。因为不加类型，那么该属性的类型就是Null类型</p></li><li><p>如果在定义属性时暂时不赋值，也可以使用符号”_”，让系统分配默认值</p><table><thead><tr><th>类型</th><th>_对应的值</th></tr></thead><tbody><tr><td>Byte/Short/Int/Long</td><td>0</td></tr><tr><td>Float/Double</td><td>0.0</td></tr><tr><td>Boolean</td><td>false</td></tr><tr><td>String和其他引用类型</td><td>null</td></tr></tbody></table></li><li><p>同一类型不同对象的属性相互独立，互不影响</p></li></ul></li></ul></li><li><p>创建对象</p><ul><li>基本语法：val | var 对象名 [: 类型] = new 类型()</li><li>说明<ul><li>如果我们不希望改变对象的引用(即内存地址)，应该声明为val，否则声明为var。scala设计者推荐使用val：因为一般来说，在程序中，我们只是改变对象的属性的值，而不是改变对象的引用</li><li>scala在声明对象变量时，可以根据创建对象的类型自动推断，所以类型声明可以省略(java不可省略,类似Kotlin)。<strong>但当类型和后面new的对象类型有继承关系即多态时，就必须写</strong></li></ul></li></ul></li><li><p>访问属性</p><ul><li><p>基本语法：对象名.属性名</p></li><li><p>原理：</p><ul><li><p>示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> test = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line">  test.age = <span class="number">5</span></span><br><span class="line">  test.name = <span class="string">"xiong"</span></span><br><span class="line">  println(<span class="string">s"age = <span class="subst">$&#123;test.age&#125;</span> , name = <span class="subst">$&#123;test.name&#125;</span> , tag = <span class="subst">$&#123;test.tag&#125;</span>"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> name: <span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">var</span> age: <span class="type">Int</span> = _</span><br><span class="line">  <span class="keyword">val</span> tag = <span class="string">"SOBXiong"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>反编译的.class文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// test$.class</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">test</span>$ </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> test$ MODULE$;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    test.Person test = <span class="keyword">new</span> test.Person();</span><br><span class="line">    <span class="comment">// 对象名.属性名修改属性是通过底层编译器翻译包装了java方法实现的</span></span><br><span class="line">    test.age_$eq(<span class="number">5</span>);</span><br><span class="line">    test.name_$eq(<span class="string">"xiong"</span>);</span><br><span class="line">    <span class="comment">// 对象名.属性名获取属性也是通过底层编译器翻译包装了java方法实现的</span></span><br><span class="line">    Predef$.MODULE$.println((<span class="keyword">new</span> StringBuilder(<span class="number">25</span>)).append(<span class="string">"age = "</span>).append(test.age()).append(<span class="string">" , name = "</span>).append(test.name()).append(<span class="string">" , tag = "</span>).append(test.tag()).toString());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> test$() &#123; MODULE$ = <span class="keyword">this</span>;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="comment">// var变量翻译后自动生成getter/setter方法</span></span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">name</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.name; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> name_$eq(String x$<span class="number">1</span>) &#123; <span class="keyword">this</span>.name = x$<span class="number">1</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">age</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.age; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> age_$eq(<span class="keyword">int</span> x$<span class="number">1</span>) &#123; <span class="keyword">this</span>.age = x$<span class="number">1</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// val变量翻译为final变量(不可变),且只提供getter方法,不提供setter修改的方法</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String tag = <span class="string">"SOBXiong"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">tag</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.tag; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>方法</p><ul><li><p>基本说明：Scala中的方法其实就是函数</p></li><li><p>基本语法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">方法名</span></span>(参数列表) [：返回值类型] = &#123;</span><br><span class="line">  方法体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>方法调用机制原理</p><ul><li>当scala程序开始执行时，先在栈区开辟一个main栈。main栈最后被销毁(scala程序终止)</li><li>当scala程序执行到一个方法时，总会开一个新的栈</li><li>每个栈是独立的空间，变量(基本数据类型)是独立的，相互不影响(引用类型除外)</li><li>当方法执行完毕后，该方法开辟的栈就会被JVM机回收</li></ul></li></ul></li><li><p>构造器</p><ul><li><p>基本介绍：构造器(constructor)又叫构造方法，是类的一种特殊的方法，<strong>主要作用是完成对新对象的初始化</strong></p></li><li><p>Java构造器回顾</p><ul><li><p>基本语法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[修饰符] 类名(参数列表) &#123;</span><br><span class="line">  构造方法体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>特点</p><ul><li>在Java中一个类可以定义多个不同的构造方法(构造方法重载)</li><li>如果没有定义构造方法，系统会自动生成一个默认无参构造方法(也叫默认构造器)，比如Person(){}</li><li>一旦定义了自己的构造方法，默认的构造方法就被覆盖了，就不能再使用默认的无参构造方法，除非显式地定义一下</li></ul></li></ul></li><li><p>Scala构造器</p><ul><li><p>介绍：和Java一样，Scala创建新对象也需要调用构造方法，并且可以有任意多个构造方法(即scala中构造器也支持重载)。Scala类的构造器包括：<strong>主构造器</strong>和<strong>辅助构造器</strong></p></li><li><p>基本语法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">类名</span>(<span class="params">形参列表</span>) </span>&#123;<span class="comment">// 主构造器</span></span><br><span class="line">  <span class="comment">// 类体</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(形参列表) &#123;...&#125;<span class="comment">// 辅助构造器1</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(形参列表) &#123;...&#125;<span class="comment">// 辅助构造器2、3...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>构造器参数</p><ul><li>Scala类的主构造器的形参若未用任何修饰符修饰，那么这个参数是局部变量</li><li>如果参数使用val关键字声明，那么Scala会将参数作为类的私有的只读属性使用</li><li>如果参数使用var关键字声明，那么那么Scala会将参数作为类的成员属性使用，并会提供属性对应的xxx()[类似getter]以及xxx_$eq()[类似setter]方法(这时的成员属性是私有的，但是可读写)</li></ul></li><li><p>Bean属性</p><ul><li>介绍<br>JavaBeans规范定义了Java的属性是像getXxx()和setXxx()的方法。许多Java框架都依赖这个命名习惯。为了与Java的互操作性，产生了@BeanProperty注解。在Scala字段前加@BeanProperty时会自动生成规范的setXxx()以及getXxx()方法。这时可以使用对象.setXxx()和对象.getXxx()来修改和获取属性值</li><li>注意<br>给某个属性加入@BeanPropetry注解后，会生成getXXX和setXXX的方法，并且对原来底层自动生成类似xxx(),xxx_$eq()方法，没有冲突，二者可以共存</li></ul></li><li><p>注意事项和细节</p><ul><li>Scala构造器作用是完成对新对象的初始化，<strong>构造器没有返回值</strong></li><li>主构造器的声明直接放置于类名之后</li><li>主构造器会执行类定义中的所有语句，这可以体会到Scala把函数式编程和面向对象编程融合在一起(构造器也是方法/函数)</li><li><strong>如果主构造器无参数，小括号可省略，构建对象时调用的构造方法的小括号也可以省略</strong></li><li>辅助构造器名称为this(和Java、Kotlin不一样)，<strong>多个辅助构造器通过不同参数列表进行区分(底层就是构造器重载)</strong></li><li>如果想让主构造器变成私有的，可以在()之前加上private，这样只能通过辅助构造器来构造对象</li><li>辅助构造器的声明不能和主构造器的声明一致，否则会发生错误(构造器名重复)</li></ul></li><li><p>示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> p1 = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"sobxiong"</span>)</span><br><span class="line">  p1.showInfo()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  println(<span class="string">"类定义语句~~~"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="meta">@BeanProperty</span> <span class="keyword">var</span> name: <span class="type">String</span> = _</span><br><span class="line">  <span class="meta">@BeanProperty</span> <span class="keyword">var</span> age: <span class="type">Int</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    辅助构造器无论是直接或间接,最终都一定要调用主构造器,执行主构造器的逻辑</span></span><br><span class="line"><span class="comment">    而且需要放在辅助构造器的第一行(这点和Java一样,Java中一个构造器要调用同类的其它构造器也需要放在第一行)</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="comment">// 直接调用主构造器</span></span><br><span class="line">    <span class="keyword">this</span>()</span><br><span class="line">    <span class="keyword">this</span>.name = name</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name: <span class="type">String</span>, age: <span class="type">Int</span>) &#123;</span><br><span class="line">    <span class="comment">// 直接调用主构造器</span></span><br><span class="line">    <span class="keyword">this</span>()</span><br><span class="line">    <span class="keyword">this</span>.name = name</span><br><span class="line">    <span class="keyword">this</span>.age = age</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(age: <span class="type">Int</span>) &#123;</span><br><span class="line">    <span class="comment">// 间接调用主构造器</span></span><br><span class="line">    <span class="keyword">this</span>(<span class="string">"匿名"</span>)</span><br><span class="line">    <span class="keyword">this</span>.age = age</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">showInfo</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"person信息如下:"</span>)</span><br><span class="line">    println(<span class="string">"name = "</span> + <span class="keyword">this</span>.name)</span><br><span class="line">    println(<span class="string">"age = "</span> + <span class="keyword">this</span>.age)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">结果如下：</span></span><br><span class="line"><span class="comment">类定义语句~~~</span></span><br><span class="line"><span class="comment">person信息如下:</span></span><br><span class="line"><span class="comment">name = sobxiong</span></span><br><span class="line"><span class="comment">age = 0</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li><li><p>反编译的.class文件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">name</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.name; &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> name_$eq(String x$<span class="number">1</span>) &#123; <span class="keyword">this</span>.name = x$<span class="number">1</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> name(); &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String x$<span class="number">1</span>)</span> </span>&#123; name_$eq(x$<span class="number">1</span>); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">age</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.age; &#125;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> age_$eq(<span class="keyword">int</span> x$<span class="number">1</span>) &#123; <span class="keyword">this</span>.age = x$<span class="number">1</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> age(); &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> x$<span class="number">1</span>)</span> </span>&#123; age_$eq(x$<span class="number">1</span>); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Predef$.MODULE$.println(<span class="string">"类定义语句~~~"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="string">""</span>);</span><br><span class="line">    age_$eq(age);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>();</span><br><span class="line">    name_$eq(name);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>();</span><br><span class="line">    name_$eq(name);</span><br><span class="line">    age_$eq(age);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">showInfo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Predef$.MODULE$.println(<span class="string">"person信息如下:"</span>);</span><br><span class="line">    Predef$.MODULE$.println((<span class="keyword">new</span> StringBuilder(<span class="number">7</span>)).append(<span class="string">"name = "</span>).append(name()).toString());</span><br><span class="line">    Predef$.MODULE$.println((<span class="keyword">new</span> StringBuilder(<span class="number">6</span>)).append(<span class="string">"age = "</span>).append(age()).toString());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h2 id="面向对象编程-中级"><a href="#面向对象编程-中级" class="headerlink" title="面向对象编程-中级"></a>面向对象编程-中级</h2><ul><li><p>包</p><ul><li><p>回顾Java包</p><ul><li>Java包的三大作用<ul><li>区分相同名字的类</li><li>当类很多时，可以很好的管理类</li><li>控制访问范围</li></ul></li><li>Java包的声明：package xxx;</li><li>Java包的本质：创建不同的文件夹保存类文件</li><li>Java包的要求：<ul><li>包名和源码所在的系统文件目录结构要一致</li><li>编译后的字节码文件路径也和包名保持一致</li></ul></li></ul></li><li><p>Scala包</p><ul><li><p>基本语法：package xxx</p></li><li><p>Scala的作用：</p><ul><li>区分相同名字的类</li><li>当类很多时，可以很好的管理类</li><li>控制访问范围</li><li>可以对类的功能进行扩展</li></ul></li><li><p><em>Scala包名和源码所在的系统文件目录可以不一致，但编译后的.class字节码文件路径和包名会保持一致</em>(该工作由编译器完成)</p></li><li><p>包的命名规则：只能包含数字、字母、下划线、小圆点(.)，但不能用数字开头，也不要使用关键字</p></li><li><p>包的命名规范：com.公司名.项目名.业务模块名</p></li><li><p>Scala自动引用的常用包：java.lang.*、scala、Predef</p></li><li><p>Scala包注意事项和使用细节：</p><ul><li><p>scala多种等价的包形式</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的包形式</span></span><br><span class="line"><span class="keyword">package</span> com.atguigu.scala</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name = <span class="string">"Nick"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">play</span></span>(message: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="keyword">this</span>.name + <span class="string">" "</span> + message)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价的第二种包形式</span></span><br><span class="line"><span class="keyword">package</span> com.atguigu</span><br><span class="line"><span class="keyword">package</span> scala</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name = <span class="string">"Nick"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">play</span></span>(message: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="keyword">this</span>.name + <span class="string">" "</span> + message)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 嵌套包形式</span></span><br><span class="line"><span class="keyword">package</span> com.atguigu&#123;</span><br><span class="line">  <span class="keyword">package</span> scala&#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">      <span class="keyword">val</span> name = <span class="string">"Nick"</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">play</span></span>(message: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="keyword">this</span>.name + <span class="string">" "</span> + message)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>嵌套包的好处：可以灵活地在同一个文件中将类(class, object)、特质(trait)创建在不同的包中</p></li><li><p>作用域原则：可以直接向上访问。即：<strong>Scala可在子包中直接访问父包中的内容，大括号体现作用域</strong>。(<em>提示: Java中子包使用父包的类,需要import</em>)。在子包和父包类重名时，默认采用就近原则，如果希望指定使用某个类，需要指定包名</p></li><li><p>父包要访问子包的内容时，需要import对应的类</p></li><li><p>可以在同一个.scala文件中，声明多个并列的package(建议嵌套的pakage不要超过3层)</p></li><li><p>包名可以相对也可以绝对。在一般情况下，我们使用相对路径来引入包，只有当包名冲突时，使用绝对路径来处理</p></li></ul></li></ul></li><li><p>包对象</p><ul><li><p>基本介绍：包可以包含类(class, object)和特质(trait)，但不能包含函数/方法或变量的定义。这是Java虚拟机的局限。为了弥补这一点不足，scala提供了包对象的概念来解决这个问题</p></li><li><p>示例：</p><ul><li><p>示例代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiong &#123;</span><br><span class="line">  <span class="comment">// 每个包都可以有一个包对象</span></span><br><span class="line">  <span class="comment">// 需要在父包中定义它,且名称与子包一样。</span></span><br><span class="line">  <span class="keyword">package</span> <span class="class"><span class="keyword">object</span> <span class="title">scala</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name = <span class="string">"jack"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sayOk</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">"package object sayOk!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">package</span> scala &#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 包对象scala中声明的name变量</span></span><br><span class="line">        println(name)</span><br><span class="line">        <span class="comment">// 调用包对象scala中声明的sayOk方法</span></span><br><span class="line">        sayOk()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">object</span> <span class="title">TestObj</span> </span>&#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> t = <span class="keyword">new</span> <span class="type">Test</span>()</span><br><span class="line">        t.test()</span><br><span class="line">        <span class="comment">// 因为TestObj和scala这个包对象在同一包,因此也可以使用name属性</span></span><br><span class="line">        println(<span class="string">"name ="</span> + name)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>机制分析：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 当创建包对象后,在该包下生成final修饰的package和package$类</span></span><br><span class="line"><span class="comment">// package$.class</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">package</span>$ </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">package</span>$ MODULE$;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">name</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>.name; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> name_$eq(String x$<span class="number">1</span>) &#123; <span class="keyword">this</span>.name = x$<span class="number">1</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sayOk</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    scala.Predef$.MODULE$.println(<span class="string">"package object sayOk!"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">package</span>$() &#123;</span><br><span class="line">    MODULE$ = <span class="keyword">this</span>;</span><br><span class="line">    <span class="keyword">this</span>.name = <span class="string">"jack"</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Test.class</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Predef$.MODULE$.println(<span class="keyword">package</span>$.MODULE$.name());</span><br><span class="line">    <span class="keyword">package</span>$.MODULE$.sayOk();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TestObj$.class</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">TestObj</span>$ </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> TestObj$ MODULE$;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Test t = <span class="keyword">new</span> Test();</span><br><span class="line">    t.test();</span><br><span class="line">    scala.Predef$.MODULE$.println((<span class="keyword">new</span> StringBuilder(<span class="number">6</span>)).append(<span class="string">"name ="</span>).append(<span class="keyword">package</span>$.MODULE$.name()).toString());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> TestObj$() &#123; MODULE$ = <span class="keyword">this</span>; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>注意事项</p><ul><li>每个包都可以有一个包对象，需要在父包中定义它</li><li>包对象名称需要和包名一致，一般用来对包的功能进行补充</li></ul></li></ul></li><li><p>包的可见性</p><ul><li><p>回顾Java</p><ul><li><p>访问修饰符介绍(控制方法和变量的访问权限、范围)</p><ul><li>公开级别：用public修饰，对外公开</li><li>受保护级别：用protected修饰，对子类和同一个包中的类公开</li><li>默认级别：没有修饰符号，向同一个包的类公开</li><li>私有级别：用private修饰，只有类本身可以访问，不对外公开</li></ul><table><thead><tr><th>访问级别</th><th>访问控制修饰符</th><th>同类</th><th>同包</th><th>子类</th><th>不同包</th></tr></thead><tbody><tr><td>公开</td><td>public</td><td>√</td><td>√</td><td>√</td><td>√</td></tr><tr><td>受保护</td><td>protected</td><td>√</td><td>√</td><td>√</td><td>×</td></tr><tr><td>默认</td><td>/</td><td>√</td><td>√</td><td>×</td><td>×</td></tr><tr><td>私有</td><td>private</td><td>√</td><td>×</td><td>×</td><td>×</td></tr></tbody></table></li><li><p>修饰符注意事项</p><ul><li>修饰符可以用来修饰类中的属性，成员方法以及类</li><li>只有默认和public才能修饰类，并且遵循上述访问权限的特点</li></ul></li></ul></li><li><p>Scala的包的可见性(四种修饰符与Java一样)</p><ul><li><p>当属性访问权限为默认时，从底层看属性是private的，但是因为提供了xxx_$eq()[类似setter]/xxx()[类似getter]方法，因此从使用效果看任何地方都可以访问</p></li><li><p>当方法访问权限为默认时，默认为public访问权限</p></li><li><p>private为私有权限，只在类的内部和伴生对象中可用</p></li><li><p>protected为受保护权限，scala中受保护权限比Java中更严格，只能子类访问，同包无法访问(编译器)</p></li><li><p>在scala中没有public关键字，即不能用public显式地修饰属性和方法</p></li><li><p>包访问权限(表示属性有了限制，同时包也有了限制)，这点和Java不一样，体现出Scala包使用的灵活性</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xiong.scala</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  增加包访问权限后</span></span><br><span class="line"><span class="comment">  1、private同时起作用,不仅同类可以使用</span></span><br><span class="line"><span class="comment">  2、同时com.xiong.scala中包下其他类也可以使用</span></span><br><span class="line"><span class="comment">  3、修饰符也可以设置为public、protected等</span></span><br><span class="line"><span class="comment">  4、包可见性可以延展到上曾,如改为xiong</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="keyword">private</span>[scala] <span class="keyword">val</span> pname = <span class="string">"hello"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>包的引入</p><ul><li><p>基本介绍：Scala引入包也是使用import，基本的原理和机制和Java一样，但Scala中的import功能更加强大，也更灵活</p></li><li><p>使用细节和注意事项：</p><ul><li><p>在Scala中，<strong>import语句可以出现在任何地方，并不仅限于文件顶部</strong>，import语句的作用一直延伸到包含该语句的块末尾。这种语法的好处是：<strong>在需要时在引入包，缩小import包的作用范围，提高效率</strong></p></li><li><p>Java中如果想要导入包中所有的类，可以通过通配符*，Scala中采用下_</p></li><li><p>如果不想要某个包中全部的类，而是其中的几个类，可以采用选取器(大括号)</p></li><li><p>如果引入的多个包中含有相同的类，那么可以将不需要的类进行重命名进行区分，这个就是<strong>重命名</strong></p></li><li><p>如果某个冲突的类根本就不会用到，那么这个类可以直接<strong>隐藏</strong>掉</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将HashMap重命名为JavaHashMap</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123; <span class="type">HashMap</span>=&gt;<span class="type">JavaHashMap</span>, <span class="type">List</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 引入java.util包的所有类,但是忽略HashMap</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123; <span class="type">HashMap</span>=&gt;_, _&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>继承</p><ul><li>Java继承回顾<ul><li>语法：class 子类名 extends 父类名 { 类体 }</li><li>子类继承父类的属性和方法</li><li>单继承</li></ul></li><li>Scala继承<ul><li>语法同Java</li><li>单继承同Java</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase</title>
      <link href="/2020/07/17/BigData/HBase/"/>
      <url>/2020/07/17/BigData/HBase/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#HBase简介">HBase简介</a></li><li><a href="#HBase快速入门">HBase快速入门</a></li><li><a href="#HBase进阶">HBase进阶</a></li><li><a href="#HBase-API">HBase-API</a></li><li><a href="#HBase优化">HBase优化</a></li></ul><a id="more"></a><h2 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h2><ul><li>HBase定义：HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库</li><li>HBase数据模型：逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构(K-V)来看，HBase更像是一个<strong>multi-dimensional map(多维度Map)</strong><ul><li>逻辑结构<br><img src="HBase%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84.png" alt="HBase逻辑结构"></li><li>物理存储结构<br><img src="HBase%E7%89%A9%E7%90%86%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.png" alt="HBase物理存储结构"></li><li>数据模型<ul><li>Name Space<br>命名空间，类似于关系型数据库的DatabBase概念，每个命名空间下有多个表。<strong>HBase有两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间</strong></li><li>Region<br>类似于关系型数据库的表概念。不同的是，<strong>HBase定义表时只需要声明列簇即可，不需要声明具体的列</strong>。这意味着，<strong>往HBase写入数据时，字段可以动态、按需指定</strong>。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景</li><li>Row<br>HBase表中的<strong>每行数据都由一个RowKey和多个Column(列)组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索</strong>，所以RowKey的设计十分重要</li><li>Column<br>HBase中的<strong>每个列都由Column Family(列簇)和Column Qualifier(列限定符)进行限定</strong>，例如info：name，info：age。建表时，只需指明列簇，而列限定符无需预先定义</li><li>Time Stamp<br>用于标识数据的不同版本(version)，每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间</li><li>Cell<br>由{rowkey,column Family:column Qualifier,time Stamp}唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮</li></ul></li></ul></li><li>HBase基本架构<br><img src="HBase%E6%9E%B6%E6%9E%84(%E4%B8%8D%E5%AE%8C%E6%95%B4%E7%89%88).png" alt="HBase架构(不完整版)"><br>架构角色：<ul><li>Region Server<br>Region Server为Region的管理者，其实现类为<strong>HRegionServer</strong>，主要作用如下:<br>对于数据的操作：get,put,delete<br>对于Region的操作：splitRegion、compactRegion</li><li>Master<br>Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：<br>对于表的操作：create,delete,alter<br>对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移</li><li>Zookeeper<br>HBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作</li><li>HDFS<br>HDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持</li></ul></li></ul><h2 id="HBase快速入门"><a href="#HBase快速入门" class="headerlink" title="HBase快速入门"></a>HBase快速入门</h2><ul><li><p>HBase安装部署</p><ul><li><p>Zookeeper正常部署</p></li><li><p>Hadoop(主要是HDFS)正常部署</p></li><li><p>HBase正常部署</p><ul><li><p>解压HBase：tar -zxvf hbase-2.2.5-bin.tar.gz -C /opt/module</p></li><li><p>修改HBase的配置文件</p><ul><li><p>修改hbase-env.sh的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自行管理zookeeper</span></span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure></li><li><p>修改hbase-site.xml的内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 设置为分布式部署 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置HBase的默认存储文件的路径 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000/HBase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置zookeeper的集群地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1,hadoop2,hadoop3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置zookeeper的data目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper-3.6.1/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>修改regionservers文件(配置集群节点)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure></li><li><p><strong>软链接hadoop配置文件到HBase</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-2.2.5/conf/core-site.xml</span><br><span class="line">ln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-2.2.5/conf/hdfs-site.xml</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>将HBase远程发送到集群其他节点：xsync hbase-2.2.5/</p></li><li><p>启动HBase服务</p><ul><li>单独启动方式(提示:如果集群之间的节点时间不同步，会导致 regionserver 无法启动，抛出ClockOutOfSyncException 异常)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hbase-daemon.sh start master</span><br><span class="line">bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure><ul><li>群起集群启动方式：bin/start-hbase.sh(停止:bin/stop-hbase.sh)</li></ul></li><li><p>查看HBase页面：<a href="http://hadoop1:16010" target="_blank" rel="noopener">http://hadoop1:16010</a></p></li></ul></li><li><p>HBase Shell操作</p><ul><li><p>基本操作</p><ul><li>进入HBase客户端命令行：bin/hbase shell</li><li>查看帮助命令：help</li><li>查看当前数据库的所有表：list</li></ul></li><li><p>表的操作</p><ul><li><p>创建表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create 'student','info'</span><br></pre></td></tr></table></figure></li><li><p>插入(更新)数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">put 'student','1001','info:sex','male'</span><br></pre></td></tr></table></figure></li><li><p>扫描查看表数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scan 'student'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 左闭右开</span></span><br><span class="line">scan 'student',&#123;STARTROW =&gt; '1001', STOPROW =&gt; '1003'&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看多版本的表数(无需在表上设置存放版本)</span></span><br><span class="line">scan 'student',&#123;RAW =&gt; true, VERSIONS =&gt; 10&#125;</span><br></pre></td></tr></table></figure></li><li><p>查看表结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe 'student'</span><br></pre></td></tr></table></figure></li><li><p>查看指定行或指定行的指定列的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">get 'student','1001'</span><br><span class="line">get 'student','1001','info:name'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看name列的三个版本的数据(需要在表上设置存放版本)</span></span><br><span class="line">get 'student','1001',&#123;COLUMN =&gt; 'info:name',VERSIONS =&gt; 3&#125;</span><br></pre></td></tr></table></figure></li><li><p>统计表数据行数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count 'student'</span><br></pre></td></tr></table></figure></li><li><p>删除数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除某rowkey的全部数据</span></span><br><span class="line">deleteall 'student','1001'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除某rowkey的某一列数据</span></span><br><span class="line">delete 'student','1002','info:sex'</span><br></pre></td></tr></table></figure></li><li><p>清空表数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 提示: 清空表的操作顺序为先<span class="built_in">disable</span>,然后再truncate</span></span><br><span class="line">truncate 'student'</span><br></pre></td></tr></table></figure></li><li><p>删除表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 首先要将表设置为<span class="built_in">disable</span>状态</span></span><br><span class="line">disable 'student'</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line">drop 'student'</span><br></pre></td></tr></table></figure></li><li><p>变更表信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置info列簇的数据存放3个版本</span></span><br><span class="line">alter 'student',&#123;NAME =&gt; 'info',VERSIONS =&gt; 3&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h2 id="HBase进阶"><a href="#HBase进阶" class="headerlink" title="HBase进阶"></a>HBase进阶</h2><ul><li><p>架构原理<br><img src="HBase%E8%AF%A6%E7%BB%86%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="HBase详细架构图"></p><ul><li>StoreFile<br>保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile(HFile)，数据在每个StoreFile中都是有序的</li><li>MemStore<br>写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的HFile</li><li>WAL<br>由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件(简称WAL)中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建</li></ul></li><li><p>写流程<br><img src="HBase%E5%86%99%E6%B5%81%E7%A8%8B.png" alt="HBase写流程"></p><ul><li>Client先访问zookeeper，获取hbase:meta表位于哪个Region Server</li><li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问</li><li>与目标Region Server进行通讯</li><li>将数据顺序写入(追加)到WAL</li><li>将数据写入对应的MemStore，数据会在MemStore进行排序</li><li>向客户端发送ack</li><li>等达到MemStore的刷写时机后，将数据刷写到HFile</li></ul></li><li><p>MemStore Flush<br><img src="MemStore-Flush%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="MemStore-Flush示意图"><br>MemStore刷写时机：</p><ul><li>当某个memstore的大小达到了hbase.hregion.memstore.flush.size(默认值128M)，其所在 region的所有memstore都会刷写。当memstore的大小达到了hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier(默认值4)时，会阻止继续往该memstore写数据</li><li>当region server中memstore的总大小达到java_heapsize * hbase.regionserver.global.memstore.size * hbase.regionserver.global.memstore.size.lower.limit(默认值0.95)，region会按照其所有memstore的大小顺序(由大到小)依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下(当前还可以写数据)。当region server中memstore的总大小达到java_heapsize * hbase.regionserver.global.memstore.size时，会阻止继续往所有的memstore写数据</li><li>到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置：hbase.regionserver.optionalcacheflushinterval(默认1小时)</li><li>当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log 以下(该属性名已经废弃——无法手动设置，默认值为32;但是可以设置每个log文件的blockSize,相当于增加单个log文件的存储量,默认为hdfs的块大小128M)</li></ul></li><li><p>读流程<br><img src="HBase%E8%AF%BB%E6%B5%81%E7%A8%8B.png" alt="HBase读流程"></p><ul><li>Client先访问zookeeper，获取hbase:meta表位于哪个Region Server</li><li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问</li><li>与目标Region Server进行通讯</li><li>分别在Block Cache(读缓存——内存)，MemStore(内存)和Store File(HFile磁盘文件)中查询目标数据(同时查内存和磁盘,因为不知道哪个timestamp先)，并将查到的所有数据进行合并(Merge)。此处所有数据是指同一条数据的不同版本(timestamp)或者不同的类型(Put/Delete)</li><li>将从文件中查询到的数据块(Block,HFile数据存储单元,默认大小为64KB)缓存到Block Cache</li><li>将合并后的最终结果返回给客户端</li></ul></li><li><p>StoreFile Compaction<br>由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本(timestamp)和不同类型(Put/Delete)有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction<br>Compaction分为两种，分别是Minor Compaction和Major Compaction。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据<br><img src="StoreFile-Compaction%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="StoreFile-Compaction示意图"></p></li><li><p>Region Split<br>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个 Region转移给其他的Region Server。<br>Region Split时机：<br>  当1个region中的某个Store下所有StoreFile的总大小超过<strong>Min(R ^ 2 * hbase.hregion.memstore.flush.size , hbase.hregion.max.filesize)</strong>，该Region就会进行拆分，其中 R为当前Region Server中属于该Table的个数</p></li></ul><h2 id="HBase-API"><a href="#HBase-API" class="headerlink" title="HBase-API"></a>HBase-API</h2><ul><li><p>环境准备<br>新建maven项目后在pom.xml中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>HBaseAPI</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestApi</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> Admin admin;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">static</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、获取配置文件信息</span></span><br><span class="line">      Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">      configuration.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"hadoop1,hadoop2,hadoop3"</span>);</span><br><span class="line">      <span class="comment">// 2、创建连接对象</span></span><br><span class="line">      connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">      <span class="comment">// 3、创建admin对象</span></span><br><span class="line">      admin = connection.getAdmin();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 关闭资源</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (admin != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        admin.close();</span><br><span class="line">        admin = <span class="keyword">null</span>;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        connection.close();</span><br><span class="line">        connection = <span class="keyword">null</span>;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 1、测试表是否存在</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTableExist</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> admin.tableExists(TableName.valueOf(tableName));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2、创建表</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String tableName, String... columnFamilies)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、判断是否存在列簇信息</span></span><br><span class="line">    <span class="keyword">if</span> (columnFamilies.length == <span class="number">0</span>) &#123;</span><br><span class="line">      System.out.println(<span class="string">"请设置列簇信息!"</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、判断表是否存在</span></span><br><span class="line">    <span class="keyword">if</span> (isTableExist(tableName)) &#123;</span><br><span class="line">      System.out.println(tableName + <span class="string">"表已存在!"</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;ColumnFamilyDescriptor&gt; columnFamilyList = <span class="keyword">new</span> ArrayList&lt;ColumnFamilyDescriptor&gt;();</span><br><span class="line">    <span class="comment">// 3、循环添加列簇信息</span></span><br><span class="line">    <span class="keyword">for</span> (String columnFamily : columnFamilies) &#123;</span><br><span class="line">      <span class="comment">// 添加列簇信息</span></span><br><span class="line">      columnFamilyList.add(ColumnFamilyDescriptorBuilder.newBuilder(columnFamily.getBytes()).build());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 4、创建表描述器</span></span><br><span class="line">    TableDescriptor tableDescriptor = TableDescriptorBuilder</span><br><span class="line">            .newBuilder(TableName.valueOf(tableName))</span><br><span class="line">            .setColumnFamilies(columnFamilyList)</span><br><span class="line">            .build();</span><br><span class="line">    <span class="comment">// 5、创建表</span></span><br><span class="line">    admin.createTable(tableDescriptor);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3、删除表</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dropTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、判断表是否存在</span></span><br><span class="line">    <span class="keyword">if</span> (!isTableExist(tableName)) &#123;</span><br><span class="line">      System.out.println(tableName + <span class="string">"表不存在!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、下线表</span></span><br><span class="line">    admin.disableTable(TableName.valueOf(tableName));</span><br><span class="line">    <span class="comment">// 3、删除表</span></span><br><span class="line">    admin.deleteTable(TableName.valueOf(tableName));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4、创建命名空间</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createNameSpace</span><span class="params">(String nameSpaceName)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1、创建命名空间描述器</span></span><br><span class="line">    NamespaceDescriptor namespaceDescriptor = NamespaceDescriptor.create(nameSpaceName)</span><br><span class="line">            .build();</span><br><span class="line">    <span class="comment">// 2、创建命名空间</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      admin.createNamespace(namespaceDescriptor);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NamespaceExistException e) &#123;</span><br><span class="line">      System.out.println(nameSpaceName + <span class="string">"命名空间已存在!"</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5、向表中插入数据</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">putData</span><span class="params">(String tableName, String rowKey, String columnFamily, String column, String value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取表对象</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">    <span class="comment">// 2、创建put对象</span></span><br><span class="line">    Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">    <span class="comment">// 3、给put对象赋值</span></span><br><span class="line">    put.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value));</span><br><span class="line">    <span class="comment">// 4、插入数据</span></span><br><span class="line">    table.put(put);</span><br><span class="line">    <span class="comment">// 5、关闭表</span></span><br><span class="line">    table.close();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6、获取数据(get)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getData</span><span class="params">(String tableName, String rowKey, String columnFamily, String column)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取表对象</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">    <span class="comment">// 2、创建get对象</span></span><br><span class="line">    Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">    <span class="comment">// 指定获取的列簇和列</span></span><br><span class="line">    <span class="keyword">if</span> (column != <span class="keyword">null</span> &amp;&amp; !column.isEmpty() &amp;&amp; columnFamily != <span class="keyword">null</span> &amp;&amp; !columnFamily.isEmpty()) &#123;</span><br><span class="line">      get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column));</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (columnFamily != <span class="keyword">null</span> &amp;&amp; !columnFamily.isEmpty()) &#123;</span><br><span class="line">      get.addFamily(Bytes.toBytes(columnFamily));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 设置获取数据的版本数</span></span><br><span class="line">    get.readVersions(<span class="number">2</span>);</span><br><span class="line">    <span class="comment">// 3、获取数据</span></span><br><span class="line">    Result result = table.get(get);</span><br><span class="line">    <span class="comment">// 4、解析result</span></span><br><span class="line">    <span class="keyword">for</span> (Cell cell : result.rawCells()) &#123;</span><br><span class="line">      <span class="comment">// 5、打印数据</span></span><br><span class="line">      System.out.println(<span class="string">"columnFamily = "</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">      System.out.println(<span class="string">"column = "</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">      System.out.println(<span class="string">"value = "</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 6、关闭表连接</span></span><br><span class="line">    table.close();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 7、获取数据(scan)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">scanTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取表对象</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">    <span class="comment">// 2、构建scan对象</span></span><br><span class="line">    Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">    <span class="comment">// 构建过滤器</span></span><br><span class="line">    <span class="comment">// RowFilter rowFilter = new RowFilter(CompareOperator.EQUAL, new SubstringComparator(uid + '_'));</span></span><br><span class="line">    <span class="comment">// scan.setFilter(rowFilter);</span></span><br><span class="line">    <span class="comment">// 3、扫描表</span></span><br><span class="line">    ResultScanner resultScanner = table.getScanner(scan);</span><br><span class="line">    <span class="comment">// 4、解析resultScanner</span></span><br><span class="line">    <span class="keyword">for</span> (Result result : resultScanner) &#123;</span><br><span class="line">      <span class="comment">// 5、解析result</span></span><br><span class="line">      <span class="keyword">for</span> (Cell cell : result.rawCells()) &#123;</span><br><span class="line">        <span class="comment">// 6、打印数据</span></span><br><span class="line">        System.out.println(<span class="string">"rowKey = "</span> + Bytes.toString(CellUtil.cloneRow(cell)));</span><br><span class="line">        System.out.println(<span class="string">"columnFamily = "</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span><br><span class="line">        System.out.println(<span class="string">"column = "</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span><br><span class="line">        System.out.println(<span class="string">"value = "</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 5、关闭table对象</span></span><br><span class="line">    table.close();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 8、删除数据,delete是一种特殊的put操作,打标记</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteData</span><span class="params">(String tableName, String rowKey, String columnFamily, String column)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取表对象</span></span><br><span class="line">    Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">    <span class="comment">// 2、构建删除对象,deleteFamily标记,rowKey下所有columnFamily的所有version</span></span><br><span class="line">    Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">    <span class="comment">// 设置删除的列簇 deleteFamily标记对应HBase Cli的deleteall命令</span></span><br><span class="line">    <span class="comment">// 删除列对应部分delete命令,删除指定columnFamily的所有version</span></span><br><span class="line">    <span class="comment">// delete.addFamily(Bytes.toBytes(columnFamily));</span></span><br><span class="line">    <span class="comment">// 设置删除的列 deleteColumn标记</span></span><br><span class="line">    <span class="comment">// delete.addColumns(Bytes.toBytes(columnFamily), Bytes.toBytes(column));</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">      设置删除的列 delete标记</span></span><br><span class="line"><span class="comment">      普通情况：上一个老值冒出来了,新值删去</span></span><br><span class="line"><span class="comment">      另一种情况：接连着push两条name,flush之后再使用该方式删除,没数据了</span></span><br><span class="line"><span class="comment">      无法确定flush时机,删除最好使用addColumns(防止出错)</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="comment">// delete.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column));</span></span><br><span class="line">    <span class="comment">// 设置删除的列 deleteColumn标记,删除在小于等于timestamp的所有版本</span></span><br><span class="line">    <span class="comment">// delete.addColumns(Bytes.toBytes(columnFamily), Bytes.toBytes(column), 1595307747400L);</span></span><br><span class="line">    <span class="comment">// 设置删除的列 delete标记,删除等于timestamp的那一个版本</span></span><br><span class="line">    delete.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column), <span class="number">1595307747419L</span>);</span><br><span class="line">    <span class="comment">// 3、执行删除操作</span></span><br><span class="line">    table.delete(delete);</span><br><span class="line">    <span class="comment">// 4、关闭连接</span></span><br><span class="line">    table.close();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// System.out.println("isTableExist = " + isTableExist("stu4"));</span></span><br><span class="line">    <span class="comment">// createTable("stu6", "info1", "info2");</span></span><br><span class="line">    <span class="comment">// System.out.println("isTableExist = " + isTableExist("stu5"));</span></span><br><span class="line">    <span class="comment">// dropTable("stu6");</span></span><br><span class="line">    <span class="comment">// createNameSpace("test");</span></span><br><span class="line">    <span class="comment">// createTable("test:xixi", "info");</span></span><br><span class="line">    <span class="comment">// putData("stu4", "1006", "info", "name", "haha");</span></span><br><span class="line">    <span class="comment">// scanTable("fruit");</span></span><br><span class="line">    deleteData(<span class="string">"stu"</span>, <span class="string">"1008"</span>, <span class="string">"info1"</span>, <span class="string">"name"</span>);</span><br><span class="line">    close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>MapReduce<br>通过HBase的相关Java API，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析</p><ul><li><p>官方HBase-MapReduce案例</p><ul><li><p>查看HBase的MapReduce任务的执行需要的依赖包：bin/hbase mapredcp以及bin/hbase classpath</p></li><li><p>环境变量的导入(/etc/profile中配置;在生产环境中最好使用临时操作,以下命令在命令行下输入,只在当前次登录生效)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> HBASE_HOME</span></span><br><span class="line">export HBASE_HOME=/opt/module/hbase-2.2.5</span><br><span class="line">export HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase classpath`</span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:`$&#123;HBASE_HOME&#125;/bin/hbase mapredcp`</span><br><span class="line"><span class="meta">#</span><span class="bash"> 保存后执行<span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure></li><li><p>执行官方的MapReduce任务(输出都在控制台上)</p><ul><li><p>案例一：统计stu表中有多少行数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar lib/hbase-mapreduce-2.2.5.jar rowcounter stu</span><br></pre></td></tr></table></figure></li><li><p>案例二：使用MapReduce将本地数据导入到HBase</p><ul><li><p>在本地创建一个tsv格式的文件：fruit.tsv(csv格式以’,’隔开,而tsv格式以’\t’隔开)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1001  Apple Red</span><br><span class="line">1002  Pear  Yellow</span><br><span class="line">1003  Pineapple Yellow</span><br></pre></td></tr></table></figure></li><li><p>创建HBase表(不存在会报错)：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create 'fruit','info'</span><br></pre></td></tr></table></figure></li><li><p>将fruit.tsv文件上传到HDFS上(根目录上)：hdfs dfs -put fruit.tsv /</p></li><li><p>执行MapReduce任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar lib/hbase-mapreduce-2.2.5.jar \</span><br><span class="line">importtsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \</span><br><span class="line">hdfs://hadoop1:9000/fruit.tsv</span><br></pre></td></tr></table></figure></li><li><p>使用scan命令查看导入后的结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan 'fruit'</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>自定义HBase-MapReduce</p><ul><li><p>导入相应的依赖包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-mapreduce<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-auth<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-jobclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>示例一：将HDFS上数据表fruit.tsv导入到HBase的fruit1表中(打包扔到集群上运行)</p><ul><li><p>FruitMapper类，用于读取HDFS上数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FruitMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    context.write(key, value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>FruitReducer类，用于将数据写入到HBase中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FruitReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(LongWritable key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、遍历values: 1001 Apple   red</span></span><br><span class="line">  <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">    <span class="comment">// 2、获取每一行数据</span></span><br><span class="line">    String[] fields = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">    <span class="comment">// 3、构建Put对象</span></span><br><span class="line">    Put put = <span class="keyword">new</span> Put(Bytes.toBytes(fields[<span class="number">0</span>]));</span><br><span class="line">    <span class="comment">// 4、给Put对象复制</span></span><br><span class="line">    put.addColumn(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"name"</span>), Bytes.toBytes(fields[<span class="number">1</span>]));</span><br><span class="line">    put.addColumn(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"color"</span>), Bytes.toBytes(fields[<span class="number">2</span>]));</span><br><span class="line">    <span class="comment">// 5、写出</span></span><br><span class="line">    context.write(NullWritable.get(), put);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>FruitDriver类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FruitDriver</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义一个Configuration</span></span><br><span class="line">  <span class="keyword">private</span> Configuration configuration = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取job对象</span></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line">    <span class="comment">// 2、设置驱动类路径</span></span><br><span class="line">    job.setJarByClass(FruitDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3、设置Mapper和Mapper输出的KV类型</span></span><br><span class="line">    job.setMapperClass(FruitMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputKeyClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、设置Reducer类</span></span><br><span class="line">    TableMapReduceUtil.initTableReducerJob(</span><br><span class="line">            args[<span class="number">1</span>],</span><br><span class="line">            FruitReducer<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">            <span class="title">job</span></span></span><br><span class="line"><span class="class">    )</span>;</span><br><span class="line">    <span class="comment">// 5、设置输入参数</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    <span class="comment">// 6、提交任务</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> result ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration configuration)</span> </span>&#123; <span class="keyword">this</span>.configuration = configuration; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> configuration; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">      <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> FruitDriver(), args);</span><br><span class="line">      System.exit(run);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用maven进行package打包操作，将jar包上传到集群上</p></li><li><p>在命令行中操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar HBaseTest-1.0-SNAPSHOT.jar com.xiong.mr.FruitDriver /fruit.tsv fruit1</span><br></pre></td></tr></table></figure></li><li><p>在HBase Cli上查看fruit1表的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan 'fruit1'</span><br></pre></td></tr></table></figure></li></ul></li><li><p>示例二：读取fruit表并过滤数据，将结果输出到fruit2表(远端连接运行)</p><ul><li><p>Fruit2Mapper类，用于读取和过滤HBase fruit表中的数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Fruit2Mapper</span> <span class="keyword">extends</span> <span class="title">TableMapper</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(ImmutableBytesWritable key, Result value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 构建Put对象</span></span><br><span class="line">    Put put = <span class="keyword">new</span> Put(key.get());</span><br><span class="line">    <span class="comment">// 1、获取数据</span></span><br><span class="line">    <span class="keyword">for</span> (Cell cell : value.rawCells()) &#123;</span><br><span class="line">      <span class="comment">// 2、判断当前的cell是否为name列</span></span><br><span class="line">      <span class="keyword">if</span> (<span class="string">"name"</span>.equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) &#123;</span><br><span class="line">        <span class="comment">// 3、给Put对象赋值</span></span><br><span class="line">        put.add(cell);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(key, put);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Fruit2Reducer类，用于将处理后的数据输出到fruit2表中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Fruit2Reducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历写出</span></span><br><span class="line">    <span class="keyword">for</span> (Put value : values) &#123;</span><br><span class="line">      context.write(NullWritable.get(), value);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Fruit2Driver类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Fruit2Driver</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义配置信息</span></span><br><span class="line">  <span class="keyword">private</span> Configuration configuration = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取Job对象</span></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line">    <span class="comment">// 2、设置主类路径</span></span><br><span class="line">    job.setJarByClass(Fruit2Driver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3、设置Mapper及输出KV类型</span></span><br><span class="line">    TableMapReduceUtil.initTableMapperJob(</span><br><span class="line">            <span class="string">"fruit"</span>,</span><br><span class="line">            <span class="keyword">new</span> Scan(),</span><br><span class="line">            Fruit2Mapper<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">            <span class="title">ImmutableBytesWritable</span>.<span class="title">class</span>,</span></span><br><span class="line"><span class="class">            <span class="title">Put</span>.<span class="title">class</span>,</span></span><br><span class="line"><span class="class">            <span class="title">job</span></span></span><br><span class="line"><span class="class">    )</span>;</span><br><span class="line">    <span class="comment">// 4、设置Reducer及输出表</span></span><br><span class="line">    TableMapReduceUtil.initTableReducerJob(</span><br><span class="line">            <span class="string">"fruit2"</span>,</span><br><span class="line">            Fruit2Reducer<span class="class">.<span class="keyword">class</span>,</span></span><br><span class="line"><span class="class">            <span class="title">job</span></span></span><br><span class="line"><span class="class">    )</span>;</span><br><span class="line">    <span class="comment">// 5、提交任务</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> result ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration configuration)</span> </span>&#123; <span class="keyword">this</span>.configuration = configuration; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> configuration; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">      <span class="keyword">int</span> run = ToolRunner.run(configuration, <span class="keyword">new</span> Fruit2Driver(), args);</span><br><span class="line">      System.exit(run);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>将集群上HBase的hbase-site.xml内容复制到当前工程下的resource/hbase-site.xml上，文件名不得修改</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- HBase默认存储文件的路径 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000/HBase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- zk的集群地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1,hadoop2,hadoop3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- zk的data目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper-3.6.1/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>./tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>运行查看结果(在HBase Cli中scan fruit2表)</p></li></ul></li></ul></li></ul></li><li><p>与Hive的集成</p><ul><li><p>HBase和Hive的对比</p><ul><li>Hive<ul><li>数据仓库：Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询</li><li>用于数据分析、清洗：Hive适用于离线的数据分析和清洗，延迟较高</li><li>基于HDFS、MapReduce：Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行</li></ul></li><li>HBase<ul><li>数据库：是一种<strong>面向列簇存储</strong>的<strong>非关系型数据库</strong></li><li>用于存储结构化和非结构化的数据：适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作</li><li>基于HDFS：数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理</li><li>延迟较低，接入在线业务使用：面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度</li></ul></li></ul></li><li><p>HBase和Hive集成使用(可能会有版本兼容问题,生产环境会采用CDH方式或者运维人员帮忙处理)</p><ul><li><p>环境准备<br>后续可能会在操作Hive的同时会对HBase产生影响，所以Hive需要持有操作HBase的Jar，那么需要拷贝Hive所依赖的Jar(或者使用软连接的形式)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可以临时设置,也可在/etc/profile中永久设置</span></span><br><span class="line">export HBASE_HOME=/opt/module/hbase-2.2.5</span><br><span class="line">export HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置软链接,<span class="string">'\'</span>为shell命令的分隔符,多行时最好采用</span></span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-common-2.2.5.jar \</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/lib/hbase-common-2.2.5.jar</span></span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-server-2.2.5.jar \</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/lib/hbase-server-2.2.5.jar</span></span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-client-2.2.5.jar \</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/lib/hbase-client-2.2.5.jar</span></span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-protocol-2.2.5.jar \</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/lib/hbase-protocol-2.2.5.jar</span></span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-it-2.2.5.jar \</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/lib/hbase-it-2.2.5.jar</span></span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-hadoop2-compat-2.2.5.jar \</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/lib/hbase-hadoop2-compat-2.2.5.jar</span></span><br><span class="line">ln -s $HBASE_HOME/lib/hbase-hadoop-compat-2.2.5.jar \</span><br><span class="line"><span class="meta">$</span><span class="bash">HIVE_HOME/lib/hbase-hadoop-compat-2.2.5.jar</span></span><br></pre></td></tr></table></figure><p>同时需要在hive-site.xml中修改zookeeper的属性(连接hbase需要与zk交互)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 设置zk节点 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1,hadoop2,hadoop3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The list of ZooKeeper servers to talk to. This is only needed for read/write locks.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置zk client通信端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The port of ZooKeeper servers to talk to. This is only needed for read/write locks.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>实操</p><ul><li><p>案例一：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表</p><ul><li><p>在Hive中创建表同时关联HBase(完成后可在Hive和HBase的Cli中查看)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE hive_hbase_emp_table(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span><br><span class="line">WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno")</span><br><span class="line">TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");</span><br></pre></td></tr></table></figure></li><li><p>在Hive中创建中间临时表，用于装载文件中的数据(因为hbase的文件格式不是txt,所有不能直接由txt导入,需要中间表)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by '\t';</span><br></pre></td></tr></table></figure></li><li><p>向Hive中间表中装载数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/emp.txt' into table emp;</span><br></pre></td></tr></table></figure></li><li><p>通过insert命令将中间表的数据导入Hive与HBase关联的那张表中：insert into table hive_hbase_emp_table select * from emp;</p></li><li><p>查看Hive和HBase各自的表中是否已同步地插入了数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive</span></span><br><span class="line">select * from hive_hbase_emp_table;</span><br><span class="line"><span class="meta">#</span><span class="bash"> HBase</span></span><br><span class="line">scan 'hbase_emp_table'</span><br></pre></td></tr></table></figure></li></ul></li><li><p>案例二：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据</p><ul><li><p>在Hive中创建外部表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE relevance_hbase_emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span><br><span class="line">WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno")</span><br><span class="line">TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");</span><br></pre></td></tr></table></figure></li><li><p>查看关联后Hive中外部表：select * from relevance_hbase_emp;</p></li><li><p>之后便可使用Hive进行一些数据分析</p></li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="HBase优化"><a href="#HBase优化" class="headerlink" title="HBase优化"></a>HBase优化</h2><ul><li><p>高可用<br>在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果 HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置</p><ul><li><p>关闭启动的HBase集群：bin/stop-hbase.sh</p></li><li><p>在conf目录下创建backup-masters文件(文件名不得更改)：touch conf/backup-masters</p></li><li><p>在backup-masters中加入备用HMaster节点(当前HMaster配置是hadoop1,文件不得多出空格和换行)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure></li><li><p>分发conf目录到其他hbase集群节点：xsync backup-masters</p></li><li><p>在hadoop1上启动HBase集群：bin/start-hbase.sh</p></li><li><p>打开页面查看：<a href="http://hadoop1:16010" target="_blank" rel="noopener">http://hadoop1:16010</a></p></li><li><p>使用jps查看hadoop1的HMaster进程，并使用kill -9 进程号杀死(杀两次,第二次杀失败,假活)</p></li><li><p>打开页面查看：<a href="http://hadoop2:16010" target="_blank" rel="noopener">http://hadoop2:16010</a>或<a href="http://hadoop3:16010" target="_blank" rel="noopener">http://hadoop3:16010</a></p></li></ul></li><li><p>预分区<br>每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致规划好，以提高HBase性能</p><ul><li><p>手动设定预分区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 具体分区信息可以在web页面的table中查看</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 四个键分五个区</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 逐个字符比较</span></span><br><span class="line">create 'staff1','info','partition1',SPLITS =&gt; ['1000','2000','3000','4000']</span><br></pre></td></tr></table></figure></li><li><p>生成16禁止序列预分区(基本不用)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create 'staff2','info','partition2',&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'&#125;</span><br></pre></td></tr></table></figure></li><li><p>按照文件中设置的规则预分区<br>创建splits.txt文件内容如下：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">aa</span><br><span class="line">bb</span><br><span class="line">dd</span><br><span class="line">cc</span><br></pre></td></tr></table></figure><p>  采用文件设置预分区形式</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认会对分区文件做排序,不然左开右闭会出问题</span></span><br><span class="line">create 'staff3','partition3',SPLITS_FILE =&gt; '/opt/module/data/hbase/splits.txt'</span><br></pre></td></tr></table></figure></li><li><p>使用JavaAPI创建预分区</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义算法，产生一系列hash散列值存储在二维数组中</span></span><br><span class="line"><span class="keyword">byte</span>[][] splitKeys = ...</span><br><span class="line"><span class="comment">// 还有四个参数的方法...</span></span><br><span class="line">admin.createTable(tableDescriptor,splitKeys);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>RowKey设计<br>一条数据的唯一标识就是RowKey，那么这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内，设计RowKey的主要目的，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈RowKey常用的设计方案</p><ul><li>生成随机数、hash、散列值：比如SHA1</li><li>字符反转</li><li>字符串拼接</li><li>示例讲解<br>电信要求统计用户的时间段内的通话流水，根据什么分区，假如要分近300个区？<br>电话号码11位，为了将对应到300个分区，而且要求一个号码需要对应到一个分区。那么如果有些号码电话打的很多，那么一个分区可能还是有问题，所以根据号码和时间进行分区。如果分到1年，时间颗粒度太大；最好采用1月。<br>最终采用取余方式：hash(15988814888(电话号码) + 2020(年) + 07(月)) % 300——此处的’+’只做一个示意作用，具体需要实践和经验。<br>那么假如我需要获取15988814888用户在2020年6月份的通话流水，怎么根据startRowKey和endRowKey获取数据呢？<br>startRowKey(xxx为分区号,是根据公式计算得出的;最后是rowKey比较规则:有比没有大)：xxx_15988814888_2020_06<br>endRowKey(‘|’的ascii码很大,或者也可以为2020_03;这里不会出现第一位大于遮蔽第二位的问题)：xxx_15988814888_2020_06|</li></ul></li><li><p>内存优化<br>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。<strong>但是不建议分配非常大的堆内存</strong>，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死</p></li><li><p>基础优化</p><ul><li><p>允许的HDFS的文件中追加内容(hdfs-site.xml、hbase-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">属性：dfs.support.append</span><br><span class="line">解释：开启HDFS追加同步，可以优秀地配合HBase的数据同步和持久化。默认值为true</span><br></pre></td></tr></table></figure></li><li><p>优化 DataNode 允许的最大文件打开数(hdfs-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">属性：dfs.datanode.max.transfer.threads</span><br><span class="line">解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096</span><br></pre></td></tr></table></figure></li><li><p>优化延迟高的数据操作的等待时间(hdfs-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">属性：dfs.image.transfer.timeout</span><br><span class="line">解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值(默认60000毫秒)，以确保socket不会被timeout掉</span><br></pre></td></tr></table></figure></li><li><p>优化数据的写入效率(mapred-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">属性：mapreduce.map.output.compress</span><br><span class="line">     mapreduce.map.output.compress.codec</span><br><span class="line">解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式</span><br></pre></td></tr></table></figure></li><li><p>设置RPC监听数量(hbase-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">属性：hbase.regionserver.handler.count</span><br><span class="line">解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值</span><br></pre></td></tr></table></figure></li><li><p>优化HStore文件大小(hbase-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">属性：hbase.hregion.max.filesize</span><br><span class="line">解释：默认值10737418240(10GB)，如果需要运行HBase的MR任务，可以减小此值，因为一个 region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile</span><br></pre></td></tr></table></figure></li><li><p>优化HBase客户端缓存(hbase-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">属性：hbase.client.write.buffer</span><br><span class="line">解释：用于指定Hbase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的</span><br></pre></td></tr></table></figure></li><li><p>指定scan.next扫描HBase所获取的行数(hbase-site.xml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">属性：hbase.client.scanner.caching</span><br><span class="line">解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大</span><br></pre></td></tr></table></figure></li><li><p>flush、compact、split机制<br>当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二<br>涉及属性：hbase.hregion.memstore.flush.size = 134217728(即128M就是Memstore的默认阈值)<br>这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase.regionserver.global.memstore.upperLimit &#x3D; 0.4</span><br><span class="line">hbase.regionserver.global.memstore.lowerLimit &#x3D; 0.38</span><br></pre></td></tr></table></figure><p>当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive</title>
      <link href="/2020/07/06/BigData/Hive/"/>
      <url>/2020/07/06/BigData/Hive/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Hive基本概念">Hive基本概念</a></li><li><a href="#Hive安装">Hive安装</a></li><li><a href="#Hive数据类型">Hive数据类型</a></li><li><a href="#DDL数据定义">DDL数据定义</a></li><li><a href="#DML数据操作">DML数据操作</a></li><li><a href="#查询">查询</a></li><li><a href="#例题实战(蚂蚁金服)">例题实战(蚂蚁金服)</a></li><li><a href="#函数">函数</a></li><li><a href="#压缩和存储">压缩和存储</a></li><li><a href="#企业级调优">企业级调优</a></li><li><a href="#谷粒影音Hive实战">谷粒影音Hive实战</a></li><li><a href="#常见错误及解决方案">常见错误及解决方案</a></li></ul><a id="more"></a><h2 id="Hive基本概念"><a href="#Hive基本概念" class="headerlink" title="Hive基本概念"></a>Hive基本概念</h2><ul><li><p>什么是Hive</p><ul><li>由Facebook开源用于解决海量结构化日志的数据统计</li><li>基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能<ul><li>Hive处理的数据存储在HDFS</li><li>Hive分析数据底层的实现是MapReduce</li><li>执行程序运行在Yarn上</li></ul></li><li>本质是将HQL(Hive Query Language)转化成MapReduce程序<br><img src="SQL-MapReduce.png" alt="SQL-MapReduce"></li></ul></li><li><p>Hive的优缺点</p><ul><li>优点：<ul><li>操作接口采用类SQL语法，提供快速开发的能力(简单、容易上手)</li><li>避免了去写MapReduce，减少开发人员的学习成本</li><li>Hive的执行延迟比较高，因此Hive常用于数据分析和对实时性要求不高的场合</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li></ul></li><li>缺点：<ul><li>Hive的HQL表达能力有限：<ul><li>迭代式算法无法表达</li><li>数据挖掘方面不擅长</li></ul></li><li>Hive的效率比较低：<ul><li>Hive自动生成的MapReduce作业，通常情况下不够智能化</li><li>Hive调优比较困难，粒度较粗</li></ul></li></ul></li></ul></li><li><p>Hive架构原理<br><img src="Hive%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png" alt="Hive架构原理"></p><ul><li>用户接口：Client<br>CLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive)</li><li>元数据：Metastore<br>元数据包括：表名、表所属的数据库(默认是default)、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在目录等；</li></ul><p><strong>默认存储在自带的derby数据库中(存在bug)，推荐使用MySQL存储Metastore</strong></p><ul><li>Hadoop：使用HDFS进行存储，使用MapReduce进行计算</li><li>驱动器：Driver<ul><li>解析器(SQL Parser)：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误</li><li>编译器(Physical Plan)：将AST编译生成逻辑执行计划</li><li>优化器(Query Optimizer)：对逻辑执行计划进行优化</li><li>执行器(Execution)：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark<br><img src="Hive%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6.png" alt="Hive运行机制"><br>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将 执行返回的结果输出到用户交互接口</li></ul></li></ul></li><li><p>Hive和数据库比较<br>由于Hive采用了类似SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。下面将从多个方面来阐述Hive和数据库的差异。数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性</p><ul><li>查询语言<br>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言 HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发</li><li>数据存储位置<br>Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中</li><li>数据更新<br>由于Hive是针对数据仓库应用设计的，而<strong>数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的</strong>。而数据库中的数据通常是需要经常进行修改的</li><li>索引<br>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。<strong>Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据</strong>，因此访问延迟较高。由于MapReduce的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive不适合在线数据查询</li><li>执行<br>Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的。而数据库通常有自己的执行引擎</li><li>执行延迟<br>Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小。当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势</li><li>可扩展性<br>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的(世界上最大的Hadoop集群在Yahoo，2009年的规模在4000台节点左右)。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右</li><li>数据规模<br>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小</li></ul></li></ul><h2 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h2><ul><li><p>安装地址</p><ul><li>Hive官网地址：<a href="http://hive.apache.org" target="_blank" rel="noopener">http://hive.apache.org</a></li><li>文档查看地址：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li><li>下载地址：<a href="http://archive.apache.org/dist/hive" target="_blank" rel="noopener">http://archive.apache.org/dist/hive</a></li><li>github地址：<a href="https://github.com/apache/hive" target="_blank" rel="noopener">https://github.com/apache/hive</a></li></ul></li><li><p>Hive安装部署</p><ul><li><p>Hive安装及配置</p><ul><li><p>上传：把apache-hive-3.1.2-bin.tar.gz上传到/opt/software目录下</p></li><li><p>解压：tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/</p></li><li><p>修改目录名：mv apache-hive-3.1.2-bin hive-3.1.2</p></li><li><p>修改配置文件(conf目录下)：</p><ul><li>备份一份配置文件：cp hive-env.sh.template hive-env.sh.template.copy</li><li>修改配置文件后缀：mv hive-env.sh.template hive-env.sh</li><li>配置hive-env.sh文件(底部加入)：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export HIVE_CONF_DIR=/opt/module/hive-3.1.2/conf</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Hadoop集群启动(hadoop1启动hdfs——sbin/start-dfs.sh,hadoop2启动yarn——sbin/start-yarn.sh)</p></li><li><p>Hive基本操作</p><ul><li>初始化默认的derby数据库：bin/schematool -dbType derby -initSchema(初始化后在hive根目录会产生derby.log和metastore目录)</li><li>启动hive：bin/hive</li><li>启动时会发生Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V错误<ul><li>这是因为hive内依赖的guava和hadoop内的版本不一致</li><li>分别查看hive(lib目录下)和hadoop(share/hadoop/common/lib目录下)的guava依赖版本：guava-19.0.jar和guava-27.0-jre.jar</li><li>删除hive的低版本guava-19.0.jar，将hadoop的高版本guava-27.0-jre.jar复制到hive的lib目录下</li><li>重新启动hive</li></ul></li><li>查看数据库：show databases;</li><li>打开默认数据库：use default;</li><li>显示default数据库中的表：show tables;</li><li>创建一张表(数据类型为java中类型)：create table student(id int,name string);</li><li>查看表的结构：desc student;</li><li>向表中插入数据：insert into student values(1,”SOBXiong”);</li><li>查询表中数据：select * from student;</li><li>退出hive：quit;</li></ul></li></ul></li><li><p>本地文件导入Hive<br>需求：将本地/opt/module/data/hive/student.txt的数据导入到hive的student表中</p><ul><li>数据准备(tab键隔开)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 xixi</span><br><span class="line">2 haha</span><br><span class="line">3 hehe</span><br></pre></td></tr></table></figure><ul><li><p>Hive操作</p><ul><li>导入student.txt的数据到之前创建的student表中：load data local inpath ‘/opt/module/data/hive/student.txt’ into table student;</li><li>查询结果(发现都是NULL NULL,因为格式不对)：select * from student;</li><li>删除已创建的student表：drop table student;</li><li>创建新的student表(声明文件分隔符’\t’)：create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</li><li>重新导入数据并重新查询结果</li><li>查看<a href="http://hadoop1:9870" target="_blank" rel="noopener">http://hadoop1:9870</a>中的HDFS文件，发现/user/hive/warehouse/student下就有数据</li><li>第二种插入数据的方式：直接将文件上传至HDFS服务器<ul><li>上传本地文件(相当于cp复制)：hadoop fs -put stu1.txt /user/hive/warehouse/student</li><li>上传HDFS文件(相当于mv移动)：hadoop fs -put /stu2.txt /user/hive/warehouse/student</li></ul></li></ul></li><li><p>derby存储元数据的问题(推荐使用mysql)：</p><ul><li>只能开启一个hive客户端</li><li>在不同的目录开启hive客户端会在当前目录下创建derby.log和metastore文件，相当于数据不共享</li></ul></li></ul></li><li><p>MySql安装</p><ul><li><p>安装包准备：</p><ul><li>查看yum中历史的mysql或者mariadb的依赖：rpm -qa | grep mysql/mariadb</li><li>如有历史依赖，删除：yum remove mysql-libs/mariadb-libs</li><li>下载mysql的rpm包：前往<a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/</a>下载5.7.30的Red Hat Enterprise Linux7版本(CentOS7)的RPM Bundle包</li></ul></li><li><p>安装MySql</p><ul><li>解压tar包：tar -xvf mysql-5.7.30-1.el7.x86_64.rpm-bundle.tar</li><li>使用rpm命令安装MySql组件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 依赖关系为common→libs→client→server</span></span><br><span class="line">rpm -ivh common</span><br><span class="line">rpm -ivh libs</span><br><span class="line">rpm -ivh client</span><br><span class="line">rpm -ivh server</span><br></pre></td></tr></table></figure><ul><li>启动MySql：systemctl start mysqld.service</li><li>查看MySql状态：systemctl status mysqld.service</li><li>查看初始化的随机密码：grep ‘temporary password’ /var/log/mysqld.log</li><li>登录MySql：mysql -u root -p</li><li>修改密码校验策略(不然设置新密码会提示密码错误)：set global validate_password_policy=0;</li><li>修改密码：alter user root@localhost identified by ‘your password’;</li><li>授权root用户远程访问权限</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'root'</span> @<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'your password'</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure><ul><li>设置MySql完毕，退出：quit;</li></ul></li></ul></li><li><p>Hive元数据配置到MySql</p><ul><li><p>拷贝mysql-connector JDBC驱动文件</p><ul><li>前往<a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/</a>下载驱动文件5.1.49版本</li><li>解压文件mysql-connector-java-5.1.49.tar.gz，拷贝mysql-connector-java-5.1.49-bin.jar到hive的lib目录下</li></ul></li><li><p>配置metastore到MySql</p><ul><li>在conf目录下创建hive-site.xml配置文件：touch hive-site.xml</li><li>修改配置文件：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- xml下的&amp;需要转义为&amp;amp; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop1:3306/metastore?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>your password<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>初始化Hive的MySql元数据数据库：bin/schematool -dbType mysql -initSchema</p></li><li><p>启动hive，MySql中新增了metastore数据库(表DBS和TBS比较重要)</p></li></ul></li><li><p>HiveJDBC访问</p><ul><li>停止hadoop：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop1</span></span><br><span class="line">stop-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop2</span></span><br><span class="line">stop-yarn.sh</span><br></pre></td></tr></table></figure><ul><li>修改hadoop配置：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml 启用webhdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">  core-site.xml 设置hadoop的代理用户</span></span><br><span class="line"><span class="comment">  hadoop.proxyuser.xxx.hosts</span></span><br><span class="line"><span class="comment">  xxx是操作的用户</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  org.apache.hadoop.security.authorize.AuthorizationException: User: sobxiong is not allowed to impersonate root(state=08S01,code=0)</span></span><br><span class="line"><span class="comment">  User:xxx即为下面该填入的用户</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.sobxiong.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.sobxiong.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改hive配置：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Bind host on which to run the HiveServer2 Thrift service.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>11000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>启动hiveserver2服务：bin/hiveserver2</li><li>启动beeline：bin/beeline</li><li>连接hiveserver2：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop1:11000</span></span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: sobxiong</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: your password(数据库的密码)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 接下来的操作就跟Hive Cli使用类似SQL</span></span><br></pre></td></tr></table></figure></li><li><p>Hive常用交互命令</p><ul><li>-e &lt;quoted-query-string&gt;：不进入hive的交互窗口执行sql语句，例如：bin/hive -e “select * from student;”</li><li>-f &lt;filename&gt;：执行脚本中sql语句<ul><li>结果打印在terminal上：bin/hive -f /opt/module/data/hive/hive.hql</li><li>结果打印在指定文件中：bin/hive -f /opt/module/data/hive/hive.hql  &gt; ./hive_result.txt</li></ul></li></ul></li><li><p>Hive其他命令操作</p><ul><li>在Hive Cli命令窗口中查看hdfs文件系统：dfs -ls /</li><li>在Hive Cli命令窗口中查看本地文件系统：! ls /</li><li>查看在hive中输入的所有历史命令：cat ~/.hivehistory</li></ul></li><li><p>Hive常见属性配置</p><ul><li>Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse</li><li><strong>在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹</strong></li><li>修改default数据仓库原始位置(hive-default.xml.template -&gt; hive-site.xml)</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>查询后信息显示配置</p><ul><li>在hive-site.xml加入如下配置：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 表列名显示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 当前使用数据库显示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>重启Hive Cli</li></ul></li><li><p>Hive运行日志信息配置</p><ul><li>Hive的日志信息默认存放在/tmp/{current_user}目录下</li><li>修改Hive的日志信息存放在hive安装目录的logs文件夹下<br>修改conf/hive-log4j.properties配置文件(hive-log4j.properties.template -&gt; hive-log4j.properties)：hive.log.dir=/opt/module/hive-3.1.2/logs</li></ul></li><li><p>参数配置方式</p><ul><li>查看当前所有的配置信息(Hive Cli命令窗口下)：set;</li><li>参数配置的三种方式<ul><li>配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml<br>注意：<strong>用户自定义配置会覆盖默认配置</strong>。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效</li><li>命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数<br>例如：bin/hive -hiveconf mapred.reduce.tasks=10;(<strong>注意：仅对本次hive启动有效</strong>)<br>查看参数设置：set mapred.reduce.tasks;</li><li>参数声明方式<br>在HQL中使用SET关键字设定参数(<strong>注意：仅对本次hive启动有效</strong>)<br>例如：set mapred.reduce.tasks=100;<br>上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了</li></ul></li></ul></li></ul></li></ul><h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><ul><li>基本数据类型(Hive数据类型大小写不敏感)</li></ul><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td></tr><tr><td>SMALLINT</td><td>short</td><td>2byte有符号整数</td></tr><tr><td><strong>INT</strong></td><td>int</td><td>4byte有符号整数</td></tr><tr><td><strong>BIGINT</strong></td><td>long</td><td>8byte有符号整数</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或false</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td></tr><tr><td><strong>DOUBLE</strong></td><td>double</td><td>双精度浮点数</td></tr><tr><td><strong>STRING</strong></td><td>string</td><td>字符系列，可以指定字符集，可以使用单引号或者双引号</td></tr><tr><td>TIMESTAMP</td><td>-</td><td>时间类型</td></tr><tr><td>BINARY</td><td>-</td><td>字节数组</td></tr></tbody></table><p>Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过他不能声明其最多能存储多少个字符，理论上它可以存储2GB的字符数</p><ul><li>集合数据类型</li></ul><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取键last对应的值数据</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用</td><td>Array()</td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套</p><ul><li><p>集合数据类型案例实操</p><ul><li>假设JSON为原始数据，具体如下：</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"songsong"</span>,</span><br><span class="line">  <span class="attr">"friends"</span>: [<span class="string">"bingbing"</span> , <span class="string">"lili"</span>],</span><br><span class="line">  <span class="attr">"children"</span>: &#123;</span><br><span class="line">      <span class="attr">"xiao song"</span>: <span class="number">18</span> ,</span><br><span class="line">      <span class="attr">"xiaoxiao song"</span>: <span class="number">19</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"address"</span>:&#123;</span><br><span class="line">    <span class="attr">"street"</span>: <span class="string">"hui long guan"</span> ,</span><br><span class="line">    <span class="attr">"city"</span>: <span class="string">"beijing"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>基于上述数据结构，建立本地测试文件test.txt，具体格式如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>注意：MAP、STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用’_’</p><ul><li>Hive上创建测试表test</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="comment">/* 设置列分隔符为',' */</span></span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line"><span class="comment">/* 设置map、struct和array的分隔符(数据分割符号)为'_' */</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span><br><span class="line"><span class="comment">/* 设置map中的key/value的分隔符为',' */</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="comment">/* 设置行分隔符为'\n'(也是默认值) */</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;</span><br></pre></td></tr></table></figure><ul><li>导入文本数据到测试表中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/test.txt' into table test;</span><br></pre></td></tr></table></figure><ul><li>访问三种集合列里的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> friends[<span class="number">1</span>],children[<span class="string">'xiao song'</span>],address.city <span class="keyword">from</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure></li><li><p>类型转换<br>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化。例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作</p><ul><li>隐式类型转换规则<ul><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT</li><li>所有整数类型、FLOAT和<strong>STRING(符合数字)</strong>类型都可以隐式地转换成DOUBLE</li><li>TINYINT、SMALLINT、INT都可以转换为FLOAT</li><li>BOOLEAN类型不可以转换为任何其它的类型</li></ul></li><li>使用CAST操作显示进行数据类型转换<br>例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值NULL</li></ul></li></ul><h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><ul><li><p>创建数据库</p><ul><li>创建一个数据库，默认在HDFS上的存储路径式/user/hive/warehouse/*.db：create database if not exists db_hive;(if not exists避免要创建的数据库已存在)</li><li>创建一个数据库，指定在HDFS上存放的路径</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive2 location <span class="string">'/db_hive2.db'</span></span><br></pre></td></tr></table></figure></li><li><p>查询数据库</p><ul><li><p>显示数据库</p><ul><li>显示数据库：show databases;</li><li>过滤查询显示的数据库</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span> <span class="keyword">like</span> <span class="string">'db_hive'</span>;</span><br></pre></td></tr></table></figure></li><li><p>查看数据库</p><ul><li>显示数据库信息：desc database db_hive;</li><li>显示数据库详细信息(extended)：desc database extended db_hive;</li></ul></li><li><p>切换当前数据库：use db_hive;</p></li></ul></li><li><p>修改数据库<br>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。<strong>数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置</strong><br>修改数据库属性值：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20200708'</span>)</span><br></pre></td></tr></table></figure><p>查看修改结果：desc database extended db_hive;</p><ul><li><p>删除数据库</p><ul><li>删除空数据库：drop database if exists db_hive;(if exists避免要删除的数据库不存在)</li><li>如果数据库中表不为空，可以采用cascade命令集联强制删除：drop database db_hive cascade;</li></ul></li><li><p>创建表</p><ul><li>建表语法</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...)</span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format]</span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure><ul><li><p>字段解释说明</p><ul><li>CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常</li><li>EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径(LOCATION)，<strong>Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</strong></li><li>COMMENT：为表和列添加注释</li><li>PARTITIONED BY创建分区表</li><li>CLUSTERED BY创建分桶表</li><li>SORTED BY不常用</li><li>ROW FORMAT<br>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]<br>| SERDE serde_name [WITH SERDEPROPERTIE (property_name=property_value, property_name=property_value, …)]<br>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化</li><li>STORED AS指定存储文件类型<br>常用的存储文件类型：SEQUENCEFILE(二进制序列文件)、TEXTFILE(文本)、RCFILE(列式存储格式文件)<br>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE</li><li>LOCATION：指定表在HDFS上的存储位置</li><li>LIKE允许用户复制现有的表结构，但是不复制数据</li></ul></li><li><p>管理表</p><ul><li><p>介绍<br>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会(或多或少地)控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。<strong>当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据</strong></p></li><li><p>实际操作</p><ul><li>创建普通表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span>;</span><br></pre></td></tr></table></figure><ul><li>根据查询结果创建表(查询的结果会添加到新创建的表中)：create table if not exists student2 as select id, name from student;</li><li>根据已存在的表结构创建表：create table if not exists student3 like student;</li><li>查询表的类型：desc formatted student;</li></ul></li></ul></li><li><p>外部表</p><ul><li>介绍：因为表是外部表，所以Hive并非认为其完全拥有这份数据。<strong>删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉</strong></li><li>实际操作(创建表,其余操作与管理表类似)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> default.dept(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>管理表与外部表</p><ul><li>相互转换</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 修改内部表为外部表：</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>);</span><br><span class="line"><span class="comment">-- 修改外部表为内部表：</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>);</span><br><span class="line"><span class="comment">-- 注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写</span></span><br></pre></td></tr></table></figure><ul><li>使用场景<br>每天将收集到的网站日志定期流入HDFS文本文件。在外部表(原始日志表)的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表</li></ul></li></ul></li><li><p>分区表<br>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<strong>Hive中的分区就是分目录</strong>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多</p><ul><li><p>分区表基本操作</p><ul><li>创建分区表语法</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition(</span><br><span class="line">deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载数据到分区表中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201709');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201708');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201707’);</span><br></pre></td></tr></table></figure><ul><li><p>查询分区表中数据</p><ul><li>单分区查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><ul><li>多分区联合查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询多个分区</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">union</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201708'</span></span><br><span class="line"><span class="keyword">union</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201707'</span>;</span><br><span class="line"><span class="comment">-- 查询全部</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>增加分区</p><ul><li>创建单个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>);</span><br></pre></td></tr></table></figure><ul><li>同时创建多个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201705'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br></pre></td></tr></table></figure></li><li><p>删除分区：</p><ul><li>删除单个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br></pre></td></tr></table></figure><ul><li>同时删除多个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201705'</span>), <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201706'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查看分区表有多少分区：show partitions dept_partition;</p></li><li><p>查看分区表结构：desc formatted dept_partition;</p></li></ul></li><li><p>分区表扩展用法</p><ul><li>创建二级分区表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载二级分区数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709', day='13');</span><br></pre></td></tr></table></figure><ul><li>查询二级分区数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'13'</span>;</span><br></pre></td></tr></table></figure><ul><li><p>将数据上传到分区目录后，让分区表和数据产生关联的方式</p><ul><li><p>上传数据后修复(适用于数据较多的情况)</p><ul><li>上传数据：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive Cli命令环境下</span></span><br><span class="line">dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">dfs -put /opt/module/data/hive/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure><ul><li>查询数据(查询不到)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'12'</span>;</span><br></pre></td></tr></table></figure><ul><li>执行修复命令：msck repair table dept_partition2;</li></ul></li><li><p>上传数据后添加分区</p><ul><li>上传数据(同上)</li><li>执行添加分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition2 <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>,<span class="keyword">day</span>=<span class="string">'11'</span>);</span><br></pre></td></tr></table></figure></li><li><p>创建文件夹后load数据到分区</p><ul><li>创建目录：dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10;</li><li>上传数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709',day='10');</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>修改表</p><ul><li>重命名表<ul><li>语法：ALTER TABLE table_name RENAME TO new_table_name</li><li>实例：alter table dept_partition2 rename to dept_partition3;</li></ul></li><li>添加、修改和删除表分区(同上)</li><li>增加、修改、替换列信息<ul><li>语法：<ul><li>更新列：ALTER TABLE table_name <strong>CHANGE</strong> [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</li><li>添加和替换列：ALTER TABLE table_name <strong>ADD|REPLACE</strong> COLUMNS (col_name data_type [COMMENT col_comment], …)</li><li><strong>注意：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段</strong></li></ul></li><li>实操<ul><li>查询表结构(用于查看修改结果)：desc dept_partition;</li><li>添加列：alter table dept_partition add columns(deptdesc string);</li><li>更新列：alter table dept_partition change column deptdesc desc int;(貌似需要符合隐式转换规则)</li><li>替换列：alter table dept_partition replace columns(deptno string, dname string, loc string);</li></ul></li></ul></li></ul></li><li><p>删除表：drop table dept_partition;</p></li></ul><h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><ul><li><p>数据导入</p><ul><li><p>向表中装载数据(Load)</p><ul><li>语法</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data [local] inpath 'path_name' [overwrite] into table table_name [partition (partcol1=val1,...)];</span><br></pre></td></tr></table></figure><ul><li><p>参数解释</p><ul><li>load data：表示加载数据</li><li>local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li><li>inpath：表示加载数据的路径</li><li>overwrite：表示覆盖表中已有数据，否则表示追加</li><li>into table：表示加载到哪张表</li><li>table_name：表示具体的表</li><li>partition：表示上传到指定分区</li></ul></li><li><p>实际操作</p><ul><li>加载本地文件到Hive：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/student.txt' into table default.student;</span><br></pre></td></tr></table></figure><ul><li>加载(覆盖)HDFS文件数据到Hive中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive Cli命令环境下</span></span><br><span class="line">dfs -put /opt/module/data/hive/student.txt /user/sobxiong/hive;</span><br><span class="line">load data inpath '/user/sobxiong/hive/student.txt' (overwrite)into table default.student;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>通过查询语句向表中插入数据(Insert)</p><ul><li>基本插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span>  student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'wangwu'</span>);</span><br></pre></td></tr></table></figure><ul><li>根据单表查询结果插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201708'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><ul><li>根据多表查询结果插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询语句中创建表并加载数据(As Select,查询的结果会添加到新创建的表中)：create table if not exists student3 as select id, name from student;</p></li><li><p>创建表时通过Location指定加载数据路径</p><ul><li>指定在HDFS上的位置创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student5(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/student5'</span>;</span><br></pre></td></tr></table></figure><ul><li>上传数据到HDFS上</li><li>查询数据</li></ul></li><li><p>Import数据到指定Hive表中(注意：先用export导出后,才能导入)</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import table student2 partition(month='201709') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure></li><li><p>数据导出</p><ul><li><p>Insert导出</p><ul><li>将查询的结果导出到本地</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/export/student'</span> <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><ul><li>将查询的结果格式化导出到本地</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/export/student1'</span> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><ul><li>将查询结果导出到HDFS上(取消local)</li></ul></li><li><p>Hadoop命令导出到本地(Hive Cli命令环境下)：dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/data/hive/export/student3.txt;</p></li><li><p>Hive Shell命令导出：基本语法(hive -f/-e 执行语句或脚本 &gt; file_name)</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -e 'select * from default.student;' &gt; /opt/module/data/hive/export/student4.txt;</span><br></pre></td></tr></table></figure><ul><li>Export导出到HDFS上(导出数据包括表数据和元数据)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export table default.student to '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><ul><li>Sqoop导出(<strong>敬请期待</strong>)</li></ul></li><li><p>清除表中数据(注意：只能删除管理表,不能删除外部表中数据)：truncate table student;</p></li></ul><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><ul><li>查询语句语法<br>官方wiki文档：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*]</span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">    | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><ul><li><p>基本查询</p><ul><li><p>全表和特定列查询<br>注意：</p><ul><li><strong>SQL大小写不敏感</strong></li><li>SQL可以写在一行/多行</li><li>关键字不能被缩写也不能分行</li><li>各子句一般要分行写</li><li>使用缩进提高语句的可读性</li></ul></li><li><p>列别名</p><ul><li>重命名一个列</li><li>便于计算</li><li>紧跟列名(或在列名和别名之间加入关键字AS)</li></ul></li><li><p>算术运算符</p></li></ul><p>+、-、*、/、%、&amp;、|、^、-</p><ul><li><p>常用函数</p><ul><li>求总行数：count()</li><li>求最大值/最小值：max()/min()</li><li>求总和：sum()</li><li>求平均值：avg()</li></ul></li><li><p>limit语句：典型的查询会返回多行数据。LIMIT子句用于限制返回的行数</p></li></ul></li><li><p>Where语句：使用Where子句可以过滤掉不满足条件的行，需要紧跟From子句</p><ul><li>比较运算符(同样可以用于Join… on和Having语句)<br>以下只介绍除=、&gt;和&lt;等简单的运算符</li></ul><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果</td></tr><tr><td>A IS [NOT] NULL</td><td>所有数据类型</td><td>如果A(不)等于NULL，则返回TRUE(FALSE)，反之返回FALSE(TRUE)</td></tr><tr><td>[NOT] IN(数值1, 数值2)</td><td>所有数据类型</td><td>(不)使用IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配</td></tr></tbody></table><ul><li><p>Like和RLike</p><ul><li><p>使用LIKE运算选择类似的值</p></li><li><p>选择条件可以包含字符或数字：</p><ul><li>% 代表零个或多个字符(任意个字符)</li><li>_ 代表一个字符</li></ul></li><li><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件</p></li><li><p>案例</p><ul><li>查找以2开头薪水的员工信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">'2%'</span>;</span><br></pre></td></tr></table></figure><ul><li>查找第二个数值为2的薪水的员工信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">'_2%'</span>;</span><br></pre></td></tr></table></figure><ul><li>查找薪水中含有2的员工信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> <span class="keyword">String</span>(sal) <span class="keyword">RLIKE</span> <span class="string">'[2]'</span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>逻辑运算符(And/Or/Not)</p></li></ul></li><li><p>分组</p><ul><li>Group By语句<br>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</li><li>Having语句<br>与where不同点：<ul><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据</li><li>where后面不能写分组函数，而having后面可以使用分组函数</li><li>having只用于group by分组统计语句</li></ul></li></ul></li><li><p>Join语句</p><ul><li>等值Join</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno, d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d</span><br><span class="line"><span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><ul><li>表的别名<br>好处：(1)简化查询；(2)有限地提高执行效率</li><li>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>多表连接：连接n个表，一般至少需要n-1个连接条件</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.ename, d.deptno, l. loc_name</span><br><span class="line"><span class="keyword">FROM</span>   emp e</span><br><span class="line"><span class="keyword">JOIN</span>   dept d</span><br><span class="line"><span class="keyword">ON</span>     d.deptno = e.deptno</span><br><span class="line"><span class="keyword">JOIN</span>   location l</span><br><span class="line"><span class="keyword">ON</span>     d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。<br><strong>注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的</strong></p><ul><li><p>笛卡尔积<br>一般会在下面情况下出现：</p><ul><li>省略连接条件</li><li>连接条件无效</li><li>所有表中的所有行互相连接<br>一般会设置禁止出现笛卡尔积，如果有特殊情况，需要在单独在命令执行前设置一次性环境</li></ul></li><li><p>连接谓词在新版中支持or</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 无实际意义(ename = dname),只做可行性试验</span></span><br><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno <span class="keyword">or</span> e.ename=d.dname;</span><br></pre></td></tr></table></figure></li><li><p>排序</p><ul><li><p>全局排序(Order By:<strong>一个Reducer</strong>)</p><ul><li>排序方式：ASC(ascend升序,默认)、DESC(descend降序)</li><li>Order By子句在Select语句的结尾</li></ul></li><li><p>多个列排序(同MySQL)</p></li><li><p>内部排序(Sort By:<strong>每个Reduce内部进行排序,对全局结果集来说不是排序</strong>)</p><ul><li>注意：如果使用sort by不使用distribute by(即没有指定分区字段)，那么就采用一种产生随机数的函数分配分区(避免数据倾斜)</li><li>设置reduce数：set mapreduce.job.reduce=3;</li><li>按照部门编号降序排序：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 三个结果文件</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/sortby-result'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li><li><p>分区排序(Distribute By:<strong>类似MR中partition,进行分区,结合sort by使用</strong>)</p></li></ul><p><strong>DISTRIBUTE BY语句要写在SORT BY语句之前</strong>。distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果</p><ul><li><p>设置reduce数：set mapreduce.job.reduces=3;</p></li><li><p>先按照部门编号分区，再按照员工编号降序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/distribute-result'</span> <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Clubster By</p><ul><li>当distribute by和sorts by字段相同时，可以使用cluster by方式。</li><li>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC</li><li>具体实操</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 以下两种写法等价</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><ul><li>注意：按照部门编号分区，不一定就是固定死的数值(要看具体数据表中的字段不同的数目以及reduce设置的数目)，可以是20号和30号部门分到一个分区里面去</li></ul></li></ul></li><li><p>分桶及抽样查询</p><ul><li><p>分桶表数据存储</p><ul><li><p>介绍：<strong>分区针对的是数据的存储路径；分桶针对的是数据文件</strong>。分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。分桶是将数据集分解成更容易管理的若干部分的另一个技术</p></li><li><p>通过导入数据文件方式创建分桶表</p><ul><li>创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>查看表结构：desc formatted stu_back;(Num Buckets: 4)</li><li>导入数据到分桶表</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/student.txt' into table stu_buck;</span><br></pre></td></tr></table></figure><ul><li>在浏览器上查看创建的分桶表是否分成4个桶(4个文件)：在新版本中分成四个桶</li></ul></li><li><p>通过子查询导入数据方式创建分桶表</p><ul><li>先创建普通的stu表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>向普通stu表中导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure><ul><li>清空stu_buck表中数据：truncate table stu_buck;</li><li>子查询方式导入数据到分桶表：insert into table stu_buck select id, name from stu;</li><li>浏览器查看：新版本中有4个分桶(文件)</li><li>需要设置Hive的属性(老版本)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置启用分桶</span></span><br><span class="line">set hive.enforce.bucketing=true;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置reduce数目为-1,会自动使用分桶数作为reduce的数目</span></span><br><span class="line">set mapreduce.job.reduces=-1;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空分桶表,重新导入数据,再去查看浏览器中的分桶</span></span><br><span class="line">truncate table stu_back;</span><br><span class="line">insert into table stu_buck select id, name from stu;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>分桶抽样查询</p><ul><li>介绍：对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求</li><li>实操：select * from stu_buck tablesample(bucket 1 out of 4 on id);</li><li>语法：tablesample是抽样语句，语法：tablesample(bucket x out of y)</li><li>参数解释<ul><li>y：y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2)2个bucket的数据，当y=8时，抽取(4/8)1/2个bucket的数据</li><li>x：<strong>x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y</strong>。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取(4/2)2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据</li><li><strong>注意：x的值必须小于等于y的值</strong></li></ul></li></ul></li></ul></li><li><p>其他常用查询函数</p><ul><li><p>空字段赋值</p><ul><li>函数说明：NVL：给值为NULL的数据赋值，它的格式是NVL(string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL，则返回NULL(replace_with可以是常量也可以是同表的另一个列)</li><li>查询(常量)：select nvl(comm,-1) from emp;</li><li>查询(另一列)：select nvl(comm,mgr) from emp;</li></ul></li><li><p>时间类</p><ul><li>date_format(格式化时间,第一个变量时间串只能是以’-‘分割)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-07-11'</span>,<span class="string">'yyyy:MM:dd'</span>);</span><br><span class="line"><span class="comment">-- 时间不以'-'分割,可以通过regex正则表达式替换/自定义函数</span></span><br><span class="line"><span class="keyword">select</span> regexp_replace(<span class="string">'2020/07/11'</span>,<span class="string">'/'</span>,<span class="string">'-'</span>);</span><br></pre></td></tr></table></figure><ul><li>date_add/sub(时间跟天数相加/相减)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">'2020-07-11'</span>,<span class="number">-5</span>/<span class="number">5</span>);</span><br></pre></td></tr></table></figure><ul><li>datediff(时间相差的间隔,前者-后者)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">'2020-07-11'</span>,<span class="string">'2020-07-08'</span>);</span><br></pre></td></tr></table></figure></li><li><p>CASE WHEN</p><ul><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">悟空  A 男</span><br><span class="line">大海  A 男</span><br><span class="line">宋宋  B 男</span><br><span class="line">凤姐  A 女</span><br><span class="line">婷姐  B 女</span><br><span class="line">婷婷  B 女</span><br></pre></td></tr></table></figure><ul><li>需求：求出不同部门的男女各多少人</li><li>创建emp_set.txt，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">dept_id <span class="keyword">string</span>,</span><br><span class="line">sex <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入本地数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/emp_sex.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure><ul><li>查询数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  dept_id,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">'男'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">'女'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  emp_sex</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  dept_id;</span><br></pre></td></tr></table></figure></li><li><p>行转列</p><ul><li>相关函数说明<br>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串<br>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间<br>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段</li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">孙悟空  白羊座  A</span><br><span class="line">大海  射手座  A</span><br><span class="line">宋宋  白羊座  B</span><br><span class="line">猪八戒  白羊座  A</span><br><span class="line">凤姐  射手座  A</span><br></pre></td></tr></table></figure><ul><li>需求：把星座和血型一样的人归类到一起。结果如下</li><li>person_info.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">constellation <span class="keyword">string</span>,</span><br><span class="line">blood_type <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/person_info.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure><ul><li>查询数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 总查询语句</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  t1.constellation_blood_type,</span><br><span class="line">  <span class="keyword">concat_ws</span>(<span class="string">'|'</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (<span class="keyword">select</span></span><br><span class="line">      <span class="keyword">name</span>,</span><br><span class="line">      <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) constellation_blood_type</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      person_info</span><br><span class="line">  ) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  t1.constellation_blood_type;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 第一步,查询出'射手座,A' '大海'</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) constellation_blood_type,</span><br><span class="line">  <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span> person_info;</span><br><span class="line"><span class="comment">-- 第二步,连接相同星座和血型的name</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  constellation_blood_type,</span><br><span class="line">  <span class="keyword">concat_ws</span>(<span class="string">'|'</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span> t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  t1.constellation_blood_type;</span><br><span class="line"><span class="comment">-- 最后一步,替换from后的t1为第一步中的临时表</span></span><br></pre></td></tr></table></figure></li><li><p>列转行</p><ul><li>函数说明<br>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行<br>LateRal View：<ul><li>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li><li>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</li></ul></li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》  悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》 悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》 战争,动作,灾难</span><br></pre></td></tr></table></figure><ul><li>需求：将电影分类中的数组数据展开，结果如下所示</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure><ul><li>创建本地movie.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">movie <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/opt/module/data/hive/movie.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure><ul><li>按需查询数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  movie,</span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure></li><li><p>窗口函数</p><ul><li>函数说明<br>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化<ul><li>参数说明<ul><li>Over()内<ul><li>CURRENT ROW：当前行</li><li>n PRECEDING：往前n行数据</li><li>n FOLLOWING：往后n行数据</li><li>UNBOUNDED：起点，UNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点</li></ul></li><li>Over()外<ul><li>LAG(col,n)：往前<strong>第</strong>n行数据</li><li>LEAD(col,n)：往后<strong>第</strong>n行数据</li><li>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型</strong></li></ul></li></ul></li></ul></li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; name,orderdate,cost</span><br><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure><ul><li>需求<ul><li>查询在2017年4月份购买过的顾客及总人数</li><li>查询顾客的购买明细及月购买总额</li><li>上述的场景，要将cost按照日期进行累加</li><li>查询顾客上次的购买时间</li><li>查询前20%时间的订单信息</li></ul></li><li>创建本地business.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">orderdate <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">cost</span> <span class="built_in">int</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/opt/module/data/hive/business.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure><ul><li><p>按需查询数据</p><ul><li>查询在2017年4月份购买过的顾客及总人数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">count</span>(*) <span class="keyword">over</span>()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">'2017-04'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><ul><li>查询顾客的购买明细及月购买总额</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,<span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate)) <span class="keyword">from</span></span><br><span class="line">business;</span><br></pre></td></tr></table></figure><ul><li>上述的场景,要将cost按照日期进行累加</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- partition by ... order by和distribute by ... sort by效果相同,可替换</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行</span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><ul><li>查看顾客上次的购买时间</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,lag(orderdate,<span class="number">1</span>,<span class="string">'1900-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> last_time <span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><ul><li>查询前20%时间的订单信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) ntile_id</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) b</span><br><span class="line"><span class="keyword">where</span> ntile_id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Rank排名函数</p><ul><li>函数说明：<ul><li>Rank()：排序相同时会重复，总数不会变</li><li>DENSE_RANK()：排序相同时会重复，总数会减少</li><li>ROW_NUMBER()：会根据顺序计算</li></ul></li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; name subject score</span><br><span class="line">孙悟空  语文  87</span><br><span class="line">孙悟空  数学  95</span><br><span class="line">孙悟空  英语  68</span><br><span class="line">大海  语文  94</span><br><span class="line">大海  数学  56</span><br><span class="line">大海  英语  84</span><br><span class="line">宋宋  语文  64</span><br><span class="line">宋宋  数学  86</span><br><span class="line">宋宋  英语  84</span><br><span class="line">婷婷  语文  65</span><br><span class="line">婷婷  数学  85</span><br><span class="line">婷婷  英语  78</span><br></pre></td></tr></table></figure><ul><li>需求：计算各学科成绩排名</li><li>创建本地score.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subject <span class="keyword">string</span>,</span><br><span class="line">score <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/score.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure><ul><li>按需查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">name</span>,</span><br><span class="line">  subject,</span><br><span class="line">  score,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">rank</span>,</span><br><span class="line">  <span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">dense_rank</span>,</span><br><span class="line">  row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) row_number</span><br><span class="line"><span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="例题实战-蚂蚁金服"><a href="#例题实战-蚂蚁金服" class="headerlink" title="例题实战(蚂蚁金服)"></a>例题实战(蚂蚁金服)</h2><ul><li>背景说明<br>用户每天的蚂蚁森林低碳生活领取的记录流水表(user_low_carbon表)</li></ul><table><thead><tr><th>字段名</th><th>注释</th></tr></thead><tbody><tr><td>user_id</td><td>用户编号</td></tr><tr><td>data_dt</td><td>日期</td></tr><tr><td>low_carbon</td><td>减少碳排放(g:克)</td></tr></tbody></table><p>用于记录申领环保植物所需要减少的碳排放量的蚂蚁森林植物换购表(plant_carbon)</p><table><thead><tr><th>字段名</th><th>注释</th></tr></thead><tbody><tr><td>plant_id</td><td>植物编号</td></tr><tr><td>plant_name</td><td>植物名</td></tr><tr><td>low_carbon</td><td>换购植物所需要的碳</td></tr></tbody></table><ul><li><p>题目</p><ul><li>蚂蚁森林蚂蚁森林植物申领统计<br>问题：假设2017年1月1日开始记录低碳数据(user_low_carbon)，假设2017年10月1日之前满足申领条件的用户都申领了一颗p004-胡杨，剩余的能量全部用来领取p002-沙柳<br>统计在10月1日累计申领p002-沙柳排名前10的用户信息、以及他比后一名多领了几颗沙柳<br>得到的统计结果如下表样式：</li></ul><table><thead><tr><th>user_id</th><th>plant_count</th><th>less_cout(比后一名多领的棵树)</th></tr></thead><tbody><tr><td>u_101</td><td>1000</td><td>100</td></tr><tr><td>u_088</td><td>900</td><td>400</td></tr><tr><td>u_103</td><td>500</td><td>…</td></tr></tbody></table><ul><li>蚂蚁森林低碳用户排名分析<br>问题：查询user_low_carbon表中每日流水记录，条件为：用户在2017年，连续三天(或以上)的天数里，每天减少碳排放(low_carbon)都超过100g的用户低碳流水。需要查询返回满足以上条件的user_low_carbon表中的记录流水。<br>例如用户u_002符合条件的记录如下，因为2017/1/2~2017/1/5连续四天的碳排放量之和都大于等于100g：</li></ul><table><thead><tr><th>seq(序号,不涉及当前列)</th><th>user_id</th><th>data_dt</th><th>low_carbon</th></tr></thead><tbody><tr><td>xxxxx10</td><td>u_002</td><td>2017/1/2</td><td>150</td></tr><tr><td>xxxxx11</td><td>u_002</td><td>2017/1/2</td><td>70</td></tr><tr><td>xxxxx12</td><td>u_002</td><td>2017/1/3</td><td>30</td></tr><tr><td>xxxxx13</td><td>u_002</td><td>2017/1/3</td><td>80</td></tr><tr><td>xxxxx14</td><td>u_002</td><td>2017/1/4</td><td>150</td></tr><tr><td>xxxxx14</td><td>u_002</td><td>2017/1/5</td><td>101</td></tr></tbody></table><p>备注：统计方法不限于sql、procedure、python,java等</p></li><li><p>解决</p><ul><li><p>前期准备</p><ul><li>创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_low_carbon(user_id <span class="keyword">String</span>,data_dt <span class="keyword">String</span>,low_carbon <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> plant_carbon(plant_id <span class="keyword">string</span>,plant_name <span class="keyword">String</span>,low_carbon <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath "/opt/module/data/hive/user_low_carbon.txt" into table user_low_carbon;</span><br><span class="line">load data local inpath "/opt/module/data/hive/plant_carbon.txt" into table plant_carbon;</span><br></pre></td></tr></table></figure><ul><li>设置本地模式(加快运行速度)：set hive.exec.mode.local.auto=true;</li></ul></li><li><p>求解问题一</p><ul><li>统计在10月1日前每个用户减少碳排放量的总和(取前11名：为了求与后一名的差值,并在第一阶段过滤数据集,加快运行速度)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t1表</span></span><br><span class="line"><span class="keyword">select</span> user_id,<span class="keyword">sum</span>(low_carbon) sum_carbon</span><br><span class="line"><span class="keyword">from</span> user_low_carbon</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">datediff</span>(regexp_replace(data_dt,<span class="string">"/"</span>,<span class="string">"-"</span>),<span class="string">"2017-10-1"</span>)&lt;<span class="number">0</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> sum_carbon <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">11</span>;</span><br></pre></td></tr></table></figure><ul><li>取出申领胡杨的碳量</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t2表</span></span><br><span class="line"><span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p004"</span>;</span><br></pre></td></tr></table></figure><ul><li>取出申领沙柳的碳量</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t3表</span></span><br><span class="line"><span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p002"</span>;</span><br></pre></td></tr></table></figure><ul><li>求出能申领沙柳的棵树</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t4表(floor下取整)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">floor</span>((t1.sum_carbon-t2.low_carbon) / t3.low_carbon) treeCount</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1,t2,t3;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t1,t2,t3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">floor</span>((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCount</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span> user_id,<span class="keyword">sum</span>(low_carbon) sum_carbon</span><br><span class="line">    <span class="keyword">from</span> user_low_carbon</span><br><span class="line">    <span class="keyword">where</span> <span class="keyword">datediff</span>(regexp_replace(data_dt,<span class="string">"/"</span>,<span class="string">"-"</span>),<span class="string">"2017-10-1"</span>)&lt;<span class="number">0</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> sum_carbon <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">11</span></span><br><span class="line">  )t1,</span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p004"</span></span><br><span class="line">  )t2,</span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p002"</span></span><br><span class="line">  )t3</span><br></pre></td></tr></table></figure><ul><li>求出前一名比后一名多几棵</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  treeCount,</span><br><span class="line">  treeCount - (<span class="keyword">lead</span>(treeCount,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> treeCount <span class="keyword">desc</span>))</span><br><span class="line"><span class="keyword">from</span> t4</span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t4表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  treeCount,</span><br><span class="line">  treeCount-(<span class="keyword">lead</span>(treeCount,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> treeCount <span class="keyword">desc</span>)) less_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      <span class="keyword">floor</span>((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCount</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span> user_id,<span class="keyword">sum</span>(low_carbon) sum_carbon</span><br><span class="line">        <span class="keyword">from</span> user_low_carbon</span><br><span class="line">        <span class="keyword">where</span> <span class="keyword">datediff</span>(regexp_replace(data_dt,<span class="string">"/"</span>,<span class="string">"-"</span>),<span class="string">"2017-10-1"</span>)&lt;<span class="number">0</span></span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line">        <span class="keyword">order</span> <span class="keyword">by</span> sum_carbon <span class="keyword">desc</span></span><br><span class="line">        <span class="keyword">limit</span> <span class="number">11</span></span><br><span class="line">      )t1,</span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p004"</span></span><br><span class="line">      )t2,</span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p002"</span></span><br><span class="line">      )t3</span><br><span class="line">  )t4</span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li><li><p>求解问题二</p><ul><li><p>方式一(Hive Sql简单版)</p><ul><li>过滤出2017年且单日低碳量超过100g</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t1表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  user_low_carbon</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_dt</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span>;</span><br></pre></td></tr></table></figure><ul><li>将前两行数据以及后两行数据的日期放至当前行</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t2表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">  lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替t1</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">  lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      user_low_carbon</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">      <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">      user_id,data_dt</span><br><span class="line">    <span class="keyword">having</span></span><br><span class="line">      <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">  )t1;</span><br></pre></td></tr></table></figure><ul><li>计算当前日期跟前后两行时间的差值</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t3表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t2</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">      lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">      <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">      <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          user_low_carbon</span><br><span class="line">        <span class="keyword">where</span></span><br><span class="line">          <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">          user_id,data_dt</span><br><span class="line">        <span class="keyword">having</span></span><br><span class="line">          <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">      )t1</span><br><span class="line">  )t2;</span><br></pre></td></tr></table></figure><ul><li>过滤出连续3天超过100g的用户</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t4表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t3</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  (lag2_diff = <span class="number">2</span> <span class="keyword">and</span> lag1_diff = <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lag1_diff = <span class="number">1</span> <span class="keyword">and</span> lead1_diff = <span class="number">-1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lead1_diff = <span class="number">-1</span> <span class="keyword">and</span> lead2_diff = <span class="number">-2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          data_dt,</span><br><span class="line">          lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">          lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">          <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">          <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              user_id,</span><br><span class="line">              <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              user_low_carbon</span><br><span class="line">            <span class="keyword">where</span></span><br><span class="line">              <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">              user_id,data_dt</span><br><span class="line">            <span class="keyword">having</span></span><br><span class="line">              <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">          )t1</span><br><span class="line">      )t2</span><br><span class="line">  )t3</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  (lag2_diff = <span class="number">2</span> <span class="keyword">and</span> lag1_diff = <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lag1_diff = <span class="number">1</span> <span class="keyword">and</span> lead1_diff = <span class="number">-1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lead1_diff = <span class="number">-1</span> <span class="keyword">and</span> lead2_diff = <span class="number">-2</span>);</span><br></pre></td></tr></table></figure><ul><li>关联原表，获取流水信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  ulc.user_id,</span><br><span class="line">  ulc.data_dt,</span><br><span class="line">  ulc.low_carbon</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t4</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  user_low_carbon ulc</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t4.user_id = ulc.user_id</span><br><span class="line">  <span class="keyword">and</span></span><br><span class="line">  t4.data_dt = <span class="keyword">date_format</span>(regexp_replace(ulc.data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t4</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  ulc.user_id,</span><br><span class="line">  ulc.data_dt,</span><br><span class="line">  ulc.low_carbon</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          data_dt,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              user_id,</span><br><span class="line">              data_dt,</span><br><span class="line">              lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">              lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">              <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">              <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              (</span><br><span class="line">                <span class="keyword">select</span></span><br><span class="line">                  user_id,</span><br><span class="line">                  <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">                <span class="keyword">from</span></span><br><span class="line">                  user_low_carbon</span><br><span class="line">                <span class="keyword">where</span></span><br><span class="line">                  <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">                <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">                  user_id,data_dt</span><br><span class="line">                <span class="keyword">having</span></span><br><span class="line">                  <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">              )t1</span><br><span class="line">          )t2</span><br><span class="line">      )t3</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">      (lag2_diff = <span class="number">2</span> <span class="keyword">and</span> lag1_diff = <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">or</span></span><br><span class="line">      (lag1_diff = <span class="number">1</span> <span class="keyword">and</span> lead1_diff = <span class="number">-1</span>)</span><br><span class="line">      <span class="keyword">or</span></span><br><span class="line">      (lead1_diff = <span class="number">-1</span> <span class="keyword">and</span> lead2_diff = <span class="number">-2</span>)</span><br><span class="line">  )t4</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  user_low_carbon ulc</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t4.user_id = ulc.user_id</span><br><span class="line">  <span class="keyword">and</span></span><br><span class="line">  t4.data_dt = <span class="keyword">date_format</span>(regexp_replace(ulc.data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>);</span><br></pre></td></tr></table></figure></li><li><p>方式二(Hive Sql困难版,使用等差数列)</p><ul><li>过滤出2017年且单日低碳量超过100g(同方式一第一步)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t1表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  user_low_carbon</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_dt</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span>;</span><br></pre></td></tr></table></figure><ul><li>按照日期进行排序,并给每一条数据一个标记</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t2表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t1表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      user_low_carbon</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">      <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">      user_id,data_dt</span><br><span class="line">    <span class="keyword">having</span></span><br><span class="line">      <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">  )t1;</span><br></pre></td></tr></table></figure><ul><li>将日期减去当前的rank值</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  如果是连续的话,data_dt - rk结果一致</span></span><br><span class="line"><span class="comment">  user_id , data_dt , rk ,data_sub_rk</span></span><br><span class="line"><span class="comment">  u_001 , 2017-01-02 , 1 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_001 , 2017-01-06 , 2 , 2017-01-04</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-02 , 1 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-03 , 2 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-04 , 3 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-05 , 4 , 2017-01-01</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">-- t3表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">date_sub</span>(data_dt,rk) data_sub_rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t2</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">date_sub</span>(data_dt,rk) data_sub_rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          user_low_carbon</span><br><span class="line">        <span class="keyword">where</span></span><br><span class="line">          <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">          user_id,data_dt</span><br><span class="line">        <span class="keyword">having</span></span><br><span class="line">          <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">      )t1</span><br><span class="line">  )t2;</span><br></pre></td></tr></table></figure><ul><li>过滤出连续3天超过100g的用户</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 当前适用于连续n天,只需要改3为n,具有通式性</span></span><br><span class="line"><span class="comment">-- 当前只能过滤出用户,流水不行</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t3</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_sub_rk</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">count</span>(*) &gt;= <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      <span class="keyword">date_sub</span>(data_dt,rk) data_sub_rk</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          data_dt,</span><br><span class="line">          <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              user_id,</span><br><span class="line">              <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              user_low_carbon</span><br><span class="line">            <span class="keyword">where</span></span><br><span class="line">              <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">              user_id,data_dt</span><br><span class="line">            <span class="keyword">having</span></span><br><span class="line">              <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">          )t1</span><br><span class="line">      )t2</span><br><span class="line">  )t3</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_sub_rk</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">count</span>(*) &gt;= <span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>方式三(MapReduce)</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">mapper(key:user_id + date,value:一行)</span><br><span class="line">grouping:user_id</span><br><span class="line">reduce()</span><br><span class="line"></span><br><span class="line">values:</span><br><span class="line">&#123;</span><br><span class="line">    date = <span class="number">1970</span>-<span class="number">01</span>-<span class="number">01</span></span><br><span class="line">    list = <span class="keyword">new</span> ArrayList();</span><br><span class="line">    values.<span class="keyword">for</span>(</span><br><span class="line">        <span class="comment">// 首次</span></span><br><span class="line">        <span class="keyword">if</span>(date == <span class="number">1970</span>-<span class="number">01</span>-<span class="number">01</span>)&#123;</span><br><span class="line">            list.add(value);</span><br><span class="line">            date = value.date;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123; <span class="comment">// 不是首次</span></span><br><span class="line">            <span class="keyword">if</span>(value.date - date == <span class="number">1</span>)&#123;</span><br><span class="line">                list.add(value);</span><br><span class="line">                date = value.date;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(list.size() &gt;= <span class="number">3</span>)&#123;</span><br><span class="line">                    context.write(list);</span><br><span class="line">                &#125;</span><br><span class="line">                list.clear();</span><br><span class="line">                list.add(value);</span><br><span class="line">                date = value.date;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 防止漏了最后一行</span></span><br><span class="line">    <span class="keyword">if</span>(list.size() &gt;= <span class="number">3</span>)&#123;</span><br><span class="line">        context.write(list);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ul><li><p>系统内置函数</p><ul><li>查看系统自带的函数：show functions;</li><li>显示自带的函数的用法：desc function split;</li><li>详细显示自带的函数的用法：desc function extened split;</li></ul></li><li><p>自定义函数</p><ul><li>Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展</li><li>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF：user-defined function)</li><li>根据用户自定义函数类别分为以下三种：<ul><li><strong>UDF(User-Defined-Function)</strong>：一进一出</li><li>UDAF(User-Defined Aggregation Function)：聚集函数，多进一出；类似于：count/max/min</li><li>UDTF(User-Defined Table-Generating Functions)：一进多出，如lateral view explore()</li></ul></li><li>官方文档地址：<a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></li><li>编程步骤：<ul><li>继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF(org.apache.hadoop.hive.ql.UDF已被废弃)</li><li>重写三个方法</li><li>在Hive的命令行窗口创建函数<ul><li>添加jar资源：add jar ‘jar_path’;</li><li>创建function：create [temporary] function [dbname.]function_name AS class_name;(temporary只在当前次使用Hive Cli有效,退出重进无效;dbname.标识限定使用函数的数据库,不写默认为default数据库)</li></ul></li><li>在Hive的命令行窗口删除函数：Drop [temporary] function [if exists] [dbname.]function_name;</li></ul></li></ul></li><li><p>自定义UDF/UDTF函数</p><ul><li>在IDE中创建一个Maven工程</li><li>导入依赖</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>创建自定义UDF函数</p><ul><li>创建类继承GenericUDF</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UDF被废弃</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 输入类型int</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> IntObjectInspector arg0;</span><br><span class="line">  <span class="comment">// 返回值类型int</span></span><br><span class="line">  <span class="keyword">private</span> IntWritable res;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个方法只调用一次,并且在evaluate()方法之前调用</span></span><br><span class="line">  <span class="comment">// 该方法接受的参数是一个ObjectInspectors数组</span></span><br><span class="line">  <span class="comment">// 该方法检查接受正确的参数类型和参数个数</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    <span class="comment">// 输入类型</span></span><br><span class="line">    <span class="keyword">this</span>.arg0 = (IntObjectInspector) arguments[<span class="number">0</span>];</span><br><span class="line">    <span class="comment">// 返回值类型</span></span><br><span class="line">    <span class="keyword">this</span>.res = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    <span class="comment">// 确定返回值类型</span></span><br><span class="line">    <span class="keyword">return</span> PrimitiveObjectInspectorFactory.writableIntObjectInspector;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个方法类似UDF的evaluate()方法。它处理真实的参数，并返回最终结果</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    Object arg0 = arguments[<span class="number">0</span>].get();</span><br><span class="line">    <span class="keyword">int</span> inputNum = <span class="keyword">this</span>.arg0.get(arg0);</span><br><span class="line">    res.set(inputNum + <span class="number">5</span>);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个方法用于当实现的GenericUDF出错的时候，打印出提示信息。而提示信息就是你实现该方法最后返回的字符串</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">assert</span> (children.length == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"param: "</span> + children[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>打成Jar包上传到服务器</li><li>将jar包添加到Hive的classpath：add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar;</li><li>创建临时函数与开发好的java class关联</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> className需要使用全类名</span></span><br><span class="line">create temporary function addFive as 'com.xiong.hive.MyUDF';</span><br></pre></td></tr></table></figure><ul><li>在Hql中使用自定义的函数：select addFive(id) from cc;</li></ul></li><li><p>创建自定义UDTF函数</p><ul><li>创建类继承GenericUDTF</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> List&lt;String&gt; dataList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义输出数据的列名和数据类型</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    <span class="comment">// 定义输出数据的列名</span></span><br><span class="line">    List&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">    fieldNames.add(<span class="string">"word"</span>);</span><br><span class="line">    <span class="comment">// 定义输出数据的类型</span></span><br><span class="line">    List&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;ObjectInspector&gt;();</span><br><span class="line">    fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">    <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取数据</span></span><br><span class="line">    String inputStr = args[<span class="number">0</span>].toString();</span><br><span class="line">    <span class="comment">// 2、获取分隔符</span></span><br><span class="line">    <span class="keyword">final</span> String splitKey = args[<span class="number">1</span>].toString();</span><br><span class="line">    <span class="comment">// 3、切粉数据</span></span><br><span class="line">    <span class="keyword">final</span> String[] words = inputStr.split(splitKey);</span><br><span class="line">    <span class="comment">// 4、遍历写出</span></span><br><span class="line">    <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">      <span class="comment">// 5、将数据放至集合</span></span><br><span class="line">      dataList.clear();</span><br><span class="line">      dataList.add(word);</span><br><span class="line">      <span class="comment">// 6、写出数据</span></span><br><span class="line">      forward(dataList);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>与自定义UDF类似</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar;</span><br><span class="line">create temporary function udtf_split as 'com.xiong.hive.MyUDTF';</span><br><span class="line">select udtf_split('hello,sobxiong,nice boy!',',');</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="压缩和存储"><a href="#压缩和存储" class="headerlink" title="压缩和存储"></a>压缩和存储</h2><ul><li><p>Hadoop源码编译支持Snappy压缩</p><ul><li>资源准备<ul><li>虚拟机准备：连接外网、<strong>采用root角色编译</strong>，减少文件夹权限问题</li><li>软件包准备</li></ul></li><li>软件包安装<ul><li>JDK安装</li><li>Maven安装</li></ul></li><li>编译源码</li></ul></li><li><p>Hadoop压缩配置</p><ul><li>MR支持的压缩编码</li></ul><table><thead><tr><th>压缩格式</th><th>是否hadoop自带</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>换成压缩格式后,原来程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是,直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样,不需要修改</td></tr><tr><td>Gzip</td><td>是,直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样,不需要修改</td></tr><tr><td>bzip2</td><td>是,直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样,不需要修改</td></tr><tr><td>LZO</td><td>否,需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引,还需要指定输入格式</td></tr><tr><td>Snappy</td><td>否,需要安装</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样,不需要修改</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>Gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr><tr><td>Snappy</td><td>8.3GB</td><td>较大</td><td>最快</td><td>最快</td></tr></tbody></table><ul><li>压缩参数配置<br>要在Hadoop中启用压缩，可以配置如下参数(mapred-site.xml文件中)<table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs(在core-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress(在mapred-site.xml中配置)</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec(在mapred-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置)</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置)</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table></li></ul></li><li><p>开启Map输出阶段压缩<br>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量<br>具体操作：</p><ul><li>开启Hive中间传输数据压缩功能：set hive.exec.compress.intermediate=true;</li><li>开启mapreduce中map输出压缩功能：set mapreduce.map.output.compress=true;</li><li>设置mapreduce中map输出数据的压缩方式：set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</li><li>执行查询语句</li></ul></li><li><p>开启Reduce输出阶段压缩<br>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能<br>具体操作：</p><ul><li>开启Hive最终输出数据压缩功能：set hive.exec.compress.output=true;</li><li>开启mapreduce最终输出数据压缩：set mapreduce.output.fileoutputformat.compress=true;</li><li>设置mapreduce最终数据输出压缩方式：set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</li><li>设置mapreduce最终数据输出压缩为块压缩：set mapreduce.output.fileoutputformat.compress.type=BLOCK;</li><li>测试输出结果是否是压缩文件：insert overwrite local directory ‘/opt/module/data/hive/distribute-result’ select * from emp distribute by deptno sort by empno desc;</li></ul></li><li><p>文件存储格式<br>Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET</p><ul><li><p>列式存储和行式存储<br><img src="%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8.png" alt="列式存储和行式存储"></p><ul><li>行存储的特点：查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快</li><li>列存储的特点：因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法</li><li>主要存储格式对应的存储方式<ul><li><strong>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的</strong></li><li><strong>ORC和PARQUET是基于列式存储的</strong></li></ul></li></ul></li><li><p>TextFile格式<br>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作</p></li><li><p>Orc格式<br>Orc(Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式<br>如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer<br>具体解释：</p><ul><li>Index Data：一个轻量级的index，<strong>默认是每隔1W行做一个索引</strong>。这里做的索引应该只是记录某行的各字段在Row Data中的offset</li><li>Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储</li><li>Stripe Footer：存的是各个Stream的类型，长度等信息<br><img src="Orc%E6%A0%BC%E5%BC%8F.png" alt="Orc格式"><br>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读</li></ul></li><li><p>Parquet格式<br>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目<br>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的<br>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度<br>一个Parquet文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页</p></li><li><p>主流文件存储格式对比实验<br>从存储文件的压缩比和查询速度两个角度对比</p><ul><li><p>存储文件的压缩比测试</p><ul><li><p>TextFile</p><ul><li>创建表(存储数据格式为TEXTFILE,默认)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><ul><li>向表中加载数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/log.data' into table log_text;</span><br></pre></td></tr></table></figure><ul><li>查看文件大小：dfs -du -h /user/hive/warehouse/log_text;</li></ul></li><li><p>ORC</p><ul><li>创建表(存储格式为ORC)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure><ul><li>向表中加载数据(不能直接导入数据)：insert into table log_orc select * from log_text;</li><li>查看文件大小：dfs -du -h /user/hive/warehouse/log_orc;</li></ul></li><li><p>Parquet</p><ul><li>创建表(存储格式为parquet)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure><ul><li>向表中加载数据(不能直接导入数据)：insert into table log_parquet select * from log_text;</li><li>查看文件大小：dfs -du -h /user/hive/warehouse/log_parquet;</li></ul></li></ul></li><li><p>存储文件的压缩比总结：ORC &gt; Parquet &gt; textFile</p></li><li><p>存储文件的查询速度总结(都运行select * from log_sortType)：<strong>查询速度相近</strong></p></li></ul></li></ul></li><li><p>存储和压缩结合</p><ul><li><p>修改Hadoop集群具有Snappy压缩方式</p><ul><li>查看hadoop本地库支持情况：hadoop checknative</li><li>将编译好的支持Snappy压缩的hadoop源码包解压，将lib/native里面的内容复制到原本的hadoop下的lib/native下替换</li><li>分发集群：xsync native/</li><li>再次查看hadoop本地库支持情况：hadoop checknative</li><li>重新启动hadoop集群和Hive</li></ul></li><li><p>测试存储和压缩</p><ul><li><p>创建一个非压缩的ORC存储方式</p><ul><li>建表语句</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"NONE"</span>);</span><br></pre></td></tr></table></figure><ul><li>插入数据：insert into table log_orc_none select * from log_text;</li><li>查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_none;</li></ul></li><li><p>创建一个SNAPPY压缩的ORC存储方式</p><ul><li>建表语句</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"SNAPPY"</span>);</span><br></pre></td></tr></table></figure><ul><li>插入数据：insert into table log_orc_snappy select * from log_text;</li><li>查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_snappy;</li></ul></li><li><p>上一节中默认的ORC存储方式，采用默认的ZLIB压缩</p></li></ul></li></ul><p>介绍ORC存储相关信息：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a><br>ORC存储方式的压缩</p><table><thead><tr><th>Key</th><th>Default</th><th>Notes</th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>high level compression(one of NONE, ZLIB, SNAPPY)</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>number of bytes in each compression chunk</td></tr><tr><td>orc.stripe.size</td><td>67,108,864</td><td>number of bytes in each stripe</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>number of rows between index entries(must be &gt;= 1000)</td></tr><tr><td>orc.create.index</td><td>true</td><td>whether to create row indexes</td></tr><tr><td>orc.bloom.filter.columns</td><td>“”</td><td>comma separated list of column names for which bloom filter should be created</td></tr><tr><td>orc.bloom.filter.fpp</td><td>0.05</td><td>false positive probability for bloom filter(must &gt; 0.0 and &lt; 1.0)</td></tr></tbody></table><ul><li>存储方式和压缩总结</li></ul><p><strong>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo</strong><br>存储方式和压缩方式不是同一个东西，文件后缀名只是人为加上的(压缩后会带有压缩格式的后缀)</p></li></ul><h2 id="企业级调优"><a href="#企业级调优" class="headerlink" title="企业级调优"></a>企业级调优</h2><ul><li>Fetch抓取<br>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees；在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。<br>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    Expects one of [none, minimal, more].</span><br><span class="line">    Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">    Currently the query should be single sourced not having any subquery and should not have</span><br><span class="line">    any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">    0. none : disable hive.fetch.task.conversion</span><br><span class="line">    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">    2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>本地模式<br>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短<br>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启本地mr</span></span><br><span class="line">set hive.exec.mode.local.auto=true;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">local</span> mr的最大输入数据量,当输入数据量小于这个值时采用<span class="built_in">local</span> mr的方式,默认为134217728,即128M</span></span><br><span class="line">set hive.exec.mode.local.auto.inputbytes.max=50000000;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">local</span> mr的最大输入文件个数,当输入文件个数小于这个值时采用<span class="built_in">local</span> mr的方式,默认为4</span></span><br><span class="line">set hive.exec.mode.local.auto.input.files.max=10;</span><br></pre></td></tr></table></figure><ul><li><p>表的优化</p><ul><li>小表、大表Join<br>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表(1000条以下的记录条数)先进内存。在map端完成reduce</li></ul><p><strong>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别</strong></p><ul><li><p>大表Join大表</p><ul><li><p>空Key过滤<br>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p></li><li><p>空Key转换<br>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如</p></li></ul></li><li><p>MapJoin<br>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理</p><ul><li>开启MapJoin参数设置<ul><li>设置自动选择MapJoin：set hive.auto.convert.join = true;(默认true)</li><li>大表小表的阈值设置：set hive.mapjoin.smalltable.filesize=25000000;(默认25MB)</li></ul></li><li>MapJoin工作机制<br><img src="MapJoin%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="MapJoin工作机制"></li></ul></li><li><p>Group By<br>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了<br>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果</p><ul><li>开启Map端聚合参数设置<ul><li>设置是否在Map端进行聚合：set hive.map.aggr = true;(默认为true)</li><li>设置在Map端进行聚合操作的条目数据：set hive.groupby.mapaggr.checkinterval = 100000;</li><li>设置有数据倾斜的时候是否进行负载均衡：set hive.groupby.skewindata = true;(默认为false)</li></ul></li></ul><p><strong>当选项设定为true，生成的查询计划会有两个MR Job</strong>。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，<strong>这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中</strong>，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中(这个过程可以保证相同的Group By Key被分布到同一个Reduce中)，最后完成最终的聚合操作</p></li><li><p>Count(Distinct)去重统计<br>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换(<strong>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的</strong>)<br>实际操作：</p><ul><li>创建一张大表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword</span><br><span class="line"><span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span></span><br><span class="line"><span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/bigtable' into table</span><br><span class="line">bigtable;</span><br></pre></td></tr></table></figure><ul><li>设置reduce个数为5：set mapreduce.job.reduces = 5;</li><li>执行去重id查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">id</span>) <span class="keyword">from</span> bigtable;</span><br></pre></td></tr></table></figure><ul><li>采用Group by去重id</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) a;</span><br></pre></td></tr></table></figure></li><li><p>笛卡尔积<br>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积</p></li><li><p>行列过滤<br>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *<br>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤行处理实际操作：</p><ul><li>测试先关联两张表，再用where条件过滤</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> ori o <span class="keyword">on</span> o.id = b.id</span><br><span class="line"><span class="keyword">where</span> o.id &lt;= <span class="number">10</span>;</span><br></pre></td></tr></table></figure><ul><li>通过子查询后，在关联表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> ori <span class="keyword">where</span> <span class="keyword">id</span> &lt;= <span class="number">10</span> ) o <span class="keyword">on</span> b.id = o.id;</span><br></pre></td></tr></table></figure></li><li><p>动态分区调整<br>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置</p><ul><li>开启动态分区参数设置<ul><li>开启动态分区功能(默认开启,为true)：hive.exec.dynamic.partition</li><li>设置为非严格模式(动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区)：hive.exec.dynamic.partition.mode</li><li>在所有执行MR的节点上最大一共可以创建动态分区的个数(默认为1000)：hive.exec.max.dynamic.partitions</li><li>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错：hive.exec.max.dynamic.partitions.pernode</li><li>整个MR Job中，最大可以创建多少个HDFS文件(默认为100000)：hive.exec.max.created.files</li><li>当有空分区生成时，是否抛出异常。一般不需要设置(默认为false)：hive.error.on.empty.partition</li></ul></li><li>案例实操</li></ul></li><li><p>分桶：详见之前介绍</p></li><li><p>分区：详见之前介绍</p></li></ul></li><li><p>数据倾斜</p><ul><li>合理设置Map数<ul><li>通常情况下，作业会通过input的目录产生一个或者多个map任务<br>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小</li><li>是不是map数越多越好？<br>答案是否定的。如果一个任务有很多小文件(远远小于块大小128m)，则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的</li><li>是不是保证每个map处理接近128m的文件块，就高枕无忧了？<br>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时<br>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数</li></ul></li><li>小文件进行合并<br>在map执行前合并小文件，减少map数；CombineHiveInputFormat具有对小文件进行合并的功能(系统默认的格式)。HiveInputFormat没有对小文件合并功能：hive.input.format</li><li>复杂文件增加Map数<br>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率<br>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize))) = blocksize = 128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数<br>实际操作<ul><li>执行查询：select count(*) from emp;</li><li>设置最大切片值为100个字节：set mapreduce.input.fileinputformat.split.maxsize=100;</li><li>再次查询</li></ul></li><li>合理设置Reduce数<ul><li>调整reduce个数的方法一<ul><li>每个Reduce处理的数据量默认是256MB：hive.exec.reducers.bytes.per.reducer</li><li>每个任务最大的reduce数，默认为1009：hive.exec.reducers.max=1009</li><li>计算reducer数的公式：N=min(参数2，总输入数据量/参数1)</li></ul></li><li>调整reduce个数的方法二<ul><li>在hadoop的mapred-default.xml文件中修改设置每个job的Reduce个数</li><li>在Hive Cli中设置：set mapreduce.job.reduces = 15;</li></ul></li><li>reduce个数并不是越多越好<ul><li>过多的启动和初始化reduce也会消耗时间和资源</li><li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题</li></ul></li><li>在设置reduce个数的时候也需要考虑这两个原则：<ul><li><strong>处理大数据量利用合适的reduce数</strong></li><li><strong>使单个reduce任务处理数据量大小要合适</strong></li></ul></li></ul></li></ul></li><li><p>并行执行<br>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。如果有更多的阶段可以并行执行，那么job可能就越快完成<br>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来<br>参数设置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 打开任务并行执行</span></span><br><span class="line">set hive.exec.parallel=true;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 同一个sql允许最大并行度,默认为8</span></span><br><span class="line">set hive.exec.parallel.thread.number=16;</span><br></pre></td></tr></table></figure></li><li><p>严格模式<br>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询<br>通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询</p><ul><li>对于分区表，<strong>除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行</strong>。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表</li><li><strong>对于使用了order by语句的查询，要求必须使用limit语句</strong>。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间</li><li><strong>限制笛卡尔积的查询</strong>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The mode in which the Hive operations are being performed.</span><br><span class="line">    In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">      Cartesian Product.</span><br><span class="line">      No partition being picked up for a query.</span><br><span class="line">      Comparing bigints and strings.</span><br><span class="line">      Comparing bigints and doubles.</span><br><span class="line">      Orderby without limit.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>JVM重用<br>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短<br>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is</span><br><span class="line">  no limit.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放</p></li><li><p>推测执行<br>在分布式集群环境下，因为程序Bug(包括Hadoop本身的bug)，负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务(比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕)，则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行(Speculative Execution)机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果</p><p>设置开启推测执行参数，Hadoop的mapred-site.xml文件中进行配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks</span><br><span class="line">              may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks</span><br><span class="line">              may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>hive本身也提供了配置项来控制reduce-side的推测执行：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.reduce.tasks.speculative.execution<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether speculative execution for reducers should be turned on. <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。<strong>如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉</strong>。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大</p></li><li><p>压缩：详见之前介绍</p></li><li><p>执行计划(Explain)</p><ul><li>基本语法：EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</li><li>案例实操<ul><li>查看执行计划：explain select deptno, avg(sal) avg_sal from emp group by deptno;</li><li>查看详细执行计划：explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</li></ul></li></ul></li></ul><h2 id="谷粒影音Hive实战"><a href="#谷粒影音Hive实战" class="headerlink" title="谷粒影音Hive实战"></a>谷粒影音Hive实战</h2><ul><li><p>需求描述<br>统计硅谷影音视频网站的常规指标，各种TopN指标：</p><ul><li>统计视频观看数Top10</li><li>统计视频类别热度Top10</li><li>统计出视频观看数Top20所属类别以及类别包含Top20视频的个数</li><li>统计视频观看数Top50所关联视频的所属类别Rank</li><li>统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10</li><li>统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</li></ul></li><li><p>项目</p><ul><li><p>数据结构</p><ul><li><p>视频表</p><table><thead><tr><th>字段</th><th>备注</th><th>详细描述</th></tr></thead><tbody><tr><td>videoId</td><td>视频唯一id</td><td>11位字符串</td></tr><tr><td>uploader</td><td>视频上传者</td><td>上传视频的用户名String</td></tr><tr><td>age</td><td>视频年龄</td><td>视频在平台上的整数天</td></tr><tr><td>category</td><td>视频类别</td><td>上传视频指定的视频分类</td></tr><tr><td>length</td><td>视频长度</td><td>整形数字标识的视频长度</td></tr><tr><td>views</td><td>观看次数</td><td>视频被浏览的次数</td></tr><tr><td>rate</td><td>视频评分</td><td>满分5分</td></tr><tr><td>ratings</td><td>流量</td><td>视频的流量,整型数字</td></tr><tr><td>conments</td><td>评论数</td><td>一个视频的整数评论数</td></tr><tr><td>relatedIds</td><td>相关视频id</td><td>相关视频的id,最多20个</td></tr></tbody></table></li><li><p>用户表</p><table><thead><tr><th>字段</th><th>备注</th><th>字符类型</th></tr></thead><tbody><tr><td>uploader</td><td>上传者用户名</td><td>string</td></tr><tr><td>videos</td><td>上传视频数</td><td>int</td></tr><tr><td>friends</td><td>朋友数量</td><td>int</td></tr></tbody></table></li></ul></li><li><p>ETL(Extraction-Transformation-Loading,数据抽取、转换和加载)原始数据<br>通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用’\t’进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用’&amp;’分割，同时去掉两边空格，多个相关视频id也使用’&amp;’进行分割</p><ul><li><p>ETLUtil清洗数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> oriStr 原始数据</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span> 过滤后的数据</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">etlStr</span><span class="params">(String oriStr)</span> </span>&#123;</span><br><span class="line">  StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">  <span class="comment">// 1、切割字符串</span></span><br><span class="line">  String[] fields = oriStr.split(<span class="string">"\t"</span>);</span><br><span class="line">  <span class="comment">// 2、过滤字段长度</span></span><br><span class="line">  <span class="keyword">if</span> (fields.length &lt; <span class="number">9</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  <span class="comment">// 3、去掉类别字段中的空格</span></span><br><span class="line">  fields[<span class="number">3</span>] = fields[<span class="number">3</span>].replaceAll(<span class="string">" "</span>, <span class="string">""</span>);</span><br><span class="line">  <span class="comment">// 4、修改相关视频ID字段的分隔符,把'\t'替换为'&amp;'</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">    <span class="comment">// 非相关id</span></span><br><span class="line">    <span class="keyword">if</span> (i &lt; <span class="number">9</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (i == fields.length - <span class="number">1</span>) sb.append(fields[i]);</span><br><span class="line">      <span class="keyword">else</span> sb.append(fields[i]).append(<span class="string">'\t'</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// 相关id</span></span><br><span class="line">      <span class="keyword">if</span> (i == fields.length - <span class="number">1</span>) sb.append(fields[i]);</span><br><span class="line">      <span class="keyword">else</span> sb.append(fields[i]).append(<span class="string">'&amp;'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 5、返回结果</span></span><br><span class="line">  <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>ETL之Mapper</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出写NullWritable,不需要排序,节省资源</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">// 定义全局value</span></span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取数据</span></span><br><span class="line">    String oriStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、过滤数据</span></span><br><span class="line">    String eltStr = ETLUtil.etlStr(oriStr);</span><br><span class="line">    <span class="comment">// 3、写出</span></span><br><span class="line">    <span class="keyword">if</span> (eltStr == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    v.set(eltStr);</span><br><span class="line">    context.write(NullWritable.get(), v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>ETL之Driver</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 官方推荐采用继承Tool方式</span></span><br><span class="line"><span class="comment">// 在ToolRunner中帮做了GenericOptionsParser</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLDriver</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Configuration conf;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取job对象</span></span><br><span class="line">    Job job = Job.getInstance(conf);</span><br><span class="line">    <span class="comment">// 2、设置jar包路径</span></span><br><span class="line">    job.setJarByClass(ETLDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3、设置Mapper类和输出KV类型</span></span><br><span class="line">    job.setMapperClass(ETLMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、设置最终输出的KV类型</span></span><br><span class="line">    job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 5、设置输入输出的路径</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 6、提交任务</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> result ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration conf)</span> </span>&#123; <span class="keyword">this</span>.conf = conf; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> conf; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 构建配置信息</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">int</span> result = ToolRunner.run(conf, <span class="keyword">new</span> ETLDriver(), args);</span><br><span class="line">      System.out.println(<span class="string">"result = "</span> + result);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>执行ETL jar包(经过maven的package,然后扔到集群上)：bin/hadoop jar /opt/module/data/hive/guli-vedio-1.0-SNAPSHOT.jar com.xiong.mr.ETLDriver /gulivideo/video/2008/0222 /guliOutput</p></li></ul></li></ul></li><li><p>准备工作</p><ul><li><p>创建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- gulivideo_ori</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_ori(</span><br><span class="line">  videoId <span class="keyword">string</span>,</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  age <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">  <span class="keyword">length</span> <span class="built_in">int</span>,</span><br><span class="line">  views <span class="built_in">int</span>,</span><br><span class="line">  rate <span class="built_in">float</span>,</span><br><span class="line">  ratings <span class="built_in">int</span>,</span><br><span class="line">  comments <span class="built_in">int</span>,</span><br><span class="line">  relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"&amp;"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- gulivideo_user_ori</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_ori(</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  videos <span class="built_in">int</span>,</span><br><span class="line">  friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- gulivideo_orc</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_orc(</span><br><span class="line">  videoId <span class="keyword">string</span>,</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  age <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">  <span class="keyword">length</span> <span class="built_in">int</span>,</span><br><span class="line">  views <span class="built_in">int</span>,</span><br><span class="line">  rate <span class="built_in">float</span>,</span><br><span class="line">  ratings <span class="built_in">int</span>,</span><br><span class="line">  comments <span class="built_in">int</span>,</span><br><span class="line">  relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line">clustered <span class="keyword">by</span> (uploader) <span class="keyword">into</span> <span class="number">8</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"&amp;"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- gulivideo_user_orc</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_orc(</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  videos <span class="built_in">int</span>,</span><br><span class="line">  friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure></li><li><p>导入ETL后的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_ori</span></span><br><span class="line">load data inpath "/guliOutput" into table gulivideo_ori;</span><br><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_user_ori</span></span><br><span class="line">load data inpath "/gulivideo/user/2008/0903" into table gulivideo_user_ori;</span><br><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_orc</span></span><br><span class="line">insert into table gulivideo_orc select * from gulivideo_ori;</span><br><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_user_orc</span></span><br><span class="line">insert into table gulivideo_user_orc select * from gulivideo_user_ori;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>业务分析</p><ul><li><p>统计视频观看数Top10</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li><li><p>统计视频类别热度Top10(某类视频的个数作为视频类别热度)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、使用UDTF函数将类别炸裂</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、按照category_name进行分组,统计每种类别视频的总数,同时按照该总数进行倒序排名,取前10</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      videoId,</span><br><span class="line">      category_name</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      gulivideo_orc</span><br><span class="line">      <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name</span><br><span class="line">  )t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li><li><p>统计出视频观看数Top20所属类别以及类别包含Top20视频的个数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、统计视频观看数Top20</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  views,</span><br><span class="line">  <span class="keyword">category</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">20</span>;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、对t1表中的category进行炸裂</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name;t2</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3、对t2表进行分组(category_name)求和(总数)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      videoId,</span><br><span class="line">      category_name</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          videoId,</span><br><span class="line">          views,</span><br><span class="line">          <span class="keyword">category</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          gulivideo_orc</span><br><span class="line">        <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">          views <span class="keyword">desc</span></span><br><span class="line">        <span class="keyword">limit</span> <span class="number">20</span></span><br><span class="line">      )t1</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name</span><br><span class="line">  )t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li><li><p>统计视频观看数Top50所关联视频的所属类别Rank</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、统计视频观看数Top50</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  relatedId,</span><br><span class="line">  views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">50</span>;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、对t1表中的relatedId进行炸裂并去重</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  related_id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(relatedId) tmp_related <span class="keyword">as</span> related_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  related_id;t2</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3、取出观看数前50视频关联ID视频的类别</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">category</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  gulivideo_orc orc</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t2.related_id = orc.videoId;t3</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4、对t3表中的category进行炸裂</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">explode</span>(<span class="keyword">category</span>) category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t3;t4</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 5、分组(类别)求和(总数)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t4</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      <span class="keyword">explode</span>(<span class="keyword">category</span>) category_name</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          <span class="keyword">category</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              related_id</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              (</span><br><span class="line">                <span class="keyword">select</span></span><br><span class="line">                  relatedId,</span><br><span class="line">                  views</span><br><span class="line">                <span class="keyword">from</span></span><br><span class="line">                  gulivideo_orc</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">                  views <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">limit</span> <span class="number">50</span></span><br><span class="line">              )t1</span><br><span class="line">            <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(relatedId) tmp_related <span class="keyword">as</span> related_id</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">              related_id</span><br><span class="line">          )t2</span><br><span class="line">        <span class="keyword">join</span></span><br><span class="line">          gulivideo_orc orc</span><br><span class="line">        <span class="keyword">on</span></span><br><span class="line">          t2.related_id = orc.videoId</span><br><span class="line">      )t3</span><br><span class="line">  )t4</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">    category_count <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li><li><p>统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、给每一种类别根据视频观看数添加rank值(倒序)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  categoryId,</span><br><span class="line">  videoId,</span><br><span class="line">  views,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> categoryId <span class="keyword">order</span> <span class="keyword">by</span> views <span class="keyword">desc</span>) rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_category;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、过滤前十</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  categoryId,</span><br><span class="line">  videoId,</span><br><span class="line">  views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      categoryId,</span><br><span class="line">      videoId,</span><br><span class="line">      views,</span><br><span class="line">      <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> categoryId <span class="keyword">order</span> <span class="keyword">by</span> views <span class="keyword">desc</span>) rk</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      gulivideo_category</span><br><span class="line">  )t1</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  rk &lt;= <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li><li><p>统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、统计上传视频最多的用户Top10</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  uploader,</span><br><span class="line">  videos</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_user_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  videos <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、取出这10个人上传的所有视频,按照观看次数进行排名,取前20</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  video.videoId,</span><br><span class="line">  video.views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  gulivideo_orc video</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t1.uploader = video.uploader</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  video.videoId,</span><br><span class="line">  video.views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      uploader,</span><br><span class="line">      videos</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      gulivideo_user_orc</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">      videos <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">10</span></span><br><span class="line">  )t1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  gulivideo_orc video</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t1.uploader = video.uploader</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">20</span>;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="常见错误及解决方案"><a href="#常见错误及解决方案" class="headerlink" title="常见错误及解决方案"></a>常见错误及解决方案</h2><ul><li><p>启动MR任务报错：virtual memory used. Killing container(虚拟内存不足)<br>修改hadoop的配置，修改检查虚拟内存的属性为false</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper</title>
      <link href="/2020/06/26/BigData/Zookeeper/"/>
      <url>/2020/06/26/BigData/Zookeeper/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Zookeeper入门">Zookeeper入门</a></li><li><a href="#Zookeeper安装">Zookeeper安装</a></li><li><a href="#Zookeeper实战">Zookeeper实战</a></li><li><a href="#Zookeeper内部原理">Zookeeper内部原理</a></li><li><a href="#面试真题">面试真题</a></li></ul><a id="more"></a><h2 id="Zookeeper入门"><a href="#Zookeeper入门" class="headerlink" title="Zookeeper入门"></a>Zookeeper入门</h2><ul><li><p>概述<br>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目<br><img src="Zookeeper%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="Zookeeper工作机制"></p></li><li><p>特点<br><img src="Zookeeper%E7%89%B9%E7%82%B9.png" alt="Zookeeper特点"></p></li><li><p>数据结构<br><img src="Zookeeper%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png" alt="Zookeeper数据结构"></p></li><li><p>应用场景<br>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等</p><ul><li>统一命名服务<br><img src="Zookeeper%E7%BB%9F%E4%B8%80%E5%91%BD%E5%90%8D%E6%9C%8D%E5%8A%A1.png" alt="Zookeeper统一命名服务"></li><li>统一配置管理<br><img src="Zookeeper%E7%BB%9F%E4%B8%80%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86.png" alt="Zookeeper统一配置管理"></li><li>统一集群管理<br><img src="Zookeeper%E7%BB%9F%E4%B8%80%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86.png" alt="Zookeeper统一集群管理"></li><li>服务器节点动态上下线<br><img src="Zookeeper%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A8%E6%80%81%E4%B8%8A%E4%B8%8B%E7%BA%BF.png" alt="Zookeeper服务器动态上下线"></li><li>软负载均衡<br><img src="Zookeeper%E8%BD%AF%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1.png" alt="Zookeeper软负载均衡"></li></ul></li><li><p>下载地址：官网地址——<a href="https://zookeeper.apache.org" target="_blank" rel="noopener">https://zookeeper.apache.org</a></p></li></ul><h2 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h2><ul><li><p>本地模式安装部署</p><ul><li>安装前准备<ul><li>安装jdk</li><li>拷贝Zookeeper安装包到Linux系统下</li><li>解压到指定目录：tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/</li></ul></li><li>配置修改<ul><li>修改配置文件(conf目录下)：mv zoo_sample.cfg zoo.cfg</li><li>打开zoo.cfg文件，修改dataDir路径：dataDir=/opt/module/zookeeper-3.6.1/zkData</li><li>新建zkData目录(不同于Hadoop目录不能存在)：mkdir zkData</li></ul></li><li>操作Zookeeper<ul><li>启动Zookeeper Server(服务端)：bin/zkServer.sh start</li><li>查看进程是否启动：jps(正常会有一个QuorumPeerMain)</li><li>查看状态：bin/zkServer.sh status</li><li>启动Zookeeper Client(客户端)：bin/zkCli.sh</li><li>查看文件列表：ls /(一开始只有[zookeeper])</li><li>退出Zookeeper Client：quit</li><li>停止Zookeeper Server：bin/zkServer.sh stop</li></ul></li></ul></li><li><p>配置参数解读<br>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p><ul><li><em>tickTime = 2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒</em><br>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。<br>它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</li><li><em>initLimit = 10：LF初始通信时限</em><br>集群中的Follower跟随者服务器与Leader领导者服务器之间<strong>初始连接时</strong>能容忍的最多心跳数(tickTime的数量)，用它来限定集群中的Zookeeper服务器连接到Leader的时限</li><li><em>syncLimit = 5：LF同步通信时限</em><br>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer</li><li><em>dataDir：数据文件目录 + 数据持久化路径</em><br>主要用于保存Zookeeper中的数据</li><li><em>clientPort = 2181：客户端连接端口</em><br>监听客户端连接的端口</li></ul></li></ul><h2 id="Zookeeper实战"><a href="#Zookeeper实战" class="headerlink" title="Zookeeper实战"></a>Zookeeper实战</h2><ul><li><p>分布式安装部署</p><ul><li><p>集群规划：在hadoop1、hadoop2、hadoop3三个节点上部署Zookeeper形成集群</p></li><li><p>安装：分发zookeeper到hadoop2、hadoop3：xsync zookeeper-3.6.1/</p></li><li><p>配置服务器编号：</p><ul><li>在zookeeper-3.6.1目录下创建zkData目录：mkdir zkData</li><li>在zkData目录下创建myid文件：touch myid</li><li>编辑myid文件(设置当前server编号)：1</li><li>同步zkData到hadoop2、hadoop3上：xsync zkData/</li><li>在hadoop2、hadoop3上修改myid中的内容为2、3</li></ul></li><li><p>配置zoo.cfg文件</p><ul><li>新增集群节点配置</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cluster</span></span><br><span class="line">server.1=hadoop1:2888:3888</span><br><span class="line">server.2=hadoop2:2888:3888</span><br><span class="line">server.3=hadoop3:2888:3888</span><br></pre></td></tr></table></figure><ul><li>同步zoo.cfg文件：xsync zoo.cfg</li><li>配置参数解读：server.A=B:C:D<ul><li>A是一个数，表示这是第几号服务器。集群模式下配置文件myid中的数字就是A的值，<strong>Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server</strong></li><li>B是这个服务器的地址</li><li>C是这个服务器Follower与集群中的Leader服务器交换信息的端口</li><li>D是用来执行选举时服务器相互通信的端口：万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader</li></ul></li></ul></li><li><p>集群操作</p><ul><li>分别启动Zookeeper(启动前需要关闭Linux防火墙,使得各节点能够相互通信)：bin/zkServer.sh start</li><li>查看状态：bin/zkServer.sh status</li></ul></li></ul></li><li><p>客户端命令行操作(启动命令行：bin/zkCli.sh)</p></li></ul><table><thead><tr><th>命令基本语法</th><th>功能描述</th></tr></thead><tbody><tr><td>help(打错也是一个效果,当前无help命令)</td><td>显示所有操作命令</td></tr><tr><td>ls [-s] [-w] [-R] path</td><td>使用ls命令来查看当前path下znode中所包含的内容(-s：查看更新次数等详细数据,替代ls2;-w：设置watcher监听器,只有效一次;-R：递归查看节点)</td></tr><tr><td>create [-s] [-e] path [data]</td><td>创建节点(-s：含有序列;-e：临时(重启或者超时消失);data：写入path的内容,如果没有data创建不出节点)</td></tr><tr><td>get [-s] [-w] path</td><td>获得节点的值(-s：获取更加详细的节点数据;-w：设置watcher监听器,只有效一次)</td></tr><tr><td>set path data</td><td>设置节点的具体值</td></tr><tr><td>stat path</td><td>查看节点状态</td></tr><tr><td>delete path</td><td>删除节点</td></tr><tr><td>deleteall path</td><td>递归删除节点</td></tr></tbody></table><ul><li><p>API应用</p><ul><li><p>IDEA环境搭建</p><ul><li>创建空maven项目</li><li>添加pom文件：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.13.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>在resources目录下新建一个日志配置文件log4j.properties</li></ul><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure></li><li><p>创建ZooKeeper客户端</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 访问的ip</span></span><br><span class="line"><span class="keyword">private</span> String connectString = <span class="string">"hadoop1:2181,hadoop2:2181,hadoop3:2181"</span>;</span><br><span class="line"><span class="comment">// 会话超时时间</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line"><span class="comment">// zookeeper客户端</span></span><br><span class="line"><span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>创建子节点</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、创建节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createNode</span><span class="params">()</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">  String path = zkClient.create(<span class="string">"/sobxiong"</span>, <span class="string">"sobxiong,xixixihahaha"</span>.getBytes(),</span><br><span class="line">          ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">  System.out.println(<span class="string">"path = "</span> + path);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>获取子节点并监听节点变化</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2、获取子节点,并监控节点的变化</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getDataAndWatch</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">  List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line">  <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">      System.out.println(<span class="string">"child = "</span> + child);</span><br><span class="line">  &#125;</span><br><span class="line">  System.out.println(<span class="string">"------------"</span>);</span><br><span class="line">  Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 设置watcher中的process方法,使其继续调用自身继续监听</span></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line">          <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">              System.out.println(<span class="string">"child = "</span> + child);</span><br><span class="line">          &#125;</span><br><span class="line">          System.out.println(<span class="string">"------------"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>判断Znode是否存在</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3、判断节点是否存在</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">judgeNodeExist</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">  Stat stat = zkClient.exists(<span class="string">"/sobxiong"</span>, <span class="keyword">false</span>);</span><br><span class="line">  System.out.println(<span class="string">"stat = "</span> + stat);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>监听服务器节点动态上下线案例</p><ul><li><p>需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线</p></li><li><p>案例分析：<br><img src="%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A8%E6%80%81%E4%B8%8A%E4%B8%8B%E7%BA%BF%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="服务器动态上下线案例分析"></p></li><li><p>具体实现</p><ul><li>先在集群上创建/servers节点：create /servers “servers”</li><li>服务端向Zookeeper注册：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeServer</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 访问的ip</span></span><br><span class="line">  <span class="keyword">private</span> String connectString = <span class="string">"hadoop1:2181,hadoop2:2181,hadoop3:2181"</span>;</span><br><span class="line">  <span class="comment">// 会话超时时间</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">  <span class="comment">// zookeeper客户端</span></span><br><span class="line">  <span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    DistributeServer server = <span class="keyword">new</span> DistributeServer();</span><br><span class="line">    <span class="comment">// 1、连接zookeeper集群</span></span><br><span class="line">    server.getConnect();</span><br><span class="line">    <span class="comment">// 2、注册节点</span></span><br><span class="line">    server.register(args[<span class="number">0</span>]);</span><br><span class="line">    <span class="comment">// 3、业务逻辑</span></span><br><span class="line">    server.business();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">register</span><span class="params">(String hostName)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    String path = zkClient.create(<span class="string">"/servers/server"</span>, hostName.getBytes(),</span><br><span class="line">            ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">    System.out.println(hostName + <span class="string">" is online..."</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;&#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>客户端注册监听：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeClient</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 访问的ip</span></span><br><span class="line">  <span class="keyword">private</span> String connectString = <span class="string">"hadoop1:2181,hadoop2:2181,hadoop3:2181"</span>;</span><br><span class="line">  <span class="comment">// 会话超时时间</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">  <span class="comment">// zookeeper客户端</span></span><br><span class="line">  <span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    DistributeClient client = <span class="keyword">new</span> DistributeClient();</span><br><span class="line">    <span class="comment">// 1、获取zookeeper集群连接</span></span><br><span class="line">    client.getConnect();</span><br><span class="line">    <span class="comment">// 2、注册监听</span></span><br><span class="line">    client.getChildren();</span><br><span class="line">    <span class="comment">// 3、业务逻辑处理</span></span><br><span class="line">    client.business();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/servers"</span>, <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">// 存储服务器节点主机名称集合</span></span><br><span class="line">    List&lt;String&gt; hostNames = <span class="keyword">new</span> ArrayList();</span><br><span class="line">    <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">      <span class="keyword">byte</span>[] data = zkClient.getData(<span class="string">"/servers/"</span> + child, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line">      hostNames.add(<span class="keyword">new</span> String(data));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将所有在线主机名称打印</span></span><br><span class="line">    System.out.println(<span class="string">"hostNames = "</span> + hostNames);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            getChildren();</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Zookeeper内部原理"><a href="#Zookeeper内部原理" class="headerlink" title="Zookeeper内部原理"></a>Zookeeper内部原理</h2><ul><li><p>节点类型<br><img src="Zookeeper%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B.png" alt="Zookeeper节点类型"></p></li><li><p>Stat结构体</p><ul><li>cZxid：创建节点的事务zxid——每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生</li><li>ctime：znode被创建的毫秒数(从1970年开始)</li><li>mzxid：znode最后更新的事务zxid</li><li>mtime：znode最后修改的毫秒数(从1970年开始)</li><li>pZxid：znode最后更新的子节点zxid</li><li>cversion：znode子节点变化号，znode子节点修改次数</li><li>dataversion：znode数据变化号</li><li>aclVersion：znode访问控制列表的变化号</li><li>ephemeralOwner：如果是临时节点，这个是znode拥有者的session id；如果不是临时节点则是0</li><li><strong>dataLength：znode的数据长度</strong></li><li><strong>numChildren：znode子节点数量</strong></li></ul></li><li><p>监听器原理<br><img src="Zookeeper%E7%9B%91%E5%90%AC%E5%99%A8%E5%8E%9F%E7%90%86.png" alt="Zookeeper监听器原理"></p></li><li><p>选举机制</p><ul><li><strong>半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器</strong></li><li>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的</li><li>选举过程的举例(假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的——没有历史数据，在存放数据量这一点上，都是一样的)：<br><img src="Zookeeper%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6.png" alt="Zookeeper选举机制"><ul><li>服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上(3票)，选举无法完成，服务器1状态保持为LOOKING</li><li>服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的(服务器1)大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1、2状态保持LOOKING</li><li>服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING</li><li>服务器4启动，发起一次选举。此时服务器1、2、3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING</li><li>服务器5启动，同4一样当小弟</li></ul></li></ul></li><li><p>写数据流程<br><img src="Zookeeper%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="Zookeeper写数据流程"></p></li></ul><h2 id="面试真题"><a href="#面试真题" class="headerlink" title="面试真题"></a>面试真题</h2><ul><li><p>请简述ZooKeeper的选举机制？<br>参见4.4</p></li><li><p>ZooKeeper的监听原理是什么？<br>参见4.3</p></li><li><p>ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？<br>(1)部署方式：单机模式、集群模式；(2)角色：Leader和Follower；(3)集群最少需要机器数：3</p></li><li><p>ZooKeeper的常用命令有哪些？<br>ls create get delete set</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring注解驱动开发</title>
      <link href="/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/"/>
      <url>/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#容器">容器</a></li><li><a href="#扩展原理">扩展原理</a></li><li><a href="#Web">Web</a></li></ul><a id="more"></a><h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><ul><li>@Configuration：类注解(配置类=配置文件,注解告诉Spring这是一个配置类)</li><li>@Bean：方法注解，在类方法中给出返回Bean的方法，并在方法上添加@Bean注解(给容器中注册一个Bean,类型为返回值的类型,id默认为方法名,可复写注解的value属性复写id)。<ul><li>@Scope：方法注解，设置作用域。常用值为：<ul><li>prototype：多实例，ioc容器诶懂并不会去调用方法创建对象放在容器中。每次获取的时候才会调用方法创建对象</li><li>singleton(默认单实例)：ioc容器启动会调用方法创建对象放到ioc容器中，以后每次获取就是直接从容器中(可看作使用map.get())拿</li></ul></li><li>@Lazy：懒加载，只有在singleton单实例下才生效，且需要在返回bean的方法上加上@Lazy注解。单实例bean默认在容器启动的时候创建对象；懒加载在容器启动时不创建对象，第一次使用(获取)Bean创建对象并初始化</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Scope</span>(<span class="string">"singleton"</span>)</span><br><span class="line"><span class="meta">@Lazy</span></span><br><span class="line"><span class="meta">@Bean</span>(<span class="string">"person"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> Person <span class="title">person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Person(<span class="string">"SOBXiong"</span>, <span class="number">22</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@ComponentScans：指定扫描规则组(value为ComponentScan集合)</li><li>@ComponentScan：类注解，指定组件扫描规则<ul><li>value：指定包名，这样Spring会扫描包下的所有组件(SpringBoot情况可能不同,不需要)</li><li>excludeFilters：指定排除的过滤器，filter可根据注解排除(排除规则)，classed指定注解的类</li><li>includeFilters：指定只需要包含的过滤器</li><li>useDefaultFilters：是否适用缺省的过滤器，默认true；如果要使includeFilters生效，则必须设置为false</li><li>FilterType：<ul><li>FilterType.ANNOTATION：按照注解方式</li><li>FilterType.ASSIGNABLE_TYPE：按照指定的类型(具体的类,包括子类和实现类)</li><li>FilterType.REGEX：适用正则表达式</li><li>FilterType.CUSTOM：使用自定义规则，需要自定义实现TypeFilter接口的类</li></ul></li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ComponentScan</span>(value = <span class="string">"packageName"</span>,excludeFilters = &#123;</span><br><span class="line">    <span class="meta">@Filter</span>(type=FilterType.ANNOTATION,classes=&#123;Controller<span class="class">.<span class="keyword">class</span>,<span class="title">Service</span>.<span class="title">class</span>&#125;)</span></span><br><span class="line"><span class="class">&#125;)</span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">MainConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Person <span class="title">person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Person(<span class="string">"SOBXiong"</span>, <span class="number">22</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestTypeFilter</span> <span class="keyword">implements</span> <span class="title">TypeFilter</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> metadataReader 读取到的当前正在扫描的类的信息</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> metadataReaderFactory 可以获取到其他任何类信息的工厂</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@return</span> boolean</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">match</span><span class="params">(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      <span class="comment">// 获取当前类注解的信息</span></span><br><span class="line">      AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata();</span><br><span class="line">      <span class="comment">// 获取当前正在扫描的类的类信息</span></span><br><span class="line">      ClassMetadata classMetadata = metadataReader.getClassMetadata();</span><br><span class="line">      <span class="comment">// 获取当前类的资源信息(类路径等)</span></span><br><span class="line">      Resource resource = metadataReader.getResource();</span><br><span class="line">      String className = classMetadata.getClassName();</span><br><span class="line">      System.out.println(<span class="string">"className = "</span> + className);</span><br><span class="line">      <span class="keyword">return</span> className.contains(<span class="string">"test"</span>);</span><br><span class="line">      <span class="comment">// return false;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@Conditional：按照一定的条件进行判断，满足条件给容器中注册bean(Spring底层大量用到);可以设置在类上，也可以设置在方法上。设置在返回bean的方法上：只根据条件解决是否注册bean。设置在类上：类中注册统一设置，满足条件时，这个类中配置的所有bean注册才能生效</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Conditional</span>(TestCondition<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line">@Bean("person2")</span><br><span class="line"><span class="function"><span class="keyword">public</span> Person <span class="title">person2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Person(<span class="string">"SOBXiong"</span>, <span class="number">22</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestCondition</span> <span class="keyword">implements</span> <span class="title">Condition</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> conditionContext      判断条件能使用的上下文(环境)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> annotatedTypeMetadata 注释信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> boolean</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1、能获取到ioc使用的beanFactory</span></span><br><span class="line">        ConfigurableListableBeanFactory beanFactory = conditionContext.getBeanFactory();</span><br><span class="line">        <span class="comment">// 2、获取类加载器</span></span><br><span class="line">        ClassLoader classLoader = conditionContext.getClassLoader();</span><br><span class="line">        <span class="comment">// 3、获取当前环境信息</span></span><br><span class="line">        Environment environment = conditionContext.getEnvironment();</span><br><span class="line">        <span class="comment">// 4、获取到bean定义的注册类</span></span><br><span class="line">        BeanDefinitionRegistry registry = conditionContext.getRegistry();</span><br><span class="line">        <span class="comment">// 可以判断容器中的bean注册情况,也可以给容器中注册bean</span></span><br><span class="line">        <span class="keyword">boolean</span> isDefinition = registry.containsBeanDefinition(<span class="string">"person"</span>);</span><br><span class="line">        <span class="comment">// 获取运行系统的名称</span></span><br><span class="line">        String osName = environment.getProperty(<span class="string">"os.name"</span>);</span><br><span class="line">        <span class="keyword">if</span> (osName.contains(<span class="string">"Windows"</span>)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@Import：导入组件，id默认是组件的全类名</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 给容器中注册组件：</span></span><br><span class="line"><span class="comment"> * 1、包扫描+组件标注注解(<span class="doctag">@Controller</span>、<span class="doctag">@Service</span>、<span class="doctag">@Repository</span>、<span class="doctag">@Component</span>)</span></span><br><span class="line"><span class="comment"> * 2、<span class="doctag">@Bean</span>[导入第三方包里面的组件]</span></span><br><span class="line"><span class="comment"> * 3、<span class="doctag">@Import</span>[快速给容器中导入一个组件]</span></span><br><span class="line"><span class="comment"> *   1、容器会自动注册这个组件,id默认是全类名</span></span><br><span class="line"><span class="comment"> *   2、ImportSelector：返回需要导入的组件的全类名数组(SpringBoot源码中许多地方用到);</span></span><br><span class="line"><span class="comment"> *   3、ImportBeanDefinitionRegistrar：手动注册bean到容器中</span></span><br><span class="line"><span class="comment"> * 4、使用Spring提供的FactoryBean(工厂Bean),其他与Spring整合的框架使用的特别多</span></span><br><span class="line"><span class="comment"> *   1、默认获取的是工厂bean调用getObject创建的对象</span></span><br><span class="line"><span class="comment"> *   2、要获取工厂bean本身,需要给id前面加一个&amp;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@Import</span>(&#123;TestImportSelector<span class="class">.<span class="keyword">class</span>, <span class="title">TestImportBeanDefinitionRegistrar</span>.<span class="title">class</span>&#125;)</span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">MainConfig</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义逻辑返回需要导入的组件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestImportSelector</span> <span class="keyword">implements</span> <span class="title">ImportSelector</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> annotationMetadata 当前标注<span class="doctag">@Import</span>注解的类的所有注解信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> String[] 导入到容器中的组件全类名数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String[] selectImports(AnnotationMetadata annotationMetadata) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String[]&#123;<span class="string">"com.xiong.test.Animal"</span>, <span class="string">"com.xiong.test.Person"</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestImportBeanDefinitionRegistrar</span> <span class="keyword">implements</span> <span class="title">ImportBeanDefinitionRegistrar</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 可以把所有需要添加到容器中的bean通过调用BeanDefinitionRegistry.registerBeanDefinition手工注册</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> importingClassMetadata 当前类的注解信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> registry               BeanDefinition注册类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registerBeanDefinitions</span><span class="params">(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> isWorldExist = registry.containsBeanDefinition(<span class="string">"World"</span>);</span><br><span class="line">        <span class="keyword">if</span> (!isWorldExist) &#123;</span><br><span class="line">            <span class="comment">// 注册一个bean,指定bean的名称和bean的定义信息(bean的类型,bean的Scope...)</span></span><br><span class="line">            registry.registerBeanDefinition(<span class="string">"world"</span>, <span class="keyword">new</span> RootBeanDefinition(World<span class="class">.<span class="keyword">class</span>))</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>FactoryBean(工厂Bean)：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个Spring定义的FactoryBean</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestFactoryBean</span> <span class="keyword">implements</span> <span class="title">FactoryBean</span>&lt;<span class="title">Animal</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 返回一个Animal对象,这个对象会添加到容器中</span></span><br><span class="line">    <span class="comment">// 调用此方法得到对象</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Animal <span class="title">getObject</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Animal();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Class&lt;?&gt; getObjectType() &#123;</span><br><span class="line">        <span class="keyword">return</span> Animal<span class="class">.<span class="keyword">class</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 是否是单实例</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSingleton</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TestFactoryBean <span class="title">testFactoryBean</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TestFactoryBean();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@Bean指定初始化和销毁方法：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Car</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Car</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Car 构造方法"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Car init"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Car destroy"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * bean的生命周期：</span></span><br><span class="line"><span class="comment"> *  bean创建 --&gt; 初始化 --&gt; 销毁</span></span><br><span class="line"><span class="comment"> * 容器管理bean的生命周期;</span></span><br><span class="line"><span class="comment"> * 我们可以自定义初始化和销毁方法;容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 构造(对象创建)：</span></span><br><span class="line"><span class="comment"> *  单实例：在容器启动的时候创建对象</span></span><br><span class="line"><span class="comment"> *  多实例：在每次获取的时候创建对象</span></span><br><span class="line"><span class="comment"> * 初始化：对象创建完成,并赋值结束,调用初始化方法</span></span><br><span class="line"><span class="comment"> * 销毁：</span></span><br><span class="line"><span class="comment"> *  单实例：容器关闭的时候</span></span><br><span class="line"><span class="comment"> *  多实例：容器不会管理这个bean,容器不会调用销毁方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1、指定初始化和销毁方法(通过<span class="doctag">@Bean</span>注解指定init-method和destroy-method)</span></span><br><span class="line"><span class="comment"> * 2、通过让Bean实现InitializingBean(定义初始化方法逻辑),DisposableBean(定义销毁逻辑)</span></span><br><span class="line"><span class="comment"> * 3、可以使用JSR250：</span></span><br><span class="line"><span class="comment"> *     <span class="doctag">@PostConstruct</span>：在bean创建完成并且属性赋值完毕再执行初始化方法</span></span><br><span class="line"><span class="comment"> *     <span class="doctag">@PreDestroy</span>：在容器销毁bean之前通知进行清理工作</span></span><br><span class="line"><span class="comment"> * 4、BeanPostProcessor：bean的后置处理器(在bean初始化前后进行一些工作)</span></span><br><span class="line"><span class="comment"> *  postProcessBeforeInitialization：在初始化之前工作</span></span><br><span class="line"><span class="comment"> *  postProcessAfterInitialization：在初始化之后工作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigLifecycle</span> </span>&#123;</span><br><span class="line">    <span class="comment">// @Scope("prototype")</span></span><br><span class="line">    <span class="meta">@Bean</span>(initMethod = <span class="string">"init"</span>, destroyMethod = <span class="string">"destroy"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Car <span class="title">car</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Car();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ComponentScan</span>(<span class="string">"com.xiong.test"</span>)</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">implements</span> <span class="title">InitializingBean</span>, <span class="title">DisposableBean</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Cat构造函数..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Cat destroy..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterPropertiesSet</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Cat init..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 后置处理器：在bean初始化前后进行处理工作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestBeanPostProcessor</span> <span class="keyword">implements</span> <span class="title">BeanPostProcessor</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">postProcessBeforeInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"postProcessBeforeInitialization: "</span> + beanName + <span class="string">" , "</span> + bean);</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">postProcessAfterInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"postProcessAfterInitialization: "</span> + beanName + <span class="string">" , "</span> + bean);</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>BeanPostProcessor原理</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 遍历得到容器中所有的BeanPostProcessor;挨个执行beforeInitialization,</span></span><br><span class="line"><span class="comment">// 一旦返回null,跳出for循环,不追执行后面的BeanPostProcessor.postProcessBeforeInitialization()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 给bean进行属性赋值</span></span><br><span class="line">populateBean(beanName, mbd, instanceWrapper);</span><br><span class="line">initializeBean(beanName, exposedObject, mbd);</span><br><span class="line">&#123; <span class="comment">// 以下就是initializeBean的粗略内容</span></span><br><span class="line">    applyBeanPostProcessorsBeforeInitialization(bean, beanName);</span><br><span class="line">    invokeInitMethods(beanName, wrappedBean, mbd);</span><br><span class="line">    applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>Spring底层对BeanPostProcessor的使用</p><ul><li>ApplicationContextAwareProcessor：可让bean获取容器对象context</li><li>BeanValidationPostProcessor：Web表单校验的处理器</li><li>InitDestroyAnnotationBeanPostProcessor：@PostConstruct和@Bean的init-method等方法的具体实现</li><li>AutowiredAnnotationBeanPostProcessor：@Autowired自动注入功能的具体实现</li></ul></li><li><p>属性赋值</p><ul><li>使用@Value赋值：<ul><li>基本数值</li><li>SpEL：#{}</li><li>${}：取出配置文件(properties或yaml)中的值(在运行环境变量里面的值)</li></ul></li><li>使用@PropertySource加载外部配置文件</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用@PropertySource读取外部配置文件中的k/v保存到运行的环境变量中</span></span><br><span class="line"><span class="comment">// 加载完外部的配置文件以后使用$&#123;&#125;取出配置文件的值</span></span><br><span class="line"><span class="comment">// 当前只能加载properties文件,yaml不能,是采用的加载器问题</span></span><br><span class="line"><span class="meta">@PropertySource</span>(value = &#123;<span class="string">"classpath:application.properties"</span>&#125;, encoding = <span class="string">"utf-8"</span>)</span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigPropertyValues</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Person <span class="title">person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Person();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"SOBXiong"</span>)</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"#&#123;22+5&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;person.nickName&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String nickName;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// application.properties</span></span><br><span class="line"><span class="comment">// person.nickName=熊哈哈</span></span><br></pre></td></tr></table></figure><ul><li>自动装配</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自动装配：</span></span><br><span class="line"><span class="comment"> * Spring利用依赖注入(DI),完成对IOC容器中各个组件的依赖关系赋值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Autowired</span>：自动注入(Spring定义的)</span></span><br><span class="line"><span class="comment"> * TestService&#123;</span></span><br><span class="line"><span class="comment"> *      <span class="doctag">@Autowired</span> TestDao testDao;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> * 1、默认优先按照类型去容器中找对应的组件：context.getBean(TestDao.class);</span></span><br><span class="line"><span class="comment"> * 2、如果找到多个相同类型的组件,将属性名作为组件的id去容器中查找</span></span><br><span class="line"><span class="comment"> * 3、<span class="doctag">@Qualifier</span>("testDao")：使用<span class="doctag">@Qualifier</span>指定需要装配的组件id,而不是使用属性名</span></span><br><span class="line"><span class="comment"> * 4、自动装配默认一定要将属性赋值好,没有就会报错(可以使用<span class="doctag">@Autowired</span>注解中的required=false避免报错)</span></span><br><span class="line"><span class="comment"> * 5、<span class="doctag">@Primary</span>：让Spring进行自动装配的时候默认使用首选的bean(此时<span class="doctag">@Qualifier</span>不能使用);也可以使用<span class="doctag">@Qualifier</span>指定需要装配的具体bean</span></span><br><span class="line"><span class="comment"> * 6、Spring还支持使用<span class="doctag">@Resource</span>(JSR250)和<span class="doctag">@Inject</span>(JSR330)[java规范的注解]</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Resource</span>：可以和<span class="doctag">@Autowired</span>一样实现自动装配功能,但默认是按照组件名称进行装配的(也可以通过name属性进行指定id);不能支持<span class="doctag">@Qualifier</span>和required=false</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Inject</span>：需要导入javax.inject的包,和<span class="doctag">@Autowired</span>的功能一样,但没有required属性</span></span><br><span class="line"><span class="comment"> * 7、<span class="doctag">@Autowired</span>可以在构造器、参数、方法和属性上标注,都是从容器中获取组件的值</span></span><br><span class="line"><span class="comment"> *      1、[标注在方法位置]：<span class="doctag">@Bean</span>标注方法的方法参数;参数从容器中获取;默认不写<span class="doctag">@Autowired</span>效果是一样的;都能自动装配</span></span><br><span class="line"><span class="comment"> *      2、[标注在构造器位置]：如果组件只有一个有参构造器,这个有参构造器的<span class="doctag">@Autowired</span>可以省略,参数位置的组件还是可以自动从容器中获取;</span></span><br><span class="line"><span class="comment"> *      但如有既有有参又有无参,会优先调用无参构造器,这使得boss的car属性和容器中的car不是同一个</span></span><br><span class="line"><span class="comment"> *      3、[标注在参数位置]</span></span><br><span class="line"><span class="comment"> * 8、自定义组件想要使用Spring容器底层的的一些组件(ApplicationContext、BeanFactory等)</span></span><br><span class="line"><span class="comment"> *  自定义组件实现xxxAware接口：在创建对象的时候,会调用接口规定的方法注入相关组件;</span></span><br><span class="line"><span class="comment"> *  xxxAware使用xxxProcessor：applicationContextAware =&gt; applicationContextAwareProcessor(BeanPostProcessor的实现类)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ComponentScan</span>(<span class="string">"com.xiong.test2"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigAutowired</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="meta">@Bean</span>(<span class="string">"testDao2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> TestDao <span class="title">testDao</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TestDao(<span class="string">"2"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// @Bean标注的方法创建对象的时候,方法参数的值从容器中获取</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boss <span class="title">boss</span><span class="params">(Car car)</span></span>&#123;</span><br><span class="line">        Boss boss = <span class="keyword">new</span> Boss();</span><br><span class="line">        boss.setCar(car);</span><br><span class="line">        <span class="keyword">return</span> boss;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Car</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认加在ioc容器中的组件，容器启动会调用无参构造器创建对象，在进行初始化赋值等操作</span></span><br><span class="line"><span class="comment">// @Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Boss</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Car car;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Boss</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">    <span class="comment">// 构造器要用的组件，都是从容器中获取</span></span><br><span class="line">    <span class="comment">// @Autowired</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Boss</span><span class="params">(@Autowired Car car)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.car = car;</span><br><span class="line">        System.out.println(<span class="string">"Boss constructor with one parameter!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Car <span class="title">getCar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> car;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 标注在方法上，Spring容器创建当前对象，就会调用方法完成赋值</span></span><br><span class="line">    <span class="comment">// 方法使用的参数，自定义类型的值从ioc容器中获取</span></span><br><span class="line">    <span class="comment">// @Autowired</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCar</span><span class="params">(Car car)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.car = car;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Boss&#123;"</span> +</span><br><span class="line">                <span class="string">"car="</span> + car +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestService</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Qualifier</span>(<span class="string">"testDao2"</span>)</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> TestDao testDao;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"TestService&#123;"</span> +</span><br><span class="line">                <span class="string">"testDao="</span> + testDao +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Repository</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestDao</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String label = <span class="string">"1"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>@Profile：Spring为我们提供的可以根据当前环境,动态地激活和切换一系列组件的共功能;指定组件在哪个环境的情况下才能被注册到容器中,不指定,任何环境下都能注册这个组件(开发、测试/生产环境,数据源(A/B/C))</p><ul><li>加了环境标志性的bean,只有这个环境被激活的时候才能注册到容器中(默认环境是default)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Profile</span>(<span class="string">"test"</span>)</span><br><span class="line"><span class="meta">@Bean</span>(<span class="string">"testDataSource"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> DataSource <span class="title">dataSourceTest</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123; ... &#125;</span><br></pre></td></tr></table></figure><ul><li><p>写在配置类上,只有是指定的环境的时候,整个配置类里面的内容才能生效</p></li><li><p>没有标注环境标识的bean在任何环境下都是加载的</p></li><li><p>环境的激活：</p><ul><li>使用命令行动态参数：虚拟机参数位置加载 -Dspring.profiles.active=test</li><li>代码的方式激活某种环境</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">contextLoads</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1、创建ioc容器</span></span><br><span class="line">    AnnotationConfigApplicationContext context =</span><br><span class="line">            <span class="keyword">new</span> AnnotationConfigApplicationContext();</span><br><span class="line">    <span class="comment">// 2、设置需要激活的环境</span></span><br><span class="line">    context.getEnvironment().setActiveProfiles(<span class="string">"test"</span>);</span><br><span class="line">    <span class="comment">// 3、注册主配置类</span></span><br><span class="line">    context.register(MainConfigProfile<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、启动刷新容器</span></span><br><span class="line">    context.refresh();</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭容器</span></span><br><span class="line">    context.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>AOP</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * AOP：指在程序运行期间动态地将某段代码切入到指定方法指定位置进行运行地编程方式</span></span><br><span class="line"><span class="comment"> * 1、导入AOP模块：Spring AOP(SpringBoot导入MVC模块会连着导入AOP模块)</span></span><br><span class="line"><span class="comment"> * 2、定义一个业务逻辑类(MathCalculator)：在业务逻辑运行地时候将日志进行打印(方法之前、方法运行结束、方法出现异常...)</span></span><br><span class="line"><span class="comment"> * 3、定义一个日志切面类(LogAspect)：切面类里面地方法需要动态感知MathCalculator.div运行到哪里然后执行</span></span><br><span class="line"><span class="comment"> *   通知方法：</span></span><br><span class="line"><span class="comment"> *   前置通知<span class="doctag">@Before</span>：logStart(在目标方法div运行之前运行)</span></span><br><span class="line"><span class="comment"> *   后置通知<span class="doctag">@After</span>：logEnd(在目标方法div运行结束之后运行——无论方法是正常结束还是异常结束)</span></span><br><span class="line"><span class="comment"> *   返回通知<span class="doctag">@AfterReturning</span>：logReturn(在目标方法div正常返回之后运行)</span></span><br><span class="line"><span class="comment"> *   异常通知<span class="doctag">@AfterThrowing</span>：logException(在目标方法div出现异常以后运行)</span></span><br><span class="line"><span class="comment"> *   环绕通知<span class="doctag">@Around</span>：动态代理,手动推进目标方法div运行(jointPoint.proceed())</span></span><br><span class="line"><span class="comment"> * 4、给切面类的目标方法标注何时何地运行(通知注解)</span></span><br><span class="line"><span class="comment"> * 5、将切面类和业务逻辑类(目标方法所在类)都加入到容器中</span></span><br><span class="line"><span class="comment"> * 6、必须告诉Spring哪个类是切面类(给切面类上加一个注解<span class="doctag">@Aspect</span>)</span></span><br><span class="line"><span class="comment"> * 7、给配置类中加<span class="doctag">@EnableAspectJAutoProxy</span>(开启基于注解的AOP模式);在Spring中有很多的<span class="doctag">@EnableXXX</span>注解,替代以前的xml配置</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 三步：</span></span><br><span class="line"><span class="comment"> *  1、将业务逻辑组件和切面类都加入到容器中;告诉Spring哪个是切面类(<span class="doctag">@Aspect</span>)</span></span><br><span class="line"><span class="comment"> *  2、在切面类上的每一个通知方法上标注通知注解,告诉Spring何时何地运行(切入点表达式)</span></span><br><span class="line"><span class="comment"> *  3、开启基于注解的AOP模式：<span class="doctag">@EnableAspectJAutoProxy</span></span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * AOP原理：[看给容器中注册了什么组件,这个组件什么时候工作以及这个组件的功能是什么?]</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@EnableAspectJAutoProxy</span>：</span></span><br><span class="line"><span class="comment"> *  1、<span class="doctag">@EnableAspectJAutoProxy</span>是什么？</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Import</span>(AspectJAutoProxyRegistrar.class)：给容器中导入AspectJAutoProxyRegistrar</span></span><br><span class="line"><span class="comment"> *      利用AspectJAutoProxyRegistrar自定义给容器中注册bean：BeanDefinition</span></span><br><span class="line"><span class="comment"> *      internalAutoProxyCreator = AnnotationAwareAspectJAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *  给容器中注册一个AnnotationAwareAspectJAutoProxyCreator,id为internalAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  2、AnnotationAwareAspectJAutoProxyCreator：</span></span><br><span class="line"><span class="comment"> *      AbstractAutoProxyCreator实现了SmartInstantiationAwareBeanPostProcessor,BeanFactoryAware接口</span></span><br><span class="line"><span class="comment"> *          AbstractAdvisorAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *              AspectJAwareAdvisorAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *                  AnnotationAwareAspectJAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *    关注后置处理器(bean初始化完成前后做事情)、自动装配beanFactory</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  阅读线索：</span></span><br><span class="line"><span class="comment"> *  AbstractAutoProxyCreator.setBeanFactory()</span></span><br><span class="line"><span class="comment"> *  AbstractAutoProxyCreator.postProcessBeforeInstantiation()</span></span><br><span class="line"><span class="comment"> *  AbstractAutoProxyCreator.postProcessAfterInitialization()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  AbstractAdvisorAutoProxyCreator.setBeanFactory()复写父类方法</span></span><br><span class="line"><span class="comment"> *  方法内还执行了initBeanFactory()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  AnnotationAwareAspectJAutoProxyCreator.initBeanFactory()复写父类方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  1、传入配置类,创建ioc容器</span></span><br><span class="line"><span class="comment"> *  2、调用配置类,调用refresh()刷新容器</span></span><br><span class="line"><span class="comment"> *  3、registerBeanPostProcessors(beanFactory)：注册bean的后置处理器来方便拦截bean的创建</span></span><br><span class="line"><span class="comment"> *      1、先获取ioc容器已定义了的需要创建的所有BeanPostProcessor(第一步传入配置类带入的)：</span></span><br><span class="line"><span class="comment"> *          String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false);</span></span><br><span class="line"><span class="comment"> *      2、给容器中加入别的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      3、优先注册实现了PriorityOrdered接口的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      4、再给容器中注册实现了Ordered接口的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      5、最后注册没实现优先级接口的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      6、注册BeanPostProcessor,实际上就是创建BeanPostProcessor对象并保存在容器中</span></span><br><span class="line"><span class="comment"> *          创建internalAutoProxyCreator的BeanPostProcessor[AnnotationAwareAspectJAutoProxyCreator对象]</span></span><br><span class="line"><span class="comment"> *              1、创建Bean的实例</span></span><br><span class="line"><span class="comment"> *              2、populateBean()：给bean的各种属性赋值</span></span><br><span class="line"><span class="comment"> *              3、initializeBean()：初始化bean</span></span><br><span class="line"><span class="comment"> *                  1、invokeAwareMethods()：处理Aware接口的方法回调</span></span><br><span class="line"><span class="comment"> *                  2、applyBeanPostProcessorsBeforeInitialization()：应用后置处理器的postProcessBeforeInitialization()方法</span></span><br><span class="line"><span class="comment"> *                  3、invokeInitMethods()：执行自定义的初始化方法</span></span><br><span class="line"><span class="comment"> *                  4、applyBeanPostProcessorsAfterInitialization()：应用后置处理器的postProcessAfterInitialization()方法</span></span><br><span class="line"><span class="comment"> *              4、BeanPostProcessor(AnnotationAwareAspectJAutoProxyCreator)创建成功 -&gt; aspectJAdvisorsBuilder</span></span><br><span class="line"><span class="comment"> *      7、把BeanPostProcessor注册到BeanFactory中：beanFactory.addBeanPostProcessor(postProcessor);</span></span><br><span class="line"><span class="comment"> * -----------------------------------------创建和注册AnnotationAwareAspectJAutoProxyCreator的过程----------------------------</span></span><br><span class="line"><span class="comment"> *  4、finishBeanFactoryInitialization(beanFactory)：完成BeanFactory初始化工作,创建剩下来的单实例bean</span></span><br><span class="line"><span class="comment"> *      1、遍历获取容器中所有的Bean,依次创建对象getBean(beanName)</span></span><br><span class="line"><span class="comment"> *          getBean -&gt; doGetBean() -&gt; getSingleton()</span></span><br><span class="line"><span class="comment"> *      2、创建bean[AnnotationAwareAspectJAutoProxyCreator在所有bean创建之前会有一个拦截,</span></span><br><span class="line"><span class="comment"> *         InstantiationAwareBeanPostProcessor,会调用postProcessBeforeInstantiation()方法]</span></span><br><span class="line"><span class="comment"> *          1、先从缓存中获取当前bean,如果能获取到,说明bean是被创建过的,直接使用;否则再创建</span></span><br><span class="line"><span class="comment"> *          只要被创建好的bean都会被缓存起来</span></span><br><span class="line"><span class="comment"> *          2、createBean()：创建bean——AnnotationAwareAspectJAutoProxyCreator会在任何bean创建之前先尝试返回bean的实例</span></span><br><span class="line"><span class="comment"> *              [BeanPostProcessor是在Bean对象创建完成初始化前后调用的]</span></span><br><span class="line"><span class="comment"> *              [InstantiationAwareBeanPostProcessor是在创建Bean实例之前先尝试用后置处理器返回对象的]</span></span><br><span class="line"><span class="comment"> *              1、resolveBeforeInstantiation()：希望后置处理器在此能返回一个代理对象,如果能返回</span></span><br><span class="line"><span class="comment"> *              代理对象就使用;</span></span><br><span class="line"><span class="comment"> *                  1、后置处理器先尝试返回对象：</span></span><br><span class="line"><span class="comment"> *                  bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName);</span></span><br><span class="line"><span class="comment"> *                      拿到所有后置处理器,如果是InstantiationAwareBeanPostProcessor</span></span><br><span class="line"><span class="comment"> *                      就执行postProcessBeforeInstantiation()方法</span></span><br><span class="line"><span class="comment"> * if (bean != null) &#123;</span></span><br><span class="line"><span class="comment"> * bean = applyBeanPostProcessorsAfterInitialization(bean, beanName);</span></span><br><span class="line"><span class="comment"> *                  &#125;</span></span><br><span class="line"><span class="comment"> *              否则就进行第二步</span></span><br><span class="line"><span class="comment"> *              2、doCreateBean()：真正地去创建bean实例,和3.6流程一样</span></span><br><span class="line"><span class="comment"> *              3、</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * AnnotationAwareAspectJAutoProxyCreator[InstantiationAwareBeanPostProcessor]的作用</span></span><br><span class="line"><span class="comment"> * 1、每一个bean创建之前,调用postProcessBeforeInstantiation()</span></span><br><span class="line"><span class="comment"> *      关心MathCalculator和LogAspect的创建</span></span><br><span class="line"><span class="comment"> *      1、判断当前bean是否在advisedBeans中(保存了需要增强的bean——需要切面)</span></span><br><span class="line"><span class="comment"> *      2、判断当前bean是否是基础类型的(Advice、Pointcut、Advisor、AopInfrastructureBean)</span></span><br><span class="line"><span class="comment"> *      或者是否是切面(<span class="doctag">@Aspect</span>)</span></span><br><span class="line"><span class="comment"> *      3、判断是否需要跳过</span></span><br><span class="line"><span class="comment"> *          1、获取候选的增强器(切面里面的通知方法),每一个封装的通知方法的增强器是InstantiationModelAwarePointcutAdvisor</span></span><br><span class="line"><span class="comment"> *          判断每一个增强器是否是AspectJPointcutAdvisor类型(返回true)</span></span><br><span class="line"><span class="comment"> *          2、返回false</span></span><br><span class="line"><span class="comment"> * 2、创建对象</span></span><br><span class="line"><span class="comment"> * postProcessAfterInitialization</span></span><br><span class="line"><span class="comment"> * // 在需要的时候包装</span></span><br><span class="line"><span class="comment"> * return wrapIfNecessary()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    1、获取当前bean的所有增强器(通知方法)</span></span><br><span class="line"><span class="comment"> *      1、找到候选的所有增强器(找哪些通知方法是需要切入当前bean方法的)</span></span><br><span class="line"><span class="comment"> *      2、获取到能在bean使用的增强器</span></span><br><span class="line"><span class="comment"> *      3、给增强器排序</span></span><br><span class="line"><span class="comment"> *    2、保存当前bean到advisedBeans</span></span><br><span class="line"><span class="comment"> *    3、如果当前bean需要增强,创建当前bean的代理对象</span></span><br><span class="line"><span class="comment"> *      1、获取所有增强器(通知方法)</span></span><br><span class="line"><span class="comment"> *      2、保存到proxyFactory</span></span><br><span class="line"><span class="comment"> *      3、创建代理对象：Spring自动决定</span></span><br><span class="line"><span class="comment"> *          JdkDynamicAopProxy()：jdk动态代理</span></span><br><span class="line"><span class="comment"> *          ObjenesisCglibAopProxy()：cglib的动态代理</span></span><br><span class="line"><span class="comment"> *    4、给容器中返回当前组件使用cglib增强了的代理对象</span></span><br><span class="line"><span class="comment"> *    5、以后容器中获取到的就是这个组件的代理对象，执行目标方法的时候，代理对象就会执行通知方法的流程</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 3、目标方法执行:</span></span><br><span class="line"><span class="comment"> *    容器中保存了组件的代理对象(cglib增强后的对象)，这个对象里面保存了详细信息(比如增强器、目标对象...)</span></span><br><span class="line"><span class="comment"> *    1、CglibAopProxy.intercept()拦截目标方法的执行</span></span><br><span class="line"><span class="comment"> *    2、根据ProxyFactory对象获取将要执行的目标方法的拦截器链</span></span><br><span class="line"><span class="comment"> *      List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);</span></span><br><span class="line"><span class="comment"> *          1、List&lt;Object&gt; interceptorList保存所有拦截器(长度为5)</span></span><br><span class="line"><span class="comment"> *              一个默认的ExposeInvocationInterceptor和四个增强器</span></span><br><span class="line"><span class="comment"> *          2、遍历所有的增强器,将其转为Interceptor</span></span><br><span class="line"><span class="comment"> *          3、将增强器转为List&lt;MethodInterceptor&gt;</span></span><br><span class="line"><span class="comment"> *             如果是MethodInterceptor,直接加入集合</span></span><br><span class="line"><span class="comment"> *             如果不是,使用AdvisorAdapter转为MethodInterceptor</span></span><br><span class="line"><span class="comment"> *             转换完成返回MethodInterceptor数组</span></span><br><span class="line"><span class="comment"> *    3、如果没有拦截器链,直接执行目标方法</span></span><br><span class="line"><span class="comment"> *          拦截器链(每一个通知方法又被包装为方法拦截器,利用MethodInterceptor机制)</span></span><br><span class="line"><span class="comment"> *    4、如果有拦截器链，把需要执行的目标对象、目标方法、拦截器链等信息传入创建一个CglibMethodInvocation对象，</span></span><br><span class="line"><span class="comment"> *    并调用它的proceed()方法</span></span><br><span class="line"><span class="comment"> *    5、拦截器的触发过程</span></span><br><span class="line"><span class="comment"> *      1、如果没有拦截器(或者拦截器的索引为拦截器数组大小-1——到了最后一个拦截器),直接执行目标方法</span></span><br><span class="line"><span class="comment"> *      2、链式获取每一个拦截器,拦截器执行invoke()方法,每一个拦截器等待下一个拦截器执行完成返回以后再来执行;</span></span><br><span class="line"><span class="comment"> *         拦截器链的机制,保证通知方法与目标方法的执行顺序</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 总结：</span></span><br><span class="line"><span class="comment"> *  1、<span class="doctag">@EnableAspectJAutoProxy</span>开始AOP功能</span></span><br><span class="line"><span class="comment"> *  2、<span class="doctag">@EnableAspectJAutoProxy</span>会给容器中注册一个组件AnnotationAwareAspectJAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *  3、AnnotationAwareAspectJAutoProxyCreator是一个后置处理器</span></span><br><span class="line"><span class="comment"> *  4、容器的创建流程：</span></span><br><span class="line"><span class="comment"> *      1、refresh()容器刷新后registerBeanPostProcessors()注册后置处理器：创建AnnotationAwareAspectJAutoProxyCreator对象</span></span><br><span class="line"><span class="comment"> *      2、finishBeanFactoryInitialization()初始化剩下的单实例bean</span></span><br><span class="line"><span class="comment"> *          1、创建业务逻辑组件和切面组件</span></span><br><span class="line"><span class="comment"> *          2、AnnotationAwareAspectJAutoProxyCreator拦截组件的创建过程</span></span><br><span class="line"><span class="comment"> *          3、组件创建完之后,判断组件是否需要增强</span></span><br><span class="line"><span class="comment"> *              是：把切面的通知方法包装成增强器(Advisor),给业务逻辑组件创建一个代理对象(cglib)</span></span><br><span class="line"><span class="comment"> *  5、执行目标方法：</span></span><br><span class="line"><span class="comment"> *      1、代理对象执行目标方法</span></span><br><span class="line"><span class="comment"> *      2、CglibAopProxy.intercept()进行拦截：</span></span><br><span class="line"><span class="comment"> *          1、得到目标方法的拦截器链(增强器包装成拦截器MethodInterceptor)</span></span><br><span class="line"><span class="comment"> *          2、利用拦截器的链式机制,依次进入每一个拦截器进行执行</span></span><br><span class="line"><span class="comment"> *          3、效果：</span></span><br><span class="line"><span class="comment"> *              正常执行：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 返回通知</span></span><br><span class="line"><span class="comment"> *              出现异常：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 异常通知</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@EnableAspectJAutoProxy</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigAOP</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 业务逻辑类加入容器中</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> MathCalculator <span class="title">calculator</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> MathCalculator();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 切面类加入到容器中</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> LogAspect <span class="title">logAspect</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LogAspect();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MathCalculator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">div</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> i / j;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 切面类,方法中joinPoint必须写在参数表的第一位(否则报错)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogAspect</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 抽取公共的切入点表达式(参考Spring官方文档)</span></span><br><span class="line">    <span class="comment">// 1、本类引用(方法名())</span></span><br><span class="line">    <span class="comment">// 2、其他的切面引用(全类名方法名())</span></span><br><span class="line">    <span class="meta">@Pointcut</span>(<span class="string">"execution(public int com.xiong.test3.MathCalculator.div(int, int))"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pointCut</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">    <span class="comment">// @Before在目标方法之前切入：切入点表达式(指定在哪个方法切入)</span></span><br><span class="line">    <span class="meta">@Before</span>(<span class="string">"pointCut()"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logStart</span><span class="params">(JoinPoint joinPoint)</span> </span>&#123;</span><br><span class="line">        Object[] args = joinPoint.getArgs();</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行开始... 参数列表是: &#123;"</span> + Arrays.asList(args) + <span class="string">"&#125;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@After</span>(<span class="string">"pointCut()"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logEnd</span><span class="params">(JoinPoint joinPoint)</span> </span>&#123;</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行结束..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@AfterReturning</span>(value = <span class="string">"pointCut()"</span>, returning = <span class="string">"result"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logReturn</span><span class="params">(JoinPoint joinPoint, Object result)</span> </span>&#123;</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行正常返回... 结果: &#123;"</span> + result + <span class="string">"&#125;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@AfterThrowing</span>(value = <span class="string">"pointCut()"</span>, throwing = <span class="string">"exception"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logException</span><span class="params">(JoinPoint joinPoint, Exception exception)</span> </span>&#123;</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行出现异常... 异常信息: &#123;"</span> + exception.getMessage() + <span class="string">"&#125;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">contextLoads</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1、创建ioc容器</span></span><br><span class="line">    AnnotationConfigApplicationContext context =</span><br><span class="line">            <span class="keyword">new</span> AnnotationConfigApplicationContext(MainConfigAOP<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 必须使用Spring容器中的组件</span></span><br><span class="line">    MathCalculator calculator = context.getBean(MathCalculator<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    calculator.div(<span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    context.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E9%80%9A%E7%9F%A5%E6%96%B9%E6%B3%95.png" alt="链式调用通知方法"></p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop</title>
      <link href="/2020/06/03/BigData/Hadoop/"/>
      <url>/2020/06/03/BigData/Hadoop/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#概论">概论</a></li><li><a href="#Hadoop介绍">Hadoop介绍</a></li><li><a href="#环境搭建">环境搭建</a></li><li><a href="#Hadoop运行模式">Hadoop运行模式</a></li><li><a href="#Hadoop编译源码">Hadoop编译源码</a></li><li><a href="#HDFS概述">HDFS概述</a></li><li><a href="#HDFS的Shell操作">HDFS的Shell操作</a></li><li><a href="#HDFS客户端操作">HDFS客户端操作</a></li><li><a href="#HDFS的数据流">HDFS的数据流</a></li><li><a href="#NameNode和SecondaryNameNode">NameNode和SecondaryNameNode</a></li><li><a href="#DataNode">DataNode</a></li><li><a href="#HDFS2.X新特性">HDFS2.X新特性</a></li><li><a href="#MapReduce概述">MapReduce概述</a></li><li><a href="#Hadoop序列化">Hadoop序列化</a></li><li><a href="#MapReduce框架原理">MapReduce框架原理</a></li><li><a href="#Hadoop数据压缩">Hadoop数据压缩</a></li><li><a href="#Yarn资源调度器">Yarn资源调度器</a></li><li><a href="#Hadoop企业优化">Hadoop企业优化</a></li><li><a href="#MapReduce扩展案例">MapReduce扩展案例</a></li><li><a href="#常见错误及解决方案">常见错误及解决方案</a></li></ul><a id="more"></a><h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><ul><li><p>概念：大数据指<strong>无法在一定时间范围</strong>内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的<strong>海量、高增长率和多样化的信息资产</strong>。需要解决的问题：海量数据的<strong>存储</strong>和海量数据的<strong>分析计算</strong>问题。</p></li><li><p>大数据特点(4V)：</p><ul><li>Volume(大量)</li><li>Velocity(高速)</li><li>Variety(多样)：<strong>结构化/非结构化数据</strong>，结构化数据以数据库/文本为主，非结构化数据包括网络日志、音频、视频、图片和地理位置信息等。</li><li>Value(低价值密度)：价值密度的高度与数据总量的大小成反比，如何<strong>快速对有价值数据“提纯”称为目前大数据背景下待解决的难题</strong>。</li></ul></li><li><p>大数据应用场景：物流仓储、零售、旅游、商品广告推荐、保险、金融、房产、人工智能</p></li><li><p>大数据部门业务流程：<br>产品人员提需求(统计总用户数、日活跃用户数、回流用户数等) =&gt; 数据部门搭建数据平台、分析数据指标 =&gt; 数据可视化(报表展示、邮件发送、大屏幕展示等)</p></li><li><p>大数据部门组织结构：<br><img src="%E9%83%A8%E9%97%A8%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84.png" alt="部门组织结构"></p></li></ul><h2 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h2><ul><li><p>Hadoop是什么：</p><ul><li>是一个由Apache基金会开发的<strong>分布式系统基础架构</strong>。</li><li>主要解决海量数据的<strong>存储</strong>和海量数据的<strong>分析计算</strong>问题。</li><li>广义上来说，Hadoop通常指更广泛的概念——Hadoop生态圈。</li></ul></li><li><p>Hadoop发展历史</p><ul><li><p>Lucene框架是<strong>Doug Cutting</strong>开创的开源软件，用Java编写，实现与Goole类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询和索引引擎。</p></li><li><p>2001年年底Lucene称为Apache基金会的一个子项目。</p></li><li><p>对于海量数据的场景，Lucene面对与Google同样的困难，<strong>存储数据困难，检索速度慢</strong>。</p></li><li><p>学习和模仿Google解决这些问题的办法：微型版Nutch。</p></li><li><p>Google是Hadoop的思想之源(其在大数据方面的三篇论文)<br><strong>GFS -&gt; HDFS Map-Reduce -&gt; MR BigTable -&gt; HBase</strong></p></li><li><p>2003年~04年，Google公开部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用<strong>2年业余时间</strong>实现了DFS和MapReduce机制，使Nutch性能飙升。</p></li><li><p>2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。</p></li><li><p>2006年3月份，Map-Reduce和Nutch Distributed File System(NDFS)分别被纳入到Hadoop项目中，Hadoop就正式诞生，标志着大数据十代来临。</p></li><li><p>Hadoop名字来源于Doug Cutting儿子的玩具大象<br><img src="logo.jpg" alt="logo"></p></li></ul></li><li><p>Hadoop三大发行版本</p><ul><li>Apache：最原始(基础)的版本，对于入门学习最好</li><li>Cloudera：在大型互联网企业中用的较多，产品主要为CDH，Cloudera Manager，Cloudera Support：<ul><li>CDH是Cloudera的Hadoop发行版，完全开源，比Apache版本在兼容性、安全性、稳定性上有所增强。</li><li>Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。</li><li>Cloudera Support即是对Hadoop的技术支持。</li></ul></li><li>Hortonworks：文档较好<ul><li>Hortonworks的主打产品是Hortonworks Data Platform(HDP)，也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari——一款开源的安装和管理系统。</li><li>HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。</li></ul></li></ul></li><li><p>Hadoop的优势(4高)</p><ul><li>高可靠性：Hadoop底层维护多个数据副本，即使某个计算元素或存储出现故障，也不会导致数据的丢失。</li><li>高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点。</li><li>高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</li><li>高容错性：能够自动将失败的任务重新分配。</li></ul></li><li><p>Hadoop组成</p><ul><li>1.x：Common(辅助工具)、HDFS(数据存储)、MapReduce(计算+资源调度)</li><li>2.x：Common(辅助工具)、HDFS(数据存储)、<strong>Yarn(资源调度)</strong>、<strong>MapReduce(计算)</strong></li></ul></li><li><p>HDFS架构概述：</p><ul><li>HDFS全名——Hadoop Distributed File System</li><li>组成：<ul><li>NameNode(nn)：存储文件的元数据，如文件名、文件目录结构、文件属性(生成时间、副本数、文件权限)以及每个文件的块列表和块所在的DataNode等——类似书的目录(索引)</li><li>DataNode(dn)：在本地文件系统存储文件块数据以及块数据的校验和——具体的书章节内容</li><li>Secondary NameNode(2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照——辅助NameNode工作</li></ul></li></ul></li><li><p>Yarn架构概述<br><img src="Yarn%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Yarn架构图"></p></li><li><p>MapReduce架构概述</p><ul><li>将计算分为两个阶段：Map和Reduce</li><li>Map阶段并行处理输入数据</li><li>Reduce阶段对Map结果进行汇总</li></ul></li><li><p>大数据技术生态体系<br><img src="%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB.png" alt="大数据技术生态体系"></p></li></ul><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><ul><li><p>配置Java环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 修改环境变量</span><br><span class="line">sudo vim &#x2F;etc&#x2F;profile</span><br><span class="line">##JAVA_HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_251</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line">&#x2F;&#x2F; 让环境变量修改生效</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">&#x2F;&#x2F; 查看Java版本</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure></li><li><p>配置Hadoop环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 修改环境变量</span><br><span class="line">sudo vim &#x2F;etc&#x2F;profile</span><br><span class="line">##HADOOP_HOME</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin</span><br><span class="line">&#x2F;&#x2F; 让环境变量修改生效</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">&#x2F;&#x2F; 查看Hadoop版本</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure></li><li><p>Hadoop目录说明</p><ul><li>bin目录：存放对Hadoop相关服务(HDFS,YARN)进行操作的脚本</li><li>etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</li><li>lib目录：存放Hadoop的本地库(对数据进行压缩解压缩功能)</li><li>sbin目录：存放启动或停止Hadoop相关服务的脚本</li><li>share目录：存放Hadoop的依赖jar包、文档、和官方案例</li></ul></li></ul><h2 id="Hadoop运行模式"><a href="#Hadoop运行模式" class="headerlink" title="Hadoop运行模式"></a>Hadoop运行模式</h2><ul><li><p>本地模式</p><ul><li>官方WordCount案例(统计单词数目)：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 创建wcinput文件夹</span><br><span class="line">mkdir wcinput</span><br><span class="line">&#x2F;&#x2F; 创建wc.input文件</span><br><span class="line">cd wcinput</span><br><span class="line">touch wc.input</span><br><span class="line">&#x2F;&#x2F; 编辑wc.input随意输入字符</span><br><span class="line">vim wc.input</span><br><span class="line">&#x2F;&#x2F; 回到Hadoop目录执行程序</span><br><span class="line">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput&#x2F; wcouput</span><br><span class="line">&#x2F;&#x2F; 查看结果</span><br><span class="line">cat wcoutput&#x2F;part-r-00000</span><br></pre></td></tr></table></figure></li><li><p>伪分布式模式</p><ul><li><p>配置集群</p><ul><li>设置hadoop-env.sh：在文件中设置JAVA_HOME为本地JDK地址</li><li>设置core-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>设置hdfs-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动集群</p><ul><li>格式化NameNode：bin/hdfs namenode -format</li><li><strong>启动NameNode和DataNode：sbin/start-dfs.sh(关闭stop)</strong></li></ul></li><li><p>查看集群</p><ul><li>查看是否启动成功：jps(JDK中的命令,不是Linux命令,类似ps)</li><li>web端查看HDFS文件系统：<a href="http://192.168.232.100:9870" target="_blank" rel="noopener">http://192.168.232.100:9870</a>(需要CentOS主机上设置关闭防火墙,在3.x版本端口号默认为9870)</li><li>查看产生的log日志：cd /hadoop/logs</li><li>注意：不能一直格式化NameNode，格式化NameNode会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。(最好关闭NameNode和DataNode)</li></ul></li><li><p>操作集群(所有命令类似于在Linux Terminal的命令行操作,需要加上固定前缀bin/hdfs dfs -)</p><ul><li>在HDFS文件系统上创建一个input文件夹：bin/hdfs dfs -mkdir -p /user/sobxiong/input</li><li>将测试文件内容上传到文件系统上：bin/hdfs dfs -put wcinput/wc.input /user/sobxiong/input/</li><li>查看上传的文件是否正确：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -ls /user/sobxiong/input/</span><br><span class="line">bin/hdfs dfs -cat /user/sobxiong/input/wc.input</span><br></pre></td></tr></table></figure><ul><li>运行MapReduce程序：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/sobxiong/input/ /user/sobxiong/output</li><li>查看输出结果：bin/hdfs dfs -cat /user/sobxiong/output/*</li><li>也可以在浏览器的文件系统中查看  </li><li>将测试文件内容下载到本地：bin/hdfs dfs -get /user/sobxiong/output/part-r-00000 ./wcoutput/</li><li>删除输出结果：bin/hdfs dfs -rm -r /user/sobxiong/output</li></ul></li><li><p>启动Yarn并运行MapReduce程序</p><ul><li><p>配置集群</p><ul><li>配置yarn-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251</li><li>配置yarn-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置Yarn应用的classPath --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 不配置出现：错误: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>命令行下输入hadoop classpath的一长串环境<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 不需要设置yarn.resourcemanager.hostname，可以直接通过ip+端口号的方式访问 --&gt;</span></span><br></pre></td></tr></table></figure><ul><li>配置mapred-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251</li><li>配置mapred-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在YARN上 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 默认是local，本地文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动集群</p><ul><li>启动前必须保证NameNode和DataNode已启动</li><li><strong>启动ResourceManager和NodeManager：sbin/start-yarn.sh(关闭stop)</strong></li></ul></li><li><p>集群操作</p><ul><li>yarn浏览器页面查看：8088端口</li><li>删除文件系统上的output文件：bin/hdfs dfs -rm -r /user/sobxiong/output</li><li>执行MapReduce程序：同上hadoop操作</li><li>查看结果：同上cat操作，也可以在浏览器端查看</li></ul></li></ul></li><li><p>配置历史服务器</p><ul><li><p>配置mapred-site.xml：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>172.16.85.130:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>172.16.85.130:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动历史服务器：bin/mapred –daemon start historyserver(stop关闭)</p></li><li><p>查看历史服务器是否启动：jps</p></li><li><p>查看JobHistory：<a href="http://172.16.85.130:19888/jobhistory" target="_blank" rel="noopener">http://172.16.85.130:19888/jobhistory</a></p></li></ul></li><li><p>配置日志的聚集：</p><ul><li>概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上</li><li>好处：可以方便的查看到程序运行详情，方便开发调试</li><li>注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryManager</li><li>配置yarn-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置文件说明</p><ul><li><p>默认配置文件：</p><ul><li>core-defalut.xml - hadoop-common-3.1.3.jar/core-default.xml</li><li>hdfs-default.xml - hadoop-hdfs-3.1.3.jar/hdfs-default.xml</li><li>yarn-default.xml - hadoop-yarn-common-3.1.3.jar/yarn-default.xml</li><li>mapred-default.xml - hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</li></ul></li><li><p>自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置(优先级高)</p></li></ul></li></ul></li><li><p>完全分布式运行模式</p><ul><li><p>虚拟机准备(3台，完全复制)</p></li><li><p>编写集群分发脚本xsync</p><ul><li><p>scp(secure copy)安全拷贝</p><ul><li>定义：scp可以实现服务器与服务器之间的数据拷贝</li><li>基本语法：<table><thead><tr><th>命令</th><th>参数</th><th>要拷贝的文件路径/名称</th><th>目的用户@主机:目的路径/名称</th></tr></thead><tbody><tr><td>scp</td><td>-r(递归)</td><td>$pdir/$fname</td><td>$user@$host:$pdir/$fname</td></tr></tbody></table></li></ul></li><li><p>rsync远程同步工具</p><ul><li>作用：主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点</li><li>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去</li><li>基本语法：<table><thead><tr><th>命令</th><th>参数</th><th>要拷贝的文件路径/名称</th><th>目的用户@主机:目的路径/名称</th></tr></thead><tbody><tr><td>rsync</td><td>-r(递归)v(显示复制过程)l(拷贝符号连接)</td><td>$pdir/$fname</td><td>$user@$host:$pdir/$fname</td></tr></tbody></table></li></ul></li><li><p>xsync集群分发脚本</p><ul><li><p>需求：循环复制文件到所有节点的相同目录下</p></li><li><p>需求分析：</p><ul><li>rsync命令原始：rsync -rvl /opt/module root@hadoop2:/opt/</li><li>期望脚本：xsync 需同步的文件名</li><li>说明：在home/sobxiong/bin这个目录下存放的脚本，sobxiong用户在系统任何地方都可以直接执行</li></ul></li><li><p>脚本实现</p><ul><li>在/home/sobxiong目录下创建bin目录，并在bin目录下创建xsync文件</li><li>在xsync中键入如下代码：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=2; host&lt;4; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>修改脚本xsync具有执行权限：chmod 777 xsync</li><li>调用脚本形式：xsync 文件名</li></ul></li></ul></li></ul></li><li><p>集群配置</p><ul><li><p>集群部署规划：</p><table><thead><tr><th>类型</th><th>hadoop1</th><th>hadoop2</th><th>hadoop3</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode、DataNode</td><td>DataNode</td><td>SecondaryNameNode、DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager、NodeManager</td><td>NodeManager</td></tr></tbody></table></li><li><p>配置集群</p><ul><li>核心配置文件core-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>HDFS配置文件hdfs-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 副本数目 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop3:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>YARN配置文件yarn-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>MapReduce配置文件mapred-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>在集群上分发配置好的hadoop配置文件：xsync /opt/module/hadoop-3.1.3/etc</p></li><li><p>查看文件分发情况  </p></li></ul></li><li><p>集群单点启动</p><ul><li>集群第一次启动，需要格式化NameNode：hadoop namenode -format(把log和data文件删除)</li><li>在hadoop1上启动NameNode：hadoop-daemon.sh start namenode</li><li>在hadoop1、2、3上分别启动DataNode：hadoop-daemon.sh start datanode</li></ul></li><li><p>SSH免密登陆配置</p><ul><li>配置ssh<ul><li>基本语法：ssh ip</li></ul></li><li>无密钥配置<ul><li>免密登录原理：</li><li>生成公钥和私钥：ssh-keygen -t rsa(生成文件id_rsa-私钥,id_rsa.pub-公钥)</li><li>将公钥拷贝到要免密登录的目标机器上：ssh-copy-id hadoop2(只是当前用户,root还需要另外配置)</li></ul></li><li>.ssh文件下(~/.ssh)的文件功能<ul><li>known_hosts：记录ssh访问过的计算机的公钥</li><li>id_rsa：生成的私钥</li><li>id_rsa.pub：生成的公钥</li><li>authorized_keys：存放授权过的无密登录服务器公钥</li></ul></li></ul></li><li><p>群起集群</p><ul><li><p>配置workers：vim etc/hadoop/workers；加入hadoop1、hadoop2、hadoop3(不能有空行和空格)；同步所有节点配置文件 - xsync etc/hadoop/workers</p></li><li><p>启动集群</p><ul><li>集群第一次启动，需要格式化NameNode(格式化前关闭启动的所有namenode和datanode进程,然后再删除data和log数据)：bin/hdfs namenode -format</li><li>启动HDFS：sbin/start-dfs.sh(在hadoop1上启动,这样hadoo1、2、3均会启动对应的进程)</li><li>启动YARN：sbin/start-yarn.sh(在hadoop2上启动,在ResourceManager所在机器hadoop2上启动Yarn)</li><li>查看NameNode：hadoop1:9870</li></ul></li><li><p>集群基本测试</p><ul><li>上传文件到集群：bin/hdfs dfs -put xx xx</li><li>查看上传文件存储位置<ul><li>查看HDFS文件存储路径：/opt/module/hadoop-3.1.3/data/tmp/dfs/data/current/BP-1002151198-172.16.85.130-1591848799222/current/finalized/subdir0/subdir0</li><li>查看HDFS在磁盘存储文件的内容：cat blk_xxx(文本文件)</li><li>拼接大文件：cat blk_xxx &gt;&gt; temp，最后temp就是初始的文件</li></ul></li></ul></li></ul></li><li><p>集群启动/停止方式总结</p><ul><li>各个服务组件逐一启动/停止<ul><li>分别启动/停止HDFS组件：hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode</li><li>启动/停止YARN：yarn-daemon.sh start/stop resourcemanager/nodemanager</li></ul></li><li>各个模块分开启动/停止(配置ssh是前提)常用<ul><li>整体启动/停止HDFS：start-dfs.sh/stop-dfs.sh</li><li>整体启动/停止YARN：start-yarn.sh/stop-yarn.sh</li></ul></li></ul></li><li><p>集群时间同步</p><ul><li><p>crontab定时任务：</p><ul><li>基本语法：crontab[选项]</li><li>选项说明<ul><li>-e：编辑crontab定时任务</li><li>-l：查询crontab任务</li><li>-r：删除当前用户所有的crontab任务</li></ul></li><li>参数说明：***** [任务]<ul><li>*的含义：<ul><li>第一个：一小时当中的第几分钟(0~59)</li><li>第二个：一天当中的第几个小时(0~23)</li><li>第三个：一个月当中的第几天(1~31)</li><li>第四个：一年当中的第几月(1~12)</li><li>第五个：一周当中的星期几(0~7,0和7均代表星期日)</li></ul></li><li>特殊符号：<ul><li><em>：代表任何时间。比如第一个“</em>”代表一小时中每分钟都执行一次</li><li>,：代表不连续的时间。如“0 8,12,16 * * *”命令，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</li><li>-：代表连续的时间范围。比如“0 5 * * 1-6”命令，代表在周一到周六的凌晨5点0分执行命令</li><li><em>/n：代表每隔多久执行一次。比如“</em>/10 * * * *”命令，代表每隔10分钟就执行一遍命令</li></ul></li></ul></li></ul></li><li><p>ntp方式进行同步</p><ul><li><p>具体思路：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。<br><img src="%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5.png" alt="集群时间同步"></p></li><li><p>具体实操</p><ul><li><p>时间服务器配置：</p><ul><li>检查ntp是否安装：rpm -qa&#124;grep ntp，有ntp、fontpackages-filesystem以及ntpdate</li><li>修改ntp配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 授权1172.16.85.0-172.16.85.255网段上的所有机器可以从这台机器上查询和同步时间</span></span><br><span class="line">restrict 172.16.85.0 mask 172.16.85.130.0 nomodify notrap</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改集群在局域网中,不使用其他互联网上的时间</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步</span></span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure><ul><li>修改/etc/sysconfig/ntpd文件：SYNC_HWCLOCK=yes(让硬件时间与系统时间一起同步)</li><li>重新启动ntpd服务：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service ntpd status</span><br><span class="line">service ntpd start</span><br></pre></td></tr></table></figure><ul><li>设置ntpd服务开机自启动：chkconfig ntpd on</li></ul></li><li><p>其他机器配置(root用户)：</p><ul><li>配置10分钟与时间服务器同步一次：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop1</span><br></pre></td></tr></table></figure><ul><li>修改任意机器时间：date -s “2020-11-11 11:11:11”</li><li>十分钟后查看机器是否与时间服务器同步：date</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="Hadoop编译源码"><a href="#Hadoop编译源码" class="headerlink" title="Hadoop编译源码"></a>Hadoop编译源码</h2><ul><li><p>前期准备</p><ul><li><p>jar包准备(hadoop源码、JDK8、Maven、Ant、Protobuf)：Protobuf在Google的github中的Release页面(3.1.3Hadoop对应2.5.0版本)</p></li><li><p>jar包安装</p><ul><li>安装JDK</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> JAVA_HOME(/etc/profile)</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><ul><li>安装Maven</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> MAVEN_HOME(/etc/profile)</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.6.3</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">mvn -version</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改maven仓库镜像</span></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><ul><li>安装Ant</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-ant-1.10.8-bin.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ANT_HOME(/etc/profile)</span></span><br><span class="line">export ANT_HOME=/opt/module/apache-ant-1.10.8</span><br><span class="line">export PATH=$PATH:$ANT_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">ant -version</span><br></pre></td></tr></table></figure><ul><li><p>安装glibc-headers和g++：yum install glibc-headers、yum install gcc-c++</p></li><li><p>安装make和cmake：yum install make</p></li><li><p>安装cmake(要装3.x版本,低版本编译不通过)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cmake-3.17.3.tar.gz -C /opt/module</span><br><span class="line">cd /opt/module/cmake-3.17.3</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> CMAKE_HOME(/etc/profile)</span></span><br><span class="line">export CMAKE_HOME=/opt/module/cmake-3.17.3</span><br><span class="line">export PATH=$PATH:$CMAKE_HOME/bin</span><br><span class="line">source /etc/profile</span><br><span class="line">cmake --version</span><br></pre></td></tr></table></figure></li><li><p>安装protobuf：</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</span><br><span class="line">cd /opt/module/protobuf-2.5.0/</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make check</span><br><span class="line">make install</span><br><span class="line">ldconfig</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> LD_LIBRARY_PATH(/etc/profile)</span></span><br><span class="line">export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</span><br><span class="line">export PATH=$PATH:$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line">protoc --version</span><br></pre></td></tr></table></figure><ul><li>安装openssl库：yum install openssl-devel</li><li>安装ncurses-devel库：yum install ncurses-devel</li></ul></li><li><p>编译源码</p><ul><li>解压源码到/opt目录</li><li>进入hadoop源码主目录</li><li>通过maven执行编译命令：mvn package -Pdist,native -DskipTests -Dtar</li></ul></li></ul></li></ul><h2 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h2><ul><li><p>HDFS产出背景及定义</p><ul><li>产生背景：随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种</li><li>定义：HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色</li><li>使用背景：<strong>适合一次写入，多次读出的场景，且不支持文件的修改</strong>。适合用来做数据分析，并不适合用来做网盘应用</li></ul></li><li><p>HDFS优缺点</p><ul><li>优点：<ul><li>高容错性<ul><li>数据自动保存多个副本。它通过增加副本的形式，提高容错性</li><li>某一个副本丢失以后，它可以自动恢复(通过将副本复制到另一个可用的节点)</li></ul></li><li>适合处理大数据<ul><li>数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据</li><li>文件规模：能够处理百万规模以上的文件数量，数量相当之大</li></ul></li><li>可构建在廉价机器上，通过多副本机制，提高可靠性</li></ul></li><li>缺点：<ul><li><strong>不适合低延时数据访问</strong>，比如毫秒级的存储数据，是做不到的</li><li><strong>无法高效的对大量小文件进行存储</strong>：<ul><li>存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的</li><li>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</li></ul></li><li>不支持并发写入、文件随机修改：<ul><li>一个文件只能有一个写，不允许多个线程同时写</li><li><strong>仅支持数据appen(追加)</strong>，不支持文件的随机修改</li></ul></li></ul></li></ul></li><li><p>HDFS组成架构<br><img src="HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84.png" alt="HDFS组成架构"></p><ul><li><p>NameNode(nn)：Master，一个主管、管理者</p><ul><li>管理HDFS的名称空间</li><li>配置副本策略</li><li>管理数据块(Block)映射信息</li><li>处理客户端读写请求</li></ul></li><li><p>DataNode：Slave。NameNode下达命令，DataNode执行实际的操作</p><ul><li>存储实际的数据块</li><li>执行数据块的读/写操作</li></ul></li><li><p>Client：客户端</p><ul><li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li><li>与NameNode交互，获取文件的位置信息</li><li>与DataNode交互，读取或者写入数据</li><li>Client提供一些命令来管理HDFS，比如NameNode格式化</li><li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li></ul></li><li><p>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务</p><ul><li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</li><li>在紧急情况下，可辅助恢复NameNode</li></ul></li></ul></li><li><p>HDFS文件块大小<br>HDFS中的文件在物理上是分块存储(Block)，块的大小可以通过配置参数(dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M<br><img src="%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F%E5%A4%A7%E8%87%B4%E8%AE%A1%E7%AE%97.png" alt="文件块大小大致计算"><br>为什么文件块的大小不能设置太小，也不能设置太大？</p><ul><li>HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置</li><li>如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢</li><li>总结：HDFS块的大小设置主要取决于磁盘传输速率</li></ul></li></ul><h2 id="HDFS的Shell操作"><a href="#HDFS的Shell操作" class="headerlink" title="HDFS的Shell操作"></a>HDFS的Shell操作</h2><ul><li><p>基本语法<br>bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令<br>其中dfs是fs的实现类</p></li><li><p>命令大全：bin/hadoop fs</p></li><li><p>使用命令：</p><ul><li>-help：输出命令的帮助(hadoop fs -help rm)</li><li>-ls：显示目录信息(hadoop fs -ls /)</li><li>-mkdir：在HDFS上创建目录[-p递归](hdoop fs -mkdir -p /sobxiong/test)</li><li>-moveFromLocal：从本地剪切粘贴到HDFS[前路径为本地,后路径为HDFS](hadoop fs -moveFromLocal ./test.txt /sobxiong/test/)</li><li>-appendToFile：追加一个文件到已经存在的文件末尾[前路径为本地,后路径为HDFS](hadoop fs -appendToFile ./test.txt /sobxiong/test/test.txt)</li><li>-cat：显示文件内容(hadoop fs -cat /sobxiong/test/test.txt)</li><li>-chgrp、-chmod、-chown：修改文件所属的权限，同Linux文件系统中的用法</li><li>-copyFromLocal：从本地文件系统拷贝文件到HDFS中，同-moveFromLocal</li><li>-copyToLocal：从HDFS拷贝文件到本地[前路径为HDFS,后路径为本地](hadoop fs -copyToLocal /sobxiong/test/test.txt ./)</li><li>-cp：把文件从HDFS的一个路径拷贝到HDFS的另一个路径</li><li>-mv：把文件从HDFS的一个路径移动到HDFS的另一个路径</li><li>-get：等同于copyToLocal(用法同copyToLocal)，从HDFS下载文件到本地</li><li>-getmerge：合并下载多个文件(hadoop fs -getmerge /sobxiong/test/* ./all.txt)</li><li>-put：等同于copyFromLocal(用法同copyFromLocal)</li><li>-tail：显示一个文件的末尾(hadoop fs -tail /sobxiong/test/test.txt)</li><li>-rm：删除文件或文件夹[-r递归删除目录]</li><li>-rmdir：删除空目录</li><li>-du：统计文件夹的大小信息[-h显示单位,-s总和](hadoop fs -du -h -s /)</li><li>-setrep：设置HDFS中文件的副本数目[这里设置的副本数只是记录在NameNode的元数据中,是否真的会有这么多副本还得看DataNode的数量.因为目前只有3台设备,最多也就3个副本,只有节点数的增加到10台时,副本数才能达到10;只要加入一台设备,就会把副本复制到设备上,直到加到10台](hadoop fs -setrep 10 /sobxiong/test/test.txt)</li></ul></li></ul><h2 id="HDFS客户端操作"><a href="#HDFS客户端操作" class="headerlink" title="HDFS客户端操作"></a>HDFS客户端操作</h2><ul><li><p>客户端环境准备</p><ul><li>将Hadoop安装到mac上，并设置环境变量</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> HADOOP_HOME(~/.bash_profile)</span></span><br><span class="line">export HADOOP_HOME="/Users/sobxiong/module/hadoop-3.1.3"</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure><ul><li>创建Maven工程测试：idea创建quickstart项目</li><li>导入依赖：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-slf4j-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>创建测试类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSClient</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, InterruptedException </span>&#123;</span><br><span class="line">      Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">      <span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop1:9000");</span></span><br><span class="line">      <span class="comment">// 1、获取hdfs客户端对象</span></span><br><span class="line">      FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 2、在hdfs上创建路径</span></span><br><span class="line">      fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/sobxiong2/test"</span>));</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 3、关闭资源</span></span><br><span class="line">      fileSystem.close();</span><br><span class="line"></span><br><span class="line">      System.out.println(<span class="string">"finish"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>HDFS的API操作</p><ul><li>文件上传</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 参数优先级：</span></span><br><span class="line"><span class="comment">  *  1、客户端代码中设置的值</span></span><br><span class="line"><span class="comment">  *  2、ClassPath(resources)下的用户自定义配置文件(hdfs-site.xml)</span></span><br><span class="line"><span class="comment">  *  3、服务器的默认配置</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// 1、文件上传</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"2"</span>);</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行上传API</span></span><br><span class="line">    fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Documents/文件块大小大致计算.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将hdfs-site.xml拷贝至项目的根目录resources资源文件夹下</span></span><br><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span>?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>文件下载</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2、文件下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行下载操作</span></span><br><span class="line">    <span class="comment">// fileSystem.copyToLocalFile(new Path("/sobxiong/test2.png"), new Path("/Users/sobxiong/Documents/test.png"));</span></span><br><span class="line">    <span class="comment">// 本地模式,true,不会产生crc文件</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Documents/test1.png"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>文件删除</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3、文件删除</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、文件删除(第二个参数,是否递归删除,文件夹时有效)</span></span><br><span class="line">    fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>文件更名</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 4、文件更名</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行更名操作</span></span><br><span class="line">    fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/sobxiong/test1.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/sobxiong/1tset.png"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>文件详情查看</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 5、文件详情查看</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、查看文件详情</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查看文件名称、权限、长度</span></span><br><span class="line">        System.out.println(<span class="string">"name: "</span> + fileStatus.getPath().getName());</span><br><span class="line">        System.out.println(<span class="string">"permission: "</span> + fileStatus.getPermission());</span><br><span class="line">        System.out.println(<span class="string">"length: "</span> + fileStatus.getLen());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查看块信息</span></span><br><span class="line">        BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">        <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">                System.out.println(<span class="string">"host = "</span> + host);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"----------------"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>判断是文件还是文件夹</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 6、判断是文件还是文件夹</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、判断操作</span></span><br><span class="line">    FileStatus[] fileStatuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"file = "</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"dir = "</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>HDFS的I/O流操作</p><ul><li>HDFS文件上传</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 把本地文件上传到HDFS根目录</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">upload</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FileInputStream fileInputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/课件.rar"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FSDataOutputStream fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/test.rar"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fsDataOutputStream);</span><br><span class="line">    IOUtils.closeStream(fileInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>HDFS文件下载</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从HDFS下载文件到本地磁盘</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">download</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/test.rar"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/test1.rar"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>定位文件获取</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载第一块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-3.1.3.tar.gz"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part1"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷(只拷贝第一个块128MB)</span></span><br><span class="line">    <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++) &#123;</span><br><span class="line">        fsDataInputStream.read(buf);</span><br><span class="line">        fileOutputStream.write(buf);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下载第二块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-3.1.3.tar.gz"</span>));</span><br><span class="line">    <span class="comment">// 3、设置指定读取的起点</span></span><br><span class="line">    fsDataInputStream.seek(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128</span>);</span><br><span class="line">    <span class="comment">// 4、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part2"</span>));</span><br><span class="line">    <span class="comment">// 5、流的对拷(拷贝剩下的两个Block块)</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下载完第一块和剩余的部分后,可通过cat hadoop-3.1.3.tar.gz.part2 &gt;&gt; hadoop-3.1.3.tar.gz.part1将剩余部分追加到第一块上,修改文件名(删去.part1),就得到完整的文件</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="HDFS的数据流"><a href="#HDFS的数据流" class="headerlink" title="HDFS的数据流"></a>HDFS的数据流</h2><ul><li><p>HDFS写数据流程</p><ul><li><p>剖析文件写入：<br><img src="HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS写数据流程"></p><ul><li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在</li><li>NameNode返回是否可以上传</li><li>客户端请求第一个Block上传到哪几个DataNode服务器上</li><li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3</li><li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</li><li>dn1、dn2、dn3逐级应答客户端</li><li>客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答</li><li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器(此后重复执行3-7步)</li></ul></li><li><p>网络拓扑-节点距离计算<br>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。节点距离：两个节点到达最近的共同祖先的距离总和<br><img src="%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97.png" alt="网络拓扑-节点距离计算"></p></li><li><p>机架感知(2.7.2版本副本节点选择,性能和安全的综合考量)</p><ul><li>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个</li><li>第二个副本和第一个副本位于相同机架，随机节点</li><li>第三个副本位于不同机架，随机节点</li></ul></li></ul></li><li><p>HDFS读数据流程<br><img src="HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS读数据流程"></p><ul><li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址</li><li>挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据</li><li>DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验)</li><li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件</li></ul></li></ul><h2 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h2><ul><li><p>NN和2NN工作机制<br>思考：NameNode中的元数据是存储在哪里的？<br>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage<br>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据<br>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并<br><img src="NameNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="NameNode工作机制"></p><ul><li><p>第一阶段：NameNode启动</p><ul><li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存</li><li>客户端对元数据进行增删改的请求</li><li>NameNode记录操作日志，更新滚动日志(先记日志,类似数据库)</li><li>NameNode在内存中对数据进行增删改</li></ul></li><li><p>第二阶段：Secondary NameNode工作</p><ul><li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果</li><li>Secondary NameNode请求执行CheckPoint</li><li>NameNode滚动正在写的Edits日志</li><li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</li><li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并</li><li>生成新的镜像文件fsimage.chkpoint</li><li>拷贝fsimage.chkpoint到NameNode</li><li>NameNode将fsimage.chkpoint重新命名成fsimage</li></ul></li><li><p>补充：<br>Fsimage：NameNode内存中元数据序列化后形成的文件。<br>Edits：记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)。<br>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息)，如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。<br>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并(所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage)。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。<br>SecondaryNameNode首先会询问NameNode是否需要CheckPoint(触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了)。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中</p></li></ul></li><li><p>Fsimage和Edits解析</p><ul><li>概念<ul><li>NameNode被格式化之后，将在/data/tmp/dfs/name/current目录中产生如下文件<br>fsimage_0000000000000000000<br>fsimage_0000000000000000000.md5<br>seen_txid<br>VERSION</li><li>Fsimage文件：HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息</li><li>Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中</li><li>seen_txid文件保存的是一个数字，就是最后一个edits_的数字</li><li><strong>每次NameNode启动的时候</strong>都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并</li></ul></li><li>查看Fsimage文件：hdfs oiv -p 文件类型(XML) -i 镜像文件 -o 转换后文件输出路径<br>例：hdfs oiv -p XML -i fsimage_0000000000000000025 o fsimage.xml<br>Fsimage中没有记录块所对应的DataNode，为什么？<br>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报</li><li>查看Edits文件：hdfs oev -p 文件类型(XML) -i 编辑日志 -o 转换后文件输出路径<br>例：hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o edits.xml<br>NameNode如何确定下次开机启动的时候合并那些Edits？<br>通过seen_txid查看</li></ul></li><li><p>CheckPoint时间设置</p><ul><li>通常情况下，SecondaryNameNode每隔一小时执行一次</li><li>一分钟检查一次操作次数</li><li>当操作次数达到1百万时，SecondaryNameNode执行一次</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-default.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure></li><li><p>NameNode故障处理</p><ul><li><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</p><ul><li>kiil -9 NameNode进程编号(用jps查看NameNode的进程编号)</li><li>删除NameNode存储的数据(data/tmp/dfs/name)：rm -rf /data/tmp/dfs/name/*</li><li>拷贝SecondaryNameNode(hadoop2)中数据到原NameNode(hadoop1)存储数据目录：scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary/* ./name/</li><li>重新启动NameNode(hadoop1)：sbin/hadoop-daemon.sh start namenode</li></ul></li><li><p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中</p><ul><li>修改hdfs-site.xml(加入下述内容)：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>kill -9 NameNode进程</li><li>删除NameNode存储的数据(同方法一)</li><li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary /data/tmp/dfs/</span><br><span class="line">cd /data/tmp/dfs/namesecondary</span><br><span class="line">rm -rf in_use.lock</span><br></pre></td></tr></table></figure><ul><li>导入检查点数据(等待一会ctrl+c结束掉)</li><li>启动NameNode：sbin/hadoop-daemon.sh start namenode</li></ul></li></ul></li><li><p>集群安全模式</p><ul><li><p>概述</p><ul><li>NameNode启动<br>NameNode启动时，首先将镜像文件(Fsimage)载入内存，并执行编辑日志(Edits)中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。<strong>这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的</strong></li><li>DataNode启动</li></ul><p><strong>系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。</strong>在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统</p><ul><li>安全模式退出判断<br>如果满足“<strong>最小副本条件</strong>”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值：dfs.replication.min=1)。<strong>在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式</strong></li></ul></li><li><p>基本语法<br>集群处于安全模式，不能执行重要操作(写操作)。集群启动完成后，自动退出安全模式</p><ul><li>查看安全模式状态：bin/hdfs dfsadmin -safemode get</li><li>进入安全模式状态：bin/hdfs dfsadmin -safemode enter</li><li>离开安全模式状态：bin/hdfs dfsadmin -safemode leave</li><li><strong>等待安全模式状态：bin/hdfs dfsadmin -safemode wait</strong></li></ul></li><li><p>案例<br>模拟等待安全模式</p><ul><li>查看当前模式：bin/hdfs dfsadmin -safemode get</li><li>先进入安全模式：bin/hdfs dfsadmin -safemode enter</li><li>创建并执行下面的脚本</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">touch safemode.sh</span><br><span class="line">vim safemode.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> safemode.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /</span><br><span class="line"></span><br><span class="line">chmod 777 safemode.sh</span><br><span class="line">./safemode.sh</span><br></pre></td></tr></table></figure><ul><li>再打开一个窗口，执行：hdfs dfsadmin -safemode leave</li><li>安全模式退出，HDFS集群上已经有上传的数据了</li></ul></li></ul></li><li><p>NameNode多目录配置</p><ul><li><p>NameNode的本地目录可以配置成多个，但每个目录存放内容相同(相当于备份)，增加了可靠性</p></li><li><p>具体配置如下</p><ul><li>在hdfs-site.xml文件中增加如下内容</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>停止集群，删除data和logs中所有数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop2：sbin/stop-yarn.sh</span><br><span class="line">hadoop2：rm -rf data/ logs/</span><br><span class="line">hadoop1：sbin/stop-dfs.sh</span><br><span class="line">hadoop2：rm -rf data/ logs/s</span><br><span class="line">hadoop3：rm -rf data/ logs/s</span><br></pre></td></tr></table></figure><ul><li>格式化集群并启动</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop1：bin/hdfs namenode -format</span><br><span class="line">hadoop1：sbin/start-dfs.sh</span><br><span class="line">hadoop2：sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><ul><li>查看结果：dfs目录下出现两个目录name1和name2</li></ul></li></ul></li></ul><h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><ul><li><p>DataNode工作机制<br><img src="DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="DataNode工作机制"></p><ul><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳</li><li>DataNode启动后向NameNode注册，通过后，周期性(1小时)的向NameNode上报所有的块信息</li><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用</li><li>集群运行中可以安全加入和退出一些机器</li></ul></li><li><p>数据完整性<br>DataNode节点保证数据完整性的方法：</p><ul><li>当DataNode读取Block的时候，它会计算CheckSum(类似crc校验位)</li><li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏</li><li>Client读取其他DataNode上的Block</li><li>DataNode在其文件创建后周期验证CheckSum</li></ul></li><li><p>掉线时限参数设置<br><img src="DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE.png" alt="DataNode掉线时限参数设置"><br>hdfs-default.xml：</p></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- heartbeat.recheck.interval单位为毫秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- dfs.heartbeat.interval单位为秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p><del>服役新数据节点(hadoop4未服役)</del></p><ul><li><p>需求：随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点</p></li><li><p>环境准备</p><ul><li>利用hadoop3主机再克隆一台hadoop4主机</li><li>修改hadoop4主机IP地址和主机名称</li><li>在hadoop1主机上将/etc/hosts下添加hadoop4的ip地址映射条目，并分发到hadoop2-4</li><li><strong>hadoop4主机删除原来HDFS文件系统留存的文件(data和log目录)——不然会发生3和4轮换出现的问题,因为3和4有着一样的data和log</strong></li><li>reboot重启加载配置</li></ul></li><li><p>服役新节点具体步骤</p><ul><li>hadoop1-3按之前步骤已启动</li><li>在hadoop4主机上单独启动：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br><span class="line">yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure><ul><li>刷新NameNode和ResourceManager：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><ul><li>刷新<a href="http://hadoop1:9870" target="_blank" rel="noopener">http://hadoop1:9870</a>web页面，等待</li><li>在hadoop4上上传文件</li><li>如果数据不均衡，可以使用命令实现集群的在平衡：sbin/start-balancer.sh</li></ul></li><li><p>结束后在workers文件中加入hadoop4，之后直接start-dfs.sh和start-yarn.sh即可启动</p></li></ul></li><li><p><del>退役旧数据节点(hadoop4未退役)</del></p><ul><li><p><del>添加白名单(hadoop4未退役)</del><br>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出</p><ul><li>在NameNode的hadoop-3.1.3/etc/hadoop目录下创建dfs.hosts文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">touch dfs.hosts</span><br><span class="line">vim dfs.hosts</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dfs.hosts(不添加hadoop4,不允许有空行和空格)</span></span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><ul><li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>配置文件分发：xsync hdfs-site.xml;xsync dfs.hosts</li><li>刷新NameNode和ResourceManager：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><ul><li>在web页面刷新等待</li></ul></li><li><p><del>黑名单退役(hadoop4未退役)</del><br>在黑名单上面的主机都会被强制退出。<strong>注意：不允许白名单和黑名单中同时出现同一个主机名称</strong></p><ul><li>在hadoop-3.1.3/etc/hadoop下创建dfs.hosts.exclude文件，并加入要退役节点</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">touch dfs.hosts.exclude</span><br><span class="line">vim dfs.hosts.exclude</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dfs.hosts.exclude</span></span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><ul><li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>刷新NameNode和ResourceManager：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><ul><li>刷新web页面等待</li></ul></li></ul></li><li><p>DataNode多目录配置</p><ul><li>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</li><li>需要在hdfs-site.xml上修改配置</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>关闭当前运行的hadoop节点</li><li>删除各节点的data和log目录</li><li>格式化dataNode</li><li>重启部署hadoop节点</li></ul></li></ul><h2 id="HDFS2-X新特性"><a href="#HDFS2-X新特性" class="headerlink" title="HDFS2.X新特性"></a>HDFS2.X新特性</h2><ul><li><p>集群间数据拷贝</p><ul><li>scp实现两个远程主机之间的文件复制</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从当前主机向目的主机 推 push</span></span><br><span class="line">scp -r hello.txt root@hadoop2:/user/sobxiong/hello.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从目的主机向当前主机 拉 pull</span></span><br><span class="line">scp -r root@hadoop2:/user/sobxiong/hello.txt hello.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过本主机中转实现两个远程主机的文件复制</span></span><br><span class="line">scp -r root@hadoop2:/user/sobxiong/hello.txt root@hadoop3:/user/sobxiong</span><br></pre></td></tr></table></figure><ul><li>采用distcp命令实现<strong>两个Hadoop集群之间</strong>的递归数据复制</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop distcp hdfs://haoop1:9000/user/sobxiong/hello.txt hdfs://hadoop2:9000/user/sobxiong/hello.txt</span><br></pre></td></tr></table></figure></li><li><p>小文件存档</p><ul><li><p>HDFS存储小文件弊端<br>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为<strong>大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关</strong>。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB</p></li><li><p>解决存储小文件办法之一<br>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存<br><img src="%E5%B0%8F%E6%96%87%E4%BB%B6%E5%BD%92%E6%A1%A3.png" alt="小文件归档"></p></li><li><p>实际操作</p><ul><li>启动YARN进程：start-yarn.sh(hadoop2)</li><li>归档文件(归档后的路径不得实现存在)<br>把/sobxiong目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/sobxiongOutput路径下：hadoop archive -archiveName input.har -p /sobxiong /sobxiongOutput</li><li>查看归档：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 普通查看文件命令</span></span><br><span class="line">hadoop fs -ls R /sobxiongOutput/input.har</span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup          0 2020-06-24 16:06 /sobxiongOutput/input.har/_SUCCESS</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup        305 2020-06-24 16:06 /sobxiongOutput/input.har/_index</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup         23 2020-06-24 16:06 /sobxiongOutput/input.har/_masterindex</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup     317029 2020-06-24 16:06 /sobxiongOutput/input.har/part-0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 采用har格式查看文件(可以像以往一样操作内部文件,也需要har格式)</span></span><br><span class="line">hadoop fs -ls R har:///sobxiongOutput/input.har</span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup      84942 2020-06-21 11:38 har:///sobxiongOutput/input.har/1tset.png</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup      84942 2020-06-21 11:33 har:///sobxiongOutput/input.har/test.png</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup     147145 2020-06-23 13:52 har:///sobxiongOutput/input.har/test6.txt</span></span><br></pre></td></tr></table></figure><ul><li>解归档文件：hadoop fs -cp har:///sobxiongOutput/input.har/* /</li></ul></li></ul></li><li><p>回收站<br>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用</p><ul><li>开启回收站功能参数说明：<ul><li>默认值fs.trash.interval = 0，0表示禁用回收站；<strong>其他值表示设置文件的存活时间(分钟为单位)</strong></li><li>默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。该值设置和fs.trash.interval的参数值相同(要求fs.trash.checkpoint.interval &lt;= fs.trash.interval)</li></ul></li><li>回收站工作机制<br><img src="%E5%9B%9E%E6%94%B6%E7%AB%99%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="回收站工作机制"></li><li>启动回收站：修改core-site.xml，配置垃圾回收时间为1分钟</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>查看回收站：回收站在集群中的路径：/user/sobxiong/.Trash/</li><li>修改访问回收站用户名称：进入垃圾回收站用户名称，默认是dr.who，修改为sobxiong(同样是core-site.xml)</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>恢复回收站数据：hadoop fs -mv /user/sobxiong/.Trash/Current/user/sobxiong/input /</li><li>清空回收站：hadoop fs -expunge</li></ul></li><li><p>快照管理</p><ul><li><p>命令介绍<br><img src="%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86.png" alt="快照管理"></p></li><li><p>实际操作</p><ul><li>开启/禁用指定目录的快照功能：hdfs dfsadmin -allowSnapshot(-disallowSnapshot) /user/sobxiong/input</li><li>对目录创建快照</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -createSnapshot /user/sobxiong/input</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 快照和源文件使用相同数据</span></span><br><span class="line">hdfs dfs -ls R /user/sobxiong/input/.snapshot/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定名称创建快照</span></span><br><span class="line">hdfs dfs -createSnapshot /user/sobxiong/input test</span><br></pre></td></tr></table></figure><ul><li>重命名快照：hdfs dfs -renameSnapshot /user/sobxiong/input test test01</li><li>列出当前用户所有可快照目录：hdfs lsSnapshottableDir</li><li><strong>比较两个快照目录的不同之处(可以是源文件和快照,使用’.’,此时”.snapshot/name”用于指定具体快照)</strong>：hdfs snapshotDiff /user/sobxiong/input . .snapshot/test01</li><li>恢复快照：hdfs dfs -cp /user/sobxiong/input/.snapshot/s20200624-134303.027 /</li></ul></li></ul></li></ul><h2 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h2><ul><li><p>MapReduce定义</p><ul><li>MapReduce是一个<strong>分布式运算程序的编程框架</strong>，是用户开发“基于Hadoop的数据分析应用”的核心框架</li><li>MapReduce核心功能是<strong>将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序</strong>，并发运行在一个Hadoop集群上</li></ul></li><li><p>MapReduce优缺点</p><ul><li>优点：<ul><li>MapReduce易于编程<br>它<strong>简单地实现一些接口，就可以完成一个分布式程序</strong>，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行</li><li>良好的扩展性(hadoop)<br>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力</li><li>高容错性<br>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。<strong>比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</strong>，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的</li><li>适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力</li></ul></li><li>缺点：<ul><li>不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果</li><li>擅长流式计算<br>流式计算的输入数据是动态的，<strong>而MapReduce的输入数据集是静态的，不能动态变化</strong>。这是因为MapReduce自身的设计特点决定了数据源必须是静态的</li><li>不擅长DAG(有向图)计算<br>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，<strong>每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下</strong></li></ul></li></ul></li><li><p>核心思想<br><img src="MapRecude%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3.png" alt="MapRecude核心编程思想"></p></li><li><p>MapReduce进程<br>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p><ul><li>MrAppMaster：负责整个程序的过程调度及状态协调</li><li>MapTask：负责Map阶段的整个数据处理流程</li><li>ReuceTask：负责Reduce阶段的整个数据处理流程</li></ul></li><li><p>常用数据序列化类型</p><table><thead><tr><th>Java类型</th><th>Hadoop Writable类型</th></tr></thead><tbody><tr><td>boolean</td><td>BooleanWritable</td></tr><tr><td>byte</td><td>ByteWritable</td></tr><tr><td>int</td><td>IntWritable</td></tr><tr><td>float</td><td>FloatWritable</td></tr><tr><td>long</td><td>LongWritable</td></tr><tr><td>double</td><td>DoubleWritable</td></tr><tr><td>String</td><td>Text</td></tr><tr><td>map</td><td>MapWritable</td></tr><tr><td>array</td><td>ArrayWritable</td></tr></tbody></table></li><li><p>MapReduce编程规范<br>用户编写的程序分成三个部分：Mapper、Reducer和Driver</p><ul><li>Mapper<ul><li>用户自定义的Mapper要继承自己的父类</li><li>Mapper的输入数据是KV对的形式(KV的类型可自定义)</li><li>Mapper中的业务逻辑写在map()方法中</li><li>Mapper的输出数据是KV对的形式(KV的类型可自定义)</li><li><strong>map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次</strong></li></ul></li><li>Reducer<ul><li>用户自定义的Reducer要继承自己的父类</li><li>Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</li><li>Reducer的业务逻辑写在reduce()方法中</li><li><strong>ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</strong></li></ul></li><li>Driver：相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象</li></ul></li><li><p>WordCount案例实操</p><ul><li><p>需求：在给定的文本文件中统计输出每一个单词出现的总次数</p></li><li><p>需求分析<br><img src="WordCount%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90.png" alt="WordCount需求分析"></p></li><li><p>环境准备</p><ul><li>创建maven空项目</li><li>改pom</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.13.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>在resources资源文件夹下新建log4j.properties文件</li></ul><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure><ul><li><p>编写MapReduce程序</p><ul><li>编写mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* map阶段</span></span><br><span class="line"><span class="comment">* KEYIN：输入数据的key类型(默认写LongWritable:偏移量)</span></span><br><span class="line"><span class="comment">* VALUEIN：输入数据的value类型</span></span><br><span class="line"><span class="comment">* KEYOUT：输出数据的key类型</span></span><br><span class="line"><span class="comment">* VALUEOUT：输出数据的value类型</span></span><br><span class="line"><span class="comment">* &lt;sobxiong,1&gt;输出数据</span></span><br><span class="line"><span class="comment">* 输出作为reduce阶段的输入</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// sobxiong sobxiong</span></span><br><span class="line">        <span class="comment">// 1、获取一行</span></span><br><span class="line">        String lineStr = value.toString();</span><br><span class="line">        <span class="comment">// 2、切割单词</span></span><br><span class="line">        String[] words = lineStr.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 3、循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="comment">// &lt;sobxiong,1&gt;</span></span><br><span class="line">            k.set(word);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* reduce阶段</span></span><br><span class="line"><span class="comment">* KEYIN,VALUEIN：map阶段输出的kv</span></span><br><span class="line"><span class="comment">* KEYOUT,VALUEOUT：reduce输出的kv</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 1、累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2、写出 &lt;sobxiong,2&gt;</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Driver驱动类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、获取Job对象</span></span><br><span class="line">      Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">      Job job = Job.getInstance(conf);</span><br><span class="line">      <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">      job.setJarByClass(WordCountDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">      job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">      job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">      job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">      FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">      FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">      <span class="comment">// 7、提交job</span></span><br><span class="line">      <span class="comment">// job.submit();</span></span><br><span class="line">      <span class="comment">// true：打印一些信息</span></span><br><span class="line">      <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">      <span class="comment">// 额外</span></span><br><span class="line">      System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>本地测试：启动旁边的下箭头Edit Configuration，新增Application，选择Main Class并在Program arguments中加入两个参数中间用空格隔开，前者是input文件所在文件夹，后者是输出文件夹，要求不能存在(不然会出错)。例如：/Users/sobxiong/Downloads/input /Users/sobxiong/Downloads/ouputTest</p></li><li><p>在集群上测试</p><ul><li>用maven打jar包，添加打包插件依赖</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.xiong.hadoop.WordCountDriver<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>将程序打成jar包，maven install即可</li><li>将获取的两个jar包(一个不带依赖,另一带)，将不带依赖的jar上传到hadoop1主机上</li><li>启动Hadoop集群</li><li>执行WordCount程序：hadoop jar WordCount.jar com.xiong.hadoop.WordCountDriver /sobxiong /outputTest(第四个参数为启动的主类名,第五个为输入文件所在文件夹,第六个为输出文件夹——不能事先存在)</li></ul></li></ul></li></ul></li></ul><h2 id="Hadoop序列化"><a href="#Hadoop序列化" class="headerlink" title="Hadoop序列化"></a>Hadoop序列化</h2><ul><li><p>序列化概述</p><ul><li>什么是序列化：<em>序列化</em>就是把<strong>内存中的对象，转换成字节序列</strong>(或其他数据传输协议)以便于存储到磁盘(持久化)和网络传输；<em>反序列化</em>就是<strong>将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象</strong></li><li>为什么要序列化：一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而<strong>序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机</strong></li><li>为什么不用Java的序列化：Java的序列化是一个重量级序列化框架(Serializable)，一个对象被序列化后，会附带很多额外的信息(各种校验信息，Header，继承体系等)，不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制(Writable)</li><li>Hadoop序列化特点：<ul><li>紧凑：高效使用存储空间</li><li>快速：读写数据的额外开销小</li><li>可扩展：随着通信协议的升级而可升级</li><li>互操作：支持多语言的交互</li></ul></li></ul></li><li><p>自定义bean对象实现序列化接口(Writable)<br>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。<br>具体实现bean对象序列化步骤如下7步：</p><ul><li>实现Writable接口</li><li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li><li>重写序列化方法</li><li>重写反序列化方法(<strong>注意反序列化的顺序和序列化的顺序完全一致</strong>)</li><li>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用</li><li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框架中的Shuffle过程要求对key必须能排序</li></ul></li><li><p>序列化案例实操</p><ul><li><p>需求：统计每一个手机号耗费的总上行流量、下行流量、总流量</p></li><li><p>案例分析<br><img src="%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="序列化案例分析"></p></li><li><p>编写MapReduce程序</p><ul><li>编写流量统计的Bean对象</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">测试数据</span></span><br><span class="line"><span class="comment">1;13736230513;192.196.100.1;www.atguigu.com;2481;24681;200</span></span><br><span class="line"><span class="comment">2;13846544121;192.196.100.2;264;0;200</span></span><br><span class="line"><span class="comment">3;13956435636;192.196.100.3;132;1512;200</span></span><br><span class="line"><span class="comment">4;13966251146;192.168.100.1;240;0;404</span></span><br><span class="line"><span class="comment">5;18271575951;192.168.100.2;www.atguigu.com;1527;2106;200</span></span><br><span class="line"><span class="comment">6;84188413;192.168.100.3;www.atguigu.com;4116;1432;200</span></span><br><span class="line"><span class="comment">7;13590439668;192.168.100.4;1116;954;200</span></span><br><span class="line"><span class="comment">8;15910133277;192.168.100.5;wwww.haol23.com;3156;2936;200</span></span><br><span class="line"><span class="comment">9;13729199489;192.168.100.6;240;0;200</span></span><br><span class="line"><span class="comment">10;13630577991;192.168.100.7;www.shouhu.com;6960;690;200</span></span><br><span class="line"><span class="comment">11;15043685818;192.168.100.8;www.baidu.com;3659;3538;200</span></span><br><span class="line"><span class="comment">12;15959002129;192.168.100.9;www.atguigu.com;1938;180;500</span></span><br><span class="line"><span class="comment">13;13560439638;192.168.100.10;918;4938;200</span></span><br><span class="line"><span class="comment">14;13470253144;192.168.100.11;180;180;200</span></span><br><span class="line"><span class="comment">15;13682846555;192.168.100.12;wwww.qq.com;1938;2910;200</span></span><br><span class="line"><span class="comment">16;13992314666;192.168.100.13;www.gaga.com;3008;3720;200</span></span><br><span class="line"><span class="comment">17;13509468723;192.168.100.14;www.qinghua.com;7335;110349;404</span></span><br><span class="line"><span class="comment">18;18390173782;192.168.100.15;www.sogou.com;9531;2412;200</span></span><br><span class="line"><span class="comment">19;13975057813;192.168.100.16;www.baidu.com;11058;48243;200</span></span><br><span class="line"><span class="comment">20;13768778790;192.168.100.17;120;120;200</span></span><br><span class="line"><span class="comment">21;13568436656;192.168.100.18;www.alibaba.com;2481;24681;200</span></span><br><span class="line"><span class="comment">22;13568436656;192.168.100.19;1116;954;200</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 上行流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">  <span class="comment">// 下行流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">  <span class="comment">// 总流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line">  <span class="comment">// 空参构造,为了后续反射</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 序列化方法</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      dataOutput.writeLong(upFlow);</span><br><span class="line">      dataOutput.writeLong(downFlow);</span><br><span class="line">      dataOutput.writeLong(sumFlow);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 反序列化方法</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      <span class="comment">// 必须要求和序列化方法顺序一致</span></span><br><span class="line">      upFlow = dataInput.readLong();</span><br><span class="line">      downFlow = dataInput.readLong();</span><br><span class="line">      sumFlow = dataInput.readLong();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">      <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">      <span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、获取一行</span></span><br><span class="line">      String lineStr = value.toString();</span><br><span class="line">      <span class="comment">// 2、切割\t</span></span><br><span class="line">      String[] fields = lineStr.split(<span class="string">"\t"</span>);</span><br><span class="line">      <span class="comment">// 3、封装对象</span></span><br><span class="line">      k.set(fields[<span class="number">1</span>]);</span><br><span class="line">      <span class="keyword">long</span> upFlow = Long.parseLong(fields[fields.length - <span class="number">3</span>]);</span><br><span class="line">      <span class="keyword">long</span> downFlow = Long.parseLong(fields[fields.length - <span class="number">2</span>]);</span><br><span class="line">      flowBean.set(upFlow, downFlow);</span><br><span class="line">      <span class="comment">// 4、写出</span></span><br><span class="line">      context.write(k, flowBean);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FlowBean v = <span class="keyword">new</span> FlowBean();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、累加求和</span></span><br><span class="line">      <span class="keyword">long</span> sumUpFlow = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">long</span> sumDownFlow = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">          sumUpFlow += value.getUpFlow();</span><br><span class="line">          sumDownFlow += value.getDownFlow();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      v.set(sumUpFlow, sumDownFlow);</span><br><span class="line">      context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Driver驱动类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取job对象</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf);</span><br><span class="line">    <span class="comment">// 2、设计jar路径</span></span><br><span class="line">    job.setJarByClass(FlowDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">    job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">    job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">    job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 7、提交job</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h2><ul><li><p>InputFormat数据输入</p><ul><li><p>切片与MapTask并行度决定机制</p><ul><li>问题引出：MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度<br>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</li><li>MapTask并行度决定机制</li></ul><p><strong>数据块</strong>：Block是HDFS物理上把数据分成一块一块<br><strong>数据切片</strong>：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储<br><img src="%E6%95%B0%E6%8D%AE%E5%88%87%E7%89%87%E4%B8%8EMapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6.png" alt="数据切片与MapTask并行度决定机制"></p></li><li><p>Job提交流程源码和切片源码详解</p><ul><li>Job提交流程源码详解</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion();</span><br><span class="line">submit();</span><br><span class="line">  <span class="comment">// 1、建立连接</span></span><br><span class="line">  connect();</span><br><span class="line">    <span class="comment">// 1)创建提交Job的代理</span></span><br><span class="line">    <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">      <span class="comment">// 判断是本地yarn还是远程</span></span><br><span class="line">      initialize(jobTrackAddr, conf);</span><br><span class="line">  <span class="comment">// 2、提交job</span></span><br><span class="line">  submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">  <span class="comment">// 1)创建给集群提交数据的Stag路径</span></span><br><span class="line">  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">  <span class="comment">// 2)获取jobid,并创建Job路径</span></span><br><span class="line">  JobID jobId = submitClient.getNewJobID();</span><br><span class="line">  <span class="comment">// 3)拷贝jar包到集群</span></span><br><span class="line">  copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">  <span class="comment">// 4)计算切片,生成切片规划文件</span></span><br><span class="line">  writeSplits(job, submitJobDir);</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">    input.getSplits(job);</span><br><span class="line">  <span class="comment">// 5)向Stag路径写XML配置文件</span></span><br><span class="line">  writeConf(conf, submitJobFile);</span><br><span class="line">  conf.writeXml(out);</span><br><span class="line">  <span class="comment">// 6)提交Job,返回提交状态</span></span><br><span class="line">  status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure><p><img src="Job%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.png" alt="Job提交流程源码分析"></p><ul><li>FileInputFormat切片源码解析(input.getSplits(job))<ul><li>程序先找到你数据存储的目录</li><li>开始遍历处理(规划切片)目录下的每一个文件</li><li>遍历第一个文件ss.txt<ul><li>获取文件大小fs.sizeOf(ss.txt)</li><li>计算切片大小：computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M(YARN集群默认128M——2.x,62M-1.x;本地运行默认32M)</li><li><strong>默认情况下，切片大小=blocksize</strong></li><li>开始切，形成第1个切片：ss.txt—0:128M、第2个切片ss.txt—128:256M、第3个切片ss.txt—256M:300M(<strong>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片</strong>)</li><li>将切片信息写到一个切片规划文件中</li><li>整个切片的核心过程在getSplit()方法中完成</li><li><strong>InputSplit只记录了切片的元数据信息</strong>，比如起始位置、长度以及所在的节点列表等</li></ul></li><li><strong>提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数</strong></li></ul></li></ul></li><li><p>FileInputFormat切片机制</p><ul><li><p>切片机制</p><ul><li>简单地按照文件的内容长度进行切片</li><li>切片大小，默认等于Block大小</li><li><strong>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</strong></li></ul></li><li><p>案例分析</p><ul><li>输入数据有两个文件：file1.txt - 320M;file2.txt - 10M</li><li>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</li></ul><table><thead><tr><th>文件</th><th>切片区间</th></tr></thead><tbody><tr><td>file1.txt.split1</td><td>0~128</td></tr><tr><td>file1.txt.split2</td><td>129~256</td></tr><tr><td>file1.txt.split3</td><td>257~320</td></tr><tr><td>file2.txt.split1</td><td>0~10</td></tr></tbody></table></li><li><p>切片大小参数配置</p><ul><li>源码中计算切片大小的公式：Math.max(minSize, Math.min(maxSize, blockSize));<br>mapreduce.input.fileinputformat.split.minsize=<strong>1</strong> 默认值为1<br>mapreduce.input.fileinputformat.split.maxsize=<strong>Long.MAXValue</strong> 默认值Long.MAXValue</li></ul><p><strong>默认情况下，切片大小=blocksize</strong></p><ul><li>切片大小设置<br>maxsize(切片最大值)：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值<br>minsize(切片最小值)：参数调的比blockSize大，则可以让切片变得比blockSize还大</li><li>获取切片信息API</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取切片的文件名称</span></span><br><span class="line">String name = inputSplit.getPath().getName();</span><br><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line">FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br></pre></td></tr></table></figure></li></ul></li><li><p>CombineTextInputFormat切片机制<br>框架默认的TextInputFormat切片机制是对任务按文件规划切片，<strong>不管文件多小，都会是一个单独的切片</strong>，都会交给一个MapTask，<strong>这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下</strong></p><ul><li>应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理</li><li>虚拟存储切片最大值设置：CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);(注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值)</li><li>切片机制切片机制：生成切片过程包括<strong>虚拟存储过程和切片过程二部分</strong><br><img src="CombineTextInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6.png" alt="CombineTextInputFormat切片机制"><ul><li>虚拟存储过程<br>将输入目录下所有文件的大小依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件<strong>均分</strong>成2个虚拟存储块(防止出现太小切片)<br>例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成(2.01M和2.01M)两个文件</li><li>切片过程<ul><li>判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片</li><li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片</li><li><strong>测试举例</strong>：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M、(2.55M、2.55M)、3.4M、(3.4M、3.4M)。最终会形成3个切片，大小分别为：(1.7M + 2.55M)，(2.55M + 3.4M)，(3.4M + 3.4M)</li></ul></li></ul></li></ul></li><li><p>CombineTextInputFormat案例实操</p><ul><li><p>需求：将输入的大量小文件合并成一个切片统一处理</p><ul><li>输入数据：准备4个小文件</li><li>期望：期望一个切片处理4个文件</li></ul></li><li><p>实现过程</p><ul><li>不做任何处理，运行之前的WordCount案例程序，观察切片个数为4——(number of splits:4)</li><li>在WordcountDriver中增加如下代码(设置切片最大值为4m)，运行程序，观察运行的切片个数为3</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure><ul><li>在WordcountDriver中增加如下代码(设置切片最大值为20m)，运行程序，观察运行的切片个数为1(代码同上,值改为20971520)</li></ul></li></ul></li><li><p>FileInputFormat实现类<br>思考：在运行MapReduce程序时，<em>输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等</em>。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢?<br>FileInputFormat常见的接口实现类包括：<strong>TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat</strong>等</p><ul><li>TextInputFormat<br>TextInputFormat是默认的FileInputFormat实现类。<strong>按行读取每条记录，键是存储该行在整个文件中的起始字节偏移量——LongWritable类型；值是这行的内容，不包括任何行终止符(换行符和回车符)——Text类型</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例：</span><br><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对：</span><br><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure><ul><li>KeyValueTextInputFormat<br>每一行均为一条记录，被分隔符分割为&lt;key,value&gt;对。<strong>可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “\t”)来设定分隔符——默认分隔符是tab(\t)</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例(其中——&gt;表示一个水平方向的制表符)：</span><br><span class="line">line1 ——&gt;Rich learning form</span><br><span class="line">line2 ——&gt;Intelligent learning engine</span><br><span class="line">line3 ——&gt;Learning more convenient</span><br><span class="line">line4 ——&gt;From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对(键是每行排在制表符之前的Text序列)：</span><br><span class="line">(line1,Rich learning form)</span><br><span class="line">(line2,Intelligent learning engine)</span><br><span class="line">(line3,Learning more convenient)</span><br><span class="line">(line4,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure><ul><li>NLineInputFormat<br>如果使用NlineInputFormat，代表每个map进程处理的<strong>InputSplit不再按Block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数 / N = 切片数，如果不整除，切片数 = 商 + 1</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例：</span><br><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 如果N是2,则每个输入分片包含两行。开启2个MapTask(键和值与TextInputFormat生成的一样)：</span><br><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br><span class="line">&#x2F;&#x2F; 另一个mapper则收到后两行：</span><br><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure></li><li><p>KeyValueTextInputFormat使用案例</p><ul><li><p>需求：统计输入文件中每一行的第一个单词相同的行数</p></li><li><p>案例分析<br><img src="KeyValueTextInputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="KeyValueTextInputFormat案例分析"></p></li><li><p>代码实现</p><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable intWritable = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、封装对象</span></span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      context.write(key, intWritable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable intWritable = <span class="keyword">new</span> IntWritable();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、累加求和</span></span><br><span class="line">      <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">          sum += value.get();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      intWritable.set(sum);</span><br><span class="line">      context.write(key, intWritable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">" "</span>);</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(KVTextDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(KVTextMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(KVTextReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setInputFormatClass(KeyValueTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>NLineInputFormat使用案例</p><ul><li><p>需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中</p></li><li><p>案例分析<br><img src="NLineInputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="NLineInputFormat案例分析"></p></li><li><p>代码实现</p><ul><li>Mapper和Reducer类参照WordCount</li><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 设置切片InputSplit中划分三条记录</span></span><br><span class="line">  NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">  <span class="comment">// 使用NLineInputFormat处理记录数</span></span><br><span class="line">  job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(NLineDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(NLineMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(NLineReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>观察控制台打印的number of splits</p></li></ul></li><li><p>自定义InputFormat<br>在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题<br>自定义InputFormat步骤如下：</p><ul><li>自定义一个类继承FileInputFormat</li><li>改写RecordReader，实现一次读取一个完整文件封装为KV</li><li>在输出时使用SequenceFileOutPutFormat输出合并文件</li></ul></li><li><p>自定义InputFormat案例实操</p><ul><li><p>需求：将多个小文件合并成一个SequenceFile文件(SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式)，SequenceFile里面存储着多个文件，存储的形式为key——文件路径 + 名称，value——文件内容</p></li><li><p>案例分析<br><img src="%E8%87%AA%E5%AE%9A%E4%B9%89InputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="自定义InputFormat案例分析"></p></li><li><p>代码实现</p><ul><li>自定义InputFormat</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      WholeRecordReader recordReader = <span class="keyword">new</span> WholeRecordReader();</span><br><span class="line">      recordReader.initialize(split, context);</span><br><span class="line">      <span class="keyword">return</span> recordReader;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>自定义RecordReader类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FileSplit split;</span><br><span class="line">  <span class="keyword">private</span> Configuration configuration;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> BytesWritable v = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">  <span class="keyword">boolean</span> isProgress = <span class="keyword">true</span>;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 初始化</span></span><br><span class="line">      <span class="keyword">this</span>.split = (FileSplit) split;</span><br><span class="line">      configuration = context.getConfiguration();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 核心业务逻辑</span></span><br><span class="line">      <span class="comment">// 每个文件创建一次reader</span></span><br><span class="line">      <span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line">          <span class="comment">// 1、获取fs对象</span></span><br><span class="line">          Path path = split.getPath();</span><br><span class="line">          FileSystem fileSystem = path.getFileSystem(configuration);</span><br><span class="line">          <span class="comment">// 2、获取输入流</span></span><br><span class="line">          FSDataInputStream fis = fileSystem.open(path);</span><br><span class="line">          <span class="comment">// 3、拷贝</span></span><br><span class="line">          <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) split.getLength()];</span><br><span class="line">          IOUtils.readFully(fis, buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line">          <span class="comment">// 4、封装v</span></span><br><span class="line">          v.set(buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line">          <span class="comment">// 5、封装key</span></span><br><span class="line">          k.set(path.toString());</span><br><span class="line">          <span class="comment">// 6、关闭资源</span></span><br><span class="line">          IOUtils.closeStream(fis);</span><br><span class="line">          isProgress = <span class="keyword">false</span>;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> k; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> v; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      context.write(key, value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 循环写出</span></span><br><span class="line">      <span class="keyword">for</span> (BytesWritable value : values) &#123;</span><br><span class="line">          context.write(key, value);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  job.setInputFormatClass(WholeFileInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputFormatClass(SequenceFileOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 2、设计jar路径</span></span><br><span class="line">  job.setJarByClass(SequenceFileDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">  job.setMapperClass(SequenceFileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(SequenceFileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>MapReduce工作流程</p><ul><li>流程示意图<br><img src="MapReduce%E8%AF%A6%E7%BB%86%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B(1).png" alt="MapReduce详细工作流程(1)"><br><img src="MapReduce%E8%AF%A6%E7%BB%86%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B(2).png" alt="MapReduce详细工作流程(2)"></li><li>流程详解<br>上面的流程是整个MapReduce的全部工作流程，Shuffle过程是从第7步开始到第16步结束，具体Shuffle过程详解，如下：<ul><li>MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</li><li>从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</li><li>多个溢出文件会被合并成大的溢出文件</li><li>在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li><li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li><li>ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并(归并排序)</li><li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程——从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法</li></ul></li><li>注意<br>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。<br>缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M</li></ul></li><li><p>Shuffle</p><ul><li><p>Shuffle机制：Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle<br><img src="Shuffle%E6%9C%BA%E5%88%B6.png" alt="Shuffle机制"></p></li><li><p>Partition分区</p><ul><li>问题引出：要求将统计结果按照条件输出到不同文件中(分区)。比如：将统计结果按照手机归属地不同省份输出到不同文件中(分区)</li><li>默认Partitionr分区：默认分区是根据key的hashCode对ReduceTasks个数取模得到的(如果分区数大于1)。用户没法控制哪个key存储到哪个分区</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(JobConf job)</span> </span>&#123;&#125;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K2 key, V2 value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>自定义Partitioner步骤</p><ul><li>自定义类继承Partitioner，重写getPartition()方法</li><li>在Job驱动中，设置自定义Partitioner：job.setPartitionerClass(CustomPartitioner.class);</li><li>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask：job.setNumReduceTasks(5);</li></ul></li><li><p><strong>分区总结</strong></p><ul><li>如果ReduceTask的数量 &gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx</li><li>如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，会抛出异常</li><li>如果ReduceTask的数量 = 1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000</li><li>分区号必须从零开始，逐一累加</li></ul></li><li><p>案例分析：假设自定义分区数为5，则</p><ul><li>job.setNumReduceTasks(1)：会正常运行，只不过会产生一个输出文件</li><li>job.setNumReduceTasks(2)：会报错</li><li>job.setNumReduceTasks(6)：大于5，程序会正常运行，会产生空文件part-r-00005</li></ul></li></ul></li><li><p>Partition分区案例实操</p><ul><li><p>需求：将统计结果按照手机归属地不同省份输出到不同文件中(分区)</p></li><li><p>案例分析<br><img src="Partition%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Partition分区案例分析"></p></li><li><p>实操</p><ul><li>在FlowBean案例基础上，增加一个分区类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// key是手机号,value是流量信息</span></span><br><span class="line">    <span class="comment">// 获取手机号前三位</span></span><br><span class="line">    String prePhoneNum = text.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="string">"136"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">2</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> partition;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在Driver驱动主函数中添加自定义数据分区设置和ReduceTask设置</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定自定义数据分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 指定ReduceTask的数目</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">6</span>);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>WritableComparable排序</p><ul><li><p>排序概述：<br>排序是MapReduce框架中最重要的操作之一。MapTask和ReduceTask均会对数据<strong>按照key进行排序</strong>，该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是<strong>按照字典顺序</strong>排序，且实现该排序的方法是<strong>快速排序</strong>。<br>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。<br>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</p></li><li><p>排序的分类</p><ul><li>部分排序：MapReduce根据输入记录的键对数据集排序。保证<strong>输出的每个文件内部有序</strong></li><li>全排序：<strong>最终输出结果只有一个文件，且文件内部有序</strong>。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构</li><li>辅助排序(GroupingComparator分组)：在Reduce端对key进行分组。应用——在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序</li><li>二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序</li></ul></li><li><p>自定义排序WritableComparable</p><ul><li><p>原理分析：bean对象做为key传输，需要<strong>实现WritableComparable接口重写compareTo方法</strong>，就可以实现排序</p></li><li><p>案例实操(全排序)</p><ul><li><p>需求：对FlowBean案例产生的结果再次对总流量进行排序</p></li><li><p>案例分析<br><img src="WritableComparable%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="WritableComparable排序案例分析"></p></li><li><p>代码实现</p><ul><li>FlowBean对象在之前案例基础上实现WritableComparable接口，实现compareTo()方法</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 按总流量倒序</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; bean.sumFlow ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、封装对象</span></span><br><span class="line">    v.set(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">long</span> upFlow = Long.parseLong(fields[fields.length - <span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">long</span> downFlow = Long.parseLong(fields[fields.length - <span class="number">2</span>]);</span><br><span class="line">    flowBean.set(upFlow, downFlow);</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(flowBean, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">        context.write(value, key);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 2、设计jar路径</span></span><br><span class="line">  job.setJarByClass(FlowCountSortDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">  job.setMapperClass(FlowCountSortMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(FlowCountSortReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>Combiner合并</p><ul><li><p>Combiner介绍<br><img src="Combiner%E4%BB%8B%E7%BB%8D.png" alt="Combiner介绍"></p></li><li><p>自定义Combiner实现步骤</p><ul><li>自定义一个Combiner继承Reducer，重写reduce方法</li><li>在Driver驱动类中设置：job.setCombinerClass(xxCombiner.class);</li></ul></li><li><p>自定义Combiner实操</p><ul><li><p>需求：统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能</p></li><li><p>案例分析<br><img src="Combiner%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Combiner案例分析"></p></li><li><p>代码实现</p><ul><li>方案一</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 新建WordCountCombiner类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、累加求和</span></span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">        sum += value.get();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、写出</span></span><br><span class="line">    v.set(sum);</span><br><span class="line">    context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在WordCountDriver驱动类中指定Combiner</span></span><br><span class="line">job.setCombinerClass(WordcountCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><ul><li>方案二：将WordCountReducer作为Combiner</li></ul></li><li><p>结果查看：控制台打印中的Map-Reduce Framework中的Combine记录</p></li></ul></li></ul></li><li><p>GroupingComparator分组(辅助排序)</p><ul><li><p>简要介绍：对Reduce阶段的数据根据某一个或几个字段进行分组</p></li><li><p>分组排序步骤</p><ul><li>自定义类继承WritableComparator</li><li>重写compare()方法</li><li>创建一个构造将比较对象的类传给父类</li></ul></li><li><p>GroupingComparator分组实操</p><ul><li><p>需求：求出每一个订单中最贵的商品</p></li><li><p>案例分析<br><img src="GroupingComparator%E5%88%86%E7%BB%84%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="GroupingComparator分组案例分析"></p><ul><li>利用“订单id和成交金额price”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序。Reduce将从Map中获取排序好的数据</li><li>在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品</li></ul></li><li><p>代码实现</p><ul><li>编写订单信息OrderBean类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 忽略了空参/全参构造器、getter/setter和toString方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> orderId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">double</span> price;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean bean)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result;</span><br><span class="line">    <span class="keyword">if</span> (orderId &gt; bean.orderId) &#123;</span><br><span class="line">        result = <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (orderId &lt; bean.orderId) &#123;</span><br><span class="line">        result = -<span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        result = price &gt; bean.price ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeInt(orderId);</span><br><span class="line">    out.writeDouble(price);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    orderId = in.readInt();</span><br><span class="line">    price = in.readDouble();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderGroupingComparator类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(OrderBean<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 要求只要id相同,就认为是相同的key</span></span><br><span class="line">    OrderBean aBean = (OrderBean) a;</span><br><span class="line">    OrderBean bBean = (OrderBean) b;</span><br><span class="line">    <span class="keyword">if</span> (aBean.getOrderId() == bBean.getOrderId()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> aBean.getOrderId() &gt; bBean.getOrderId() ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderMapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> OrderBean orderBean = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、封装对象</span></span><br><span class="line">    orderBean.setOrderId(Integer.parseInt(fields[<span class="number">0</span>]));</span><br><span class="line">    orderBean.setPrice(Double.parseDouble(fields[<span class="number">2</span>]));</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(orderBean, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderReducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 输出第一个</span></span><br><span class="line">    context.write(key, NullWritable.get());</span><br><span class="line">    <span class="comment">// 循环几次输出前几</span></span><br><span class="line">    <span class="comment">//for (NullWritable value : values) &#123;</span></span><br><span class="line">    <span class="comment">//    context.write(key, NullWritable.get());</span></span><br><span class="line">    <span class="comment">//&#125;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderDriver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(OrderDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(OrderMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(OrderReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setGroupingComparatorClass(OrderGroupingComparator<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>MapTask工作机制<br><img src="MapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="MapTask工作机制"></p><ul><li>Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value</li><li>Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value</li><li>Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区(调用Partitioner)，并写入一个环形内存缓冲区中</li><li>Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作<ul><li>溢写步骤1：利用快速排序算法对缓存区内的数据进行排序。排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序</li><li>溢写步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out(N表示当前溢写次数)中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作</li><li>溢写步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中</li></ul></li><li>Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件<br>当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。<br>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor(默认10)个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。<br>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销</li></ul></li><li><p>ReduceTask</p><ul><li><p>ReduceTask工作机制<br><img src="ReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="ReduceTask工作机制"></p><ul><li>Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中</li><li>Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多</li><li>Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可</li><li>Reduce阶段：reduce()函数将计算结果写到HDFS上</li></ul></li><li><p>设置ReduceTask并行度(个数)<br>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置(默认值为1)：job.setNumReduceTasks(4);</p></li><li><p>一个测试ReduceTask数目的实验</p><ul><li>实验环境：1个Master节点，16个Slave节点；CPU：8GHZ，内存: 2G</li><li>实验结论(数据量为1G)</li></ul><table><thead><tr><th>ReduceTask数目</th><th>总时间</th></tr></thead><tbody><tr><td>1</td><td>892</td></tr><tr><td>5</td><td>146</td></tr><tr><td>10</td><td>110</td></tr><tr><td>15</td><td>92</td></tr><tr><td>16</td><td>88</td></tr><tr><td>20</td><td>100</td></tr><tr><td>25</td><td>128</td></tr><tr><td>30</td><td>101</td></tr><tr><td>45</td><td>145</td></tr><tr><td>60</td><td>104</td></tr></tbody></table></li><li><p>注意事项</p><ul><li>ReduceTask = 0，表示没有Reduce阶段，输出文件个数和Map个数一致</li><li>ReduceTask默认值就是1，所以输出文件个数为一个</li><li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜(一些节点很忙,其余空闲)</li><li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask</li><li>具体多少个ReduceTask，需要根据集群性能而定</li><li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程？答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行</li></ul></li></ul></li><li><p>OutputFormat数据输出</p><ul><li><p>OutputFormat接口实现类<br>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。以下是几种常见的OutputFormat实现类</p><ul><li><p>文本输出TextOutputFormat<br>默认的输出格式是TextOutputFormat，<strong>它把每条记录写为文本行</strong>。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串</p></li><li><p>SequenceFileOutputFormat<br>将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的<strong>格式紧凑，很容易被压缩</strong></p></li><li><p>自定义OutputFormat：根据用户需求，自定义实现输出</p><ul><li><p>使用场景<br>为了实现<strong>控制最终文件的输出路径和输出格式</strong>，可以自定义OutputFormat<br>例如：要在一个MapReduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义OutputFormat来实现</p></li><li><p>自定义OutputFormat步骤</p><ul><li>自定义一个类继承FileOutputFormat</li><li>改写RecordWriter，具体改写输出数据的方法write()</li></ul></li><li><p>自定义OutputFormat案例实操</p><ul><li><p>需求：过滤输入的log日志，指定包含某特定字段的记录输出到一个文件，其他则输出到另一个文件</p></li><li><p>案例分析<br><img src="%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="自定义OutputFormat案例分析"></p></li><li><p>代码编写</p><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// http://www.baidu.com</span></span><br><span class="line">    context.write(value, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    String line = key.toString();</span><br><span class="line">    line += <span class="string">"\r\n"</span>;</span><br><span class="line">    k.set(line);</span><br><span class="line">    <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">        context.write(k, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>自定义OutputFormat、RecordWriter类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> FRecordWriter(job);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FSDataOutputStream fosSobxiong;</span><br><span class="line">  <span class="keyword">private</span> FSDataOutputStream fosOther;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、获取文件系统</span></span><br><span class="line">      FileSystem fs = FileSystem.get(job.getConfiguration());</span><br><span class="line">      <span class="comment">// 2、创建输出到sobxiong.log的输出流</span></span><br><span class="line">      fosSobxiong = fs.create(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Downloads/sobxiong.log"</span>));</span><br><span class="line">      <span class="comment">// 3、创建输出到other.log的输出流</span></span><br><span class="line">      fosOther = fs.create(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Downloads/other.log"</span>));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 判断key中是否有sobxiong,如果有写出到sobxiong,否则输出到other</span></span><br><span class="line">    <span class="keyword">if</span> (key.toString().contains(<span class="string">"sobxiong"</span>)) &#123;</span><br><span class="line">        fosSobxiong.write(key.toString().getBytes());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fosOther.write(key.toString().getBytes());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    IOUtils.closeStream(fosOther);</span><br><span class="line">    IOUtils.closeStream(fosSobxiong);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">  job.setJarByClass(FilterDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapperClass(FilterMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(FilterReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">  job.setOutputFormatClass(FilterOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 虽然我们自定义了outputFormat,但是因为我们的outputFormat继承自fileOutputFormat</span></span><br><span class="line">  <span class="comment">// 而fileOutputFormat要输出一个_SUCCESS文件,所以,在这还得指定一个输出目录</span></span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li></ul></li><li><p>Join多种应用</p><ul><li><p>Reduce Join</p><ul><li><p>工作原理<br>Map端的主要工作：为来自不同表或文件的key/value对，<strong>打标签以区别不同来源的记录</strong>。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出<br>Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些<strong>来源于不同文件的记录(在Map阶段已经打标志)分开</strong>，最后进行合并</p></li><li><p>案例实操</p><ul><li><p>需求：将商品信息表中数据根据商品pid合并到订单数据表中</p></li><li><p>案例分析：通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联<br><img src="Reduce%E7%AB%AF%E8%A1%A8%E5%90%88%E5%B9%B6.png" alt="Reduce端表合并"></p></li><li><p>代码编写</p><ul><li>合并后的Bean类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 略去空参/全参构造器、getter/setter方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 订单id</span></span><br><span class="line">  <span class="keyword">private</span> String id;</span><br><span class="line">  <span class="comment">// 产品id</span></span><br><span class="line">  <span class="keyword">private</span> String pid;</span><br><span class="line">  <span class="comment">// 数量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> amount;</span><br><span class="line">  <span class="comment">// 产品名称</span></span><br><span class="line">  <span class="keyword">private</span> String pName;</span><br><span class="line">  <span class="comment">// 标记: 产品/订单</span></span><br><span class="line">  <span class="keyword">private</span> String flag;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> id + <span class="string">'\t'</span> + amount + <span class="string">'\t'</span> + pName;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeUTF(id);</span><br><span class="line">    out.writeUTF(pid);</span><br><span class="line">    out.writeInt(amount);</span><br><span class="line">    out.writeUTF(pName);</span><br><span class="line">    out.writeUTF(flag);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    id = in.readUTF();</span><br><span class="line">    pid = in.readUTF();</span><br><span class="line">    amount = in.readInt();</span><br><span class="line">    pName = in.readUTF();</span><br><span class="line">    flag = in.readUTF();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String fileName;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TableBean tableBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Text key = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取文件的名称</span></span><br><span class="line">    FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">    fileName = inputSplit.getPath().getName();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// id pid amount</span></span><br><span class="line">  <span class="comment">// 1001 01 1</span></span><br><span class="line">  <span class="comment">// pid pname</span></span><br><span class="line">  <span class="comment">// 01 小米</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="keyword">if</span> (fileName.startsWith(<span class="string">"order"</span>)) &#123; <span class="comment">// 订单表</span></span><br><span class="line">        <span class="comment">// 封装kv</span></span><br><span class="line">        tableBean.setId(fields[<span class="number">0</span>]);</span><br><span class="line">        tableBean.setPid(fields[<span class="number">1</span>]);</span><br><span class="line">        tableBean.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">        <span class="comment">// 属性不能为空,不然会序列化会出错</span></span><br><span class="line">        tableBean.setpName(<span class="string">""</span>);</span><br><span class="line">        tableBean.setFlag(<span class="string">"order"</span>);</span><br><span class="line">        <span class="keyword">this</span>.key.set(fields[<span class="number">1</span>]);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// 产品表</span></span><br><span class="line">        <span class="comment">// 封装kv</span></span><br><span class="line">        tableBean.setId(<span class="string">""</span>);</span><br><span class="line">        tableBean.setPid(fields[<span class="number">0</span>]);</span><br><span class="line">        tableBean.setAmount(<span class="number">0</span>);</span><br><span class="line">        tableBean.setpName(fields[<span class="number">1</span>]);</span><br><span class="line">        tableBean.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">        <span class="keyword">this</span>.key.set(fields[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 写出</span></span><br><span class="line">    context.write(<span class="keyword">this</span>.key, tableBean);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 存储所有订单集合</span></span><br><span class="line">    List&lt;TableBean&gt; beans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="comment">// 存储产品信息</span></span><br><span class="line">    TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">    <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"order"</span>.equals(value.getFlag())) &#123;</span><br><span class="line">            TableBean tmpBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// value是引用,tmpBean是实实在在的对象</span></span><br><span class="line">                BeanUtils.copyProperties(tmpBean, value);</span><br><span class="line">                beans.add(tmpBean);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                BeanUtils.copyProperties(pdBean, value);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 拼接表,设置商品名称</span></span><br><span class="line">    <span class="keyword">for</span> (TableBean bean : beans) &#123;</span><br><span class="line">        bean.setpName(pdBean.getpName());</span><br><span class="line">        context.write(bean, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取配置信息,创建job对象实例</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定本程序的jar包所在的本地路径</span></span><br><span class="line">  job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定本业务job要使用的Mapper/Reducer业务类</span></span><br><span class="line">  job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定Mapper输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始文件所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>测试结果：</li></ul><table><thead><tr><th>pid</th><th>pname</th><th>amount</th></tr></thead><tbody><tr><td>1001</td><td>小米</td><td>1</td></tr><tr><td>1001</td><td>小米</td><td>1</td></tr><tr><td>1002</td><td>华为</td><td>2</td></tr><tr><td>1002</td><td>华为</td><td>2</td></tr><tr><td>1003</td><td>格力</td><td>3</td></tr><tr><td>1003</td><td>格力</td><td>3</td></tr></tbody></table><ul><li>总结<ul><li>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜</li><li>解决方案：Map端实现数据合并</li></ul></li></ul></li></ul></li></ul></li><li><p>Map Join</p><ul><li><p>使用场景：适用于一张表十分小、一张表很大的场景</p></li><li><p>优点<br>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？<br>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜</p></li><li><p>具体方法：采用DistributedCache</p><ul><li>在Mapper的setup阶段，将文件读取到缓存集合中</li><li>在驱动函数中加载缓存(缓存普通文件到Task运行节点)：job.addCacheFile(new URI(“file:///Users/sobxiong/Downloads/testInput3/pd.txt”))</li></ul></li><li><p>案例实操</p><ul><li><p>需求同Reduce Join</p></li><li><p>案例分析<br><img src="Map%E7%AB%AF%E8%A1%A8%E5%90%88%E5%B9%B6.png" alt="Map端表合并"></p></li><li><p>代码编写</p><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job信息</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置加载jar包路径</span></span><br><span class="line">  job.setJarByClass(DistributedCacheDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联map</span></span><br><span class="line">  job.setMapperClass(DistributedCacheMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 没有reduce阶段,map阶段输出即为最终输出</span></span><br><span class="line">  <span class="comment">// 4、设置最终输出数据类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 6、加载缓存数据</span></span><br><span class="line">  job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///Users/sobxiong/Downloads/testInput3/pd.txt"</span>));</span><br><span class="line">  <span class="comment">// 7、Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span></span><br><span class="line">  job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 8、提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">5</span>);</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 缓存小表</span></span><br><span class="line">    String cachePath = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">    BufferedReader bufferedReader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(cachePath), StandardCharsets.UTF_8));</span><br><span class="line">    String lineStr;</span><br><span class="line">    <span class="keyword">while</span> (StringUtils.isNotEmpty(lineStr = bufferedReader.readLine())) &#123;</span><br><span class="line">        <span class="comment">// 1、切割</span></span><br><span class="line">        <span class="comment">// pid pname</span></span><br><span class="line">        <span class="comment">// 01 小米</span></span><br><span class="line">        String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">        <span class="comment">// 2、封装到集合去</span></span><br><span class="line">        pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(bufferedReader);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// id pid amount</span></span><br><span class="line">    <span class="comment">// 1001 01 1</span></span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、获取pid</span></span><br><span class="line">    String pid = fields[<span class="number">1</span>];</span><br><span class="line">    <span class="comment">// 4、取出pname</span></span><br><span class="line">    String pName = pdMap.get(pid);</span><br><span class="line">    <span class="comment">// 5、拼接</span></span><br><span class="line">    lineStr = lineStr.replace(<span class="string">';'</span>, <span class="string">'\t'</span>) + <span class="string">'\t'</span> + pName;</span><br><span class="line">    k.set(lineStr);</span><br><span class="line">    <span class="comment">// 6、写出</span></span><br><span class="line">    context.write(k, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>计数器应用<br>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量</p><ul><li><p>计数器API</p><ul><li>采用枚举的方式统计计数</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> MyCounter&#123;MALFORORMED,NORMAL&#125;</span><br><span class="line"><span class="comment">//对枚举定义的自定义计数器加1</span></span><br><span class="line">context.getCounter(MyCounter.MALFORORMED).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><ul><li>采用计数器组、计数器名称的方式统计</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 组名和计数器名称随便起,但最好有意义</span></span><br><span class="line">context.getCounter(<span class="string">"counterGroup"</span>, <span class="string">"counter"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><ul><li>计数结果在程序运行后的控制台上查看</li></ul></li></ul></li><li><p>数据清洗(ETL)<br>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序</p><ul><li><p>案例实操(简单解析版——运用计数器)——复杂版(字段多,过滤的需求多,思路与下面无差)</p><ul><li><p>需求：去除日志中字段长度小于等于11的日志</p></li><li><p>代码编写</p><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    <span class="comment">// 2、解析数据</span></span><br><span class="line">    <span class="keyword">boolean</span> isDirty = parseLog(line, context);</span><br><span class="line">    <span class="keyword">if</span> (!isDirty) &#123;</span><br><span class="line">        <span class="comment">// 3、解析通过,写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line">    String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line">    <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"clean"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"dirty"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job信息</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 2、加载jar包</span></span><br><span class="line">  job.setJarByClass(LogDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联map</span></span><br><span class="line">  job.setMapperClass(LogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置最终输出类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 设置reduceTask个数为0</span></span><br><span class="line">  job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 5、设置输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 6、提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>MapReduce开发总结<br>编写MapReduce程序时，需要考虑如下方面</p><ul><li><p>输入数据接口：InputFormat</p><ul><li>默认使用的实现类是：TextInputFormat</li><li>TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</li><li>KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key/value。默认分隔符是tab(\t)</li><li>NlineInputFormat按照指定的行数N来划分切片</li><li>CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率</li><li>自定义InputFormat</li></ul></li><li><p>逻辑处理接口：Mapper<br>用户根据业务需求实现其中三个方法：map()、setup()、cleanup()</p></li><li><p>Partitioner分区</p><ul><li>默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号(分区数大于1时)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key.hashCode() &amp; Integer.MAXVALUE % numReduces</span><br></pre></td></tr></table></figure><ul><li>如果业务上有特别的需求，可以自定义分区</li></ul></li><li><p>Comparable排序</p><ul><li>当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法</li><li>部分排序：对最终输出的每一个文件进行内部排序</li><li>全排序：对所有数据进行排序，通常只有一个Reduce</li><li>二次排序：排序的条件有两个</li></ul></li><li><p>Combiner合并<br>Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果</p></li><li><p>Reduce端分组：GroupingComparator<br>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序</p></li><li><p>逻辑处理接口：Reduce<br>用户根据业务需求实现其中三个方法：reduce()、setup()、cleanup()</p></li><li><p>输出数据接口：OutputFormat</p><ul><li>默认实现类是TextOutputFormat，功能逻辑是：将每一个kv对向目标文本文件输出一行</li><li>将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩</li><li>自定义OutputFormat</li></ul></li></ul></li></ul><h2 id="Hadoop数据压缩"><a href="#Hadoop数据压缩" class="headerlink" title="Hadoop数据压缩"></a>Hadoop数据压缩</h2><ul><li><p>概述<br>压缩技术能够有效减少底层存储系统(HDFS)读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要<br>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，<strong>数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩</strong>。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价</p><ul><li>压缩策略和原则<br>压缩是提高Hadoop运行效率的一种优化策略<br>通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度</li></ul><p><strong>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能</strong><br>压缩基本原则</p><ul><li>运算密集型的job，少用压缩</li><li>IO密集型的job，多用压缩</li></ul></li><li><p>MR支持的压缩编码</p></li></ul><table><thead><tr><th>压缩格式</th><th>是否hadoop自带</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>换成压缩格式后,原来程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是,直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样,不需要修改</td></tr><tr><td>Gzip</td><td>是,直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样,不需要修改</td></tr><tr><td>bzip2</td><td>是,直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样,不需要修改</td></tr><tr><td>LZO</td><td>否,需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引,还需要指定输入格式</td></tr><tr><td>Snappy</td><td>否,需要安装</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样,不需要修改</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>Gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr><tr><td>Snappy</td><td>8.3GB</td><td>较大</td><td>最快</td><td>最快</td></tr></tbody></table><ul><li><p>压缩方式选择</p><ul><li>Gzip压缩<ul><li>优点<br>压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便</li><li>缺点：不支持Split(切片)</li><li>应用场景</li></ul><strong>当每个文件压缩之后在130M以内的(1个块大小内)，都可以考虑用Gzip压缩格式</strong>。例如说一天或者一个小时的日志压缩成一个Gzip文件</li><li>Bzip2压缩<ul><li>优点：支持Split(切片)；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便</li><li>缺点：压缩/解压速度慢</li><li>应用场景<br>适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split(切片)，而且兼容之前的应用程序的情况</li></ul></li><li>Lzo压缩<ul><li>优点：压缩/解压速度也比较快，合理的压缩率；支持Split(切片)，是Hadoop中最流行的压缩格式之一；可以在Linux系统下安装lzop命令，使用方便</li><li>缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建索引，还需要指定InputFormat为Lzo格式)</li><li>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显</li></ul></li><li>Snappy压缩<ul><li>优点：高速压缩速度和合理的压缩率</li><li>缺点：不支持Split(切片)；压缩率比Gzip要低；Hadoop本身不支持，需要安装</li><li>应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入</li></ul></li></ul></li><li><p>压缩位置选择<br>压缩可以在MapReduce作用的任意阶段启用<br><img src="MapReduce%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE.png" alt="MapReduce压缩位置"></p></li><li><p>压缩参数配置<br>要在Hadoop中启用压缩，可配置如下参数：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs(在core-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress(在mapred-site.xml中配置)</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec(在mapred-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置)</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置)</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table></li><li><p>压缩实操</p><ul><li>数据流的压缩和解压缩<br>CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据：<ul><li>要想对正在被写入一个输出流的数据进行<strong>压缩</strong>，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流</li><li>相反，要想对从输入流读取而来的数据进行<strong>解压缩</strong>，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据<br>测试如下的压缩方式：<table><thead><tr><th>压缩格式</th><th>编解码类</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr></tbody></table></li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  compress(<span class="string">"/Users/sobxiong/Downloads/test.txt"</span>, <span class="string">"org.apache.hadoop.io.compress.BZip2Codec"</span>);</span><br><span class="line">  decompress(<span class="string">"/Users/sobxiong/Downloads/test.txt.bz2"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 解压缩</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、压缩方式检查</span></span><br><span class="line">  CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line">  CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(filePath));</span><br><span class="line">  <span class="keyword">if</span> (codec == <span class="keyword">null</span>) &#123;</span><br><span class="line">      System.out.println(<span class="string">"Can't process!"</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 2、获取输入流</span></span><br><span class="line">  FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filePath));</span><br><span class="line">  CompressionInputStream cis = codec.createInputStream(fis);</span><br><span class="line">  <span class="comment">// 3、获取输出流</span></span><br><span class="line">  FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filePath + <span class="string">".decode"</span>));</span><br><span class="line">  <span class="comment">// 4、流的对拷</span></span><br><span class="line">  IOUtils.copyBytes(cis, fos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">10</span>, <span class="keyword">false</span>);</span><br><span class="line">  <span class="comment">// 5、关闭资源</span></span><br><span class="line">  IOUtils.closeStream(fos);</span><br><span class="line">  IOUtils.closeStream(cis);</span><br><span class="line">  IOUtils.closeStream(fis);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 压缩</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String filePath, String compressTypeClassName)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取输入流</span></span><br><span class="line">  FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filePath));</span><br><span class="line">  <span class="comment">// 2、获取输出流</span></span><br><span class="line">  Class&lt;?&gt; classCodec = Class.forName(compressTypeClassName);</span><br><span class="line">  CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(classCodec, <span class="keyword">new</span> Configuration());</span><br><span class="line">  FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filePath + codec.getDefaultExtension()));</span><br><span class="line">  CompressionOutputStream cos = codec.createOutputStream(fos);</span><br><span class="line">  <span class="comment">// 3、流的对拷</span></span><br><span class="line">  IOUtils.copyBytes(fis, cos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">10</span>, <span class="keyword">false</span>);</span><br><span class="line">  <span class="comment">// 4、关闭资源</span></span><br><span class="line">  IOUtils.closeStream(cos);</span><br><span class="line">  IOUtils.closeStream(fos);</span><br><span class="line">  IOUtils.closeStream(fis);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Map输出端采用压缩(以万能的WordCount案例为例)<br>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可<br>具体实现(只修改Driver部分代码,Mapper和Reducer不变)：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、获取Job对象</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec<span class="class">.<span class="keyword">class</span>, <span class="title">CompressionCodec</span>.<span class="title">class</span>)</span>;</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 之后代码保持原样</span></span><br></pre></td></tr></table></figure><ul><li>Reduce输出端采用压缩(以万能的WordCount案例为例)<br>具体实现(只修改Driver部分代码,Mapper和Reducer不变)：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 之前代码保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, GzipCodec<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 之后代码保持不变</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="Yarn资源调度器"><a href="#Yarn资源调度器" class="headerlink" title="Yarn资源调度器"></a>Yarn资源调度器</h2><p>Yarn是一个资源调度平台，<strong>负责为运算程序提供服务器运算资源</strong>，相当于一个<strong>分布式的操作系统平台</strong>，而MapReduce等运算程序则相当于<strong>运行于操作系统之上的应用程序</strong></p><ul><li><p>Yarn基本架构<br>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成<br><img src="Yarn%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png" alt="Yarn基本架构"></p></li><li><p>Yarn工作机制</p><ul><li>Yarn工作机制图解<br><img src="Yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="Yarn工作机制"></li><li>工作机制详解<ul><li>MR程序提交到客户端所在的节点</li><li>YarnRunner向ResourceManager申请一个Application</li><li>RM将该应用程序的资源路径返回给YarnRunner</li><li>该程序将运行所需资源提交到HDFS上</li><li>程序资源提交完毕后，申请运行MrAppMaster</li><li>RM将用户的请求初始化成一个Task</li><li>其中一个NodeManager领取到Task任务</li><li>该NodeManager创建容器Container，并产生MrAppmaster</li><li>Container从HDFS上拷贝资源到本地</li><li>MrAppmaster向RM申请运行MapTask的资源</li><li>RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器</li><li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序</li><li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器运行ReduceTask</li><li>ReduceTask向MapTask获取相应分区的数据</li><li>程序运行完毕后，MR会向RM申请注销自己</li></ul></li></ul></li><li><p>作业提交全过程</p><ul><li>作业提交过程之Yarn图解<br><img src="%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BYarn.png" alt="作业提交过程之Yarn"></li><li>作业提交过程之MapReduce图解<br><img src="%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BMapReduce.png" alt="作业提交过程之MapReduce"></li><li>作业提交过程详解<ul><li>作业提交<ul><li>Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业</li><li>Client向RM申请一个作业id</li><li>RM给Client返回该job资源的提交路径和作业id</li><li>Client提交jar包、切片信息和配置文件到指定的资源提交路径</li><li>Client提交完资源后，向RM申请运行MrAppMaster</li></ul></li><li>作业初始化<ul><li>当RM收到Client的请求后，将该job添加到容量调度器中</li><li>某一个空闲的NM领取到该Job</li><li>该NM创建Container，并产生MrAppmaster</li><li>下载Client提交的资源到本地</li></ul></li><li>任务分配<ul><li>MrAppMaster向RM申请运行多个MapTask任务资源</li><li>RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器</li></ul></li><li>任务运行<ul><li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序</li><li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask</li><li>ReduceTask向MapTask获取相应分区的数据</li><li>程序运行完毕后，MR会向RM申请注销自己</li></ul></li><li>进度和状态更新<br>YARN中的任务将其进度和状态(包括counter)返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新，展示给用户</li><li>作业完成<br>除了向应用管理器请求作业进度外，客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后，应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查</li></ul></li></ul></li><li><p>资源调度器<br>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。</p><ul><li>先进先出调度器(FIFO)<br><img src="FIFO%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="FIFO调度器"></li><li>容量调度器(Capacity Scheduler)<br><img src="%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="容量调度器"></li><li>公平调度器(Fair Scheduler)<br><img src="%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="公平调度器"></li></ul><p>Hadoop3.1.3默认的资源调度器是Capacity Scheduler。具体设置详见yarn-default.xml文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>任务的推测执行</p><ul><li><p>作业完成时间取决于最慢的任务完成时间<br>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。<br>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p></li><li><p>推测执行机制<br>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果</p></li><li><p>执行推测任务的前提条件</p><ul><li>每个Task只能有一个备份任务</li><li>当前Job已完成的Task必须不小于0.05(5%)</li><li>开启推测执行参数设置。mapred-site.xml文件中默认是打开的</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>不能启用推测执行机制情况<ul><li>任务间存在严重的负载倾斜</li><li>特殊任务，比如任务向数据库中写数据</li></ul></li><li>算法原理<br><img src="%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86.png" alt="推测执行算法原理"></li></ul></li></ul></li></ul><h2 id="Hadoop企业优化"><a href="#Hadoop企业优化" class="headerlink" title="Hadoop企业优化"></a>Hadoop企业优化</h2><ul><li><p>MapReduce跑的慢的原因<br>MapReduce程序效率的瓶颈在于两点：</p><ul><li>计算机性能：CPU、内存、磁盘健康、网络</li><li>I/O操作优化<ul><li>I/O操作优化</li><li>Map和Reduce数设置不合理</li><li>Map运行时间太长，导致Reduce等待过久</li><li>小文件过多</li><li>大量的不可分块的超大文件</li><li>Spill次数过多</li><li>Merge次数过多等</li></ul></li></ul></li><li><p>MapReduce优化方法<br>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数</p><ul><li><p>数据输入</p><ul><li>合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢</li><li>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景</li></ul></li><li><p>Map阶段</p><ul><li><strong>减少溢写(Spill)次数</strong>：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO</li><li><strong>减少合并(Merge)次数</strong>：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间</li><li>在Map之后，<strong>不影响业务逻辑前提下，先进行Combine处理</strong>，减少I/O</li></ul></li><li><p>Reduce阶段</p><ul><li><strong>合理设置Map和Reduce数</strong>：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误</li><li><strong>设置Map、Reduce共存</strong>：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</li><li><strong>规避使用Reduce</strong>：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗</li><li><strong>合理设置Reduce端的Buffer</strong>：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：<strong>mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整</strong></li></ul></li><li><p>I/O传输</p><ul><li><strong>采用数据压缩的方式</strong>，减少网络IO的的时间。安装Snappy和LZO压缩编码器</li><li><strong>使用SequenceFile二进制文件</strong></li></ul></li><li><p>数据倾斜问题</p><ul><li>数据倾斜现象<ul><li>数据频率倾斜：某一个区域的数据量要远远大于其他区域</li><li>数据大小倾斜：部分记录的大小远远大于平均值</li></ul></li><li>减少数据倾斜的方法<ul><li><strong>抽样和范围分区</strong>：可以通过对原始数据进行抽样得到的结果集来预设分区边界值</li><li><strong>自定义分区</strong>：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例</li><li><strong>Combine</strong>：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据</li><li><strong>采用Map Join，尽量避免Reduce Join</strong></li></ul></li></ul></li><li><p>常用的调优参数</p><ul><li><p>资源相关参数</p><ul><li>以下参数是在用户自己的MR应用程序中配置就可以生效(mapred-default.xml)</li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.memory.mb</td><td>一个MapTask可使用的资源上限(单位:MB)，默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死</td></tr><tr><td>mapreduce.reduce.memory.mb</td><td>一个ReduceTask可使用的资源上限(单位:MB)，默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死</td></tr><tr><td>mapreduce.map.cpu.vcores</td><td>每个MapTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.cpu.vcores</td><td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.shuffle.parallelcopies</td><td>每个Reduce去Map中取数据的并行数。默认值是5</td></tr><tr><td>mapreduce.reduce.shuffle.merge.percent</td><td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td></tr><tr><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>Buffer大小占Reduce可用内存的比例。默认值0.7</td></tr><tr><td>mapreduce.reduce.input.buffer.percent</td><td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td></tr></tbody></table><ul><li>应该在YARN启动之前就配置在服务器的配置文件中才能生效(yarn-default.xml)</li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>给应用程序Container分配的最小内存，默认值：1024</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>给应用程序Container分配的最大内存，默认值：8192</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container申请的最小CPU核数，默认值：1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container申请的最大CPU核数，默认值：32</td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>给Containers分配的最大物理内存，默认值：8192</td></tr></tbody></table><ul><li>Shuffle性能优化的关键参数，应在YARN启动之前就配置好(mapred-default.xml)</li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.task.io.sort.mb</td><td>Shuffle的环形缓冲区大小，默认100m</td></tr><tr><td>mapreduce.map.sort.spill.percent</td><td>环形缓冲区溢出的阈值，默认80%</td></tr></tbody></table></li><li><p>容错相关参数(MapReduce性能优化)</p></li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.maxattempts</td><td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4</td></tr><tr><td>mapreduce.reduce.maxattempts</td><td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4</td></tr><tr><td>mapreduce.task.timeout</td><td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间(单位毫秒)，默认是600000。如果你的程序对每条输入数据的处理时间过长(比如会访问数据库，通过网络拉取数据等)，建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”</td></tr></tbody></table></li></ul></li><li><p>HDFS小文件优化方法</p><ul><li>HDFS小文件弊端<br>HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件。<strong>一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢</strong></li><li>HDFS小文件解决方案<ul><li>在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS</li><li>在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并</li><li>在MapReduce处理时，可采用CombineTextInputFormat提高效率</li></ul></li><li>具体方案<ul><li>Hadoop Archive<br>是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了NameNode的内存使用</li><li>Sequence File<br>Sequence File由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件</li><li>CombineFileInputFormat<br>CombineFileInputFormat是一种新的InputFormat，用于将多个文件合并成一个单独的Split，另外，它会考虑数据的存储位置</li><li>开启JVM重用<br>对于大量小文件Job，可以开启JVM重用会减少45%运行时间。JVM重用原理：一个Map运行在一个JVM上，开启重用的话，该Map在JVM上运行完毕后，JVM继续运行其他Map。<br>具体设置：mapreduce.job.jvm.numtasks值在10-20之间</li></ul></li></ul></li></ul><h2 id="MapReduce扩展案例"><a href="#MapReduce扩展案例" class="headerlink" title="MapReduce扩展案例"></a>MapReduce扩展案例</h2><ul><li><p>倒排索引案例(多job串联)</p><ul><li><p>需求：有大量的文本(文档、网页)，需要建立搜索索引</p></li><li><p>案例分析<br><img src="%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95(%E5%A4%9Ajob%E4%B8%B2%E8%81%94)%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="倒排索引(多job串联)案例分析"></p></li><li><p>第一次处理</p><ul><li>OneIndexMapper</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneIndexMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String fileName;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">    fileName = fileSplit.getPath().getName();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">" "</span>);</span><br><span class="line">    <span class="comment">// 3、写出</span></span><br><span class="line">    <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line">      k.set(field + <span class="string">"--"</span> + fileName);</span><br><span class="line">      context.write(k, v);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>OneIndexReducer</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneIndexReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 1、累加求和</span></span><br><span class="line">    <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">      sum += value.get();</span><br><span class="line">    &#125;</span><br><span class="line">    v.set(sum);</span><br><span class="line">    <span class="comment">// 2、写出</span></span><br><span class="line">    context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>OneIndexDriver</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  job.setJarByClass(OneIndexDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapperClass(OneIndexMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(OneIndexReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>第二次处理</p><ul><li>TwoIndexMapper</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoIndexMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// haha--a.txt 2</span></span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">"--"</span>);</span><br><span class="line">    <span class="comment">// 3、封装</span></span><br><span class="line">    k.set(fields[<span class="number">0</span>]);</span><br><span class="line">    v.set(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(k, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>TwoIndexReducer</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoIndexReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">      sb.append(value.toString().replace(<span class="string">"\t"</span>, <span class="string">"--&gt;"</span>))</span><br><span class="line">              .append(<span class="string">'\t'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    v.set(sb.toString());</span><br><span class="line">    context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>TwoIndexDriver</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  Configuration config = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(config);</span><br><span class="line">  job.setJarByClass(TwoIndexDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapperClass(TwoIndexMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TwoIndexReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>先运行OneIndexDriver，将得到的输入作为TwoIndexDriver的输入，在运行TwoIndexDriver得到最终结果</p></li></ul></li><li><p>TopN案例</p><ul><li><p>需求：输出流量使用量在前10的用户信息</p></li><li><p>案例分析<br><img src="Top10%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Top10案例分析"></p></li><li><p>代码实现</p><ul><li>TopNMapper</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="comment">// 定义一个TreeMap作为存储数据的容器(天然按key排序)</span></span><br><span class="line">  <span class="keyword">private</span> TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> TreeMap&lt;FlowBean, Text&gt;();</span><br><span class="line">  <span class="keyword">private</span> FlowBean kBean;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    kBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    Text v = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">    <span class="comment">// 3、封装数据</span></span><br><span class="line">    String phoneNum = fields[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">long</span> sumFlow = Long.parseLong(fields[<span class="number">3</span>]);</span><br><span class="line">    kBean.setDownFlow(downFlow);</span><br><span class="line">    kBean.setUpFlow(upFlow);</span><br><span class="line">    kBean.setSumFlow(sumFlow);</span><br><span class="line">    v.set(phoneNum);</span><br><span class="line">    <span class="comment">// 4、向TreeMap中添加数据</span></span><br><span class="line">    flowMap.put(kBean, v);</span><br><span class="line">    <span class="comment">// 5、限制TreeMap的数据量,超过10条就删除掉流量最小的一条数据</span></span><br><span class="line">    <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">      flowMap.remove(flowMap.lastKey());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 6、遍历treeMap集合,输出数据</span></span><br><span class="line">    Iterator&lt;FlowBean&gt; bean = flowMap.keySet().iterator();</span><br><span class="line">    <span class="keyword">while</span> (bean.hasNext()) &#123;</span><br><span class="line">      FlowBean k = bean.next();</span><br><span class="line">      context.write(k, flowMap.get(k));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>TopNReducer</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">// 定义一个TreeMap作为存储数据的容器（天然按key排序）</span></span><br><span class="line">  TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> TreeMap&lt;FlowBean, Text&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">      FlowBean bean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">      bean.set(key.getDownFlow(), key.getUpFlow());</span><br><span class="line">      <span class="comment">// 1、向treeMap集合中添加数据</span></span><br><span class="line">      flowMap.put(bean, <span class="keyword">new</span> Text(value));</span><br><span class="line">      <span class="comment">// 2、限制TreeMap数据量,超过10条就删除掉流量最小的一条数据</span></span><br><span class="line">      <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">        flowMap.remove(flowMap.lastKey());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 3、遍历集合,输出数据</span></span><br><span class="line">    Iterator&lt;FlowBean&gt; it = flowMap.keySet().iterator();</span><br><span class="line">    <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">      FlowBean v = it.next();</span><br><span class="line">      context.write(<span class="keyword">new</span> Text(flowMap.get(v)), v);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>TopNDriver</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取配置信息,或者job对象实例</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定本程序的jar包所在的本地路径</span></span><br><span class="line">  job.setJarByClass(TopNDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">  job.setMapperClass(TopNMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TopNReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定mapper输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始文件所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>找博客共同好友案例</p><ul><li>需求<br>以下是博客的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友(数据中的好友关系是单向的)。<br>求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？</li><li>示例数据</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J</span><br></pre></td></tr></table></figure><ul><li>案例分析</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 第一次先求出A、B、C...等是谁的好友</span><br><span class="line">A I,K,C,B,G,F,H,O,D</span><br><span class="line">B A,F,J,E</span><br><span class="line">C A,E,B,H,F,G,K</span><br><span class="line">D G,C,K,A,L,F,E,H</span><br><span class="line">E G,M,L,H,A,F,B,D</span><br><span class="line">F L,M,D,C,G,A</span><br><span class="line">G M</span><br><span class="line">H O</span><br><span class="line">I O,C</span><br><span class="line">J O</span><br><span class="line">K B</span><br><span class="line">L D,E</span><br><span class="line">M E,F</span><br><span class="line">O A,H,I,J,F</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 第二次找出共同好友</span><br><span class="line">A-B E C</span><br><span class="line">A-C D F</span><br><span class="line">A-D E F</span><br><span class="line">A-E D B C</span><br><span class="line">A-F O B C D E</span><br><span class="line">A-G F E C D</span><br><span class="line">A-H E C D O</span><br><span class="line">A-I O</span><br><span class="line">A-J O B</span><br><span class="line">A-K D C</span><br><span class="line">A-L F E D</span><br><span class="line">A-M E F</span><br><span class="line">B-C A</span><br><span class="line">B-D A E</span><br><span class="line">B-E C</span><br><span class="line">B-F E A C</span><br><span class="line">B-G C E A</span><br><span class="line">B-H A E C</span><br><span class="line">B-I A</span><br><span class="line">B-K C A</span><br><span class="line">B-L E</span><br><span class="line">B-M E</span><br><span class="line">B-O A</span><br><span class="line">C-D A F</span><br><span class="line">C-E D</span><br><span class="line">C-F D A</span><br><span class="line">C-G D F A</span><br><span class="line">C-H D A</span><br><span class="line">C-I A</span><br><span class="line">C-K A D</span><br><span class="line">C-L D F</span><br><span class="line">C-M F</span><br><span class="line">C-O I A</span><br><span class="line">D-E L</span><br><span class="line">D-F A E</span><br><span class="line">D-G E A F</span><br><span class="line">D-H A E</span><br><span class="line">D-I A</span><br><span class="line">D-K A</span><br><span class="line">D-L E F</span><br><span class="line">D-M F E</span><br><span class="line">D-O A</span><br><span class="line">E-F D M C B</span><br><span class="line">E-G C D</span><br><span class="line">E-H C D</span><br><span class="line">E-J B</span><br><span class="line">E-K C D</span><br><span class="line">E-L D</span><br><span class="line">F-G D C A E</span><br><span class="line">F-H A D O E C</span><br><span class="line">F-I O A</span><br><span class="line">F-J B O</span><br><span class="line">F-K D C A</span><br><span class="line">F-L E D</span><br><span class="line">F-M E</span><br><span class="line">F-O A</span><br><span class="line">G-H D C E A</span><br><span class="line">G-I A</span><br><span class="line">G-K D A C</span><br><span class="line">G-L D F E</span><br><span class="line">G-M E F</span><br><span class="line">G-O A</span><br><span class="line">H-I O A</span><br><span class="line">H-J O</span><br><span class="line">H-K A C D</span><br><span class="line">H-L D E</span><br><span class="line">H-M E</span><br><span class="line">H-O A</span><br><span class="line">I-J O</span><br><span class="line">I-K A</span><br><span class="line">I-O A</span><br><span class="line">K-L D</span><br><span class="line">K-O A</span><br><span class="line">L-M E F</span><br></pre></td></tr></table></figure><ul><li><p>代码实现</p><ul><li>第一次Mapper</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneShareFriendsMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行 A:B,C,D,F,E,O</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = line.split(<span class="string">":"</span>);</span><br><span class="line">    <span class="comment">// 3、获取person和好友</span></span><br><span class="line">    String person = fields[<span class="number">0</span>];</span><br><span class="line">    String[] friends = fields[<span class="number">1</span>].split(<span class="string">","</span>);</span><br><span class="line">    <span class="comment">// 4、写出去</span></span><br><span class="line">    <span class="keyword">for</span>(String friend: friends)&#123;</span><br><span class="line">      <span class="comment">// 输出 &lt;好友，人&gt;</span></span><br><span class="line">      context.write(<span class="keyword">new</span> Text(friend), <span class="keyword">new</span> Text(person));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>第一次Reducer</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneShareFriendsReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">    <span class="comment">// 1、拼接</span></span><br><span class="line">    <span class="keyword">for</span>(Text person: values)&#123;</span><br><span class="line">      sb.append(person).append(<span class="string">","</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 取出多余的','</span></span><br><span class="line">    sb.deleteCharAt(sb.length() - <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 2、写出</span></span><br><span class="line">    context.write(key, <span class="keyword">new</span> Text(sb.toString()));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>第一次Driver</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定jar包运行的路径</span></span><br><span class="line">  job.setJarByClass(OneShareFriendsDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定map/reduce使用的类</span></span><br><span class="line">  job.setMapperClass(OneShareFriendsMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(OneShareFriendsReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定map输出的数据类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7 提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>第二次Mapper</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoShareFriendsMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// A I,K,C,B,G,F,H,O,D</span></span><br><span class="line">    <span class="comment">// 友 人,人,人</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    String[] friend_persons = line.split(<span class="string">"\t"</span>);</span><br><span class="line">    String friend = friend_persons[<span class="number">0</span>];</span><br><span class="line">    String[] persons = friend_persons[<span class="number">1</span>].split(<span class="string">","</span>);</span><br><span class="line">    Arrays.sort(persons);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; persons.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; persons.length; j++) &#123;</span><br><span class="line">        <span class="comment">// 发出 &lt;人-人,好友&gt;,这样，相同的“人-人”对的所有好友就会到同1个reduce中去</span></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(persons[i] + <span class="string">"-"</span> + persons[j]), <span class="keyword">new</span> Text(friend));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>第二次Reducer</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoShareFriendsReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">    <span class="keyword">for</span> (Text friend : values) &#123;</span><br><span class="line">      sb.append(friend).append(<span class="string">" "</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    context.write(key, <span class="keyword">new</span> Text(sb.toString()));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>第二次Driver</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定jar包运行的路径</span></span><br><span class="line">  job.setJarByClass(TwoShareFriendsDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定map/reduce使用的类</span></span><br><span class="line">  job.setMapperClass(TwoShareFriendsMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TwoShareFriendsReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定map输出的数据类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7 提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="常见错误及解决方案"><a href="#常见错误及解决方案" class="headerlink" title="常见错误及解决方案"></a>常见错误及解决方案</h2><ul><li>导包错误，尤其是Text和CombineTextInputFormat</li><li>Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable。报的错误是类型转换异常</li><li>java.lang.Exception: java.io.IOException: Illegal partition for 13926435656(4)；说明Partition和ReduceTask个数没对上，调整ReduceTask个数</li><li>如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行</li><li>报类型转换异常：通常都是在驱动函数中设置Map输出和最终输出时编写错误；Map输出的key如果没有排序，也会报类型转换异常</li><li>集群中运行wc.jar时出现了无法获得输入文件。原因：WordCount案例的输入文件不能放用HDFS集群的根目录</li><li>自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空</li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
