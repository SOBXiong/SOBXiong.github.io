<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hive</title>
      <link href="/2020/07/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/"/>
      <url>/2020/07/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Hive基本概念">Hive基本概念</a></li><li><a href="#Hive安装">Hive安装</a></li><li><a href="#Hive数据类型">Hive数据类型</a></li><li><a href="#DDL数据定义">DDL数据定义</a></li><li><a href="#DML数据操作">DML数据操作</a></li><li><a href="#查询">查询</a></li><li><a href="#函数">函数</a></li></ul><a id="more"></a><h2 id="Hive基本概念"><a href="#Hive基本概念" class="headerlink" title="Hive基本概念"></a>Hive基本概念</h2><ul><li><p>什么是Hive</p><ul><li>由Facebook开源用于解决海量结构化日志的数据统计</li><li>基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能<ul><li>Hive处理的数据存储在HDFS</li><li>Hive分析数据底层的实现是MapReduce</li><li>执行程序运行在Yarn上</li></ul></li><li>本质是将HQL(Hive Query Language)转化成MapReduce程序<br><img src="SQL-MapReduce.png" alt="SQL-MapReduce"></li></ul></li><li><p>Hive的优缺点</p><ul><li>优点：<ul><li>操作接口采用类SQL语法，提供快速开发的能力(简单、容易上手)</li><li>避免了去写MapReduce，减少开发人员的学习成本</li><li>Hive的执行延迟比较高，因此Hive常用于数据分析和对实时性要求不高的场合</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li></ul></li><li>缺点：<ul><li>Hive的HQL表达能力有限：<ul><li>迭代式算法无法表达</li><li>数据挖掘方面不擅长</li></ul></li><li>Hive的效率比较低：<ul><li>Hive自动生成的MapReduce作业，通常情况下不够智能化</li><li>Hive调优比较困难，粒度较粗</li></ul></li></ul></li></ul></li><li><p>Hive架构原理<br><img src="Hive%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png" alt="Hive架构原理"></p><ul><li>用户接口：Client<br>CLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive)</li><li>元数据：Metastore<br>元数据包括：表名、表所属的数据库(默认是default)、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在目录等；</li></ul><p><strong>默认存储在自带的derby数据库中(存在bug)，推荐使用MySQL存储Metastore</strong></p><ul><li>Hadoop：使用HDFS进行存储，使用MapReduce进行计算</li><li>驱动器：Driver<ul><li>解析器(SQL Parser)：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误</li><li>编译器(Physical Plan)：将AST编译生成逻辑执行计划</li><li>优化器(Query Optimizer)：对逻辑执行计划进行优化</li><li>执行器(Execution)：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark<br><img src="Hive%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6.png" alt="Hive运行机制"><br>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将 执行返回的结果输出到用户交互接口</li></ul></li></ul></li><li><p>Hive和数据库比较<br>由于Hive采用了类似SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。下面将从多个方面来阐述Hive和数据库的差异。数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性</p><ul><li>查询语言<br>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言 HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发</li><li>数据存储位置<br>Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中</li><li>数据更新<br>由于Hive是针对数据仓库应用设计的，而<strong>数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的</strong>。而数据库中的数据通常是需要经常进行修改的</li><li>索引<br>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。<strong>Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据</strong>，因此访问延迟较高。由于MapReduce的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive不适合在线数据查询</li><li>执行<br>Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的。而数据库通常有自己的执行引擎</li><li>执行延迟<br>Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小。当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势</li><li>可扩展性<br>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的(世界上最大的Hadoop集群在Yahoo，2009年的规模在4000台节点左右)。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右</li><li>数据规模<br>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小</li></ul></li></ul><h2 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h2><ul><li><p>安装地址</p><ul><li>Hive官网地址：<a href="http://hive.apache.org" target="_blank" rel="noopener">http://hive.apache.org</a></li><li>文档查看地址：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li><li>下载地址：<a href="http://archive.apache.org/dist/hive" target="_blank" rel="noopener">http://archive.apache.org/dist/hive</a></li><li>github地址：<a href="https://github.com/apache/hive" target="_blank" rel="noopener">https://github.com/apache/hive</a></li></ul></li><li><p>Hive安装部署</p><ul><li><p>Hive安装及配置</p><ul><li><p>上传：把apache-hive-3.1.2-bin.tar.gz上传到/opt/software目录下</p></li><li><p>解压：tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/</p></li><li><p>修改目录名：mv apache-hive-3.1.2-bin hive-3.1.2</p></li><li><p>修改配置文件(conf目录下)：</p><ul><li>备份一份配置文件：cp hive-env.sh.template hive-env.sh.template.copy</li><li>修改配置文件后缀：mv hive-env.sh.template hive-env.sh</li><li>配置hive-env.sh文件(底部加入)：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export HIVE_CONF_DIR=/opt/module/hive-3.1.2/conf</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Hadoop集群启动(hadoop1启动hdfs——sbin/start-dfs.sh,hadoop2启动yarn——sbin/start-yarn.sh)</p></li><li><p>Hive基本操作</p><ul><li>初始化默认的derby数据库：bin/schematool -dbType derby -initSchema(初始化后在hive根目录会产生derby.log和metastore目录)</li><li>启动hive：bin/hive</li><li>启动时会发生Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V错误<ul><li>这是因为hive内依赖的guava和hadoop内的版本不一致</li><li>分别查看hive(lib目录下)和hadoop(share/hadoop/common/lib目录下)的guava依赖版本：guava-19.0.jar和guava-27.0-jre.jar</li><li>删除hive的低版本guava-19.0.jar，将hadoop的高版本guava-27.0-jre.jar复制到hive的lib目录下</li><li>重启启动hive</li></ul></li><li>查看数据库：show databases;</li><li>打开默认数据库：use default;</li><li>显示default数据库中的表：show tables;</li><li>创建一张表(数据类型为java中类型)：create table student(id int,name string);</li><li>查看表的结构：desc student;</li><li>向表中插入数据：insert into student values(1,”SOBXiong”);</li><li>查询表中数据：select * from student;</li><li>退出hive：quit;</li></ul></li></ul></li><li><p>本地文件导入Hive<br>需求：将本地/opt/module/data/hive/student.txt的数据导入到hive的student表中</p><ul><li>数据准备(tab键隔开)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 xixi</span><br><span class="line">2 haha</span><br><span class="line">3 hehe</span><br></pre></td></tr></table></figure><ul><li><p>Hive操作</p><ul><li>导入student.txt的数据到之前创建的student表中：load data local inpath ‘/opt/module/data/hive/student.txt’ into table student;</li><li>查询结果(发现都是NULL NULL,因为格式不对)：select * from student;</li><li>删除已创建的student表：drop table student;</li><li>创建新的student表(声明文件分隔符’\t’)：create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</li><li>重新导入数据并重新查询结果</li><li>查看<a href="http://hadoop1:9870" target="_blank" rel="noopener">http://hadoop1:9870</a>中的HDFS文件，发现/user/hive/warehouse/student下就有数据</li><li>第二种插入数据的方式：直接将文件上传至HDFS服务器<ul><li>上传本地文件(相当于cp复制)：hadoop fs -put stu1.txt /user/hive/warehouse/student</li><li>上传HDFS文件(相当于mv移动)：hadoop fs -put /stu2.txt /user/hive/warehouse/student</li></ul></li></ul></li><li><p>derby存储元数据的问题(推荐使用mysql)：</p><ul><li>只能开启一个hive客户端</li><li>在不同的目录开启hive客户端会在当前目录下创建derby.log和metastore文件，相当于数据不共享</li></ul></li></ul></li><li><p>MySql安装</p><ul><li><p>安装包准备：</p><ul><li>查看yum中历史的mysql或者mariadb的依赖：rpm -qa | grep mysql/mariadb</li><li>如有历史依赖，删除：yum remove mysql-libs/mariadb-libs</li><li>下载mysql的rpm包：前往<a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/</a>下载5.7.30的Red Hat Enterprise Linux7版本(CentOS7)的RPM Bundle包</li></ul></li><li><p>安装MySql</p><ul><li>解压tar包：tar -xvf mysql-5.7.30-1.el7.x86_64.rpm-bundle.tar</li><li>使用rpm命令安装MySql组件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 依赖关系为common→libs→client→server</span></span><br><span class="line">rpm -ivh common</span><br><span class="line">rpm -ivh libs</span><br><span class="line">rpm -ivh client</span><br><span class="line">rpm -ivh server</span><br></pre></td></tr></table></figure><ul><li>启动MySql：systemctl start mysqld.service</li><li>查看MySql状态：systemctl status mysqld.service</li><li>查看初始化的随机密码：grep ‘temporary password’ /var/log/mysqld.log</li><li>登录MySql：mysql -u root -p</li><li>修改密码校验策略(不然设置新密码会提示密码错误)：set global validate_password_policy=0;</li><li>修改密码：alter user root@localhost identified by ‘your password’;</li><li>授权root用户远程访问权限</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'root'</span> @<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'your password'</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure><ul><li>设置MySql完毕，退出：quit;</li></ul></li></ul></li><li><p>Hive元数据配置到MySql</p><ul><li><p>拷贝mysql-connector JDBC驱动文件</p><ul><li>前往<a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/</a>下载驱动文件5.1.49版本</li><li>解压文件mysql-connector-java-5.1.49.tar.gz，拷贝mysql-connector-java-5.1.49-bin.jar到hive的lib目录下</li></ul></li><li><p>配置metastore到MySql</p><ul><li>在conf目录下创建hive-site.xml配置文件：touch hive-site.xml</li><li>修改配置文件：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- xml下的&amp;需要转义为&amp;amp; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop1:3306/metastore?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>your password<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>初始化Hive的MySql元数据数据库：bin/schematool -dbType mysql -initSchema</p></li><li><p>启动hive，MySql中新增了metastore数据库(表DBS和TBS比较重要)</p></li></ul></li><li><p>HiveJDBC访问</p><ul><li>停止hadoop：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop1</span></span><br><span class="line">stop-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop2</span></span><br><span class="line">stop-yarn.sh</span><br></pre></td></tr></table></figure><ul><li>修改hadoop配置：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml 启用webhdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">  core-site.xml 设置hadoop的代理用户</span></span><br><span class="line"><span class="comment">  hadoop.proxyuser.xxx.hosts</span></span><br><span class="line"><span class="comment">  xxx是操作的用户</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  org.apache.hadoop.security.authorize.AuthorizationException: User: sobxiong is not allowed to impersonate root(state=08S01,code=0)</span></span><br><span class="line"><span class="comment">  User:xxx即为下面该填入的用户</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.sobxiong.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.sobxiong.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改hive配置：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Bind host on which to run the HiveServer2 Thrift service.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>11000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>启动hiveserver2服务：bin/hiveserver2</li><li>启动beeline：bin/beeline</li><li>连接hiveserver2：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop1:11000</span></span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: sobxiong</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: your password(数据库的密码)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 接下来的操作就跟Hive Cli使用类似SQL</span></span><br></pre></td></tr></table></figure></li><li><p>Hive常用交互命令</p><ul><li>-e &lt;quoted-query-string&gt;：不进入hive的交互窗口执行sql语句，例如：bin/hive -e “select * from student;”</li><li>-f &lt;filename&gt;：执行脚本中sql语句<ul><li>结果打印在terminal上：bin/hive -f /opt/module/data/hive/hive.hql</li><li>结果打印在指定文件中：bin/hive -f /opt/module/data/hive/hive.hql  &gt; ./hive_result.txt</li></ul></li></ul></li><li><p>Hive其他命令操作</p><ul><li>在Hive Cli命令窗口中查看hdfs文件系统：dfs -ls /</li><li>在Hive Cli命令窗口中查看本地文件系统：! ls /</li><li>查看在hive中输入的所有历史命令：cat ~/.hivehistory</li></ul></li><li><p>Hive常见属性配置</p><ul><li>Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse</li><li><strong>在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹</strong></li><li>修改default数据仓库原始位置(hive-default.xml.template -&gt; hive-site.xml)</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>查询后信息显示配置</p><ul><li>在hive-site.xml加入如下配置：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 表列名显示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 当前使用数据库显示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>重启Hive Cli</li></ul></li><li><p>Hive运行日志信息配置</p><ul><li>Hive的日志信息默认存放在/tmp/{current_user}目录下</li><li>修改Hive的日志信息存放在hive安装目录的logs文件夹下<br>修改conf/hive-log4j.properties配置文件(hive-log4j.properties.template -&gt; hive-log4j.properties)：hive.log.dir=/opt/module/hive-3.1.2/logs</li></ul></li><li><p>参数配置方式</p><ul><li>查看当前所有的配置信息(Hive Cli命令窗口下)：set;</li><li>参数配置的三种方式<ul><li>配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml<br>注意：<strong>用户自定义配置会覆盖默认配置</strong>。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效</li><li>命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数<br>例如：bin/hive -hiveconf mapred.reduce.tasks=10;(<strong>注意：仅对本次hive启动有效</strong>)<br>查看参数设置：set mapred.reduce.tasks;</li><li>参数声明方式<br>在HQL中使用SET关键字设定参数(<strong>注意：仅对本次hive启动有效</strong>)<br>例如：set mapred.reduce.tasks=100;<br>上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了</li></ul></li></ul></li></ul></li></ul><h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><ul><li>基本数据类型(Hive数据类型大小写不敏感)</li></ul><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td></tr><tr><td>SMALLINT</td><td>short</td><td>2byte有符号整数</td></tr><tr><td><strong>INT</strong></td><td>int</td><td>4byte有符号整数</td></tr><tr><td><strong>BIGINT</strong></td><td>long</td><td>8byte有符号整数</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或false</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td></tr><tr><td><strong>DOUBLE</strong></td><td>double</td><td>双精度浮点数</td></tr><tr><td><strong>STRING</strong></td><td>string</td><td>字符系列，可以指定字符集，可以使用单引号或者双引号</td></tr><tr><td>TIMESTAMP</td><td>-</td><td>时间类型</td></tr><tr><td>BINARY</td><td>-</td><td>字节数组</td></tr></tbody></table><p>Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过他不能声明其最多能存储多少个字符，理论上它可以存储2GB的字符数</p><ul><li>集合数据类型</li></ul><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取键last对应的值数据</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用</td><td>Array()</td></tr></tbody></table><p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套</p><ul><li><p>集合数据类型案例实操</p><ul><li>假设JSON为原始数据，具体如下：</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"songsong"</span>,</span><br><span class="line">  <span class="attr">"friends"</span>: [<span class="string">"bingbing"</span> , <span class="string">"lili"</span>],</span><br><span class="line">  <span class="attr">"children"</span>: &#123;</span><br><span class="line">      <span class="attr">"xiao song"</span>: <span class="number">18</span> ,</span><br><span class="line">      <span class="attr">"xiaoxiao song"</span>: <span class="number">19</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"address"</span>:&#123;</span><br><span class="line">    <span class="attr">"street"</span>: <span class="string">"hui long guan"</span> ,</span><br><span class="line">    <span class="attr">"city"</span>: <span class="string">"beijing"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>基于上述数据结构，建立本地测试文件test.txt，具体格式如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure><p>注意：MAP、STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用’_’</p><ul><li>Hive上创建测试表test</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="comment">/* 设置列分隔符为',' */</span></span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line"><span class="comment">/* 设置map、struct和array的分隔符(数据分割符号)为'_' */</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span><br><span class="line"><span class="comment">/* 设置map中的key/value的分隔符为',' */</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="comment">/* 设置行分隔符为'\n'(也是默认值) */</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;</span><br></pre></td></tr></table></figure><ul><li>导入文本数据到测试表中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/test.txt' into table test;</span><br></pre></td></tr></table></figure><ul><li>访问三种集合列里的数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> friends[<span class="number">1</span>],children[<span class="string">'xiao song'</span>],address.city <span class="keyword">from</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure></li><li><p>类型转换<br>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化。例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作</p><ul><li>隐式类型转换规则<ul><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT</li><li>所有整数类型、FLOAT和<strong>STRING(符合数字)</strong>类型都可以隐式地转换成DOUBLE</li><li>TINYINT、SMALLINT、INT都可以转换为FLOAT</li><li>BOOLEAN类型不可以转换为任何其它的类型</li></ul></li><li>使用CAST操作显示进行数据类型转换<br>例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值NULL</li></ul></li></ul><h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><ul><li><p>创建数据库</p><ul><li>创建一个数据库，默认在HDFS上的存储路径式/user/hive/warehouse/*.db：create database if not exists db_hive;(if not exists避免要创建的数据库已存在)</li><li>创建一个数据库，指定在HDFS上存放的路径</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive2 location <span class="string">'/db_hive2.db'</span></span><br></pre></td></tr></table></figure></li><li><p>查询数据库</p><ul><li><p>显示数据库</p><ul><li>显示数据库：show databases;</li><li>过滤查询显示的数据库</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span> <span class="keyword">like</span> <span class="string">'db_hive'</span>;</span><br></pre></td></tr></table></figure></li><li><p>查看数据库</p><ul><li>显示数据库信息：desc database db_hive;</li><li>显示数据库详细信息(extended)：desc database extended db_hive;</li></ul></li><li><p>切换当前数据库：use db_hive;</p></li></ul></li><li><p>修改数据库<br>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。<strong>数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置</strong><br>修改数据库属性值：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20200708'</span>)</span><br></pre></td></tr></table></figure><p>查看修改结果：desc database extended db_hive;</p><ul><li><p>删除数据库</p><ul><li>删除空数据库：drop database if exists db_hive;(if exists避免要删除的数据库不存在)</li><li>如果数据库中表不为空，可以采用cascade命令集联强制删除：drop database db_hive cascade;</li></ul></li><li><p>创建表</p><ul><li>建表语法</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...)</span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format]</span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure><ul><li><p>字段解释说明</p><ul><li>CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常</li><li>EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径(LOCATION)，<strong>Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</strong></li><li>COMMENT：为表和列添加注释</li><li>PARTITIONED BY创建分区表</li><li>CLUSTERED BY创建分桶表</li><li>SORTED BY不常用</li><li>ROW FORMAT<br>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]<br>| SERDE serde_name [WITH SERDEPROPERTIE (property_name=property_value, property_name=property_value, …)]<br>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化</li><li>STORED AS指定存储文件类型<br>常用的存储文件类型：SEQUENCEFILE(二进制序列文件)、TEXTFILE(文本)、RCFILE(列式存储格式文件)<br>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE</li><li>LOCATION：指定表在HDFS上的存储位置</li><li>LIKE允许用户复制现有的表结构，但是不复制数据</li></ul></li><li><p>管理表</p><ul><li><p>介绍<br>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会(或多或少地)控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。<strong>当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据</strong></p></li><li><p>实际操作</p><ul><li>创建普通表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span>;</span><br></pre></td></tr></table></figure><ul><li>根据查询结果创建表(查询的结果会添加到新创建的表中)：create table if not exists student2 as select id, name from student;</li><li>根据已存在的表结构创建表：create table if not exists student3 like student;</li><li>查询表的类型：desc formatted student;</li></ul></li></ul></li><li><p>外部表</p><ul><li>介绍：因为表是外部表，所以Hive并非认为其完全拥有这份数据。<strong>删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉</strong></li><li>实际操作(创建表,其余操作与管理表类似)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> default.dept(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure></li><li><p>管理表与外部表</p><ul><li>相互转换</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 修改内部表为外部表：</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>);</span><br><span class="line"><span class="comment">-- 修改外部表为内部表：</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>);</span><br><span class="line"><span class="comment">-- 注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写</span></span><br></pre></td></tr></table></figure><ul><li>使用场景<br>每天将收集到的网站日志定期流入HDFS文本文件。在外部表(原始日志表)的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表</li></ul></li></ul></li><li><p>分区表<br>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<strong>Hive中的分区就是分目录</strong>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多</p><ul><li><p>分区表基本操作</p><ul><li>创建分区表语法</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition(</span><br><span class="line">deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载数据到分区表中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201709');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201708');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201707’);</span><br></pre></td></tr></table></figure><ul><li><p>查询分区表中数据</p><ul><li>单分区查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><ul><li>多分区联合查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询多个分区</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">union</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201708'</span></span><br><span class="line"><span class="keyword">union</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201707'</span>;</span><br><span class="line"><span class="comment">-- 查询全部</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition;</span><br></pre></td></tr></table></figure></li><li><p>增加分区</p><ul><li>创建单个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>);</span><br></pre></td></tr></table></figure><ul><li>同时创建多个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201705'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br></pre></td></tr></table></figure></li><li><p>删除分区：</p><ul><li>删除单个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br></pre></td></tr></table></figure><ul><li>同时删除多个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201705'</span>), <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201706'</span>);</span><br></pre></td></tr></table></figure></li><li><p>查看分区表有多少分区：show partitions dept_partition;</p></li><li><p>查看分区表结构：desc formatted dept_partition;</p></li></ul></li><li><p>分区表扩展用法</p><ul><li>创建二级分区表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>加载二级分区数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709', day='13');</span><br></pre></td></tr></table></figure><ul><li>查询二级分区数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'13'</span>;</span><br></pre></td></tr></table></figure><ul><li><p>将数据上传到分区目录后，让分区表和数据产生关联的方式</p><ul><li><p>上传数据后修复(适用于数据较多的情况)</p><ul><li>上传数据：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive Cli命令环境下</span></span><br><span class="line">dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">dfs -put /opt/module/data/hive/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure><ul><li>查询数据(查询不到)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'12'</span>;</span><br></pre></td></tr></table></figure><ul><li>执行修复命令：msck repair table dept_partition2;</li></ul></li><li><p>上传数据后添加分区</p><ul><li>上传数据(同上)</li><li>执行添加分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition2 <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>,<span class="keyword">day</span>=<span class="string">'11'</span>);</span><br></pre></td></tr></table></figure></li><li><p>创建文件夹后load数据到分区</p><ul><li>创建目录：dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10;</li><li>上传数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709',day='10');</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>修改表</p><ul><li>重命名表<ul><li>语法：ALTER TABLE table_name RENAME TO new_table_name</li><li>实例：alter table dept_partition2 rename to dept_partition3;</li></ul></li><li>添加、修改和删除表分区(同上)</li><li>增加、修改、替换列信息<ul><li>语法：<ul><li>更新列：ALTER TABLE table_name <strong>CHANGE</strong> [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</li><li>添加和替换列：ALTER TABLE table_name <strong>ADD|REPLACE</strong> COLUMNS (col_name data_type [COMMENT col_comment], …)</li><li><strong>注意：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段</strong></li></ul></li><li>实操<ul><li>查询表结构(用于查看修改结果)：desc dept_partition;</li><li>添加列：alter table dept_partition add columns(deptdesc string);</li><li>更新列：alter table dept_partition change column deptdesc desc int;(貌似需要符合隐式转换规则)</li><li>替换列：alter table dept_partition replace columns(deptno string, dname string, loc string);</li></ul></li></ul></li></ul></li><li><p>删除表：drop table dept_partition;</p></li></ul><h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><ul><li><p>数据导入</p><ul><li><p>向表中装载数据(Load)</p><ul><li>语法</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data [local] inpath 'path_name' [overwrite] into table table_name [partition (partcol1=val1,...)];</span><br></pre></td></tr></table></figure><ul><li><p>参数解释</p><ul><li>load data：表示加载数据</li><li>local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li><li>inpath：表示加载数据的路径</li><li>overwrite：表示覆盖表中已有数据，否则表示追加</li><li>into table：表示加载到哪张表</li><li>table_name：表示具体的表</li><li>partition：表示上传到指定分区</li></ul></li><li><p>实际操作</p><ul><li>加载本地文件到Hive：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/student.txt' into table default.student;</span><br></pre></td></tr></table></figure><ul><li>加载(覆盖)HDFS文件数据到Hive中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive Cli命令环境下</span></span><br><span class="line">dfs -put /opt/module/data/hive/student.txt /user/sobxiong/hive;</span><br><span class="line">load data inpath '/user/sobxiong/hive/student.txt' (overwrite)into table default.student;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>通过查询语句向表中插入数据(Insert)</p><ul><li>基本插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span>  student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'wangwu'</span>);</span><br></pre></td></tr></table></figure><ul><li>根据单表查询结果插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201708'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><ul><li>根据多表查询结果插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure></li><li><p>查询语句中创建表并加载数据(As Select,查询的结果会添加到新创建的表中)：create table if not exists student3 as select id, name from student;</p></li><li><p>创建表时通过Location指定加载数据路径</p><ul><li>指定在HDFS上的位置创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student5(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/student5'</span>;</span><br></pre></td></tr></table></figure><ul><li>上传数据到HDFS上</li><li>查询数据</li></ul></li><li><p>Import数据到指定Hive表中(注意：先用export导出后,才能导入)</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import table student2 partition(month='201709') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure></li><li><p>数据导出</p><ul><li><p>Insert导出</p><ul><li>将查询的结果导出到本地</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/export/student'</span> <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><ul><li>将查询的结果格式化导出到本地</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/export/student1'</span> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><ul><li>将查询结果导出到HDFS上(取消local)</li></ul></li><li><p>Hadoop命令导出到本地(Hive Cli命令环境下)：dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/data/hive/export/student3.txt;</p></li><li><p>Hive Shell命令导出：基本语法(hive -f/-e 执行语句或脚本 &gt; file_name)</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -e 'select * from default.student;' &gt; /opt/module/data/hive/export/student4.txt;</span><br></pre></td></tr></table></figure><ul><li>Export导出到HDFS上(导出数据包括表数据和元数据)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export table default.student to '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><ul><li>Sqoop导出(<strong>敬请期待</strong>)</li></ul></li><li><p>清除表中数据(注意：只能删除管理表,不能删除外部表中数据)：truncate table student;</p></li></ul><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><ul><li>查询语句语法<br>官方wiki文档：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*]</span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">    | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><ul><li><p>基本查询</p><ul><li><p>全表和特定列查询<br>注意：</p><ul><li><strong>SQL大小写不敏感</strong></li><li>SQL可以写在一行/多行</li><li>关键字不能被缩写也不能分行</li><li>各子句一般要分行写</li><li>使用缩进提高语句的可读性</li></ul></li><li><p>列别名</p><ul><li>重命名一个列</li><li>便于计算</li><li>紧跟列名(或在列名和别名之间加入关键字AS)</li></ul></li><li><p>算术运算符</p></li></ul><p>+、-、*、/、%、&amp;、|、^、-</p><ul><li><p>常用函数</p><ul><li>求总行数：count()</li><li>求最大值/最小值：max()/min()</li><li>求总和：sum()</li><li>求平均值：avg()</li></ul></li><li><p>limit语句：典型的查询会返回多行数据。LIMIT子句用于限制返回的行数</p></li></ul></li><li><p>Where语句：使用Where子句可以过滤掉不满足条件的行，需要紧跟From子句</p><ul><li>比较运算符(同样可以用于Join… on和Having语句)<br>以下只介绍除=、&gt;和&lt;等简单的运算符</li></ul><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果</td></tr><tr><td>A IS [NOT] NULL</td><td>所有数据类型</td><td>如果A(不)等于NULL，则返回TRUE(FALSE)，反之返回FALSE(TRUE)</td></tr><tr><td>[NOT] IN(数值1, 数值2)</td><td>所有数据类型</td><td>(不)使用IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配</td></tr></tbody></table><ul><li><p>Like和RLike</p><ul><li><p>使用LIKE运算选择类似的值</p></li><li><p>选择条件可以包含字符或数字：</p><ul><li>% 代表零个或多个字符(任意个字符)</li><li>_ 代表一个字符</li></ul></li><li><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件</p></li><li><p>案例</p><ul><li>查找以2开头薪水的员工信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">'2%'</span>;</span><br></pre></td></tr></table></figure><ul><li>查找第二个数值为2的薪水的员工信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">'_2%'</span>;</span><br></pre></td></tr></table></figure><ul><li>查找薪水中含有2的员工信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> <span class="keyword">String</span>(sal) <span class="keyword">RLIKE</span> <span class="string">'[2]'</span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>逻辑运算符(And/Or/Not)</p></li></ul></li><li><p>分组</p><ul><li>Group By语句<br>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</li><li>Having语句<br>与where不同点：<ul><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据</li><li>where后面不能写分组函数，而having后面可以使用分组函数</li><li>having只用于group by分组统计语句</li></ul></li></ul></li><li><p>Join语句</p><ul><li>等值Join</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno, d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d</span><br><span class="line"><span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure><ul><li>表的别名<br>好处：(1)简化查询；(2)有限地提高执行效率</li><li>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure><ul><li>多表连接：连接n个表，一般至少需要n-1个连接条件</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.ename, d.deptno, l. loc_name</span><br><span class="line"><span class="keyword">FROM</span>   emp e</span><br><span class="line"><span class="keyword">JOIN</span>   dept d</span><br><span class="line"><span class="keyword">ON</span>     d.deptno = e.deptno</span><br><span class="line"><span class="keyword">JOIN</span>   location l</span><br><span class="line"><span class="keyword">ON</span>     d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。<br><strong>注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的</strong></p><ul><li><p>笛卡尔积<br>一般会在下面情况下出现：</p><ul><li>省略连接条件</li><li>连接条件无效</li><li>所有表中的所有行互相连接<br>一般会设置禁止出现笛卡尔积，如果有特殊情况，需要在单独在命令执行前设置一次性环境</li></ul></li><li><p>连接谓词在新版中支持or</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 无实际意义(ename = dname),只做可行性试验</span></span><br><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno <span class="keyword">or</span> e.ename=d.dname;</span><br></pre></td></tr></table></figure></li><li><p>排序</p><ul><li><p>全局排序(Order By:<strong>一个Reducer</strong>)</p><ul><li>排序方式：ASC(ascend升序,默认)、DESC(descend降序)</li><li>Order By子句在Select语句的结尾</li></ul></li><li><p>多个列排序(同MySQL)</p></li><li><p>内部排序(Sort By:<strong>每个Reduce内部进行排序,对全局结果集来说不是排序</strong>)</p><ul><li>注意：如果使用sort by不使用distribute by(即没有指定分区字段)，那么就采用一种产生随机数的函数分配分区(避免数据倾斜)</li><li>设置reduce数：set mapreduce.job.reduce=3;</li><li>按照部门编号降序排序：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 三个结果文件</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/sortby-result'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li><li><p>分区排序(Distribute By:<strong>类似MR中partition,进行分区,结合sort by使用</strong>)</p></li></ul><p><strong>DISTRIBUTE BY语句要写在SORT BY语句之前</strong>。distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果</p><ul><li><p>设置reduce数：set mapreduce.job.reduces=3;</p></li><li><p>先按照部门编号分区，再按照员工编号降序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/distribute-result'</span> <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Clubster By</p><ul><li>当distribute by和sorts by字段相同时，可以使用cluster by方式。</li><li>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC</li><li>具体实操</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 以下两种写法等价</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><ul><li>注意：按照部门编号分区，不一定就是固定死的数值(要看具体数据表中的字段不同的数目以及reduce设置的数目)，可以是20号和30号部门分到一个分区里面去</li></ul></li></ul></li><li><p>分桶及抽样查询</p><ul><li><p>分桶表数据存储</p><ul><li><p>介绍：<strong>分区针对的是数据的存储路径；分桶针对的是数据文件</strong>。分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。分桶是将数据集分解成更容易管理的若干部分的另一个技术</p></li><li><p>通过导入数据文件方式创建分桶表</p><ul><li>创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>查看表结构：desc formatted stu_back;(Num Buckets: 4)</li><li>导入数据到分桶表</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/student.txt' into table stu_buck;</span><br></pre></td></tr></table></figure><ul><li>在浏览器上查看创建的分桶表是否分成4个桶(4个文件)：在新版本中分成四个桶</li></ul></li><li><p>通过子查询导入数据方式创建分桶表</p><ul><li>先创建普通的stu表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><ul><li>向普通stu表中导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure><ul><li>清空stu_buck表中数据：truncate table stu_buck;</li><li>子查询方式导入数据到分桶表：insert into table stu_buck select id, name from stu;</li><li>浏览器查看：新版本中有4个分桶(文件)</li><li>需要设置Hive的属性(老版本)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置启用分桶</span></span><br><span class="line">set hive.enforce.bucketing=true;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置reduce数目为-1,会自动使用分桶数作为reduce的数目</span></span><br><span class="line">set mapreduce.job.reduces=-1;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空分桶表,重新导入数据,再去查看浏览器中的分桶</span></span><br><span class="line">truncate table stu_back;</span><br><span class="line">insert into table stu_buck select id, name from stu;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>分桶抽样查询</p><ul><li>介绍：对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求</li><li>实操：select * from stu_buck tablesample(bucket 1 out of 4 on id);</li><li>语法：tablesample是抽样语句，语法：tablesample(bucket x out of y)</li><li>参数解释<ul><li>y：y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2)2个bucket的数据，当y=8时，抽取(4/8)1/2个bucket的数据</li><li>x：<strong>x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y</strong>。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取(4/2)2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据</li><li><strong>注意：x的值必须小于等于y的值</strong></li></ul></li></ul></li></ul></li><li><p>其他常用查询函数</p><ul><li><p>空字段赋值</p><ul><li>函数说明：NVL：给值为NULL的数据赋值，它的格式是NVL(string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL，则返回NULL(replace_with可以是常量也可以是同表的另一个列)</li><li>查询(常量)：select nvl(comm,-1) from emp;</li><li>查询(另一列)：select nvl(comm,mgr) from emp;</li></ul></li><li><p>时间类</p><ul><li>date_format(格式化时间,第一个变量时间串只能是以’-‘分割)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-07-11'</span>,<span class="string">'yyyy:MM:dd'</span>);</span><br><span class="line"><span class="comment">-- 时间不以'-'分割,可以通过regex正则表达式替换/自定义函数</span></span><br><span class="line"><span class="keyword">select</span> regexp_replace(<span class="string">'2020/07/11'</span>,<span class="string">'/'</span>,<span class="string">'-'</span>);</span><br></pre></td></tr></table></figure><ul><li>date_add/sub(时间跟天数相加/相减)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">'2020-07-11'</span>,<span class="number">-5</span>/<span class="number">5</span>);</span><br></pre></td></tr></table></figure><ul><li>datediff(时间相差的间隔,前者-后者)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">'2020-07-11'</span>,<span class="string">'2020-07-08'</span>);</span><br></pre></td></tr></table></figure></li><li><p>CASE WHEN</p><ul><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">悟空  A 男</span><br><span class="line">大海  A 男</span><br><span class="line">宋宋  B 男</span><br><span class="line">凤姐  A 女</span><br><span class="line">婷姐  B 女</span><br><span class="line">婷婷  B 女</span><br></pre></td></tr></table></figure><ul><li>需求：求出不同部门的男女各多少人</li><li>创建emp_set.txt，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">dept_id <span class="keyword">string</span>,</span><br><span class="line">sex <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入本地数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/emp_sex.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure><ul><li>查询数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  dept_id,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">'男'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">'女'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  emp_sex</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  dept_id;</span><br></pre></td></tr></table></figure></li><li><p>行转列</p><ul><li>相关函数说明<br>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串<br>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间<br>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段</li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">孙悟空  白羊座  A</span><br><span class="line">大海  射手座  A</span><br><span class="line">宋宋  白羊座  B</span><br><span class="line">猪八戒  白羊座  A</span><br><span class="line">凤姐  射手座  A</span><br></pre></td></tr></table></figure><ul><li>需求：把星座和血型一样的人归类到一起。结果如下</li><li>person_info.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">constellation <span class="keyword">string</span>,</span><br><span class="line">blood_type <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/person_info.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure><ul><li>查询数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 总查询语句</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  t1.constellation_blood_type,</span><br><span class="line">  <span class="keyword">concat_ws</span>(<span class="string">'|'</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (<span class="keyword">select</span></span><br><span class="line">      <span class="keyword">name</span>,</span><br><span class="line">      <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) constellation_blood_type</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      person_info</span><br><span class="line">  ) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  t1.constellation_blood_type;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 第一步,查询出'射手座,A' '大海'</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) constellation_blood_type,</span><br><span class="line">  <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span> person_info;</span><br><span class="line"><span class="comment">-- 第二步,连接相同星座和血型的name</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  constellation_blood_type,</span><br><span class="line">  <span class="keyword">concat_ws</span>(<span class="string">'|'</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span> t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  t1.constellation_blood_type;</span><br><span class="line"><span class="comment">-- 最后一步,替换from后的t1为第一步中的临时表</span></span><br></pre></td></tr></table></figure></li><li><p>列转行</p><ul><li>函数说明<br>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行<br>LateRal View：<ul><li>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li><li>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</li></ul></li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》  悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》 悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》 战争,动作,灾难</span><br></pre></td></tr></table></figure><ul><li>需求：将电影分类中的数组数据展开，结果如下所示</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure><ul><li>创建本地movie.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">movie <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/opt/module/data/hive/movie.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure><ul><li>按需查询数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  movie,</span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure></li><li><p>窗口函数</p><ul><li>函数说明<br>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化<ul><li>参数说明<ul><li>Over()内<ul><li>CURRENT ROW：当前行</li><li>n PRECEDING：往前n行数据</li><li>n FOLLOWING：往后n行数据</li><li>UNBOUNDED：起点，UNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点</li></ul></li><li>Over()外<ul><li>LAG(col,n)：往前<strong>第</strong>n行数据</li><li>LEAD(col,n)：往后<strong>第</strong>n行数据</li><li>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型</strong></li></ul></li></ul></li></ul></li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; name,orderdate,cost</span><br><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure><ul><li>需求<ul><li>查询在2017年4月份购买过的顾客及总人数</li><li>查询顾客的购买明细及月购买总额</li><li>上述的场景，要将cost按照日期进行累加</li><li>查询顾客上次的购买时间</li><li>查询前20%时间的订单信息</li></ul></li><li>创建本地business.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">orderdate <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">cost</span> <span class="built_in">int</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/opt/module/data/hive/business.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure><ul><li><p>按需查询数据</p><ul><li>查询在2017年4月份购买过的顾客及总人数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">count</span>(*) <span class="keyword">over</span>()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">'2017-04'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure><ul><li>查询顾客的购买明细及月购买总额</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,<span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate)) <span class="keyword">from</span></span><br><span class="line">business;</span><br></pre></td></tr></table></figure><ul><li>上述的场景,要将cost按照日期进行累加</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- partition by ... order by和distribute by ... sort by效果相同,可替换</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行</span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><ul><li>查看顾客上次的购买时间</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,lag(orderdate,<span class="number">1</span>,<span class="string">'1900-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> last_time <span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><ul><li>查询前20%时间的订单信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) ntile_id</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) b</span><br><span class="line"><span class="keyword">where</span> ntile_id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Rank排名函数</p><ul><li>函数说明：<ul><li>Rank()：排序相同时会重复，总数不会变</li><li>DENSE_RANK()：排序相同时会重复，总数会减少</li><li>ROW_NUMBER()：会根据顺序计算</li></ul></li><li>数据准备</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; name subject score</span><br><span class="line">孙悟空  语文  87</span><br><span class="line">孙悟空  数学  95</span><br><span class="line">孙悟空  英语  68</span><br><span class="line">大海  语文  94</span><br><span class="line">大海  数学  56</span><br><span class="line">大海  英语  84</span><br><span class="line">宋宋  语文  64</span><br><span class="line">宋宋  数学  86</span><br><span class="line">宋宋  英语  84</span><br><span class="line">婷婷  语文  65</span><br><span class="line">婷婷  数学  85</span><br><span class="line">婷婷  英语  78</span><br></pre></td></tr></table></figure><ul><li>需求：计算各学科成绩排名</li><li>创建本地score.txt文件，复制数据</li><li>创建Hive表并导入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subject <span class="keyword">string</span>,</span><br><span class="line">score <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/score.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure><ul><li>按需查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">name</span>,</span><br><span class="line">  subject,</span><br><span class="line">  score,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">rank</span>,</span><br><span class="line">  <span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">dense_rank</span>,</span><br><span class="line">  row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) row_number</span><br><span class="line"><span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MFC</title>
      <link href="/2020/07/04/MFC/"/>
      <url>/2020/07/04/MFC/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#MFC入门">MFC入门</a></li><li><a href="#基于对话框编程">基于对话框编程</a></li><li><a href="#常用控件">常用控件</a></li><li><a href="#综合案例">综合案例</a></li></ul><a id="more"></a><h2 id="MFC入门"><a href="#MFC入门" class="headerlink" title="MFC入门"></a>MFC入门</h2><ul><li><p>MFC介绍：MFC用于在Windows平台上做GUI开发，MFC是微软基础类库的缩写</p></li><li><p>为什么要学MFC：向现实低头</p></li><li><p>Windows消息机制</p><ul><li><p>基本概念解释<br>我们在编写标准C程序的时候,经常会调用各种库函数来辅助完成某些功能。初学者使用得最多的C库函数就是printf了，这些库函数是由你所使用的编译器厂商提供的。在Windows平台下，也有类似的函数可供调用：不同的是，这些函数是由Windows操作系统本身提供的</p><ul><li><p>SDK<br>软件开发工具包(Software Development Kit)，一般都是一些被软件工程师用于为特定的软件包、软件框架、硬件平台、操作系统等建立应用软件的开发工具的集合</p></li><li><p>API函数<br>提供给应用程序编程的接口(Application Programming Interface)<br>Windows应用程序API函数是通过C语言实现的，所有主要的Windows函数都在Windows.h头文件中进行了声明</p></li><li><p>窗口</p><ul><li>窗口是Windows应用程序中一个非常重要的元素，一个Windows应用程序至少要有一个窗口，称为主窗口</li><li>窗口是屏幕上的一块矩形区域，是Windows应用程序与用户进行交互的接口。利用窗口可以接收用户的输入、以及显示输出</li><li>一个应用程序窗口通常都包含标题栏、菜单栏、系统菜单、最小化框、最大化框、 可调边框，有的还有滚动条。典型的窗口如下图所示：<br><img src="%E5%85%B8%E5%9E%8B%E7%AA%97%E5%8F%A3.png" alt="典型窗口"></li><li><strong>窗口可以分为客户区和非客户区</strong>，如上图。客户区是窗口的一部分，应用程序通常在客户区中显示文字或者绘制图形</li><li>标题栏、菜单栏、系统菜单、最小化框和最大化框、可调边框统称为窗口的非客户区，它们由Windows系统来管理，而应用程序则主要管理客户区的外观及操作</li><li><strong>窗口可以有一个父窗口，有父窗口的窗口称为子窗口</strong>。除了上图所示类型的窗口外，对话框和消息框也是一种窗口。在对话框上通常还包含许多子窗口，这些子窗口的形式有按钮、单选按钮、复选框、组框、文本编辑框等</li><li><strong>在Windows应用程序中，窗口是通过窗口句柄(HWND)来标识的。我们要对某个窗口进行操作，首先就要得到这个窗口的句柄</strong></li></ul></li><li><p>句柄<br>句柄(HANDLE)是Windows程序中一个重要的概念，使用也非常频繁。在Windows程序中，有各种各样的资源(窗口、图标、光标、画刷等)，系统在创建这些资源时会为它们分配内存，并返回标识这些资源的标识号，即句柄。在后面的内容中我们还会看到图标句柄(HICON)、光标句柄(HCURSOR)和画刷句柄(HBRUSH)</p></li><li><p>消息与消息队列</p><ul><li>Windows程序设计是一种完全不同于传统的DOS方式的程序设计方法。<strong>它是一种事件驱动方式的程序设计模式，主要是基于消息的(类似Android的Handle机制?)</strong></li><li>每一个Windows应用程序开始执行后，系统都会为该程序创建一个消息队列，这个消息队列用来存放该程序创建的窗口的消息</li><li>例如，当用户在窗口中画图的时候，按下鼠标左键，此时，操作系统会感知到这一事件，于是将这个事件包装成一个消息，投递到应用程序的消息队列中，等待应用程序的处理</li><li>然后应用程序通过一个消息循环不断地从消息队列中取出消息，并进行响应</li><li>在这个处理过程中，操作系统也会给应用程序“发送消息”。<strong>所谓“发送消息”，实际上是操作系统调用程序中一个专门负责处理消息的函数，这个函数称为窗口过程</strong><br><img src="%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B.png" alt="消息处理过程"></li></ul></li><li><p>WinMain函数<br>当Windows操作系统启动一个程序时，它调用的就是该程序的WinMain函数(实际是由插入到可执行文件中的启动代码调用的)。<strong>WinMain是Windows程序的入口点函数，与DOS程序的入口点函数main的作用相同，当WinMain函数结束或返回时，Windows应用程序结束</strong></p></li></ul></li><li><p>Windows编程模型<br>一个完整的Win32程序(#include &lt;windows.h&gt;)实现的功能是创建一个窗口，并在该窗口中响应键盘及鼠标消息，程序的实现步骤为：</p><ul><li>WinMain函数的定义</li><li>创建一个窗口</li><li>进行消息循环</li><li>编写窗口过程函数<br>具体案例实操(基于VS2017)：</li><li>创建项目：文件 -&gt; 新建 -&gt; 项目 -&gt; Visual C++ -&gt; 空项目</li><li>创建主函数文件：解决方案资源管理器 -&gt; 源文件 -&gt; 添加 -&gt; 新建项 -&gt; Vistual C++ -&gt; C++文件(.cpp) -&gt; 在名称中打入xxx.c(使用C语言编写)</li><li>WinMain函数介绍</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 程序入口函数</span></span><br><span class="line"><span class="comment">// WINAPI 代表__stdcall 参数的传递顺序：从右到左依次入栈,并且在函数返回前清空堆栈</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> WINAPI <span class="title">WinMain</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="comment">// 应用程序实例句柄</span></span></span></span><br><span class="line"><span class="function"><span class="params">    HINSTANCE hInstance,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="comment">// 上一个应用程序句柄,在win32环境下,参数一般为NULL,不起作用了</span></span></span></span><br><span class="line"><span class="function"><span class="params">    HINSTANCE hPrevInstance,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="comment">// char * argv[],命令行参数</span></span></span></span><br><span class="line"><span class="function"><span class="params">    LPSTR lpCmdLine,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="comment">// 显示命令 最大化、最小化、正常</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> nShowCmd)</span></span></span><br><span class="line"><span class="function"><span class="comment">/*</span></span></span><br><span class="line"><span class="function"><span class="comment">具体解释：</span></span></span><br><span class="line"><span class="function"><span class="comment"></span></span></span><br><span class="line"><span class="function"><span class="comment">WINAPI：是一个宏，它代表的是__stdcall(两个下划线)，表示的是参数传递的顺序：从右往左入栈，同时在函数返回前自动清空堆栈</span></span></span><br><span class="line"><span class="function"><span class="comment"></span></span></span><br><span class="line"><span class="function"><span class="comment">hInstance：表示该程序当前运行的实例的句柄，这是一个数值。当程序在Windows下运行时，它唯一标识运行中的实例(注意，只有运行中的程序实例，才有实例句柄)。一个应用程序可以运行多个实例，每运行一个实例，系统都会给该实例分配一个句柄值，并通过hInstance参数传递给WinMain函数</span></span></span><br><span class="line"><span class="function"><span class="comment"></span></span></span><br><span class="line"><span class="function"><span class="comment">hPrevInstance：表示当前实例的前一个实例的句柄。在Win32环境下，这个参数总是NULL，即在Win32环境下，这个参数不再起作用</span></span></span><br><span class="line"><span class="function"><span class="comment"></span></span></span><br><span class="line"><span class="function"><span class="comment">lpCmdLine：是一个以空终止的字符串，指定传递给应用程序的命令行参数，相当于C或C++中的main函数中的参数char *argv[]</span></span></span><br><span class="line"><span class="function"><span class="comment"></span></span></span><br><span class="line"><span class="function"><span class="comment">nShowCmd：表示一个窗口的显示，表示它是要最大化显示、最小化显示、正常大小显示还是隐藏显示</span></span></span><br><span class="line"><span class="function"><span class="comment">*/</span></span></span><br></pre></td></tr></table></figure><ul><li><p>创建一个窗口<br>创建一个完整的窗口，需要经过下面几个步骤：</p><ul><li><p>设计一个窗口类<br>一个完整的窗口具有许多特征，包括光标(鼠标进入该窗口时的形状)、图标、背景色等<br>在创建一个窗口前，要对该类型的窗口进行设计，指定窗口的特征。在Windows中，窗口的特征就是由WNDCLASS结构体来定义的，我们只需给WNDCLASS结构体对应的成员赋值，即可完成窗口类的设计<br><br>以下是WNDCLASS结构体相应成员变量的解释</p><ul><li>style：指定窗口的样式(风格)，常用的样式如下</li></ul><table><thead><tr><th>类型</th><th>含义</th></tr></thead><tbody><tr><td>CS_HREDRAW</td><td>当窗口水平方向上的宽度发生变化时，将重新绘制整个窗口。当窗口发生重绘时，窗口中的文字和图形将被擦除。如果没有指定这一样式，那么在水平方向上调整窗口宽度时，将不会重绘窗口</td></tr><tr><td>CS_VREDRAW</td><td>当窗口垂直方向上的高度发生变化时，将重新绘制整个窗口。如果没有指定这一样式，那么在垂直方向上调整窗口高度时，将不会重绘窗口</td></tr><tr><td>CS_NOCLOSE</td><td>禁用系统菜单的Close命令，这将导致窗口没有关闭按钮</td></tr><tr><td>CS_DBLCLKS</td><td>当用户在窗口中双击鼠标时，向窗口过程发送鼠标双击消息</td></tr></tbody></table></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WNDCLASS结构体</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> _<span class="title">WNDCLASS</span>&#123;</span></span><br><span class="line">    UINT        style;</span><br><span class="line">    WNDPROC     lpfnWndProc;</span><br><span class="line">    <span class="keyword">int</span>         cbClsExtra;</span><br><span class="line">    <span class="keyword">int</span>         cbWndExtra;</span><br><span class="line">    HINSTANCE   hInstance;</span><br><span class="line">    HICON       hIcon;</span><br><span class="line">    HCURSOR     hCursor;</span><br><span class="line">    HBRUSH      hbrBackground;</span><br><span class="line">    LPCWSTR     lpszMenuName;</span><br><span class="line">    LPCWSTR     lpszClassName;</span><br><span class="line">&#125; WNDCLASS;</span><br></pre></td></tr></table></figure><ul><li>注册窗口类</li><li>创建窗口</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MFC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态规划</title>
      <link href="/2020/06/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
      <url>/2020/06/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#背景与介绍">背景与介绍</a></li><li><a href="#10-正则表达式匹配">10-正则表达式匹配</a></li><li><a href="#139-单词拆分">139-单词拆分</a></li></ul><a id="more"></a><h2 id="背景与介绍"><a href="#背景与介绍" class="headerlink" title="背景与介绍"></a>背景与介绍</h2><ul><li>背景：经常在题目中get不到这道题是要用动态规划求解，而且当意会是动态规划题目后也经常很难一下子得出状态转移方程等解题要素…</li><li>介绍：动态规划算法与分治法类似，其基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次。如果我们能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。我们可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其结果填入表中。这就是动态规划法的基本思路。具体的动态规划算法多种多样，但它们具有相同的填表格式。</li></ul><h2 id="10-正则表达式匹配"><a href="#10-正则表达式匹配" class="headerlink" title="10-正则表达式匹配"></a>10-正则表达式匹配</h2><ul><li>题目描述：<br><img src="10-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D.png" alt="10-正则表达式匹配"></li></ul><h2 id="139-单词拆分"><a href="#139-单词拆分" class="headerlink" title="139-单词拆分"></a>139-单词拆分</h2><ul><li>题目描述：<br><img src="139-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86.png" alt="139-单词拆分"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针</title>
      <link href="/2020/06/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/%E5%8F%8C%E6%8C%87%E9%92%88/"/>
      <url>/2020/06/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/%E5%8F%8C%E6%8C%87%E9%92%88/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#背景与介绍">背景与介绍</a></li><li><a href="#16-最接近的三数之和">16-最接近的三数之和</a></li></ul><a id="more"></a><h2 id="背景与介绍"><a href="#背景与介绍" class="headerlink" title="背景与介绍"></a>背景与介绍</h2><ul><li>背景：因为写的时候想不到，以及不会做</li><li>介绍：双指针法有时也叫快慢指针，在数组里是用两个整型值代表下标，在链表里是两个指针，一般能实现O（n）的时间解决问题，两个指针的位置一般在第一个元素和第二个元素或者第一个元素和最后一个元素，快指针在前“探路”，当符合某种条件时慢指针向前挪</li></ul><h2 id="16-最接近的三数之和"><a href="#16-最接近的三数之和" class="headerlink" title="16-最接近的三数之和"></a>16-最接近的三数之和</h2><ul><li>题目描述：<br><img src="16-%E6%9C%80%E6%8E%A5%E8%BF%91%E7%9A%84%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C.png" alt="16-最接近的三数之和"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper入门</title>
      <link href="/2020/06/26/%E5%A4%A7%E6%95%B0%E6%8D%AE/Zookeeper/"/>
      <url>/2020/06/26/%E5%A4%A7%E6%95%B0%E6%8D%AE/Zookeeper/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#Zookeeper入门">Zookeeper入门</a></li><li><a href="#Zookeeper安装">Zookeeper安装</a></li><li><a href="#Zookeeper实战">Zookeeper实战</a></li><li><a href="#Zookeeper内部原理">Zookeeper内部原理</a></li><li><a href="#面试真题">面试真题</a></li></ul><a id="more"></a><h2 id="Zookeeper入门"><a href="#Zookeeper入门" class="headerlink" title="Zookeeper入门"></a>Zookeeper入门</h2><ul><li><p>概述<br>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目<br><img src="Zookeeper%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="Zookeeper工作机制"></p></li><li><p>特点<br><img src="Zookeeper%E7%89%B9%E7%82%B9.png" alt="Zookeeper特点"></p></li><li><p>数据结构<br><img src="Zookeeper%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png" alt="Zookeeper数据结构"></p></li><li><p>应用场景<br>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等</p><ul><li>统一命名服务<br><img src="Zookeeper%E7%BB%9F%E4%B8%80%E5%91%BD%E5%90%8D%E6%9C%8D%E5%8A%A1.png" alt="Zookeeper统一命名服务"></li><li>统一配置管理<br><img src="Zookeeper%E7%BB%9F%E4%B8%80%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86.png" alt="Zookeeper统一配置管理"></li><li>统一集群管理<br><img src="Zookeeper%E7%BB%9F%E4%B8%80%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86.png" alt="Zookeeper统一集群管理"></li><li>服务器节点动态上下线<br><img src="Zookeeper%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A8%E6%80%81%E4%B8%8A%E4%B8%8B%E7%BA%BF.png" alt="Zookeeper服务器动态上下线"></li><li>软负载均衡<br><img src="Zookeeper%E8%BD%AF%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1.png" alt="Zookeeper软负载均衡"></li></ul></li><li><p>下载地址：官网地址——<a href="https://zookeeper.apache.org" target="_blank" rel="noopener">https://zookeeper.apache.org</a></p></li></ul><h2 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h2><ul><li><p>本地模式安装部署</p><ul><li>安装前准备<ul><li>安装jdk</li><li>拷贝Zookeeper安装包到Linux系统下</li><li>解压到指定目录：tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/</li></ul></li><li>配置修改<ul><li>修改配置文件(conf目录下)：mv zoo_sample.cfg zoo.cfg</li><li>打开zoo.cfg文件，修改dataDir路径：dataDir=/opt/module/zookeeper-3.6.1/zkData</li><li>新建zkData目录(不同于Hadoop目录不能存在)：mkdir zkData</li></ul></li><li>操作Zookeeper<ul><li>启动Zookeeper Server(服务端)：bin/zkServer.sh start</li><li>查看进程是否启动：jps(正常会有一个QuorumPeerMain)</li><li>查看状态：bin/zkServer.sh status</li><li>启动Zookeeper Client(客户端)：bin/zkCli.sh</li><li>查看文件列表：ls /(一开始只有[zookeeper])</li><li>退出Zookeeper Client：quit</li><li>停止Zookeeper Server：bin/zkServer.sh stop</li></ul></li></ul></li><li><p>配置参数解读<br>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p><ul><li><em>tickTime = 2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒</em><br>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。<br>它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</li><li><em>initLimit = 10：LF初始通信时限</em><br>集群中的Follower跟随者服务器与Leader领导者服务器之间<strong>初始连接时</strong>能容忍的最多心跳数(tickTime的数量)，用它来限定集群中的Zookeeper服务器连接到Leader的时限</li><li><em>syncLimit = 5：LF同步通信时限</em><br>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer</li><li><em>dataDir：数据文件目录 + 数据持久化路径</em><br>主要用于保存Zookeeper中的数据</li><li><em>clientPort = 2181：客户端连接端口</em><br>监听客户端连接的端口</li></ul></li></ul><h2 id="Zookeeper实战"><a href="#Zookeeper实战" class="headerlink" title="Zookeeper实战"></a>Zookeeper实战</h2><ul><li><p>分布式安装部署</p><ul><li><p>集群规划：在hadoop1、hadoop2、hadoop3三个节点上部署Zookeeper形成集群</p></li><li><p>安装：分发zookeeper到hadoop2、hadoop3：xsync zookeeper-3.6.1/</p></li><li><p>配置服务器编号：</p><ul><li>在zookeeper-3.6.1目录下创建zkData目录：mkdir zkData</li><li>在zkData目录下创建myid文件：touch myid</li><li>编辑myid文件(设置当前server编号)：1</li><li>同步zkData到hadoop2、hadoop3上：xsync zkData/</li><li>在hadoop2、hadoop3上修改myid中的内容为2、3</li></ul></li><li><p>配置zoo.cfg文件</p><ul><li>新增集群节点配置</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cluster</span></span><br><span class="line">server.1=hadoop1:2888:3888</span><br><span class="line">server.2=hadoop2:2888:3888</span><br><span class="line">server.3=hadoop3:2888:3888</span><br></pre></td></tr></table></figure><ul><li>同步zoo.cfg文件：xsync zoo.cfg</li><li>配置参数解读：server.A=B:C:D<ul><li>A是一个数，表示这是第几号服务器。集群模式下配置文件myid中的数字就是A的值，<strong>Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server</strong></li><li>B是这个服务器的地址</li><li>C是这个服务器Follower与集群中的Leader服务器交换信息的端口</li><li>D是用来执行选举时服务器相互通信的端口：万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader</li></ul></li></ul></li><li><p>集群操作</p><ul><li>分别启动Zookeeper(启动前需要关闭Linux防火墙,使得各节点能够相互通信)：bin/zkServer.sh start</li><li>查看状态：bin/zkServer.sh status</li></ul></li></ul></li><li><p>客户端命令行操作(启动命令行：bin/zkCli.sh)</p></li></ul><table><thead><tr><th>命令基本语法</th><th>功能描述</th></tr></thead><tbody><tr><td>help(打错也是一个效果,当前无help命令)</td><td>显示所有操作命令</td></tr><tr><td>ls [-s] [-w] [-R] path</td><td>使用ls命令来查看当前path下znode中所包含的内容(-s：查看更新次数等详细数据,替代ls2;-w：设置watcher监听器,只有效一次;-R：递归查看节点)</td></tr><tr><td>create [-s] [-e] path [data]</td><td>创建节点(-s：含有序列;-e：临时(重启或者超时消失);data：写入path的内容,如果没有data创建不出节点)</td></tr><tr><td>get [-s] [-w] path</td><td>获得节点的值(-s：获取更加详细的节点数据;-w：设置watcher监听器,只有效一次)</td></tr><tr><td>set path data</td><td>设置节点的具体值</td></tr><tr><td>stat path</td><td>查看节点状态</td></tr><tr><td>delete path</td><td>删除节点</td></tr><tr><td>deleteall path</td><td>递归删除节点</td></tr></tbody></table><ul><li><p>API应用</p><ul><li><p>IDEA环境搭建</p><ul><li>创建空maven项目</li><li>添加pom文件：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.13.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>在resources目录下新建一个日志配置文件log4j.properties</li></ul><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure></li><li><p>创建ZooKeeper客户端</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 访问的ip</span></span><br><span class="line"><span class="keyword">private</span> String connectString = <span class="string">"hadoop1:2181,hadoop2:2181,hadoop3:2181"</span>;</span><br><span class="line"><span class="comment">// 会话超时时间</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line"><span class="comment">// zookeeper客户端</span></span><br><span class="line"><span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>创建子节点</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、创建节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createNode</span><span class="params">()</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">  String path = zkClient.create(<span class="string">"/sobxiong"</span>, <span class="string">"sobxiong,xixixihahaha"</span>.getBytes(),</span><br><span class="line">          ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">  System.out.println(<span class="string">"path = "</span> + path);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>获取子节点并监听节点变化</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2、获取子节点,并监控节点的变化</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getDataAndWatch</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">  List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line">  <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">      System.out.println(<span class="string">"child = "</span> + child);</span><br><span class="line">  &#125;</span><br><span class="line">  System.out.println(<span class="string">"------------"</span>);</span><br><span class="line">  Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 设置watcher中的process方法,使其继续调用自身继续监听</span></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line">          <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">              System.out.println(<span class="string">"child = "</span> + child);</span><br><span class="line">          &#125;</span><br><span class="line">          System.out.println(<span class="string">"------------"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">          e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>判断Znode是否存在</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3、判断节点是否存在</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">judgeNodeExist</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">  Stat stat = zkClient.exists(<span class="string">"/sobxiong"</span>, <span class="keyword">false</span>);</span><br><span class="line">  System.out.println(<span class="string">"stat = "</span> + stat);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>监听服务器节点动态上下线案例</p><ul><li><p>需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线</p></li><li><p>案例分析：<br><img src="%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A8%E6%80%81%E4%B8%8A%E4%B8%8B%E7%BA%BF%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="服务器动态上下线案例分析"></p></li><li><p>具体实现</p><ul><li>先在集群上创建/servers节点：create /servers “servers”</li><li>服务端向Zookeeper注册：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeServer</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 访问的ip</span></span><br><span class="line">  <span class="keyword">private</span> String connectString = <span class="string">"hadoop1:2181,hadoop2:2181,hadoop3:2181"</span>;</span><br><span class="line">  <span class="comment">// 会话超时时间</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">  <span class="comment">// zookeeper客户端</span></span><br><span class="line">  <span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    DistributeServer server = <span class="keyword">new</span> DistributeServer();</span><br><span class="line">    <span class="comment">// 1、连接zookeeper集群</span></span><br><span class="line">    server.getConnect();</span><br><span class="line">    <span class="comment">// 2、注册节点</span></span><br><span class="line">    server.register(args[<span class="number">0</span>]);</span><br><span class="line">    <span class="comment">// 3、业务逻辑</span></span><br><span class="line">    server.business();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">register</span><span class="params">(String hostName)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    String path = zkClient.create(<span class="string">"/servers/server"</span>, hostName.getBytes(),</span><br><span class="line">            ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">    System.out.println(hostName + <span class="string">" is online..."</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;&#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>客户端注册监听：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeClient</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 访问的ip</span></span><br><span class="line">  <span class="keyword">private</span> String connectString = <span class="string">"hadoop1:2181,hadoop2:2181,hadoop3:2181"</span>;</span><br><span class="line">  <span class="comment">// 会话超时时间</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> sessionTimeout = <span class="number">2000</span>;</span><br><span class="line">  <span class="comment">// zookeeper客户端</span></span><br><span class="line">  <span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    DistributeClient client = <span class="keyword">new</span> DistributeClient();</span><br><span class="line">    <span class="comment">// 1、获取zookeeper集群连接</span></span><br><span class="line">    client.getConnect();</span><br><span class="line">    <span class="comment">// 2、注册监听</span></span><br><span class="line">    client.getChildren();</span><br><span class="line">    <span class="comment">// 3、业务逻辑处理</span></span><br><span class="line">    client.business();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">    List&lt;String&gt; children = zkClient.getChildren(<span class="string">"/servers"</span>, <span class="keyword">true</span>);</span><br><span class="line">    <span class="comment">// 存储服务器节点主机名称集合</span></span><br><span class="line">    List&lt;String&gt; hostNames = <span class="keyword">new</span> ArrayList();</span><br><span class="line">    <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">      <span class="keyword">byte</span>[] data = zkClient.getData(<span class="string">"/servers/"</span> + child, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line">      hostNames.add(<span class="keyword">new</span> String(data));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将所有在线主机名称打印</span></span><br><span class="line">    System.out.println(<span class="string">"hostNames = "</span> + hostNames);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            getChildren();</span><br><span class="line">          &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Zookeeper内部原理"><a href="#Zookeeper内部原理" class="headerlink" title="Zookeeper内部原理"></a>Zookeeper内部原理</h2><ul><li><p>节点类型<br><img src="Zookeeper%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B.png" alt="Zookeeper节点类型"></p></li><li><p>Stat结构体</p><ul><li>cZxid：创建节点的事务zxid——每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生</li><li>ctime：znode被创建的毫秒数(从1970年开始)</li><li>mzxid：znode最后更新的事务zxid</li><li>mtime：znode最后修改的毫秒数(从1970年开始)</li><li>pZxid：znode最后更新的子节点zxid</li><li>cversion：znode子节点变化号，znode子节点修改次数</li><li>dataversion：znode数据变化号</li><li>aclVersion：znode访问控制列表的变化号</li><li>ephemeralOwner：如果是临时节点，这个是znode拥有者的session id；如果不是临时节点则是0</li><li><strong>dataLength：znode的数据长度</strong></li><li><strong>numChildren：znode子节点数量</strong></li></ul></li><li><p>监听器原理<br><img src="Zookeeper%E7%9B%91%E5%90%AC%E5%99%A8%E5%8E%9F%E7%90%86.png" alt="Zookeeper监听器原理"></p></li><li><p>选举机制</p><ul><li><strong>半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器</strong></li><li>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的</li><li>选举过程的举例(假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的——没有历史数据，在存放数据量这一点上，都是一样的)：<br><img src="Zookeeper%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6.png" alt="Zookeeper选举机制"><ul><li>服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上(3票)，选举无法完成，服务器1状态保持为LOOKING</li><li>服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的(服务器1)大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1、2状态保持LOOKING</li><li>服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING</li><li>服务器4启动，发起一次选举。此时服务器1、2、3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING</li><li>服务器5启动，同4一样当小弟</li></ul></li></ul></li><li><p>写数据流程<br><img src="Zookeeper%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="Zookeeper写数据流程"></p></li></ul><h2 id="面试真题"><a href="#面试真题" class="headerlink" title="面试真题"></a>面试真题</h2><ul><li><p>请简述ZooKeeper的选举机制？<br>参见4.4</p></li><li><p>ZooKeeper的监听原理是什么？<br>参见4.3</p></li><li><p>ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？<br>(1)部署方式：单机模式、集群模式；(2)角色：Leader和Follower；(3)集群最少需要机器数：3</p></li><li><p>ZooKeeper的常用命令有哪些？<br>ls create get delete set</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud</title>
      <link href="/2020/06/21/Spring/SpringCloud/"/>
      <url>/2020/06/21/Spring/SpringCloud/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#前期准备">前期准备</a></li><li><a href="#Eureka介绍">Eureka介绍</a></li></ul><a id="more"></a><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><ul><li><p>SpringCloud：分布式微服务架构的一站式解决方案，是多种微服务架构落地技术的集合体，俗称微服务全家桶；SpringCloud已成为微服务开发的主流技术栈</p></li><li><p>版本选择：</p><ul><li>SpringBoot：2.2.2.RELEASE<ul><li>git源码地址：<a href="https://github.com/spring-projects/spring-boot/releases/" target="_blank" rel="noopener">https://github.com/spring-projects/spring-boot/releases/</a></li><li>官方文档：<a href="https://docs.spring.io/spring-boot/docs/2.2.2.RELEASE/reference/htmlsingle/" target="_blank" rel="noopener">https://docs.spring.io/spring-boot/docs/2.2.2.RELEASE/reference/htmlsingle/</a></li></ul></li><li>SpringCloud：Hoxton.SR1、Alibaba 2.1.0.RELEASE(boot和cloud一起运用时,boot要照顾cloud)<ul><li>git源码地址：<a href="https://github.com/spring-projects/spring-cloud/wiki" target="_blank" rel="noopener">https://github.com/spring-projects/spring-cloud/wiki</a></li><li>官网：<a href="https://spring.io/projects/spring-cloud" target="_blank" rel="noopener">https://spring.io/projects/spring-cloud</a></li><li>官方文档：<a href="https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/htmlsingle/" target="_blank" rel="noopener">https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/htmlsingle/</a></li><li>中文文档：<a href="https://www.bookstack.cn/read/spring-cloud-docs/docs-index.md" target="_blank" rel="noopener">https://www.bookstack.cn/read/spring-cloud-docs/docs-index.md</a></li><li>cloud与boot之间的依赖关系<ul><li><a href="https://spring.io/projects/spring-cloud#overview" target="_blank" rel="noopener">https://spring.io/projects/spring-cloud#overview</a>(大版本)</li><li><a href="https://start.spring.io/actuator/info" target="_blank" rel="noopener">https://start.spring.io/actuator/info</a>(具体版本)</li></ul></li></ul></li></ul></li><li><p>架构编码构建</p><ul><li><p>重要规矩：约定 &gt; 配置 &gt; 编码</p></li><li><p>IDEA创建project工作空间</p><ul><li>父工程project(pom项目)<ul><li>设置项目字符编码：Editor -&gt; File Encodings -&gt; Global Encoding、Project Encoding、Default encoding for properties files设置UTF-8，勾选上Transparent native-to-ascii conversion</li><li>设置注解生效激活：Build,… -&gt; Complier -&gt; Annotatoin Processors -&gt; 勾选Enable annotation processing</li><li>java编译版本选择8：Build,… -&gt; Complier -&gt; Java Compiler -&gt; 设置父工程java编译版本为1.8</li></ul></li><li>父工程project的pom文件设置：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 统一管理jar包版本 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">  子模块继承之后,提供作用：锁定版本 + 子modlue不用写groupId和version</span></span><br><span class="line"><span class="comment">  父工程声明后不会直接引入,在子工程pom文件声明后才会正式引用</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">      ...</span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>子模块构建</p><ul><li><p>五个步骤：建module、改pom、写yml、主启动、业务类</p></li><li><p>设置热部署Devtools：</p><ul><li>添加devtools的pom依赖：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-devtools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>添加plugin插件的pom依赖：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>      <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fork</span>&gt;</span>true<span class="tag">&lt;/<span class="name">fork</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">addResources</span>&gt;</span>true<span class="tag">&lt;/<span class="name">addResources</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>设置启用自动build：Compiler -&gt; ADBC四个选项(首字母)打勾</li><li>更新idea属性值：command+shift+A调出搜索action，键入Registry，compiler.automake.allow.when.app.running和actionSystem.assertFocusAccessFromEdt打勾</li><li>重启idea</li></ul></li></ul></li><li><p>RestTemplate介绍：提供了多种便捷访问远程Http服务的方法，是一种简单便捷的访问restful服务模版类，是Spring提供的用于访问Rest服务的<strong>客户端模板工具集</strong></p><ul><li>官网地址：<a href="https://docs.spring.io/spring-framework/docs/5.2.2.RELEASE/javadoc-api/org/springframework/web/client/RestTemplate.html" target="_blank" rel="noopener">https://docs.spring.io/spring-framework/docs/5.2.2.RELEASE/javadoc-api/org/springframework/web/client/RestTemplate.html</a></li><li>使用：<br>使用restTemplate访问restful接口非常地简单粗暴。(url、requestMap、ResponseBean.class)这三个参数分别代表REST请求地址、请求参数、HTTP响应需转换成的对象类型</li></ul></li></ul><h2 id="Eureka介绍"><a href="#Eureka介绍" class="headerlink" title="Eureka介绍"></a>Eureka介绍</h2><ul><li><p>什么是服务注册与发现<br>Eureka采用了CS的设计架构，Eureka Server作为服务注册功能的服务器，它是服务注册中心，而系统中的其他微服务，使用 Eureka的客户端连接到Eureka Server并维持心跳连接。这样系统维护人员就可以通过Eureka Server来监控各个微服务是否正常运行<br>在服务注册与发现中有一个注册中心。当服务器启动时，会把当前自己的服务器信息比如自己服务地址、通信地址等以别名方式注册到注册中心上，另一方(消费者｜服务提供者)以别名的方式去注册中心上获取实际的服务器通讯地址，然后再实现本地RPC调用RPC远程调用。框架核心设计思想在于注册中心，因为使用注册中心管理每个服务与服务之间的一个依赖关系(服务治理概念)。在任何RPC远程框架中，都会有一个注册中心(存放服务地址相关信息(接口地址))<br><img src="Eureka%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C.png" alt="Eureka服务注册"></p></li><li><p>两个组件</p><ul><li><strong>Eureka Server</strong>提供服务注册服务<br>各个微服务节点通过配置启动后，会在EurekaServer中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息</li><li><strong>EurekaClient</strong>通过注册中心进行访问<br>是一个Java客户端，用于简化与Eureka Server的交互，客户端也同时具备一个内置的，使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会向Eureka Server发送心跳(默认周期30秒)。如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，Eureka Server将会从服务注册表中将这个服务节点移除(默认90秒)</li></ul></li><li><p>单机Eureka</p><ul><li><p>服务端创建</p><ul><li>创建module：cloud-eureka-server7001</li><li>改pom：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 主要加入eureka-server依赖 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-starter-netflix-eureka-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>写yml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">7001</span></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">instance:</span></span><br><span class="line">    <span class="comment"># eureka服务端实例名称</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">localhost</span></span><br><span class="line">  <span class="attr">client:</span></span><br><span class="line">    <span class="comment"># false表示不向注册中心注册自己</span></span><br><span class="line">    <span class="attr">register-with-eureka:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># false表示自己就是注册中心，职责是维护服务实例，并不需要去检索服务</span></span><br><span class="line">    <span class="attr">fetch-registry:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">service-url:</span></span><br><span class="line">      <span class="comment"># 设置与 eureka server交互的地址查询服务和注册服务都需要依赖这个地址</span></span><br><span class="line">      <span class="attr">defaultZone:</span>  <span class="string">http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka</span></span><br></pre></td></tr></table></figure><ul><li>主启动</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="comment">// 声明自己是eureka的服务端</span></span><br><span class="line"><span class="meta">@EnableEurekaServer</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EurekaMainApplication</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(EurekaMainApplication<span class="class">.<span class="keyword">class</span>,<span class="title">args</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>测试：<a href="http://localhost:7001" target="_blank" rel="noopener">http://localhost:7001</a>查看eureka的服务页面</li></ul></li><li><p>客户端设置<br>Eureka Client端cloud-provider-payment8001将注册进EurekaServer成为服务提供者 provider，cloud-consumer-order80同理</p><ul><li>改pom</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- eureka client --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-starter-netflix-eureka-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>写yml</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">client:</span></span><br><span class="line">    <span class="comment"># 表示是否将自己注册进Eureka Server,默认为true</span></span><br><span class="line">    <span class="attr">register-with-eureka:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 是否从Eureka Server抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合Ribbon使用负载均衡</span></span><br><span class="line">    <span class="attr">fetch-registry:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">service-url:</span></span><br><span class="line">      <span class="attr">defaultZone:</span>  <span class="string">http://localhost:7001/eureka</span></span><br></pre></td></tr></table></figure><ul><li>添加注解：在SpringBootApplication启动类上添加@EnableEurekaClient注解</li><li>测试：<a href="http://localhost:7001" target="_blank" rel="noopener">http://localhost:7001</a>查看eureka的服务页面，看看Application Instances</li></ul></li></ul></li><li><p>集群Eureka构建</p><ul><li><p>集群目的：高可用，如果注册中心只有一个，出了故障就会导致整个服务环境不可用(解决方法：搭建Eureka注册中心集群，实现负载均衡+故障排错)</p></li><li><p>集群注册原理：互相注册，相互守望(多个eureka server相互测住,保障信息共享)</p></li><li><p>搭建集群</p><ul><li>新建7002模块</li><li>复制7001的pom、yml和主启动文件</li><li>修改hosts(新增)</li></ul><p>127.0.0.1 eureka7001.com<br>127.0.0.1 eureka7002.com</p><ul><li>修改yml(相互注册)</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">instance:</span></span><br><span class="line">    <span class="comment"># eureka服务端实例名称(为区分,取不同)</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">eureka7001.com</span></span><br><span class="line">  <span class="attr">client:</span></span><br><span class="line">    <span class="attr">service-url:</span></span><br><span class="line">      <span class="comment"># 两台集群所以相互注册</span></span><br><span class="line">      <span class="attr">defaultZone:</span>  <span class="string">http://eureka7002.com:7002/eureka</span></span><br></pre></td></tr></table></figure><ul><li>测试：访问<a href="http://eureka7001.com:7001" target="_blank" rel="noopener">http://eureka7001.com:7001</a>以及<a href="http://eureka7002.com:7002" target="_blank" rel="noopener">http://eureka7002.com:7002</a>，查看DS Replicas</li></ul></li><li><p>部署微服务模块到集群</p><ul><li>修改两个模块(payment与order)的yml：defaultZone: <a href="http://eureka7002.com:7002/eureka" target="_blank" rel="noopener">http://eureka7002.com:7002/eureka</a>,<a href="http://eureka7001.com:7001/eureka" target="_blank" rel="noopener">http://eureka7001.com:7001/eureka</a></li><li>启动微服务测试</li></ul></li><li><p>支付模块微服务的配置</p><ul><li>新建另一支付模块payment8002，复制payment8001的pom、yml(更改端口8002)和主启动类</li><li>获取端口号</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取yml设置的端口号</span></span><br><span class="line"><span class="meta">@Value</span>(<span class="string">"$&#123;server.port&#125;"</span>)</span><br><span class="line"><span class="keyword">private</span> String serverPort;</span><br></pre></td></tr></table></figure><ul><li>修改consumer80中controller中的支付url(改为动态)<ul><li>之前单机eurake中写死为8001，集群中有8001和8002</li><li>查看eurake页面<a href="http://eureka7001.com:7001" target="_blank" rel="noopener">http://eureka7001.com:7001</a>中8001和8002对应的application名称(就是yml设置的application.name)</li><li>将url设置为：http + application名(例：<a href="http://CLOUD-PAYMENT-SERVICE" target="_blank" rel="noopener">http://CLOUD-PAYMENT-SERVICE</a>)</li><li>此时未开启负载均衡不能访问页面：将consumer80模块下的配置类ApplicationContextConfig声明的RestTemplate的bean方法上添加注解@LoadBalanced开启负载均衡</li><li>访问<a href="http://localhost/consumer/payment/get/1" target="_blank" rel="noopener">http://localhost/consumer/payment/get/1</a>查看json串，如在payment的controller方法中返回方法中设置了port字符串，则可以看到8001和8002回来变换(线性负载均衡)</li></ul></li></ul></li><li><p>actuator微服务信息完善<br>修改主机名与暴露ip地址，可在<a href="http://eureka7002.com:7002" target="_blank" rel="noopener">http://eureka7002.com:7002</a>查看自定义主机名的变化(鼠标在主机名上方，浏览器左下角会出现完整访问地址)</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">instance:</span></span><br><span class="line">    <span class="comment"># 自定义主机名</span></span><br><span class="line">    <span class="attr">instance-id:</span> <span class="string">payment8002</span></span><br><span class="line">    <span class="comment"># 设置暴露ip地址</span></span><br><span class="line">    <span class="attr">prefer-ip-address:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>服务发现Discovery</p><ul><li>payment8001的controller添加代码</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Resource</span></span><br><span class="line"><span class="keyword">private</span> DiscoveryClient discoveryClient;</span><br><span class="line"></span><br><span class="line"><span class="meta">@GetMapping</span>(<span class="string">"/discovery"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">discovery</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取服务列表清单</span></span><br><span class="line">    List&lt;String&gt; services = discoveryClient.getServices();</span><br><span class="line">    <span class="keyword">for</span> (String service : services) &#123;</span><br><span class="line">        log.info(<span class="string">"service: &#123;&#125;"</span>, service);</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(<span class="string">"CLOUD-PAYMENT-SERVICE"</span>);</span><br><span class="line">    <span class="keyword">for</span> (ServiceInstance instance : instances) &#123;</span><br><span class="line">        log.info(<span class="string">"instance: id = &#123;&#125;, host = &#123;&#125;, port = &#123;&#125;, uri = &#123;&#125;"</span>,</span><br><span class="line">                instance.getInstanceId(), instance.getHost(), instance.getPort(), instance.getUri());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> discoveryClient;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>payment8001主类添加注解@EnableDiscoveryClient</li><li>等待热重启，观察日志</li></ul></li></ul></li><li><p>eureka自我保护(宁可保留错误的服务注册信息，也不盲目注销任何可能健康的服务实例——好死不如赖活着)</p><ul><li>概述：保护模式主要用于一组客户端和Eureka Server之间存在网络分区场景下的保护。一旦进入保护模式。<strong>Eureka Server将会尝试保护其服务注册表中的信息，不再删除服务注册表中的数据，也就是不会注销任何微服务</strong></li><li>是否开启保护模式：默认开始，如果在Eureka Server页面看到提示(EMERGENCY!……JUST TO BE SAFE)，则说明开启</li><li>为什么会产生自我保护机制：为了防止EurekaClient可以正常运行，但是在EurekaServer网络不通的情况下，EurekaServer<strong>不会立刻</strong>将EurekaClient服务剔除</li><li>什么是自我保护模式：默认情况下，如果EurekaServer在一定时间内没有接收到某个微服务实例的心跳，EurekaServer将会注销该实例(默认90s)。但是当网络分区故障发生(延时、卡顿、拥挤)时，微服务与EurekaServer之间无法正常通信，以上行为可能变得非常危险——<strong>因为微服务本身其实是健康的，此时本不应该注销这个微服务</strong>。Eureka通过“自我保护模式”来解决这个问题——当EurekaServer节点在短时间内丢失过多客户端时(可能发生了网络分区故障)，那么这个节点就会进入自我保护模式</li><li>关闭自我保护(访问eureka发现红字消息，关闭8001服务页面服务立马消失)</li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># eureka7001</span></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">server:</span></span><br><span class="line">    <span class="comment"># 关闭自我保护机制</span></span><br><span class="line">    <span class="attr">enable-self-preservation:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 心跳时间默认90s，改为2000ms，即2s</span></span><br><span class="line">    <span class="attr">eviction-interval-timer-in-ms:</span> <span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># payment8001</span></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">instance:</span></span><br><span class="line">    <span class="comment"># eureka客户端发送心跳的时间间隔，默认30s</span></span><br><span class="line">    <span class="attr">lease-renewal-interval-in-seconds:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># eureka服务端在收到最后一次心跳等待的时间上线，默认90s</span></span><br><span class="line">    <span class="attr">lease-expiration-duration-in-seconds:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring注解驱动开发</title>
      <link href="/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/"/>
      <url>/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#容器">容器</a></li><li><a href="#扩展原理">扩展原理</a></li><li><a href="#Web">Web</a></li></ul><a id="more"></a><h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><ul><li>@Configuration：类注解(配置类=配置文件,注解告诉Spring这是一个配置类)</li><li>@Bean：方法注解，在类方法中给出返回Bean的方法，并在方法上添加@Bean注解(给容器中注册一个Bean,类型为返回值的类型,id默认为方法名,可复写注解的value属性复写id)。<ul><li>@Scope：方法注解，设置作用域。常用值为：<ul><li>prototype：多实例，ioc容器诶懂并不会去调用方法创建对象放在容器中。每次获取的时候才会调用方法创建对象</li><li>singleton(默认单实例)：ioc容器启动会调用方法创建对象放到ioc容器中，以后每次获取就是直接从容器中(可看作使用map.get())拿</li></ul></li><li>@Lazy：懒加载，只有在singleton单实例下才生效，且需要在返回bean的方法上加上@Lazy注解。单实例bean默认在容器启动的时候创建对象；懒加载在容器启动时不创建对象，第一次使用(获取)Bean创建对象并初始化</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Scope</span>(<span class="string">"singleton"</span>)</span><br><span class="line"><span class="meta">@Lazy</span></span><br><span class="line"><span class="meta">@Bean</span>(<span class="string">"person"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> Person <span class="title">person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Person(<span class="string">"SOBXiong"</span>, <span class="number">22</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@ComponentScans：指定扫描规则组(value为ComponentScan集合)</li><li>@ComponentScan：类注解，指定组件扫描规则<ul><li>value：指定包名，这样Spring会扫描包下的所有组件(SpringBoot情况可能不同,不需要)</li><li>excludeFilters：指定排除的过滤器，filter可根据注解排除(排除规则)，classed指定注解的类</li><li>includeFilters：指定只需要包含的过滤器</li><li>useDefaultFilters：是否适用缺省的过滤器，默认true；如果要使includeFilters生效，则必须设置为false</li><li>FilterType：<ul><li>FilterType.ANNOTATION：按照注解方式</li><li>FilterType.ASSIGNABLE_TYPE：按照指定的类型(具体的类,包括子类和实现类)</li><li>FilterType.REGEX：适用正则表达式</li><li>FilterType.CUSTOM：使用自定义规则，需要自定义实现TypeFilter接口的类</li></ul></li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ComponentScan</span>(value = <span class="string">"packageName"</span>,excludeFilters = &#123;</span><br><span class="line">    <span class="meta">@Filter</span>(type=FilterType.ANNOTATION,classes=&#123;Controller<span class="class">.<span class="keyword">class</span>,<span class="title">Service</span>.<span class="title">class</span>&#125;)</span></span><br><span class="line"><span class="class">&#125;)</span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">MainConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Person <span class="title">person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Person(<span class="string">"SOBXiong"</span>, <span class="number">22</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestTypeFilter</span> <span class="keyword">implements</span> <span class="title">TypeFilter</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> metadataReader 读取到的当前正在扫描的类的信息</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> metadataReaderFactory 可以获取到其他任何类信息的工厂</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@return</span> boolean</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">match</span><span class="params">(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      <span class="comment">// 获取当前类注解的信息</span></span><br><span class="line">      AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata();</span><br><span class="line">      <span class="comment">// 获取当前正在扫描的类的类信息</span></span><br><span class="line">      ClassMetadata classMetadata = metadataReader.getClassMetadata();</span><br><span class="line">      <span class="comment">// 获取当前类的资源信息(类路径等)</span></span><br><span class="line">      Resource resource = metadataReader.getResource();</span><br><span class="line">      String className = classMetadata.getClassName();</span><br><span class="line">      System.out.println(<span class="string">"className = "</span> + className);</span><br><span class="line">      <span class="keyword">return</span> className.contains(<span class="string">"test"</span>);</span><br><span class="line">      <span class="comment">// return false;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@Conditional：按照一定的条件进行判断，满足条件给容器中注册bean(Spring底层大量用到);可以设置在类上，也可以设置在方法上。设置在返回bean的方法上：只根据条件解决是否注册bean。设置在类上：类中注册统一设置，满足条件时，这个类中配置的所有bean注册才能生效</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Conditional</span>(TestCondition<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line">@Bean("person2")</span><br><span class="line"><span class="function"><span class="keyword">public</span> Person <span class="title">person2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Person(<span class="string">"SOBXiong"</span>, <span class="number">22</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestCondition</span> <span class="keyword">implements</span> <span class="title">Condition</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> conditionContext      判断条件能使用的上下文(环境)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> annotatedTypeMetadata 注释信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> boolean</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">matches</span><span class="params">(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1、能获取到ioc使用的beanFactory</span></span><br><span class="line">        ConfigurableListableBeanFactory beanFactory = conditionContext.getBeanFactory();</span><br><span class="line">        <span class="comment">// 2、获取类加载器</span></span><br><span class="line">        ClassLoader classLoader = conditionContext.getClassLoader();</span><br><span class="line">        <span class="comment">// 3、获取当前环境信息</span></span><br><span class="line">        Environment environment = conditionContext.getEnvironment();</span><br><span class="line">        <span class="comment">// 4、获取到bean定义的注册类</span></span><br><span class="line">        BeanDefinitionRegistry registry = conditionContext.getRegistry();</span><br><span class="line">        <span class="comment">// 可以判断容器中的bean注册情况,也可以给容器中注册bean</span></span><br><span class="line">        <span class="keyword">boolean</span> isDefinition = registry.containsBeanDefinition(<span class="string">"person"</span>);</span><br><span class="line">        <span class="comment">// 获取运行系统的名称</span></span><br><span class="line">        String osName = environment.getProperty(<span class="string">"os.name"</span>);</span><br><span class="line">        <span class="keyword">if</span> (osName.contains(<span class="string">"Windows"</span>)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@Import：导入组件，id默认是组件的全类名</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 给容器中注册组件：</span></span><br><span class="line"><span class="comment"> * 1、包扫描+组件标注注解(<span class="doctag">@Controller</span>、<span class="doctag">@Service</span>、<span class="doctag">@Repository</span>、<span class="doctag">@Component</span>)</span></span><br><span class="line"><span class="comment"> * 2、<span class="doctag">@Bean</span>[导入第三方包里面的组件]</span></span><br><span class="line"><span class="comment"> * 3、<span class="doctag">@Import</span>[快速给容器中导入一个组件]</span></span><br><span class="line"><span class="comment"> *   1、容器会自动注册这个组件,id默认是全类名</span></span><br><span class="line"><span class="comment"> *   2、ImportSelector：返回需要导入的组件的全类名数组(SpringBoot源码中许多地方用到);</span></span><br><span class="line"><span class="comment"> *   3、ImportBeanDefinitionRegistrar：手动注册bean到容器中</span></span><br><span class="line"><span class="comment"> * 4、使用Spring提供的FactoryBean(工厂Bean),其他与Spring整合的框架使用的特别多</span></span><br><span class="line"><span class="comment"> *   1、默认获取的是工厂bean调用getObject创建的对象</span></span><br><span class="line"><span class="comment"> *   2、要获取工厂bean本身,需要给id前面加一个&amp;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@Import</span>(&#123;TestImportSelector<span class="class">.<span class="keyword">class</span>, <span class="title">TestImportBeanDefinitionRegistrar</span>.<span class="title">class</span>&#125;)</span></span><br><span class="line"><span class="class"><span class="title">public</span> <span class="title">class</span> <span class="title">MainConfig</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义逻辑返回需要导入的组件</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestImportSelector</span> <span class="keyword">implements</span> <span class="title">ImportSelector</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> annotationMetadata 当前标注<span class="doctag">@Import</span>注解的类的所有注解信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> String[] 导入到容器中的组件全类名数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String[] selectImports(AnnotationMetadata annotationMetadata) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String[]&#123;<span class="string">"com.xiong.test.Animal"</span>, <span class="string">"com.xiong.test.Person"</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestImportBeanDefinitionRegistrar</span> <span class="keyword">implements</span> <span class="title">ImportBeanDefinitionRegistrar</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 可以把所有需要添加到容器中的bean通过调用BeanDefinitionRegistry.registerBeanDefinition手工注册</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> importingClassMetadata 当前类的注解信息</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> registry               BeanDefinition注册类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registerBeanDefinitions</span><span class="params">(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> isWorldExist = registry.containsBeanDefinition(<span class="string">"World"</span>);</span><br><span class="line">        <span class="keyword">if</span> (!isWorldExist) &#123;</span><br><span class="line">            <span class="comment">// 注册一个bean,指定bean的名称和bean的定义信息(bean的类型,bean的Scope...)</span></span><br><span class="line">            registry.registerBeanDefinition(<span class="string">"world"</span>, <span class="keyword">new</span> RootBeanDefinition(World<span class="class">.<span class="keyword">class</span>))</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>FactoryBean(工厂Bean)：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个Spring定义的FactoryBean</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestFactoryBean</span> <span class="keyword">implements</span> <span class="title">FactoryBean</span>&lt;<span class="title">Animal</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 返回一个Animal对象,这个对象会添加到容器中</span></span><br><span class="line">    <span class="comment">// 调用此方法得到对象</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Animal <span class="title">getObject</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Animal();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Class&lt;?&gt; getObjectType() &#123;</span><br><span class="line">        <span class="keyword">return</span> Animal<span class="class">.<span class="keyword">class</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 是否是单实例</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSingleton</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfig</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TestFactoryBean <span class="title">testFactoryBean</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TestFactoryBean();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>@Bean指定初始化和销毁方法：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Car</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Car</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Car 构造方法"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Car init"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Car destroy"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * bean的生命周期：</span></span><br><span class="line"><span class="comment"> *  bean创建 --&gt; 初始化 --&gt; 销毁</span></span><br><span class="line"><span class="comment"> * 容器管理bean的生命周期;</span></span><br><span class="line"><span class="comment"> * 我们可以自定义初始化和销毁方法;容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 构造(对象创建)：</span></span><br><span class="line"><span class="comment"> *  单实例：在容器启动的时候创建对象</span></span><br><span class="line"><span class="comment"> *  多实例：在每次获取的时候创建对象</span></span><br><span class="line"><span class="comment"> * 初始化：对象创建完成,并赋值结束,调用初始化方法</span></span><br><span class="line"><span class="comment"> * 销毁：</span></span><br><span class="line"><span class="comment"> *  单实例：容器关闭的时候</span></span><br><span class="line"><span class="comment"> *  多实例：容器不会管理这个bean,容器不会调用销毁方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1、指定初始化和销毁方法(通过<span class="doctag">@Bean</span>注解指定init-method和destroy-method)</span></span><br><span class="line"><span class="comment"> * 2、通过让Bean实现InitializingBean(定义初始化方法逻辑),DisposableBean(定义销毁逻辑)</span></span><br><span class="line"><span class="comment"> * 3、可以使用JSR250：</span></span><br><span class="line"><span class="comment"> *     <span class="doctag">@PostConstruct</span>：在bean创建完成并且属性赋值完毕再执行初始化方法</span></span><br><span class="line"><span class="comment"> *     <span class="doctag">@PreDestroy</span>：在容器销毁bean之前通知进行清理工作</span></span><br><span class="line"><span class="comment"> * 4、BeanPostProcessor：bean的后置处理器(在bean初始化前后进行一些工作)</span></span><br><span class="line"><span class="comment"> *  postProcessBeforeInitialization：在初始化之前工作</span></span><br><span class="line"><span class="comment"> *  postProcessAfterInitialization：在初始化之后工作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigLifecycle</span> </span>&#123;</span><br><span class="line">    <span class="comment">// @Scope("prototype")</span></span><br><span class="line">    <span class="meta">@Bean</span>(initMethod = <span class="string">"init"</span>, destroyMethod = <span class="string">"destroy"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Car <span class="title">car</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Car();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ComponentScan</span>(<span class="string">"com.xiong.test"</span>)</span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">implements</span> <span class="title">InitializingBean</span>, <span class="title">DisposableBean</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cat</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Cat构造函数..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Cat destroy..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterPropertiesSet</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Cat init..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 后置处理器：在bean初始化前后进行处理工作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestBeanPostProcessor</span> <span class="keyword">implements</span> <span class="title">BeanPostProcessor</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">postProcessBeforeInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"postProcessBeforeInitialization: "</span> + beanName + <span class="string">" , "</span> + bean);</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">postProcessAfterInitialization</span><span class="params">(Object bean, String beanName)</span> <span class="keyword">throws</span> BeansException </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"postProcessAfterInitialization: "</span> + beanName + <span class="string">" , "</span> + bean);</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>BeanPostProcessor原理</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 遍历得到容器中所有的BeanPostProcessor;挨个执行beforeInitialization,</span></span><br><span class="line"><span class="comment">// 一旦返回null,跳出for循环,不追执行后面的BeanPostProcessor.postProcessBeforeInitialization()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 给bean进行属性赋值</span></span><br><span class="line">populateBean(beanName, mbd, instanceWrapper);</span><br><span class="line">initializeBean(beanName, exposedObject, mbd);</span><br><span class="line">&#123; <span class="comment">// 以下就是initializeBean的粗略内容</span></span><br><span class="line">    applyBeanPostProcessorsBeforeInitialization(bean, beanName);</span><br><span class="line">    invokeInitMethods(beanName, wrappedBean, mbd);</span><br><span class="line">    applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>Spring底层对BeanPostProcessor的使用</p><ul><li>ApplicationContextAwareProcessor：可让bean获取容器对象context</li><li>BeanValidationPostProcessor：Web表单校验的处理器</li><li>InitDestroyAnnotationBeanPostProcessor：@PostConstruct和@Bean的init-method等方法的具体实现</li><li>AutowiredAnnotationBeanPostProcessor：@Autowired自动注入功能的具体实现</li></ul></li><li><p>属性赋值</p><ul><li>使用@Value赋值：<ul><li>基本数值</li><li>SpEL：#{}</li><li>${}：取出配置文件(properties或yaml)中的值(在运行环境变量里面的值)</li></ul></li><li>使用@PropertySource加载外部配置文件</li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用@PropertySource读取外部配置文件中的k/v保存到运行的环境变量中</span></span><br><span class="line"><span class="comment">// 加载完外部的配置文件以后使用$&#123;&#125;取出配置文件的值</span></span><br><span class="line"><span class="comment">// 当前只能加载properties文件,yaml不能,是采用的加载器问题</span></span><br><span class="line"><span class="meta">@PropertySource</span>(value = &#123;<span class="string">"classpath:application.properties"</span>&#125;, encoding = <span class="string">"utf-8"</span>)</span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigPropertyValues</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Person <span class="title">person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Person();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"SOBXiong"</span>)</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"#&#123;22+5&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="meta">@Value</span>(<span class="string">"$&#123;person.nickName&#125;"</span>)</span><br><span class="line">    <span class="keyword">private</span> String nickName;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// application.properties</span></span><br><span class="line"><span class="comment">// person.nickName=熊哈哈</span></span><br></pre></td></tr></table></figure><ul><li>自动装配</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自动装配：</span></span><br><span class="line"><span class="comment"> * Spring利用依赖注入(DI),完成对IOC容器中各个组件的依赖关系赋值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Autowired</span>：自动注入(Spring定义的)</span></span><br><span class="line"><span class="comment"> * TestService&#123;</span></span><br><span class="line"><span class="comment"> *      <span class="doctag">@Autowired</span> TestDao testDao;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> * 1、默认优先按照类型去容器中找对应的组件：context.getBean(TestDao.class);</span></span><br><span class="line"><span class="comment"> * 2、如果找到多个相同类型的组件,将属性名作为组件的id去容器中查找</span></span><br><span class="line"><span class="comment"> * 3、<span class="doctag">@Qualifier</span>("testDao")：使用<span class="doctag">@Qualifier</span>指定需要装配的组件id,而不是使用属性名</span></span><br><span class="line"><span class="comment"> * 4、自动装配默认一定要将属性赋值好,没有就会报错(可以使用<span class="doctag">@Autowired</span>注解中的required=false避免报错)</span></span><br><span class="line"><span class="comment"> * 5、<span class="doctag">@Primary</span>：让Spring进行自动装配的时候默认使用首选的bean(此时<span class="doctag">@Qualifier</span>不能使用);也可以使用<span class="doctag">@Qualifier</span>指定需要装配的具体bean</span></span><br><span class="line"><span class="comment"> * 6、Spring还支持使用<span class="doctag">@Resource</span>(JSR250)和<span class="doctag">@Inject</span>(JSR330)[java规范的注解]</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Resource</span>：可以和<span class="doctag">@Autowired</span>一样实现自动装配功能,但默认是按照组件名称进行装配的(也可以通过name属性进行指定id);不能支持<span class="doctag">@Qualifier</span>和required=false</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Inject</span>：需要导入javax.inject的包,和<span class="doctag">@Autowired</span>的功能一样,但没有required属性</span></span><br><span class="line"><span class="comment"> * 7、<span class="doctag">@Autowired</span>可以在构造器、参数、方法和属性上标注,都是从容器中获取组件的值</span></span><br><span class="line"><span class="comment"> *      1、[标注在方法位置]：<span class="doctag">@Bean</span>标注方法的方法参数;参数从容器中获取;默认不写<span class="doctag">@Autowired</span>效果是一样的;都能自动装配</span></span><br><span class="line"><span class="comment"> *      2、[标注在构造器位置]：如果组件只有一个有参构造器,这个有参构造器的<span class="doctag">@Autowired</span>可以省略,参数位置的组件还是可以自动从容器中获取;</span></span><br><span class="line"><span class="comment"> *      但如有既有有参又有无参,会优先调用无参构造器,这使得boss的car属性和容器中的car不是同一个</span></span><br><span class="line"><span class="comment"> *      3、[标注在参数位置]</span></span><br><span class="line"><span class="comment"> * 8、自定义组件想要使用Spring容器底层的的一些组件(ApplicationContext、BeanFactory等)</span></span><br><span class="line"><span class="comment"> *  自定义组件实现xxxAware接口：在创建对象的时候,会调用接口规定的方法注入相关组件;</span></span><br><span class="line"><span class="comment"> *  xxxAware使用xxxProcessor：applicationContextAware =&gt; applicationContextAwareProcessor(BeanPostProcessor的实现类)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@ComponentScan</span>(<span class="string">"com.xiong.test2"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigAutowired</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Primary</span></span><br><span class="line">    <span class="meta">@Bean</span>(<span class="string">"testDao2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> TestDao <span class="title">testDao</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TestDao(<span class="string">"2"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// @Bean标注的方法创建对象的时候,方法参数的值从容器中获取</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boss <span class="title">boss</span><span class="params">(Car car)</span></span>&#123;</span><br><span class="line">        Boss boss = <span class="keyword">new</span> Boss();</span><br><span class="line">        boss.setCar(car);</span><br><span class="line">        <span class="keyword">return</span> boss;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Car</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 默认加在ioc容器中的组件，容器启动会调用无参构造器创建对象，在进行初始化赋值等操作</span></span><br><span class="line"><span class="comment">// @Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Boss</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Car car;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Boss</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">    <span class="comment">// 构造器要用的组件，都是从容器中获取</span></span><br><span class="line">    <span class="comment">// @Autowired</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Boss</span><span class="params">(@Autowired Car car)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.car = car;</span><br><span class="line">        System.out.println(<span class="string">"Boss constructor with one parameter!"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Car <span class="title">getCar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> car;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 标注在方法上，Spring容器创建当前对象，就会调用方法完成赋值</span></span><br><span class="line">    <span class="comment">// 方法使用的参数，自定义类型的值从ioc容器中获取</span></span><br><span class="line">    <span class="comment">// @Autowired</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCar</span><span class="params">(Car car)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.car = car;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Boss&#123;"</span> +</span><br><span class="line">                <span class="string">"car="</span> + car +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestService</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Qualifier</span>(<span class="string">"testDao2"</span>)</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> TestDao testDao;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"TestService&#123;"</span> +</span><br><span class="line">                <span class="string">"testDao="</span> + testDao +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Repository</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestDao</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String label = <span class="string">"1"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>@Profile：Spring为我们提供的可以根据当前环境,动态地激活和切换一系列组件的共功能;指定组件在哪个环境的情况下才能被注册到容器中,不指定,任何环境下都能注册这个组件(开发、测试/生产环境,数据源(A/B/C))</p><ul><li>加了环境标志性的bean,只有这个环境被激活的时候才能注册到容器中(默认环境是default)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Profile</span>(<span class="string">"test"</span>)</span><br><span class="line"><span class="meta">@Bean</span>(<span class="string">"testDataSource"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> DataSource <span class="title">dataSourceTest</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123; ... &#125;</span><br></pre></td></tr></table></figure><ul><li><p>写在配置类上,只有是指定的环境的时候,整个配置类里面的内容才能生效</p></li><li><p>没有标注环境标识的bean在任何环境下都是加载的</p></li><li><p>环境的激活：</p><ul><li>使用命令行动态参数：虚拟机参数位置加载 -Dspring.profiles.active=test</li><li>代码的方式激活某种环境</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">contextLoads</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1、创建ioc容器</span></span><br><span class="line">    AnnotationConfigApplicationContext context =</span><br><span class="line">            <span class="keyword">new</span> AnnotationConfigApplicationContext();</span><br><span class="line">    <span class="comment">// 2、设置需要激活的环境</span></span><br><span class="line">    context.getEnvironment().setActiveProfiles(<span class="string">"test"</span>);</span><br><span class="line">    <span class="comment">// 3、注册主配置类</span></span><br><span class="line">    context.register(MainConfigProfile<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、启动刷新容器</span></span><br><span class="line">    context.refresh();</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭容器</span></span><br><span class="line">    context.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>AOP</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * AOP：指在程序运行期间动态地将某段代码切入到指定方法指定位置进行运行地编程方式</span></span><br><span class="line"><span class="comment"> * 1、导入AOP模块：Spring AOP(SpringBoot导入MVC模块会连着导入AOP模块)</span></span><br><span class="line"><span class="comment"> * 2、定义一个业务逻辑类(MathCalculator)：在业务逻辑运行地时候将日志进行打印(方法之前、方法运行结束、方法出现异常...)</span></span><br><span class="line"><span class="comment"> * 3、定义一个日志切面类(LogAspect)：切面类里面地方法需要动态感知MathCalculator.div运行到哪里然后执行</span></span><br><span class="line"><span class="comment"> *   通知方法：</span></span><br><span class="line"><span class="comment"> *   前置通知<span class="doctag">@Before</span>：logStart(在目标方法div运行之前运行)</span></span><br><span class="line"><span class="comment"> *   后置通知<span class="doctag">@After</span>：logEnd(在目标方法div运行结束之后运行——无论方法是正常结束还是异常结束)</span></span><br><span class="line"><span class="comment"> *   返回通知<span class="doctag">@AfterReturning</span>：logReturn(在目标方法div正常返回之后运行)</span></span><br><span class="line"><span class="comment"> *   异常通知<span class="doctag">@AfterThrowing</span>：logException(在目标方法div出现异常以后运行)</span></span><br><span class="line"><span class="comment"> *   环绕通知<span class="doctag">@Around</span>：动态代理,手动推进目标方法div运行(jointPoint.proceed())</span></span><br><span class="line"><span class="comment"> * 4、给切面类的目标方法标注何时何地运行(通知注解)</span></span><br><span class="line"><span class="comment"> * 5、将切面类和业务逻辑类(目标方法所在类)都加入到容器中</span></span><br><span class="line"><span class="comment"> * 6、必须告诉Spring哪个类是切面类(给切面类上加一个注解<span class="doctag">@Aspect</span>)</span></span><br><span class="line"><span class="comment"> * 7、给配置类中加<span class="doctag">@EnableAspectJAutoProxy</span>(开启基于注解的AOP模式);在Spring中有很多的<span class="doctag">@EnableXXX</span>注解,替代以前的xml配置</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 三步：</span></span><br><span class="line"><span class="comment"> *  1、将业务逻辑组件和切面类都加入到容器中;告诉Spring哪个是切面类(<span class="doctag">@Aspect</span>)</span></span><br><span class="line"><span class="comment"> *  2、在切面类上的每一个通知方法上标注通知注解,告诉Spring何时何地运行(切入点表达式)</span></span><br><span class="line"><span class="comment"> *  3、开启基于注解的AOP模式：<span class="doctag">@EnableAspectJAutoProxy</span></span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * AOP原理：[看给容器中注册了什么组件,这个组件什么时候工作以及这个组件的功能是什么?]</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@EnableAspectJAutoProxy</span>：</span></span><br><span class="line"><span class="comment"> *  1、<span class="doctag">@EnableAspectJAutoProxy</span>是什么？</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Import</span>(AspectJAutoProxyRegistrar.class)：给容器中导入AspectJAutoProxyRegistrar</span></span><br><span class="line"><span class="comment"> *      利用AspectJAutoProxyRegistrar自定义给容器中注册bean：BeanDefinition</span></span><br><span class="line"><span class="comment"> *      internalAutoProxyCreator = AnnotationAwareAspectJAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *  给容器中注册一个AnnotationAwareAspectJAutoProxyCreator,id为internalAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  2、AnnotationAwareAspectJAutoProxyCreator：</span></span><br><span class="line"><span class="comment"> *      AbstractAutoProxyCreator实现了SmartInstantiationAwareBeanPostProcessor,BeanFactoryAware接口</span></span><br><span class="line"><span class="comment"> *          AbstractAdvisorAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *              AspectJAwareAdvisorAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *                  AnnotationAwareAspectJAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *    关注后置处理器(bean初始化完成前后做事情)、自动装配beanFactory</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  阅读线索：</span></span><br><span class="line"><span class="comment"> *  AbstractAutoProxyCreator.setBeanFactory()</span></span><br><span class="line"><span class="comment"> *  AbstractAutoProxyCreator.postProcessBeforeInstantiation()</span></span><br><span class="line"><span class="comment"> *  AbstractAutoProxyCreator.postProcessAfterInitialization()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  AbstractAdvisorAutoProxyCreator.setBeanFactory()复写父类方法</span></span><br><span class="line"><span class="comment"> *  方法内还执行了initBeanFactory()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  AnnotationAwareAspectJAutoProxyCreator.initBeanFactory()复写父类方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  1、传入配置类,创建ioc容器</span></span><br><span class="line"><span class="comment"> *  2、调用配置类,调用refresh()刷新容器</span></span><br><span class="line"><span class="comment"> *  3、registerBeanPostProcessors(beanFactory)：注册bean的后置处理器来方便拦截bean的创建</span></span><br><span class="line"><span class="comment"> *      1、先获取ioc容器已定义了的需要创建的所有BeanPostProcessor(第一步传入配置类带入的)：</span></span><br><span class="line"><span class="comment"> *          String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false);</span></span><br><span class="line"><span class="comment"> *      2、给容器中加入别的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      3、优先注册实现了PriorityOrdered接口的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      4、再给容器中注册实现了Ordered接口的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      5、最后注册没实现优先级接口的BeanPostProcessor</span></span><br><span class="line"><span class="comment"> *      6、注册BeanPostProcessor,实际上就是创建BeanPostProcessor对象并保存在容器中</span></span><br><span class="line"><span class="comment"> *          创建internalAutoProxyCreator的BeanPostProcessor[AnnotationAwareAspectJAutoProxyCreator对象]</span></span><br><span class="line"><span class="comment"> *              1、创建Bean的实例</span></span><br><span class="line"><span class="comment"> *              2、populateBean()：给bean的各种属性赋值</span></span><br><span class="line"><span class="comment"> *              3、initializeBean()：初始化bean</span></span><br><span class="line"><span class="comment"> *                  1、invokeAwareMethods()：处理Aware接口的方法回调</span></span><br><span class="line"><span class="comment"> *                  2、applyBeanPostProcessorsBeforeInitialization()：应用后置处理器的postProcessBeforeInitialization()方法</span></span><br><span class="line"><span class="comment"> *                  3、invokeInitMethods()：执行自定义的初始化方法</span></span><br><span class="line"><span class="comment"> *                  4、applyBeanPostProcessorsAfterInitialization()：应用后置处理器的postProcessAfterInitialization()方法</span></span><br><span class="line"><span class="comment"> *              4、BeanPostProcessor(AnnotationAwareAspectJAutoProxyCreator)创建成功 -&gt; aspectJAdvisorsBuilder</span></span><br><span class="line"><span class="comment"> *      7、把BeanPostProcessor注册到BeanFactory中：beanFactory.addBeanPostProcessor(postProcessor);</span></span><br><span class="line"><span class="comment"> * -----------------------------------------创建和注册AnnotationAwareAspectJAutoProxyCreator的过程----------------------------</span></span><br><span class="line"><span class="comment"> *  4、finishBeanFactoryInitialization(beanFactory)：完成BeanFactory初始化工作,创建剩下来的单实例bean</span></span><br><span class="line"><span class="comment"> *      1、遍历获取容器中所有的Bean,依次创建对象getBean(beanName)</span></span><br><span class="line"><span class="comment"> *          getBean -&gt; doGetBean() -&gt; getSingleton()</span></span><br><span class="line"><span class="comment"> *      2、创建bean[AnnotationAwareAspectJAutoProxyCreator在所有bean创建之前会有一个拦截,</span></span><br><span class="line"><span class="comment"> *         InstantiationAwareBeanPostProcessor,会调用postProcessBeforeInstantiation()方法]</span></span><br><span class="line"><span class="comment"> *          1、先从缓存中获取当前bean,如果能获取到,说明bean是被创建过的,直接使用;否则再创建</span></span><br><span class="line"><span class="comment"> *          只要被创建好的bean都会被缓存起来</span></span><br><span class="line"><span class="comment"> *          2、createBean()：创建bean——AnnotationAwareAspectJAutoProxyCreator会在任何bean创建之前先尝试返回bean的实例</span></span><br><span class="line"><span class="comment"> *              [BeanPostProcessor是在Bean对象创建完成初始化前后调用的]</span></span><br><span class="line"><span class="comment"> *              [InstantiationAwareBeanPostProcessor是在创建Bean实例之前先尝试用后置处理器返回对象的]</span></span><br><span class="line"><span class="comment"> *              1、resolveBeforeInstantiation()：希望后置处理器在此能返回一个代理对象,如果能返回</span></span><br><span class="line"><span class="comment"> *              代理对象就使用;</span></span><br><span class="line"><span class="comment"> *                  1、后置处理器先尝试返回对象：</span></span><br><span class="line"><span class="comment"> *                  bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName);</span></span><br><span class="line"><span class="comment"> *                      拿到所有后置处理器,如果是InstantiationAwareBeanPostProcessor</span></span><br><span class="line"><span class="comment"> *                      就执行postProcessBeforeInstantiation()方法</span></span><br><span class="line"><span class="comment"> * if (bean != null) &#123;</span></span><br><span class="line"><span class="comment"> * bean = applyBeanPostProcessorsAfterInitialization(bean, beanName);</span></span><br><span class="line"><span class="comment"> *                  &#125;</span></span><br><span class="line"><span class="comment"> *              否则就进行第二步</span></span><br><span class="line"><span class="comment"> *              2、doCreateBean()：真正地去创建bean实例,和3.6流程一样</span></span><br><span class="line"><span class="comment"> *              3、</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * AnnotationAwareAspectJAutoProxyCreator[InstantiationAwareBeanPostProcessor]的作用</span></span><br><span class="line"><span class="comment"> * 1、每一个bean创建之前,调用postProcessBeforeInstantiation()</span></span><br><span class="line"><span class="comment"> *      关心MathCalculator和LogAspect的创建</span></span><br><span class="line"><span class="comment"> *      1、判断当前bean是否在advisedBeans中(保存了需要增强的bean——需要切面)</span></span><br><span class="line"><span class="comment"> *      2、判断当前bean是否是基础类型的(Advice、Pointcut、Advisor、AopInfrastructureBean)</span></span><br><span class="line"><span class="comment"> *      或者是否是切面(<span class="doctag">@Aspect</span>)</span></span><br><span class="line"><span class="comment"> *      3、判断是否需要跳过</span></span><br><span class="line"><span class="comment"> *          1、获取候选的增强器(切面里面的通知方法),每一个封装的通知方法的增强器是InstantiationModelAwarePointcutAdvisor</span></span><br><span class="line"><span class="comment"> *          判断每一个增强器是否是AspectJPointcutAdvisor类型(返回true)</span></span><br><span class="line"><span class="comment"> *          2、返回false</span></span><br><span class="line"><span class="comment"> * 2、创建对象</span></span><br><span class="line"><span class="comment"> * postProcessAfterInitialization</span></span><br><span class="line"><span class="comment"> * // 在需要的时候包装</span></span><br><span class="line"><span class="comment"> * return wrapIfNecessary()</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    1、获取当前bean的所有增强器(通知方法)</span></span><br><span class="line"><span class="comment"> *      1、找到候选的所有增强器(找哪些通知方法是需要切入当前bean方法的)</span></span><br><span class="line"><span class="comment"> *      2、获取到能在bean使用的增强器</span></span><br><span class="line"><span class="comment"> *      3、给增强器排序</span></span><br><span class="line"><span class="comment"> *    2、保存当前bean到advisedBeans</span></span><br><span class="line"><span class="comment"> *    3、如果当前bean需要增强,创建当前bean的代理对象</span></span><br><span class="line"><span class="comment"> *      1、获取所有增强器(通知方法)</span></span><br><span class="line"><span class="comment"> *      2、保存到proxyFactory</span></span><br><span class="line"><span class="comment"> *      3、创建代理对象：Spring自动决定</span></span><br><span class="line"><span class="comment"> *          JdkDynamicAopProxy()：jdk动态代理</span></span><br><span class="line"><span class="comment"> *          ObjenesisCglibAopProxy()：cglib的动态代理</span></span><br><span class="line"><span class="comment"> *    4、给容器中返回当前组件使用cglib增强了的代理对象</span></span><br><span class="line"><span class="comment"> *    5、以后容器中获取到的就是这个组件的代理对象，执行目标方法的时候，代理对象就会执行通知方法的流程</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 3、目标方法执行:</span></span><br><span class="line"><span class="comment"> *    容器中保存了组件的代理对象(cglib增强后的对象)，这个对象里面保存了详细信息(比如增强器、目标对象...)</span></span><br><span class="line"><span class="comment"> *    1、CglibAopProxy.intercept()拦截目标方法的执行</span></span><br><span class="line"><span class="comment"> *    2、根据ProxyFactory对象获取将要执行的目标方法的拦截器链</span></span><br><span class="line"><span class="comment"> *      List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);</span></span><br><span class="line"><span class="comment"> *          1、List&lt;Object&gt; interceptorList保存所有拦截器(长度为5)</span></span><br><span class="line"><span class="comment"> *              一个默认的ExposeInvocationInterceptor和四个增强器</span></span><br><span class="line"><span class="comment"> *          2、遍历所有的增强器,将其转为Interceptor</span></span><br><span class="line"><span class="comment"> *          3、将增强器转为List&lt;MethodInterceptor&gt;</span></span><br><span class="line"><span class="comment"> *             如果是MethodInterceptor,直接加入集合</span></span><br><span class="line"><span class="comment"> *             如果不是,使用AdvisorAdapter转为MethodInterceptor</span></span><br><span class="line"><span class="comment"> *             转换完成返回MethodInterceptor数组</span></span><br><span class="line"><span class="comment"> *    3、如果没有拦截器链,直接执行目标方法</span></span><br><span class="line"><span class="comment"> *          拦截器链(每一个通知方法又被包装为方法拦截器,利用MethodInterceptor机制)</span></span><br><span class="line"><span class="comment"> *    4、如果有拦截器链，把需要执行的目标对象、目标方法、拦截器链等信息传入创建一个CglibMethodInvocation对象，</span></span><br><span class="line"><span class="comment"> *    并调用它的proceed()方法</span></span><br><span class="line"><span class="comment"> *    5、拦截器的触发过程</span></span><br><span class="line"><span class="comment"> *      1、如果没有拦截器(或者拦截器的索引为拦截器数组大小-1——到了最后一个拦截器),直接执行目标方法</span></span><br><span class="line"><span class="comment"> *      2、链式获取每一个拦截器,拦截器执行invoke()方法,每一个拦截器等待下一个拦截器执行完成返回以后再来执行;</span></span><br><span class="line"><span class="comment"> *         拦截器链的机制,保证通知方法与目标方法的执行顺序</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 总结：</span></span><br><span class="line"><span class="comment"> *  1、<span class="doctag">@EnableAspectJAutoProxy</span>开始AOP功能</span></span><br><span class="line"><span class="comment"> *  2、<span class="doctag">@EnableAspectJAutoProxy</span>会给容器中注册一个组件AnnotationAwareAspectJAutoProxyCreator</span></span><br><span class="line"><span class="comment"> *  3、AnnotationAwareAspectJAutoProxyCreator是一个后置处理器</span></span><br><span class="line"><span class="comment"> *  4、容器的创建流程：</span></span><br><span class="line"><span class="comment"> *      1、refresh()容器刷新后registerBeanPostProcessors()注册后置处理器：创建AnnotationAwareAspectJAutoProxyCreator对象</span></span><br><span class="line"><span class="comment"> *      2、finishBeanFactoryInitialization()初始化剩下的单实例bean</span></span><br><span class="line"><span class="comment"> *          1、创建业务逻辑组件和切面组件</span></span><br><span class="line"><span class="comment"> *          2、AnnotationAwareAspectJAutoProxyCreator拦截组件的创建过程</span></span><br><span class="line"><span class="comment"> *          3、组件创建完之后,判断组件是否需要增强</span></span><br><span class="line"><span class="comment"> *              是：把切面的通知方法包装成增强器(Advisor),给业务逻辑组件创建一个代理对象(cglib)</span></span><br><span class="line"><span class="comment"> *  5、执行目标方法：</span></span><br><span class="line"><span class="comment"> *      1、代理对象执行目标方法</span></span><br><span class="line"><span class="comment"> *      2、CglibAopProxy.intercept()进行拦截：</span></span><br><span class="line"><span class="comment"> *          1、得到目标方法的拦截器链(增强器包装成拦截器MethodInterceptor)</span></span><br><span class="line"><span class="comment"> *          2、利用拦截器的链式机制,依次进入每一个拦截器进行执行</span></span><br><span class="line"><span class="comment"> *          3、效果：</span></span><br><span class="line"><span class="comment"> *              正常执行：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 返回通知</span></span><br><span class="line"><span class="comment"> *              出现异常：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 异常通知</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@EnableAspectJAutoProxy</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainConfigAOP</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 业务逻辑类加入容器中</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> MathCalculator <span class="title">calculator</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> MathCalculator();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 切面类加入到容器中</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> LogAspect <span class="title">logAspect</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LogAspect();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MathCalculator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">div</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> i / j;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 切面类,方法中joinPoint必须写在参数表的第一位(否则报错)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Aspect</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogAspect</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 抽取公共的切入点表达式(参考Spring官方文档)</span></span><br><span class="line">    <span class="comment">// 1、本类引用(方法名())</span></span><br><span class="line">    <span class="comment">// 2、其他的切面引用(全类名方法名())</span></span><br><span class="line">    <span class="meta">@Pointcut</span>(<span class="string">"execution(public int com.xiong.test3.MathCalculator.div(int, int))"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pointCut</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">    <span class="comment">// @Before在目标方法之前切入：切入点表达式(指定在哪个方法切入)</span></span><br><span class="line">    <span class="meta">@Before</span>(<span class="string">"pointCut()"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logStart</span><span class="params">(JoinPoint joinPoint)</span> </span>&#123;</span><br><span class="line">        Object[] args = joinPoint.getArgs();</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行开始... 参数列表是: &#123;"</span> + Arrays.asList(args) + <span class="string">"&#125;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@After</span>(<span class="string">"pointCut()"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logEnd</span><span class="params">(JoinPoint joinPoint)</span> </span>&#123;</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行结束..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@AfterReturning</span>(value = <span class="string">"pointCut()"</span>, returning = <span class="string">"result"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logReturn</span><span class="params">(JoinPoint joinPoint, Object result)</span> </span>&#123;</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行正常返回... 结果: &#123;"</span> + result + <span class="string">"&#125;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@AfterThrowing</span>(value = <span class="string">"pointCut()"</span>, throwing = <span class="string">"exception"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">logException</span><span class="params">(JoinPoint joinPoint, Exception exception)</span> </span>&#123;</span><br><span class="line">        System.out.println(joinPoint.getSignature().getName() + <span class="string">"运行出现异常... 异常信息: &#123;"</span> + exception.getMessage() + <span class="string">"&#125;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">contextLoads</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1、创建ioc容器</span></span><br><span class="line">    AnnotationConfigApplicationContext context =</span><br><span class="line">            <span class="keyword">new</span> AnnotationConfigApplicationContext(MainConfigAOP<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 必须使用Spring容器中的组件</span></span><br><span class="line">    MathCalculator calculator = context.getBean(MathCalculator<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    calculator.div(<span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    context.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E9%80%9A%E7%9F%A5%E6%96%B9%E6%B3%95.png" alt="链式调用通知方法"></p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop入门</title>
      <link href="/2020/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/"/>
      <url>/2020/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/</url>
      
        <content type="html"><![CDATA[<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li><a href="#概论">概论</a></li><li><a href="#Hadoop介绍">Hadoop介绍</a></li><li><a href="#环境搭建">环境搭建</a></li><li><a href="#Hadoop运行模式">Hadoop运行模式</a></li><li><a href="#Hadoop编译源码">Hadoop编译源码</a></li><li><a href="#HDFS概述">HDFS概述</a></li><li><a href="#HDFS的Shell操作">HDFS的Shell操作</a></li><li><a href="#HDFS客户端操作">HDFS客户端操作</a></li><li><a href="#HDFS的数据流">HDFS的数据流</a></li><li><a href="#NameNode和SecondaryNameNode">NameNode和SecondaryNameNode</a></li><li><a href="#DataNode">DataNode</a></li><li><a href="#HDFS2.X新特性">HDFS2.X新特性</a></li><li><a href="#MapReduce概述">MapReduce概述</a></li><li><a href="#Hadoop序列化">Hadoop序列化</a></li><li><a href="#MapReduce框架原理">MapReduce框架原理</a></li><li><a href="#Hadoop数据压缩">Hadoop数据压缩</a></li><li><a href="#Yarn资源调度器">Yarn资源调度器</a></li><li><a href="#Hadoop企业优化">Hadoop企业优化</a></li></ul><a id="more"></a><h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><ul><li><p>概念：大数据指<strong>无法在一定时间范围</strong>内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的<strong>海量、高增长率和多样化的信息资产</strong>。需要解决的问题：海量数据的<strong>存储</strong>和海量数据的<strong>分析计算</strong>问题。</p></li><li><p>大数据特点(4V)：</p><ul><li>Volume(大量)</li><li>Velocity(高速)</li><li>Variety(多样)：<strong>结构化/非结构化数据</strong>，结构化数据以数据库/文本为主，非结构化数据包括网络日志、音频、视频、图片和地理位置信息等。</li><li>Value(低价值密度)：价值密度的高度与数据总量的大小成反比，如何<strong>快速对有价值数据“提纯”称为目前大数据背景下待解决的难题</strong>。</li></ul></li><li><p>大数据应用场景：物流仓储、零售、旅游、商品广告推荐、保险、金融、房产、人工智能</p></li><li><p>大数据部门业务流程：<br>产品人员提需求(统计总用户数、日活跃用户数、回流用户数等) =&gt; 数据部门搭建数据平台、分析数据指标 =&gt; 数据可视化(报表展示、邮件发送、大屏幕展示等)</p></li><li><p>大数据部门组织结构：<br><img src="%E9%83%A8%E9%97%A8%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84.png" alt="部门组织结构"></p></li></ul><h2 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h2><ul><li><p>Hadoop是什么：</p><ul><li>是一个由Apache基金会开发的<strong>分布式系统基础架构</strong>。</li><li>主要解决海量数据的<strong>存储</strong>和海量数据的<strong>分析计算</strong>问题。</li><li>广义上来说，Hadoop通常指更广泛的概念——Hadoop生态圈。</li></ul></li><li><p>Hadoop发展历史</p><ul><li><p>Lucene框架是<strong>Doug Cutting</strong>开创的开源软件，用Java编写，实现与Goole类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询和索引引擎。</p></li><li><p>2001年年底Lucene称为Apache基金会的一个子项目。</p></li><li><p>对于海量数据的场景，Lucene面对与Google同样的困难，<strong>存储数据困难，检索速度慢</strong>。</p></li><li><p>学习和模仿Google解决这些问题的办法：微型版Nutch。</p></li><li><p>Google是Hadoop的思想之源(其在大数据方面的三篇论文)<br><strong>GFS -&gt; HDFS Map-Reduce -&gt; MR BigTable -&gt; HBase</strong></p></li><li><p>2003年~04年，Google公开部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用<strong>2年业余时间</strong>实现了DFS和MapReduce机制，使Nutch性能飙升。</p></li><li><p>2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。</p></li><li><p>2006年3月份，Map-Reduce和Nutch Distributed File System(NDFS)分别被纳入到Hadoop项目中，Hadoop就正式诞生，标志着大数据十代来临。</p></li><li><p>Hadoop名字来源于Doug Cutting儿子的玩具大象<br><img src="logo.jpg" alt="logo"></p></li></ul></li><li><p>Hadoop三大发行版本</p><ul><li>Apache：最原始(基础)的版本，对于入门学习最好</li><li>Cloudera：在大型互联网企业中用的较多，产品主要为CDH，Cloudera Manager，Cloudera Support：<ul><li>CDH是Cloudera的Hadoop发行版，完全开源，比Apache版本在兼容性、安全性、稳定性上有所增强。</li><li>Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。</li><li>Cloudera Support即是对Hadoop的技术支持。</li></ul></li><li>Hortonworks：文档较好<ul><li>Hortonworks的主打产品是Hortonworks Data Platform(HDP)，也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari——一款开源的安装和管理系统。</li><li>HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。</li></ul></li></ul></li><li><p>Hadoop的优势(4高)</p><ul><li>高可靠性：Hadoop底层维护多个数据副本，即使某个计算元素或存储出现故障，也不会导致数据的丢失。</li><li>高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点。</li><li>高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</li><li>高容错性：能够自动将失败的任务重新分配。</li></ul></li><li><p>Hadoop组成</p><ul><li>1.x：Common(辅助工具)、HDFS(数据存储)、MapReduce(计算+资源调度)</li><li>2.x：Common(辅助工具)、HDFS(数据存储)、<strong>Yarn(资源调度)</strong>、<strong>MapReduce(计算)</strong></li></ul></li><li><p>HDFS架构概述：</p><ul><li>HDFS全名——Hadoop Distributed File System</li><li>组成：<ul><li>NameNode(nn)：存储文件的元数据，如文件名、文件目录结构、文件属性(生成时间、副本数、文件权限)以及每个文件的块列表和块所在的DataNode等——类似书的目录(索引)</li><li>DataNode(dn)：在本地文件系统存储文件块数据以及块数据的校验和——具体的书章节内容</li><li>Secondary NameNode(2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照——辅助NameNode工作</li></ul></li></ul></li><li><p>Yarn架构概述<br><img src="Yarn%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Yarn架构图"></p></li><li><p>MapReduce架构概述</p><ul><li>将计算分为两个阶段：Map和Reduce</li><li>Map阶段并行处理输入数据</li><li>Reduce阶段对Map结果进行汇总</li></ul></li><li><p>大数据技术生态体系<br><img src="%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB.png" alt="大数据技术生态体系"></p></li></ul><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><ul><li><p>配置Java环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 修改环境变量</span><br><span class="line">sudo vim &#x2F;etc&#x2F;profile</span><br><span class="line">##JAVA_HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_251</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line">&#x2F;&#x2F; 让环境变量修改生效</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">&#x2F;&#x2F; 查看Java版本</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure></li><li><p>配置Hadoop环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 修改环境变量</span><br><span class="line">sudo vim &#x2F;etc&#x2F;profile</span><br><span class="line">##HADOOP_HOME</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin</span><br><span class="line">&#x2F;&#x2F; 让环境变量修改生效</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">&#x2F;&#x2F; 查看Hadoop版本</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure></li><li><p>Hadoop目录说明</p><ul><li>bin目录：存放对Hadoop相关服务(HDFS,YARN)进行操作的脚本</li><li>etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</li><li>lib目录：存放Hadoop的本地库(对数据进行压缩解压缩功能)</li><li>sbin目录：存放启动或停止Hadoop相关服务的脚本</li><li>share目录：存放Hadoop的依赖jar包、文档、和官方案例</li></ul></li></ul><h2 id="Hadoop运行模式"><a href="#Hadoop运行模式" class="headerlink" title="Hadoop运行模式"></a>Hadoop运行模式</h2><ul><li><p>本地模式</p><ul><li>官方WordCount案例(统计单词数目)：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 创建wcinput文件夹</span><br><span class="line">mkdir wcinput</span><br><span class="line">&#x2F;&#x2F; 创建wc.input文件</span><br><span class="line">cd wcinput</span><br><span class="line">touch wc.input</span><br><span class="line">&#x2F;&#x2F; 编辑wc.input随意输入字符</span><br><span class="line">vim wc.input</span><br><span class="line">&#x2F;&#x2F; 回到Hadoop目录执行程序</span><br><span class="line">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput&#x2F; wcouput</span><br><span class="line">&#x2F;&#x2F; 查看结果</span><br><span class="line">cat wcoutput&#x2F;part-r-00000</span><br></pre></td></tr></table></figure></li><li><p>伪分布式模式</p><ul><li><p>配置集群</p><ul><li>设置hadoop-env.sh：在文件中设置JAVA_HOME为本地JDK地址</li><li>设置core-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>设置hdfs-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动集群</p><ul><li>格式化NameNode：bin/hdfs namenode -format</li><li><strong>启动NameNode和DataNode：sbin/start-dfs.sh(关闭stop)</strong></li></ul></li><li><p>查看集群</p><ul><li>查看是否启动成功：jps(JDK中的命令,不是Linux命令,类似ps)</li><li>web端查看HDFS文件系统：<a href="http://192.168.232.100:9870" target="_blank" rel="noopener">http://192.168.232.100:9870</a>(需要CentOS主机上设置关闭防火墙,在3.x版本端口号默认为9870)</li><li>查看产生的log日志：cd /hadoop/logs</li><li>注意：不能一直格式化NameNode，格式化NameNode会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。(最好关闭NameNode和DataNode)</li></ul></li><li><p>操作集群(所有命令类似于在Linux Terminal的命令行操作,需要加上固定前缀bin/hdfs dfs -)</p><ul><li>在HDFS文件系统上创建一个input文件夹：bin/hdfs dfs -mkdir -p /user/sobxiong/input</li><li>将测试文件内容上传到文件系统上：bin/hdfs dfs -put wcinput/wc.input /user/sobxiong/input/</li><li>查看上传的文件是否正确：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -ls /user/sobxiong/input/</span><br><span class="line">bin/hdfs dfs -cat /user/sobxiong/input/wc.input</span><br></pre></td></tr></table></figure><ul><li>运行MapReduce程序：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/sobxiong/input/ /user/sobxiong/output</li><li>查看输出结果：bin/hdfs dfs -cat /user/sobxiong/output/*</li><li>也可以在浏览器的文件系统中查看  </li><li>将测试文件内容下载到本地：bin/hdfs dfs -get /user/sobxiong/output/part-r-00000 ./wcoutput/</li><li>删除输出结果：bin/hdfs dfs -rm -r /user/sobxiong/output</li></ul></li><li><p>启动Yarn并运行MapReduce程序</p><ul><li><p>配置集群</p><ul><li>配置yarn-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251</li><li>配置yarn-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置Yarn应用的classPath --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 不配置出现：错误: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>命令行下输入hadoop classpath的一长串环境<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 不需要设置yarn.resourcemanager.hostname，可以直接通过ip+端口号的方式访问 --&gt;</span></span><br></pre></td></tr></table></figure><ul><li>配置mapred-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251</li><li>配置mapred-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在YARN上 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 默认是local，本地文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动集群</p><ul><li>启动前必须保证NameNode和DataNode已启动</li><li><strong>启动ResourceManager和NodeManager：sbin/start-yarn.sh(关闭stop)</strong></li></ul></li><li><p>集群操作</p><ul><li>yarn浏览器页面查看：8088端口</li><li>删除文件系统上的output文件：bin/hdfs dfs -rm -r /user/sobxiong/output</li><li>执行MapReduce程序：同上hadoop操作</li><li>查看结果：同上cat操作，也可以在浏览器端查看</li></ul></li></ul></li><li><p>配置历史服务器</p><ul><li><p>配置mapred-site.xml：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>172.16.85.130:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>172.16.85.130:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动历史服务器：bin/mapred –daemon start historyserver(stop关闭)</p></li><li><p>查看历史服务器是否启动：jps</p></li><li><p>查看JobHistory：<a href="http://172.16.85.130:19888/jobhistory" target="_blank" rel="noopener">http://172.16.85.130:19888/jobhistory</a></p></li></ul></li><li><p>配置日志的聚集：</p><ul><li>概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上</li><li>好处：可以方便的查看到程序运行详情，方便开发调试</li><li>注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryManager</li><li>配置yarn-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置文件说明</p><ul><li><p>默认配置文件：</p><ul><li>core-defalut.xml - hadoop-common-3.1.3.jar/core-default.xml</li><li>hdfs-default.xml - hadoop-hdfs-3.1.3.jar/hdfs-default.xml</li><li>yarn-default.xml - hadoop-yarn-common-3.1.3.jar/yarn-default.xml</li><li>mapred-default.xml - hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</li></ul></li><li><p>自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置(优先级高)</p></li></ul></li></ul></li><li><p>完全分布式运行模式</p><ul><li><p>虚拟机准备(3台，完全复制)</p></li><li><p>编写集群分发脚本xsync</p><ul><li><p>scp(secure copy)安全拷贝</p><ul><li>定义：scp可以实现服务器与服务器之间的数据拷贝</li><li>基本语法：<table><thead><tr><th>命令</th><th>参数</th><th>要拷贝的文件路径/名称</th><th>目的用户@主机:目的路径/名称</th></tr></thead><tbody><tr><td>scp</td><td>-r(递归)</td><td>$pdir/$fname</td><td>$user@$host:$pdir/$fname</td></tr></tbody></table></li></ul></li><li><p>rsync远程同步工具</p><ul><li>作用：主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点</li><li>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去</li><li>基本语法：<table><thead><tr><th>命令</th><th>参数</th><th>要拷贝的文件路径/名称</th><th>目的用户@主机:目的路径/名称</th></tr></thead><tbody><tr><td>rsync</td><td>-r(递归)v(显示复制过程)l(拷贝符号连接)</td><td>$pdir/$fname</td><td>$user@$host:$pdir/$fname</td></tr></tbody></table></li></ul></li><li><p>xsync集群分发脚本</p><ul><li><p>需求：循环复制文件到所有节点的相同目录下</p></li><li><p>需求分析：</p><ul><li>rsync命令原始：rsync -rvl /opt/module root@hadoop2:/opt/</li><li>期望脚本：xsync 需同步的文件名</li><li>说明：在home/sobxiong/bin这个目录下存放的脚本，sobxiong用户在系统任何地方都可以直接执行</li></ul></li><li><p>脚本实现</p><ul><li>在/home/sobxiong目录下创建bin目录，并在bin目录下创建xsync文件</li><li>在xsync中键入如下代码：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=2; host&lt;4; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>修改脚本xsync具有执行权限：chmod 777 xsync</li><li>调用脚本形式：xsync 文件名</li></ul></li></ul></li></ul></li><li><p>集群配置</p><ul><li><p>集群部署规划：</p><table><thead><tr><th>类型</th><th>hadoop1</th><th>hadoop2</th><th>hadoop3</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode、DataNode</td><td>DataNode</td><td>SecondaryNameNode、DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager、NodeManager</td><td>NodeManager</td></tr></tbody></table></li><li><p>配置集群</p><ul><li>核心配置文件core-site.xml：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>HDFS配置文件hdfs-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 副本数目 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop3:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>YARN配置文件yarn-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>MapReduce配置文件mapred-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>在集群上分发配置好的hadoop配置文件：xsync /opt/module/hadoop-3.1.3/etc</p></li><li><p>查看文件分发情况  </p></li></ul></li><li><p>集群单点启动</p><ul><li>集群第一次启动，需要格式化NameNode：hadoop namenode -format(把log和data文件删除)</li><li>在hadoop1上启动NameNode：hadoop-daemon.sh start namenode</li><li>在hadoop1、2、3上分别启动DataNode：hadoop-daemon.sh start datanode</li></ul></li><li><p>SSH免密登陆配置</p><ul><li>配置ssh<ul><li>基本语法：ssh ip</li></ul></li><li>无密钥配置<ul><li>免密登录原理：</li><li>生成公钥和私钥：ssh-keygen -t rsa(生成文件id_rsa-私钥,id_rsa.pub-公钥)</li><li>将公钥拷贝到要免密登录的目标机器上：ssh-copy-id hadoop2(只是当前用户,root还需要另外配置)</li></ul></li><li>.ssh文件下(~/.ssh)的文件功能<ul><li>known_hosts：记录ssh访问过的计算机的公钥</li><li>id_rsa：生成的私钥</li><li>id_rsa.pub：生成的公钥</li><li>authorized_keys：存放授权过的无密登录服务器公钥</li></ul></li></ul></li><li><p>群起集群</p><ul><li><p>配置workers：vim etc/hadoop/workers；加入hadoop1、hadoop2、hadoop3(不能有空行和空格)；同步所有节点配置文件 - xsync etc/hadoop/workers</p></li><li><p>启动集群</p><ul><li>集群第一次启动，需要格式化NameNode(格式化前关闭启动的所有namenode和datanode进程,然后再删除data和log数据)：bin/hdfs namenode -format</li><li>启动HDFS：sbin/start-dfs.sh(在hadoop1上启动,这样hadoo1、2、3均会启动对应的进程)</li><li>启动YARN：sbin/start-yarn.sh(在hadoop2上启动,在ResourceManager所在机器hadoop2上启动Yarn)</li><li>查看NameNode：hadoop1:9870</li></ul></li><li><p>集群基本测试</p><ul><li>上传文件到集群：bin/hdfs dfs -put xx xx</li><li>查看上传文件存储位置<ul><li>查看HDFS文件存储路径：/opt/module/hadoop-3.1.3/data/tmp/dfs/data/current/BP-1002151198-172.16.85.130-1591848799222/current/finalized/subdir0/subdir0</li><li>查看HDFS在磁盘存储文件的内容：cat blk_xxx(文本文件)</li><li>拼接大文件：cat blk_xxx &gt;&gt; temp，最后temp就是初始的文件</li></ul></li></ul></li></ul></li><li><p>集群启动/停止方式总结</p><ul><li>各个服务组件逐一启动/停止<ul><li>分别启动/停止HDFS组件：hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode</li><li>启动/停止YARN：yarn-daemon.sh start/stop resourcemanager/nodemanager</li></ul></li><li>各个模块分开启动/停止(配置ssh是前提)常用<ul><li>整体启动/停止HDFS：start-dfs.sh/stop-dfs.sh</li><li>整体启动/停止YARN：start-yarn.sh/stop-yarn.sh</li></ul></li></ul></li><li><p>集群时间同步</p><ul><li><p>crontab定时任务：</p><ul><li>基本语法：crontab[选项]</li><li>选项说明<ul><li>-e：编辑crontab定时任务</li><li>-l：查询crontab任务</li><li>-r：删除当前用户所有的crontab任务</li></ul></li><li>参数说明：***** [任务]<ul><li>*的含义：<ul><li>第一个：一小时当中的第几分钟(0~59)</li><li>第二个：一天当中的第几个小时(0~23)</li><li>第三个：一个月当中的第几天(1~31)</li><li>第四个：一年当中的第几月(1~12)</li><li>第五个：一周当中的星期几(0~7,0和7均代表星期日)</li></ul></li><li>特殊符号：<ul><li><em>：代表任何时间。比如第一个“</em>”代表一小时中每分钟都执行一次</li><li>,：代表不连续的时间。如“0 8,12,16 * * *”命令，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</li><li>-：代表连续的时间范围。比如“0 5 * * 1-6”命令，代表在周一到周六的凌晨5点0分执行命令</li><li><em>/n：代表每隔多久执行一次。比如“</em>/10 * * * *”命令，代表每隔10分钟就执行一遍命令</li></ul></li></ul></li></ul></li><li><p>ntp方式进行同步</p><ul><li><p>具体思路：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。<br><img src="%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5.png" alt="集群时间同步"></p></li><li><p>具体实操</p><ul><li><p>时间服务器配置：</p><ul><li>检查ntp是否安装：rpm -qa&#124;grep ntp，有ntp、fontpackages-filesystem以及ntpdate</li><li>修改ntp配置文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 授权1172.16.85.0-172.16.85.255网段上的所有机器可以从这台机器上查询和同步时间</span></span><br><span class="line">restrict 172.16.85.0 mask 172.16.85.130.0 nomodify notrap</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改集群在局域网中,不使用其他互联网上的时间</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步</span></span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure><ul><li>修改/etc/sysconfig/ntpd文件：SYNC_HWCLOCK=yes(让硬件时间与系统时间一起同步)</li><li>重新启动ntpd服务：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service ntpd status</span><br><span class="line">service ntpd start</span><br></pre></td></tr></table></figure><ul><li>设置ntpd服务开机自启动：chkconfig ntpd on</li></ul></li><li><p>其他机器配置(root用户)：</p><ul><li>配置10分钟与时间服务器同步一次：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop1</span><br></pre></td></tr></table></figure><ul><li>修改任意机器时间：date -s “2020-11-11 11:11:11”</li><li>十分钟后查看机器是否与时间服务器同步：date</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h2 id="Hadoop编译源码"><a href="#Hadoop编译源码" class="headerlink" title="Hadoop编译源码"></a>Hadoop编译源码</h2><ul><li><p>前期准备</p><ul><li><p>jar包准备(hadoop源码、JDK8、Maven、Ant、Protobuf)：Protobuf在Google的github中的Release页面(3.1.3Hadoop对应2.5.0版本)</p></li><li><p>jar包安装</p><ul><li>安装JDK</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> JAVA_HOME(/etc/profile)</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><ul><li>安装Maven</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> MAVEN_HOME(/etc/profile)</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.6.3</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">mvn -version</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改maven仓库镜像</span></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><ul><li>安装Ant</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-ant-1.10.8-bin.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ANT_HOME(/etc/profile)</span></span><br><span class="line">export ANT_HOME=/opt/module/apache-ant-1.10.8</span><br><span class="line">export PATH=$PATH:$ANT_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">ant -version</span><br></pre></td></tr></table></figure><ul><li><p>安装glibc-headers和g++：yum install glibc-headers、yum install gcc-c++</p></li><li><p>安装make和cmake：yum install make</p></li><li><p>安装cmake(要装3.x版本,低版本编译不通过)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cmake-3.17.3.tar.gz -C /opt/module</span><br><span class="line">cd /opt/module/cmake-3.17.3</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> CMAKE_HOME(/etc/profile)</span></span><br><span class="line">export CMAKE_HOME=/opt/module/cmake-3.17.3</span><br><span class="line">export PATH=$PATH:$CMAKE_HOME/bin</span><br><span class="line">source /etc/profile</span><br><span class="line">cmake --version</span><br></pre></td></tr></table></figure></li><li><p>安装protobuf：</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</span><br><span class="line">cd /opt/module/protobuf-2.5.0/</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make check</span><br><span class="line">make install</span><br><span class="line">ldconfig</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> LD_LIBRARY_PATH(/etc/profile)</span></span><br><span class="line">export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</span><br><span class="line">export PATH=$PATH:$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line">protoc --version</span><br></pre></td></tr></table></figure><ul><li>安装openssl库：yum install openssl-devel</li><li>安装ncurses-devel库：yum install ncurses-devel</li></ul></li><li><p>编译源码</p><ul><li>解压源码到/opt目录</li><li>进入hadoop源码主目录</li><li>通过maven执行编译命令：mvn package -Pdist,native -DskipTests -Dtar</li></ul></li></ul></li></ul><h2 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h2><ul><li><p>HDFS产出背景及定义</p><ul><li>产生背景：随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种</li><li>定义：HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色</li><li>使用背景：<strong>适合一次写入，多次读出的场景，且不支持文件的修改</strong>。适合用来做数据分析，并不适合用来做网盘应用</li></ul></li><li><p>HDFS优缺点</p><ul><li>优点：<ul><li>高容错性<ul><li>数据自动保存多个副本。它通过增加副本的形式，提高容错性</li><li>某一个副本丢失以后，它可以自动恢复(通过将副本复制到另一个可用的节点)</li></ul></li><li>适合处理大数据<ul><li>数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据</li><li>文件规模：能够处理百万规模以上的文件数量，数量相当之大</li></ul></li><li>可构建在廉价机器上，通过多副本机制，提高可靠性</li></ul></li><li>缺点：<ul><li><strong>不适合低延时数据访问</strong>，比如毫秒级的存储数据，是做不到的</li><li><strong>无法高效的对大量小文件进行存储</strong>：<ul><li>存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的</li><li>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</li></ul></li><li>不支持并发写入、文件随机修改：<ul><li>一个文件只能有一个写，不允许多个线程同时写</li><li><strong>仅支持数据appen(追加)</strong>，不支持文件的随机修改</li></ul></li></ul></li></ul></li><li><p>HDFS组成架构<br><img src="HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84.png" alt="HDFS组成架构"></p><ul><li><p>NameNode(nn)：Master，一个主管、管理者</p><ul><li>管理HDFS的名称空间</li><li>配置副本策略</li><li>管理数据块(Block)映射信息</li><li>处理客户端读写请求</li></ul></li><li><p>DataNode：Slave。NameNode下达命令，DataNode执行实际的操作</p><ul><li>存储实际的数据块</li><li>执行数据块的读/写操作</li></ul></li><li><p>Client：客户端</p><ul><li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li><li>与NameNode交互，获取文件的位置信息</li><li>与DataNode交互，读取或者写入数据</li><li>Client提供一些命令来管理HDFS，比如NameNode格式化</li><li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li></ul></li><li><p>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务</p><ul><li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</li><li>在紧急情况下，可辅助恢复NameNode</li></ul></li></ul></li><li><p>HDFS文件块大小<br>HDFS中的文件在物理上是分块存储(Block)，块的大小可以通过配置参数(dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M<br><img src="%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F%E5%A4%A7%E8%87%B4%E8%AE%A1%E7%AE%97.png" alt="文件块大小大致计算"><br>为什么文件块的大小不能设置太小，也不能设置太大？</p><ul><li>HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置</li><li>如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢</li><li>总结：HDFS块的大小设置主要取决于磁盘传输速率</li></ul></li></ul><h2 id="HDFS的Shell操作"><a href="#HDFS的Shell操作" class="headerlink" title="HDFS的Shell操作"></a>HDFS的Shell操作</h2><ul><li><p>基本语法<br>bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令<br>其中dfs是fs的实现类</p></li><li><p>命令大全：bin/hadoop fs</p></li><li><p>使用命令：</p><ul><li>-help：输出命令的帮助(hadoop fs -help rm)</li><li>-ls：显示目录信息(hadoop fs -ls /)</li><li>-mkdir：在HDFS上创建目录[-p递归](hdoop fs -mkdir -p /sobxiong/test)</li><li>-moveFromLocal：从本地剪切粘贴到HDFS[前路径为本地,后路径为HDFS](hadoop fs -moveFromLocal ./test.txt /sobxiong/test/)</li><li>-appendToFile：追加一个文件到已经存在的文件末尾[前路径为本地,后路径为HDFS](hadoop fs -appendToFile ./test.txt /sobxiong/test/test.txt)</li><li>-cat：显示文件内容(hadoop fs -cat /sobxiong/test/test.txt)</li><li>-chgrp、-chmod、-chown：修改文件所属的权限，同Linux文件系统中的用法</li><li>-copyFromLocal：从本地文件系统拷贝文件到HDFS中，同-moveFromLocal</li><li>-copyToLocal：从HDFS拷贝文件到本地[前路径为HDFS,后路径为本地](hadoop fs -copyToLocal /sobxiong/test/test.txt ./)</li><li>-cp：把文件从HDFS的一个路径拷贝到HDFS的另一个路径</li><li>-mv：把文件从HDFS的一个路径移动到HDFS的另一个路径</li><li>-get：等同于copyToLocal(用法同copyToLocal)，从HDFS下载文件到本地</li><li>-getmerge：合并下载多个文件(hadoop fs -getmerge /sobxiong/test/* ./all.txt)</li><li>-put：等同于copyFromLocal(用法同copyFromLocal)</li><li>-tail：显示一个文件的末尾(hadoop fs -tail /sobxiong/test/test.txt)</li><li>-rm：删除文件或文件夹[-r递归删除目录]</li><li>-rmdir：删除空目录</li><li>-du：统计文件夹的大小信息[-h显示单位,-s总和](hadoop fs -du -h -s /)</li><li>-setrep：设置HDFS中文件的副本数目[这里设置的副本数只是记录在NameNode的元数据中,是否真的会有这么多副本还得看DataNode的数量.因为目前只有3台设备,最多也就3个副本,只有节点数的增加到10台时,副本数才能达到10;只要加入一台设备,就会把副本复制到设备上,直到加到10台](hadoop fs -setrep 10 /sobxiong/test/test.txt)</li></ul></li></ul><h2 id="HDFS客户端操作"><a href="#HDFS客户端操作" class="headerlink" title="HDFS客户端操作"></a>HDFS客户端操作</h2><ul><li><p>客户端环境准备</p><ul><li>将Hadoop安装到mac上，并设置环境变量</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> HADOOP_HOME(~/.bash_profile)</span></span><br><span class="line">export HADOOP_HOME="/Users/sobxiong/module/hadoop-3.1.3"</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure><ul><li>创建Maven工程测试：idea创建quickstart项目</li><li>导入依赖：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-slf4j-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>创建测试类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSClient</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, InterruptedException </span>&#123;</span><br><span class="line">      Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">      <span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop1:9000");</span></span><br><span class="line">      <span class="comment">// 1、获取hdfs客户端对象</span></span><br><span class="line">      FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 2、在hdfs上创建路径</span></span><br><span class="line">      fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/sobxiong2/test"</span>));</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 3、关闭资源</span></span><br><span class="line">      fileSystem.close();</span><br><span class="line"></span><br><span class="line">      System.out.println(<span class="string">"finish"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>HDFS的API操作</p><ul><li>文件上传</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 参数优先级：</span></span><br><span class="line"><span class="comment">  *  1、客户端代码中设置的值</span></span><br><span class="line"><span class="comment">  *  2、ClassPath(resources)下的用户自定义配置文件(hdfs-site.xml)</span></span><br><span class="line"><span class="comment">  *  3、服务器的默认配置</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// 1、文件上传</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"2"</span>);</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行上传API</span></span><br><span class="line">    fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Documents/文件块大小大致计算.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将hdfs-site.xml拷贝至项目的根目录resources资源文件夹下</span></span><br><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span>?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>文件下载</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2、文件下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行下载操作</span></span><br><span class="line">    <span class="comment">// fileSystem.copyToLocalFile(new Path("/sobxiong/test2.png"), new Path("/Users/sobxiong/Documents/test.png"));</span></span><br><span class="line">    <span class="comment">// 本地模式,true,不会产生crc文件</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Documents/test1.png"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>文件删除</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3、文件删除</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、文件删除(第二个参数,是否递归删除,文件夹时有效)</span></span><br><span class="line">    fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>文件更名</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 4、文件更名</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行更名操作</span></span><br><span class="line">    fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/sobxiong/test1.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/sobxiong/1tset.png"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>文件详情查看</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 5、文件详情查看</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、查看文件详情</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查看文件名称、权限、长度</span></span><br><span class="line">        System.out.println(<span class="string">"name: "</span> + fileStatus.getPath().getName());</span><br><span class="line">        System.out.println(<span class="string">"permission: "</span> + fileStatus.getPermission());</span><br><span class="line">        System.out.println(<span class="string">"length: "</span> + fileStatus.getLen());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查看块信息</span></span><br><span class="line">        BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">        <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">                System.out.println(<span class="string">"host = "</span> + host);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"----------------"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>判断是文件还是文件夹</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 6、判断是文件还是文件夹</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、判断操作</span></span><br><span class="line">    FileStatus[] fileStatuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"file = "</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"dir = "</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>HDFS的I/O流操作</p><ul><li>HDFS文件上传</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 把本地文件上传到HDFS根目录</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">upload</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FileInputStream fileInputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/课件.rar"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FSDataOutputStream fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/test.rar"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fsDataOutputStream);</span><br><span class="line">    IOUtils.closeStream(fileInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>HDFS文件下载</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从HDFS下载文件到本地磁盘</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">download</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/test.rar"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/test1.rar"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>定位文件获取</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载第一块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-3.1.3.tar.gz"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part1"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷(只拷贝第一个块128MB)</span></span><br><span class="line">    <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++) &#123;</span><br><span class="line">        fsDataInputStream.read(buf);</span><br><span class="line">        fileOutputStream.write(buf);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下载第二块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-3.1.3.tar.gz"</span>));</span><br><span class="line">    <span class="comment">// 3、设置指定读取的起点</span></span><br><span class="line">    fsDataInputStream.seek(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128</span>);</span><br><span class="line">    <span class="comment">// 4、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part2"</span>));</span><br><span class="line">    <span class="comment">// 5、流的对拷(拷贝剩下的两个Block块)</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下载完第一块和剩余的部分后,可通过cat hadoop-3.1.3.tar.gz.part2 &gt;&gt; hadoop-3.1.3.tar.gz.part1将剩余部分追加到第一块上,修改文件名(删去.part1),就得到完整的文件</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="HDFS的数据流"><a href="#HDFS的数据流" class="headerlink" title="HDFS的数据流"></a>HDFS的数据流</h2><ul><li><p>HDFS写数据流程</p><ul><li><p>剖析文件写入：<br><img src="HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS写数据流程"></p><ul><li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在</li><li>NameNode返回是否可以上传</li><li>客户端请求第一个Block上传到哪几个DataNode服务器上</li><li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3</li><li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</li><li>dn1、dn2、dn3逐级应答客户端</li><li>客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答</li><li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器(此后重复执行3-7步)</li></ul></li><li><p>网络拓扑-节点距离计算<br>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。节点距离：两个节点到达最近的共同祖先的距离总和<br><img src="%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97.png" alt="网络拓扑-节点距离计算"></p></li><li><p>机架感知(2.7.2版本副本节点选择,性能和安全的综合考量)</p><ul><li>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个</li><li>第二个副本和第一个副本位于相同机架，随机节点</li><li>第三个副本位于不同机架，随机节点</li></ul></li></ul></li><li><p>HDFS读数据流程<br><img src="HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS读数据流程"></p><ul><li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址</li><li>挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据</li><li>DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验)</li><li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件</li></ul></li></ul><h2 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h2><ul><li><p>NN和2NN工作机制<br>思考：NameNode中的元数据是存储在哪里的？<br>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage<br>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据<br>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并<br><img src="NameNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="NameNode工作机制"></p><ul><li><p>第一阶段：NameNode启动</p><ul><li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存</li><li>客户端对元数据进行增删改的请求</li><li>NameNode记录操作日志，更新滚动日志(先记日志,类似数据库)</li><li>NameNode在内存中对数据进行增删改</li></ul></li><li><p>第二阶段：Secondary NameNode工作</p><ul><li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果</li><li>Secondary NameNode请求执行CheckPoint</li><li>NameNode滚动正在写的Edits日志</li><li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</li><li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并</li><li>生成新的镜像文件fsimage.chkpoint</li><li>拷贝fsimage.chkpoint到NameNode</li><li>NameNode将fsimage.chkpoint重新命名成fsimage</li></ul></li><li><p>补充：<br>Fsimage：NameNode内存中元数据序列化后形成的文件。<br>Edits：记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)。<br>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息)，如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。<br>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并(所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage)。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。<br>SecondaryNameNode首先会询问NameNode是否需要CheckPoint(触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了)。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中</p></li></ul></li><li><p>Fsimage和Edits解析</p><ul><li>概念<ul><li>NameNode被格式化之后，将在/data/tmp/dfs/name/current目录中产生如下文件<br>fsimage_0000000000000000000<br>fsimage_0000000000000000000.md5<br>seen_txid<br>VERSION</li><li>Fsimage文件：HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息</li><li>Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中</li><li>seen_txid文件保存的是一个数字，就是最后一个edits_的数字</li><li><strong>每次NameNode启动的时候</strong>都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并</li></ul></li><li>查看Fsimage文件：hdfs oiv -p 文件类型(XML) -i 镜像文件 -o 转换后文件输出路径<br>例：hdfs oiv -p XML -i fsimage_0000000000000000025 o fsimage.xml<br>Fsimage中没有记录块所对应的DataNode，为什么？<br>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报</li><li>查看Edits文件：hdfs oev -p 文件类型(XML) -i 编辑日志 -o 转换后文件输出路径<br>例：hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o edits.xml<br>NameNode如何确定下次开机启动的时候合并那些Edits？<br>通过seen_txid查看</li></ul></li><li><p>CheckPoint时间设置</p><ul><li>通常情况下，SecondaryNameNode每隔一小时执行一次</li><li>一分钟检查一次操作次数</li><li>当操作次数达到1百万时，SecondaryNameNode执行一次</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-default.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure></li><li><p>NameNode故障处理</p><ul><li><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</p><ul><li>kiil -9 NameNode进程编号(用jps查看NameNode的进程编号)</li><li>删除NameNode存储的数据(data/tmp/dfs/name)：rm -rf /data/tmp/dfs/name/*</li><li>拷贝SecondaryNameNode(hadoop2)中数据到原NameNode(hadoop1)存储数据目录：scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary/* ./name/</li><li>重新启动NameNode(hadoop1)：sbin/hadoop-daemon.sh start namenode</li></ul></li><li><p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中</p><ul><li>修改hdfs-site.xml(加入下述内容)：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>kill -9 NameNode进程</li><li>删除NameNode存储的数据(同方法一)</li><li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary /data/tmp/dfs/</span><br><span class="line">cd /data/tmp/dfs/namesecondary</span><br><span class="line">rm -rf in_use.lock</span><br></pre></td></tr></table></figure><ul><li>导入检查点数据(等待一会ctrl+c结束掉)</li><li>启动NameNode：sbin/hadoop-daemon.sh start namenode</li></ul></li></ul></li><li><p>集群安全模式</p><ul><li><p>概述</p><ul><li>NameNode启动<br>NameNode启动时，首先将镜像文件(Fsimage)载入内存，并执行编辑日志(Edits)中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。<strong>这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的</strong></li><li>DataNode启动</li></ul><p><strong>系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。</strong>在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统</p><ul><li>安全模式退出判断<br>如果满足“<strong>最小副本条件</strong>”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值：dfs.replication.min=1)。<strong>在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式</strong></li></ul></li><li><p>基本语法<br>集群处于安全模式，不能执行重要操作(写操作)。集群启动完成后，自动退出安全模式</p><ul><li>查看安全模式状态：bin/hdfs dfsadmin -safemode get</li><li>进入安全模式状态：bin/hdfs dfsadmin -safemode enter</li><li>离开安全模式状态：bin/hdfs dfsadmin -safemode leave</li><li><strong>等待安全模式状态：bin/hdfs dfsadmin -safemode wait</strong></li></ul></li><li><p>案例<br>模拟等待安全模式</p><ul><li>查看当前模式：bin/hdfs dfsadmin -safemode get</li><li>先进入安全模式：bin/hdfs dfsadmin -safemode enter</li><li>创建并执行下面的脚本</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">touch safemode.sh</span><br><span class="line">vim safemode.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> safemode.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /</span><br><span class="line"></span><br><span class="line">chmod 777 safemode.sh</span><br><span class="line">./safemode.sh</span><br></pre></td></tr></table></figure><ul><li>再打开一个窗口，执行：hdfs dfsadmin -safemode leave</li><li>安全模式退出，HDFS集群上已经有上传的数据了</li></ul></li></ul></li><li><p>NameNode多目录配置</p><ul><li><p>NameNode的本地目录可以配置成多个，但每个目录存放内容相同(相当于备份)，增加了可靠性</p></li><li><p>具体配置如下</p><ul><li>在hdfs-site.xml文件中增加如下内容</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>停止集群，删除data和logs中所有数据</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop2：sbin/stop-yarn.sh</span><br><span class="line">hadoop2：rm -rf data/ logs/</span><br><span class="line">hadoop1：sbin/stop-dfs.sh</span><br><span class="line">hadoop2：rm -rf data/ logs/s</span><br><span class="line">hadoop3：rm -rf data/ logs/s</span><br></pre></td></tr></table></figure><ul><li>格式化集群并启动</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop1：bin/hdfs namenode -format</span><br><span class="line">hadoop1：sbin/start-dfs.sh</span><br><span class="line">hadoop2：sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><ul><li>查看结果：dfs目录下出现两个目录name1和name2</li></ul></li></ul></li></ul><h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><ul><li><p>DataNode工作机制<br><img src="DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="DataNode工作机制"></p><ul><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳</li><li>DataNode启动后向NameNode注册，通过后，周期性(1小时)的向NameNode上报所有的块信息</li><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用</li><li>集群运行中可以安全加入和退出一些机器</li></ul></li><li><p>数据完整性<br>DataNode节点保证数据完整性的方法：</p><ul><li>当DataNode读取Block的时候，它会计算CheckSum(类似crc校验位)</li><li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏</li><li>Client读取其他DataNode上的Block</li><li>DataNode在其文件创建后周期验证CheckSum</li></ul></li><li><p>掉线时限参数设置<br><img src="DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE.png" alt="DataNode掉线时限参数设置"><br>hdfs-default.xml：</p></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- heartbeat.recheck.interval单位为毫秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- dfs.heartbeat.interval单位为秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p><del>服役新数据节点(hadoop4未服役)</del></p><ul><li><p>需求：随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点</p></li><li><p>环境准备</p><ul><li>利用hadoop3主机再克隆一台hadoop4主机</li><li>修改hadoop4主机IP地址和主机名称</li><li>在hadoop1主机上将/etc/hosts下添加hadoop4的ip地址映射条目，并分发到hadoop2-4</li><li><strong>hadoop4主机删除原来HDFS文件系统留存的文件(data和log目录)——不然会发生3和4轮换出现的问题,因为3和4有着一样的data和log</strong></li><li>reboot重启加载配置</li></ul></li><li><p>服役新节点具体步骤</p><ul><li>hadoop1-3按之前步骤已启动</li><li>在hadoop4主机上单独启动：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br><span class="line">yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure><ul><li>刷新NameNode和ResourceManager：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><ul><li>刷新<a href="http://hadoop1:9870" target="_blank" rel="noopener">http://hadoop1:9870</a>web页面，等待</li><li>在hadoop4上上传文件</li><li>如果数据不均衡，可以使用命令实现集群的在平衡：sbin/start-balancer.sh</li></ul></li><li><p>结束后在workers文件中加入hadoop4，之后直接start-dfs.sh和start-yarn.sh即可启动</p></li></ul></li><li><p><del>退役旧数据节点(hadoop4未退役)</del></p><ul><li><p><del>添加白名单(hadoop4未退役)</del><br>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出</p><ul><li>在NameNode的hadoop-3.1.3/etc/hadoop目录下创建dfs.hosts文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">touch dfs.hosts</span><br><span class="line">vim dfs.hosts</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dfs.hosts(不添加hadoop4,不允许有空行和空格)</span></span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><ul><li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>配置文件分发：xsync hdfs-site.xml;xsync dfs.hosts</li><li>刷新NameNode和ResourceManager：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><ul><li>在web页面刷新等待</li></ul></li><li><p><del>黑名单退役(hadoop4未退役)</del><br>在黑名单上面的主机都会被强制退出。<strong>注意：不允许白名单和黑名单中同时出现同一个主机名称</strong></p><ul><li>在hadoop-3.1.3/etc/hadoop下创建dfs.hosts.exclude文件，并加入要退役节点</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">touch dfs.hosts.exclude</span><br><span class="line">vim dfs.hosts.exclude</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dfs.hosts.exclude</span></span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure><ul><li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>刷新NameNode和ResourceManager：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure><ul><li>刷新web页面等待</li></ul></li></ul></li><li><p>DataNode多目录配置</p><ul><li>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</li><li>需要在hdfs-site.xml上修改配置</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>关闭当前运行的hadoop节点</li><li>删除各节点的data和log目录</li><li>格式化dataNode</li><li>重启部署hadoop节点</li></ul></li></ul><h2 id="HDFS2-X新特性"><a href="#HDFS2-X新特性" class="headerlink" title="HDFS2.X新特性"></a>HDFS2.X新特性</h2><ul><li><p>集群间数据拷贝</p><ul><li>scp实现两个远程主机之间的文件复制</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从当前主机向目的主机 推 push</span></span><br><span class="line">scp -r hello.txt root@hadoop2:/user/sobxiong/hello.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从目的主机向当前主机 拉 pull</span></span><br><span class="line">scp -r root@hadoop2:/user/sobxiong/hello.txt hello.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过本主机中转实现两个远程主机的文件复制</span></span><br><span class="line">scp -r root@hadoop2:/user/sobxiong/hello.txt root@hadoop3:/user/sobxiong</span><br></pre></td></tr></table></figure><ul><li>采用distcp命令实现<strong>两个Hadoop集群之间</strong>的递归数据复制</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop distcp hdfs://haoop1:9000/user/sobxiong/hello.txt hdfs://hadoop2:9000/user/sobxiong/hello.txt</span><br></pre></td></tr></table></figure></li><li><p>小文件存档</p><ul><li><p>HDFS存储小文件弊端<br>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为<strong>大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关</strong>。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB</p></li><li><p>解决存储小文件办法之一<br>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存<br><img src="%E5%B0%8F%E6%96%87%E4%BB%B6%E5%BD%92%E6%A1%A3.png" alt="小文件归档"></p></li><li><p>实际操作</p><ul><li>启动YARN进程：start-yarn.sh(hadoop2)</li><li>归档文件(归档后的路径不得实现存在)<br>把/sobxiong目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/sobxiongOutput路径下：hadoop archive -archiveName input.har -p /sobxiong /sobxiongOutput</li><li>查看归档：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 普通查看文件命令</span></span><br><span class="line">hadoop fs -ls R /sobxiongOutput/input.har</span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup          0 2020-06-24 16:06 /sobxiongOutput/input.har/_SUCCESS</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup        305 2020-06-24 16:06 /sobxiongOutput/input.har/_index</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup         23 2020-06-24 16:06 /sobxiongOutput/input.har/_masterindex</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup     317029 2020-06-24 16:06 /sobxiongOutput/input.har/part-0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 采用har格式查看文件(可以像以往一样操作内部文件,也需要har格式)</span></span><br><span class="line">hadoop fs -ls R har:///sobxiongOutput/input.har</span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup      84942 2020-06-21 11:38 har:///sobxiongOutput/input.har/1tset.png</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup      84942 2020-06-21 11:33 har:///sobxiongOutput/input.har/test.png</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup     147145 2020-06-23 13:52 har:///sobxiongOutput/input.har/test6.txt</span></span><br></pre></td></tr></table></figure><ul><li>解归档文件：hadoop fs -cp har:///sobxiongOutput/input.har/* /</li></ul></li></ul></li><li><p>回收站<br>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用</p><ul><li>开启回收站功能参数说明：<ul><li>默认值fs.trash.interval = 0，0表示禁用回收站；<strong>其他值表示设置文件的存活时间(分钟为单位)</strong></li><li>默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。该值设置和fs.trash.interval的参数值相同(要求fs.trash.checkpoint.interval &lt;= fs.trash.interval)</li></ul></li><li>回收站工作机制<br><img src="%E5%9B%9E%E6%94%B6%E7%AB%99%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="回收站工作机制"></li><li>启动回收站：修改core-site.xml，配置垃圾回收时间为1分钟</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>查看回收站：回收站在集群中的路径：/user/sobxiong/.Trash/</li><li>修改访问回收站用户名称：进入垃圾回收站用户名称，默认是dr.who，修改为sobxiong(同样是core-site.xml)</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>恢复回收站数据：hadoop fs -mv /user/sobxiong/.Trash/Current/user/sobxiong/input /</li><li>清空回收站：hadoop fs -expunge</li></ul></li><li><p>快照管理</p><ul><li><p>命令介绍<br><img src="%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86.png" alt="快照管理"></p></li><li><p>实际操作</p><ul><li>开启/禁用指定目录的快照功能：hdfs dfsadmin -allowSnapshot(-disallowSnapshot) /user/sobxiong/input</li><li>对目录创建快照</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -createSnapshot /user/sobxiong/input</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 快照和源文件使用相同数据</span></span><br><span class="line">hdfs dfs -ls R /user/sobxiong/input/.snapshot/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定名称创建快照</span></span><br><span class="line">hdfs dfs -createSnapshot /user/sobxiong/input test</span><br></pre></td></tr></table></figure><ul><li>重命名快照：hdfs dfs -renameSnapshot /user/sobxiong/input test test01</li><li>列出当前用户所有可快照目录：hdfs lsSnapshottableDir</li><li><strong>比较两个快照目录的不同之处(可以是源文件和快照,使用’.’,此时”.snapshot/name”用于指定具体快照)</strong>：hdfs snapshotDiff /user/sobxiong/input . .snapshot/test01</li><li>恢复快照：hdfs dfs -cp /user/sobxiong/input/.snapshot/s20200624-134303.027 /</li></ul></li></ul></li></ul><h2 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h2><ul><li><p>MapReduce定义</p><ul><li>MapReduce是一个<strong>分布式运算程序的编程框架</strong>，是用户开发“基于Hadoop的数据分析应用”的核心框架</li><li>MapReduce核心功能是<strong>将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序</strong>，并发运行在一个Hadoop集群上</li></ul></li><li><p>MapReduce优缺点</p><ul><li>优点：<ul><li>MapReduce易于编程<br>它<strong>简单地实现一些接口，就可以完成一个分布式程序</strong>，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行</li><li>良好的扩展性(hadoop)<br>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力</li><li>高容错性<br>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。<strong>比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</strong>，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的</li><li>适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力</li></ul></li><li>缺点：<ul><li>不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果</li><li>擅长流式计算<br>流式计算的输入数据是动态的，<strong>而MapReduce的输入数据集是静态的，不能动态变化</strong>。这是因为MapReduce自身的设计特点决定了数据源必须是静态的</li><li>不擅长DAG(有向图)计算<br>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，<strong>每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下</strong></li></ul></li></ul></li><li><p>核心思想<br><img src="MapRecude%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3.png" alt="MapRecude核心编程思想"></p></li><li><p>MapReduce进程<br>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p><ul><li>MrAppMaster：负责整个程序的过程调度及状态协调</li><li>MapTask：负责Map阶段的整个数据处理流程</li><li>ReuceTask：负责Reduce阶段的整个数据处理流程</li></ul></li><li><p>常用数据序列化类型</p><table><thead><tr><th>Java类型</th><th>Hadoop Writable类型</th></tr></thead><tbody><tr><td>boolean</td><td>BooleanWritable</td></tr><tr><td>byte</td><td>ByteWritable</td></tr><tr><td>int</td><td>IntWritable</td></tr><tr><td>float</td><td>FloatWritable</td></tr><tr><td>long</td><td>LongWritable</td></tr><tr><td>double</td><td>DoubleWritable</td></tr><tr><td>String</td><td>Text</td></tr><tr><td>map</td><td>MapWritable</td></tr><tr><td>array</td><td>ArrayWritable</td></tr></tbody></table></li><li><p>MapReduce编程规范<br>用户编写的程序分成三个部分：Mapper、Reducer和Driver</p><ul><li>Mapper<ul><li>用户自定义的Mapper要继承自己的父类</li><li>Mapper的输入数据是KV对的形式(KV的类型可自定义)</li><li>Mapper中的业务逻辑写在map()方法中</li><li>Mapper的输出数据是KV对的形式(KV的类型可自定义)</li><li><strong>map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次</strong></li></ul></li><li>Reducer<ul><li>用户自定义的Reducer要继承自己的父类</li><li>Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</li><li>Reducer的业务逻辑写在reduce()方法中</li><li><strong>ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</strong></li></ul></li><li>Driver：相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象</li></ul></li><li><p>WordCount案例实操</p><ul><li><p>需求：在给定的文本文件中统计输出每一个单词出现的总次数</p></li><li><p>需求分析<br><img src="WordCount%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90.png" alt="WordCount需求分析"></p></li><li><p>环境准备</p><ul><li>创建maven空项目</li><li>改pom</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.13.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>在resources资源文件夹下新建log4j.properties文件</li></ul><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure><ul><li><p>编写MapReduce程序</p><ul><li>编写mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* map阶段</span></span><br><span class="line"><span class="comment">* KEYIN：输入数据的key类型(默认写LongWritable:偏移量)</span></span><br><span class="line"><span class="comment">* VALUEIN：输入数据的value类型</span></span><br><span class="line"><span class="comment">* KEYOUT：输出数据的key类型</span></span><br><span class="line"><span class="comment">* VALUEOUT：输出数据的value类型</span></span><br><span class="line"><span class="comment">* &lt;sobxiong,1&gt;输出数据</span></span><br><span class="line"><span class="comment">* 输出作为reduce阶段的输入</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// sobxiong sobxiong</span></span><br><span class="line">        <span class="comment">// 1、获取一行</span></span><br><span class="line">        String lineStr = value.toString();</span><br><span class="line">        <span class="comment">// 2、切割单词</span></span><br><span class="line">        String[] words = lineStr.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 3、循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="comment">// &lt;sobxiong,1&gt;</span></span><br><span class="line">            k.set(word);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* reduce阶段</span></span><br><span class="line"><span class="comment">* KEYIN,VALUEIN：map阶段输出的kv</span></span><br><span class="line"><span class="comment">* KEYOUT,VALUEOUT：reduce输出的kv</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 1、累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2、写出 &lt;sobxiong,2&gt;</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Driver驱动类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、获取Job对象</span></span><br><span class="line">      Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">      Job job = Job.getInstance(conf);</span><br><span class="line">      <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">      job.setJarByClass(WordCountDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">      job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">      job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">      job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">      FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">      FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">      <span class="comment">// 7、提交job</span></span><br><span class="line">      <span class="comment">// job.submit();</span></span><br><span class="line">      <span class="comment">// true：打印一些信息</span></span><br><span class="line">      <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">      <span class="comment">// 额外</span></span><br><span class="line">      System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>本地测试：启动旁边的下箭头Edit Configuration，新增Application，选择Main Class并在Program arguments中加入两个参数中间用空格隔开，前者是input文件所在文件夹，后者是输出文件夹，要求不能存在(不然会出错)。例如：/Users/sobxiong/Downloads/input /Users/sobxiong/Downloads/ouputTest</p></li><li><p>在集群上测试</p><ul><li>用maven打jar包，添加打包插件依赖</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.xiong.hadoop.WordCountDriver<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>将程序打成jar包，maven install即可</li><li>将获取的两个jar包(一个不带依赖,另一带)，将不带依赖的jar上传到hadoop1主机上</li><li>启动Hadoop集群</li><li>执行WordCount程序：hadoop jar WordCount.jar com.xiong.hadoop.WordCountDriver /sobxiong /outputTest(第四个参数为启动的主类名,第五个为输入文件所在文件夹,第六个为输出文件夹——不能事先存在)</li></ul></li></ul></li></ul></li></ul><h2 id="Hadoop序列化"><a href="#Hadoop序列化" class="headerlink" title="Hadoop序列化"></a>Hadoop序列化</h2><ul><li><p>序列化概述</p><ul><li>什么是序列化：<em>序列化</em>就是把<strong>内存中的对象，转换成字节序列</strong>(或其他数据传输协议)以便于存储到磁盘(持久化)和网络传输；<em>反序列化</em>就是<strong>将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象</strong></li><li>为什么要序列化：一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而<strong>序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机</strong></li><li>为什么不用Java的序列化：Java的序列化是一个重量级序列化框架(Serializable)，一个对象被序列化后，会附带很多额外的信息(各种校验信息，Header，继承体系等)，不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制(Writable)</li><li>Hadoop序列化特点：<ul><li>紧凑：高效使用存储空间</li><li>快速：读写数据的额外开销小</li><li>可扩展：随着通信协议的升级而可升级</li><li>互操作：支持多语言的交互</li></ul></li></ul></li><li><p>自定义bean对象实现序列化接口(Writable)<br>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。<br>具体实现bean对象序列化步骤如下7步：</p><ul><li>实现Writable接口</li><li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li><li>重写序列化方法</li><li>重写反序列化方法(<strong>注意反序列化的顺序和序列化的顺序完全一致</strong>)</li><li>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用</li><li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框架中的Shuffle过程要求对key必须能排序</li></ul></li><li><p>序列化案例实操</p><ul><li><p>需求：统计每一个手机号耗费的总上行流量、下行流量、总流量</p></li><li><p>案例分析<br><img src="%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="序列化案例分析"></p></li><li><p>编写MapReduce程序</p><ul><li>编写流量统计的Bean对象</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">测试数据</span></span><br><span class="line"><span class="comment">1;13736230513;192.196.100.1;www.atguigu.com;2481;24681;200</span></span><br><span class="line"><span class="comment">2;13846544121;192.196.100.2;264;0;200</span></span><br><span class="line"><span class="comment">3;13956435636;192.196.100.3;132;1512;200</span></span><br><span class="line"><span class="comment">4;13966251146;192.168.100.1;240;0;404</span></span><br><span class="line"><span class="comment">5;18271575951;192.168.100.2;www.atguigu.com;1527;2106;200</span></span><br><span class="line"><span class="comment">6;84188413;192.168.100.3;www.atguigu.com;4116;1432;200</span></span><br><span class="line"><span class="comment">7;13590439668;192.168.100.4;1116;954;200</span></span><br><span class="line"><span class="comment">8;15910133277;192.168.100.5;wwww.haol23.com;3156;2936;200</span></span><br><span class="line"><span class="comment">9;13729199489;192.168.100.6;240;0;200</span></span><br><span class="line"><span class="comment">10;13630577991;192.168.100.7;www.shouhu.com;6960;690;200</span></span><br><span class="line"><span class="comment">11;15043685818;192.168.100.8;www.baidu.com;3659;3538;200</span></span><br><span class="line"><span class="comment">12;15959002129;192.168.100.9;www.atguigu.com;1938;180;500</span></span><br><span class="line"><span class="comment">13;13560439638;192.168.100.10;918;4938;200</span></span><br><span class="line"><span class="comment">14;13470253144;192.168.100.11;180;180;200</span></span><br><span class="line"><span class="comment">15;13682846555;192.168.100.12;wwww.qq.com;1938;2910;200</span></span><br><span class="line"><span class="comment">16;13992314666;192.168.100.13;www.gaga.com;3008;3720;200</span></span><br><span class="line"><span class="comment">17;13509468723;192.168.100.14;www.qinghua.com;7335;110349;404</span></span><br><span class="line"><span class="comment">18;18390173782;192.168.100.15;www.sogou.com;9531;2412;200</span></span><br><span class="line"><span class="comment">19;13975057813;192.168.100.16;www.baidu.com;11058;48243;200</span></span><br><span class="line"><span class="comment">20;13768778790;192.168.100.17;120;120;200</span></span><br><span class="line"><span class="comment">21;13568436656;192.168.100.18;www.alibaba.com;2481;24681;200</span></span><br><span class="line"><span class="comment">22;13568436656;192.168.100.19;1116;954;200</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 上行流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">  <span class="comment">// 下行流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">  <span class="comment">// 总流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line">  <span class="comment">// 空参构造,为了后续反射</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 序列化方法</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      dataOutput.writeLong(upFlow);</span><br><span class="line">      dataOutput.writeLong(downFlow);</span><br><span class="line">      dataOutput.writeLong(sumFlow);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 反序列化方法</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      <span class="comment">// 必须要求和序列化方法顺序一致</span></span><br><span class="line">      upFlow = dataInput.readLong();</span><br><span class="line">      downFlow = dataInput.readLong();</span><br><span class="line">      sumFlow = dataInput.readLong();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">      <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">      <span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、获取一行</span></span><br><span class="line">      String lineStr = value.toString();</span><br><span class="line">      <span class="comment">// 2、切割\t</span></span><br><span class="line">      String[] fields = lineStr.split(<span class="string">"\t"</span>);</span><br><span class="line">      <span class="comment">// 3、封装对象</span></span><br><span class="line">      k.set(fields[<span class="number">1</span>]);</span><br><span class="line">      <span class="keyword">long</span> upFlow = Long.parseLong(fields[fields.length - <span class="number">3</span>]);</span><br><span class="line">      <span class="keyword">long</span> downFlow = Long.parseLong(fields[fields.length - <span class="number">2</span>]);</span><br><span class="line">      flowBean.set(upFlow, downFlow);</span><br><span class="line">      <span class="comment">// 4、写出</span></span><br><span class="line">      context.write(k, flowBean);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FlowBean v = <span class="keyword">new</span> FlowBean();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、累加求和</span></span><br><span class="line">      <span class="keyword">long</span> sumUpFlow = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">long</span> sumDownFlow = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">          sumUpFlow += value.getUpFlow();</span><br><span class="line">          sumDownFlow += value.getDownFlow();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      v.set(sumUpFlow, sumDownFlow);</span><br><span class="line">      context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Driver驱动类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取job对象</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf);</span><br><span class="line">    <span class="comment">// 2、设计jar路径</span></span><br><span class="line">    job.setJarByClass(FlowDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">    job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">    job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">    job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 7、提交job</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h2><ul><li><p>InputFormat数据输入</p><ul><li><p>切片与MapTask并行度决定机制</p><ul><li>问题引出：MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度<br>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</li><li>MapTask并行度决定机制</li></ul><p><strong>数据块</strong>：Block是HDFS物理上把数据分成一块一块<br><strong>数据切片</strong>：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储<br><img src="%E6%95%B0%E6%8D%AE%E5%88%87%E7%89%87%E4%B8%8EMapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6.png" alt="数据切片与MapTask并行度决定机制"></p></li><li><p>Job提交流程源码和切片源码详解</p><ul><li>Job提交流程源码详解</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion();</span><br><span class="line">submit();</span><br><span class="line">  <span class="comment">// 1、建立连接</span></span><br><span class="line">  connect();</span><br><span class="line">    <span class="comment">// 1)创建提交Job的代理</span></span><br><span class="line">    <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">      <span class="comment">// 判断是本地yarn还是远程</span></span><br><span class="line">      initialize(jobTrackAddr, conf);</span><br><span class="line">  <span class="comment">// 2、提交job</span></span><br><span class="line">  submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">  <span class="comment">// 1)创建给集群提交数据的Stag路径</span></span><br><span class="line">  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">  <span class="comment">// 2)获取jobid,并创建Job路径</span></span><br><span class="line">  JobID jobId = submitClient.getNewJobID();</span><br><span class="line">  <span class="comment">// 3)拷贝jar包到集群</span></span><br><span class="line">  copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">  <span class="comment">// 4)计算切片,生成切片规划文件</span></span><br><span class="line">  writeSplits(job, submitJobDir);</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">    input.getSplits(job);</span><br><span class="line">  <span class="comment">// 5)向Stag路径写XML配置文件</span></span><br><span class="line">  writeConf(conf, submitJobFile);</span><br><span class="line">  conf.writeXml(out);</span><br><span class="line">  <span class="comment">// 6)提交Job,返回提交状态</span></span><br><span class="line">  status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure><p><img src="Job%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.png" alt="Job提交流程源码分析"></p><ul><li>FileInputFormat切片源码解析(input.getSplits(job))<ul><li>程序先找到你数据存储的目录</li><li>开始遍历处理(规划切片)目录下的每一个文件</li><li>遍历第一个文件ss.txt<ul><li>获取文件大小fs.sizeOf(ss.txt)</li><li>计算切片大小：computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M(YARN集群默认128M——2.x,62M-1.x;本地运行默认32M)</li><li><strong>默认情况下，切片大小=blocksize</strong></li><li>开始切，形成第1个切片：ss.txt—0:128M、第2个切片ss.txt—128:256M、第3个切片ss.txt—256M:300M(<strong>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片</strong>)</li><li>将切片信息写到一个切片规划文件中</li><li>整个切片的核心过程在getSplit()方法中完成</li><li><strong>InputSplit只记录了切片的元数据信息</strong>，比如起始位置、长度以及所在的节点列表等</li></ul></li><li><strong>提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数</strong></li></ul></li></ul></li><li><p>FileInputFormat切片机制</p><ul><li><p>切片机制</p><ul><li>简单地按照文件的内容长度进行切片</li><li>切片大小，默认等于Block大小</li><li><strong>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</strong></li></ul></li><li><p>案例分析</p><ul><li>输入数据有两个文件：file1.txt - 320M;file2.txt - 10M</li><li>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</li></ul><table><thead><tr><th>文件</th><th>切片区间</th></tr></thead><tbody><tr><td>file1.txt.split1</td><td>0~128</td></tr><tr><td>file1.txt.split2</td><td>129~256</td></tr><tr><td>file1.txt.split3</td><td>257~320</td></tr><tr><td>file2.txt.split1</td><td>0~10</td></tr></tbody></table></li><li><p>切片大小参数配置</p><ul><li>源码中计算切片大小的公式：Math.max(minSize, Math.min(maxSize, blockSize));<br>mapreduce.input.fileinputformat.split.minsize=<strong>1</strong> 默认值为1<br>mapreduce.input.fileinputformat.split.maxsize=<strong>Long.MAXValue</strong> 默认值Long.MAXValue</li></ul><p><strong>默认情况下，切片大小=blocksize</strong></p><ul><li>切片大小设置<br>maxsize(切片最大值)：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值<br>minsize(切片最小值)：参数调的比blockSize大，则可以让切片变得比blockSize还大</li><li>获取切片信息API</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取切片的文件名称</span></span><br><span class="line">String name = inputSplit.getPath().getName();</span><br><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line">FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br></pre></td></tr></table></figure></li></ul></li><li><p>CombineTextInputFormat切片机制<br>框架默认的TextInputFormat切片机制是对任务按文件规划切片，<strong>不管文件多小，都会是一个单独的切片</strong>，都会交给一个MapTask，<strong>这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下</strong></p><ul><li>应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理</li><li>虚拟存储切片最大值设置：CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);(注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值)</li><li>切片机制切片机制：生成切片过程包括<strong>虚拟存储过程和切片过程二部分</strong><br><img src="CombineTextInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6.png" alt="CombineTextInputFormat切片机制"><ul><li>虚拟存储过程<br>将输入目录下所有文件的大小依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件<strong>均分</strong>成2个虚拟存储块(防止出现太小切片)<br>例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成(2.01M和2.01M)两个文件</li><li>切片过程<ul><li>判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片</li><li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片</li><li><strong>测试举例</strong>：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M、(2.55M、2.55M)、3.4M、(3.4M、3.4M)。最终会形成3个切片，大小分别为：(1.7M + 2.55M)，(2.55M + 3.4M)，(3.4M + 3.4M)</li></ul></li></ul></li></ul></li><li><p>CombineTextInputFormat案例实操</p><ul><li><p>需求：将输入的大量小文件合并成一个切片统一处理</p><ul><li>输入数据：准备4个小文件</li><li>期望：期望一个切片处理4个文件</li></ul></li><li><p>实现过程</p><ul><li>不做任何处理，运行之前的WordCount案例程序，观察切片个数为4——(number of splits:4)</li><li>在WordcountDriver中增加如下代码(设置切片最大值为4m)，运行程序，观察运行的切片个数为3</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure><ul><li>在WordcountDriver中增加如下代码(设置切片最大值为20m)，运行程序，观察运行的切片个数为1(代码同上,值改为20971520)</li></ul></li></ul></li><li><p>FileInputFormat实现类<br>思考：在运行MapReduce程序时，<em>输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等</em>。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢?<br>FileInputFormat常见的接口实现类包括：<strong>TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat</strong>等</p><ul><li>TextInputFormat<br>TextInputFormat是默认的FileInputFormat实现类。<strong>按行读取每条记录，键是存储该行在整个文件中的起始字节偏移量——LongWritable类型；值是这行的内容，不包括任何行终止符(换行符和回车符)——Text类型</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例：</span><br><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对：</span><br><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure><ul><li>KeyValueTextInputFormat<br>每一行均为一条记录，被分隔符分割为&lt;key,value&gt;对。<strong>可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “\t”)来设定分隔符——默认分隔符是tab(\t)</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例(其中——&gt;表示一个水平方向的制表符)：</span><br><span class="line">line1 ——&gt;Rich learning form</span><br><span class="line">line2 ——&gt;Intelligent learning engine</span><br><span class="line">line3 ——&gt;Learning more convenient</span><br><span class="line">line4 ——&gt;From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对(键是每行排在制表符之前的Text序列)：</span><br><span class="line">(line1,Rich learning form)</span><br><span class="line">(line2,Intelligent learning engine)</span><br><span class="line">(line3,Learning more convenient)</span><br><span class="line">(line4,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure><ul><li>NLineInputFormat<br>如果使用NlineInputFormat，代表每个map进程处理的<strong>InputSplit不再按Block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数 / N = 切片数，如果不整除，切片数 = 商 + 1</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例：</span><br><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 如果N是2,则每个输入分片包含两行。开启2个MapTask(键和值与TextInputFormat生成的一样)：</span><br><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br><span class="line">&#x2F;&#x2F; 另一个mapper则收到后两行：</span><br><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure></li><li><p>KeyValueTextInputFormat使用案例</p><ul><li><p>需求：统计输入文件中每一行的第一个单词相同的行数</p></li><li><p>案例分析<br><img src="KeyValueTextInputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="KeyValueTextInputFormat案例分析"></p></li><li><p>代码实现</p><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable intWritable = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、封装对象</span></span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      context.write(key, intWritable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable intWritable = <span class="keyword">new</span> IntWritable();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、累加求和</span></span><br><span class="line">      <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">          sum += value.get();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      intWritable.set(sum);</span><br><span class="line">      context.write(key, intWritable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">" "</span>);</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(KVTextDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(KVTextMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(KVTextReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setInputFormatClass(KeyValueTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>NLineInputFormat使用案例</p><ul><li><p>需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中</p></li><li><p>案例分析<br><img src="NLineInputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="NLineInputFormat案例分析"></p></li><li><p>代码实现</p><ul><li>Mapper和Reducer类参照WordCount</li><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 设置切片InputSplit中划分三条记录</span></span><br><span class="line">  NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">  <span class="comment">// 使用NLineInputFormat处理记录数</span></span><br><span class="line">  job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(NLineDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(NLineMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(NLineReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>观察控制台打印的number of splits</p></li></ul></li><li><p>自定义InputFormat<br>在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题<br>自定义InputFormat步骤如下：</p><ul><li>自定义一个类继承FileInputFormat</li><li>改写RecordReader，实现一次读取一个完整文件封装为KV</li><li>在输出时使用SequenceFileOutPutFormat输出合并文件</li></ul></li><li><p>自定义InputFormat案例实操</p><ul><li><p>需求：将多个小文件合并成一个SequenceFile文件(SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式)，SequenceFile里面存储着多个文件，存储的形式为key——文件路径 + 名称，value——文件内容</p></li><li><p>案例分析<br><img src="%E8%87%AA%E5%AE%9A%E4%B9%89InputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="自定义InputFormat案例分析"></p></li><li><p>代码实现</p><ul><li>自定义InputFormat</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      WholeRecordReader recordReader = <span class="keyword">new</span> WholeRecordReader();</span><br><span class="line">      recordReader.initialize(split, context);</span><br><span class="line">      <span class="keyword">return</span> recordReader;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>自定义RecordReader类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FileSplit split;</span><br><span class="line">  <span class="keyword">private</span> Configuration configuration;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> BytesWritable v = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">  <span class="keyword">boolean</span> isProgress = <span class="keyword">true</span>;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 初始化</span></span><br><span class="line">      <span class="keyword">this</span>.split = (FileSplit) split;</span><br><span class="line">      configuration = context.getConfiguration();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 核心业务逻辑</span></span><br><span class="line">      <span class="comment">// 每个文件创建一次reader</span></span><br><span class="line">      <span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line">          <span class="comment">// 1、获取fs对象</span></span><br><span class="line">          Path path = split.getPath();</span><br><span class="line">          FileSystem fileSystem = path.getFileSystem(configuration);</span><br><span class="line">          <span class="comment">// 2、获取输入流</span></span><br><span class="line">          FSDataInputStream fis = fileSystem.open(path);</span><br><span class="line">          <span class="comment">// 3、拷贝</span></span><br><span class="line">          <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) split.getLength()];</span><br><span class="line">          IOUtils.readFully(fis, buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line">          <span class="comment">// 4、封装v</span></span><br><span class="line">          v.set(buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line">          <span class="comment">// 5、封装key</span></span><br><span class="line">          k.set(path.toString());</span><br><span class="line">          <span class="comment">// 6、关闭资源</span></span><br><span class="line">          IOUtils.closeStream(fis);</span><br><span class="line">          isProgress = <span class="keyword">false</span>;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> k; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> v; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      context.write(key, value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 循环写出</span></span><br><span class="line">      <span class="keyword">for</span> (BytesWritable value : values) &#123;</span><br><span class="line">          context.write(key, value);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  job.setInputFormatClass(WholeFileInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputFormatClass(SequenceFileOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 2、设计jar路径</span></span><br><span class="line">  job.setJarByClass(SequenceFileDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">  job.setMapperClass(SequenceFileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(SequenceFileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>MapReduce工作流程</p><ul><li>流程示意图<br><img src="MapReduce%E8%AF%A6%E7%BB%86%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B(1).png" alt="MapReduce详细工作流程(1)"><br><img src="MapReduce%E8%AF%A6%E7%BB%86%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B(2).png" alt="MapReduce详细工作流程(2)"></li><li>流程详解<br>上面的流程是整个MapReduce的全部工作流程，Shuffle过程是从第7步开始到第16步结束，具体Shuffle过程详解，如下：<ul><li>MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</li><li>从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</li><li>多个溢出文件会被合并成大的溢出文件</li><li>在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li><li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li><li>ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并(归并排序)</li><li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程——从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法</li></ul></li><li>注意<br>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。<br>缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M</li></ul></li><li><p>Shuffle</p><ul><li><p>Shuffle机制：Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle<br><img src="Shuffle%E6%9C%BA%E5%88%B6.png" alt="Shuffle机制"></p></li><li><p>Partition分区</p><ul><li>问题引出：要求将统计结果按照条件输出到不同文件中(分区)。比如：将统计结果按照手机归属地不同省份输出到不同文件中(分区)</li><li>默认Partitionr分区：默认分区是根据key的hashCode对ReduceTasks个数取模得到的(如果分区数大于1)。用户没法控制哪个key存储到哪个分区</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(JobConf job)</span> </span>&#123;&#125;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K2 key, V2 value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>自定义Partitioner步骤</p><ul><li>自定义类继承Partitioner，重写getPartition()方法</li><li>在Job驱动中，设置自定义Partitioner：job.setPartitionerClass(CustomPartitioner.class);</li><li>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask：job.setNumReduceTasks(5);</li></ul></li><li><p><strong>分区总结</strong></p><ul><li>如果ReduceTask的数量 &gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx</li><li>如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，会抛出异常</li><li>如果ReduceTask的数量 = 1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000</li><li>分区号必须从零开始，逐一累加</li></ul></li><li><p>案例分析：假设自定义分区数为5，则</p><ul><li>job.setNumReduceTasks(1)：会正常运行，只不过会产生一个输出文件</li><li>job.setNumReduceTasks(2)：会报错</li><li>job.setNumReduceTasks(6)：大于5，程序会正常运行，会产生空文件part-r-00005</li></ul></li></ul></li><li><p>Partition分区案例实操</p><ul><li><p>需求：将统计结果按照手机归属地不同省份输出到不同文件中(分区)</p></li><li><p>案例分析<br><img src="Partition%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Partition分区案例分析"></p></li><li><p>实操</p><ul><li>在FlowBean案例基础上，增加一个分区类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// key是手机号,value是流量信息</span></span><br><span class="line">    <span class="comment">// 获取手机号前三位</span></span><br><span class="line">    String prePhoneNum = text.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="string">"136"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">2</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> partition;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在Driver驱动主函数中添加自定义数据分区设置和ReduceTask设置</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定自定义数据分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 指定ReduceTask的数目</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">6</span>);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>WritableComparable排序</p><ul><li><p>排序概述：<br>排序是MapReduce框架中最重要的操作之一。MapTask和ReduceTask均会对数据<strong>按照key进行排序</strong>，该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是<strong>按照字典顺序</strong>排序，且实现该排序的方法是<strong>快速排序</strong>。<br>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。<br>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</p></li><li><p>排序的分类</p><ul><li>部分排序：MapReduce根据输入记录的键对数据集排序。保证<strong>输出的每个文件内部有序</strong></li><li>全排序：<strong>最终输出结果只有一个文件，且文件内部有序</strong>。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构</li><li>辅助排序(GroupingComparator分组)：在Reduce端对key进行分组。应用——在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序</li><li>二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序</li></ul></li><li><p>自定义排序WritableComparable</p><ul><li><p>原理分析：bean对象做为key传输，需要<strong>实现WritableComparable接口重写compareTo方法</strong>，就可以实现排序</p></li><li><p>案例实操(全排序)</p><ul><li><p>需求：对FlowBean案例产生的结果再次对总流量进行排序</p></li><li><p>案例分析<br><img src="WritableComparable%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="WritableComparable排序案例分析"></p></li><li><p>代码实现</p><ul><li>FlowBean对象在之前案例基础上实现WritableComparable接口，实现compareTo()方法</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 按总流量倒序</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; bean.sumFlow ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、封装对象</span></span><br><span class="line">    v.set(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">long</span> upFlow = Long.parseLong(fields[fields.length - <span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">long</span> downFlow = Long.parseLong(fields[fields.length - <span class="number">2</span>]);</span><br><span class="line">    flowBean.set(upFlow, downFlow);</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(flowBean, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">        context.write(value, key);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 2、设计jar路径</span></span><br><span class="line">  job.setJarByClass(FlowCountSortDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">  job.setMapperClass(FlowCountSortMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(FlowCountSortReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>Combiner合并</p><ul><li><p>Combiner介绍<br><img src="Combiner%E4%BB%8B%E7%BB%8D.png" alt="Combiner介绍"></p></li><li><p>自定义Combiner实现步骤</p><ul><li>自定义一个Combiner继承Reducer，重写reduce方法</li><li>在Driver驱动类中设置：job.setCombinerClass(xxCombiner.class);</li></ul></li><li><p>自定义Combiner实操</p><ul><li><p>需求：统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能</p></li><li><p>案例分析<br><img src="Combiner%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Combiner案例分析"></p></li><li><p>代码实现</p><ul><li>方案一</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 新建WordCountCombiner类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、累加求和</span></span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">        sum += value.get();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、写出</span></span><br><span class="line">    v.set(sum);</span><br><span class="line">    context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在WordCountDriver驱动类中指定Combiner</span></span><br><span class="line">job.setCombinerClass(WordcountCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure><ul><li>方案二：将WordCountReducer作为Combiner</li></ul></li><li><p>结果查看：控制台打印中的Map-Reduce Framework中的Combine记录</p></li></ul></li></ul></li><li><p>GroupingComparator分组(辅助排序)</p><ul><li><p>简要介绍：对Reduce阶段的数据根据某一个或几个字段进行分组</p></li><li><p>分组排序步骤</p><ul><li>自定义类继承WritableComparator</li><li>重写compare()方法</li><li>创建一个构造将比较对象的类传给父类</li></ul></li><li><p>GroupingComparator分组实操</p><ul><li><p>需求：求出每一个订单中最贵的商品</p></li><li><p>案例分析<br><img src="GroupingComparator%E5%88%86%E7%BB%84%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="GroupingComparator分组案例分析"></p><ul><li>利用“订单id和成交金额price”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序。Reduce将从Map中获取排序好的数据</li><li>在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品</li></ul></li><li><p>代码实现</p><ul><li>编写订单信息OrderBean类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 忽略了空参/全参构造器、getter/setter和toString方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> orderId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">double</span> price;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean bean)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result;</span><br><span class="line">    <span class="keyword">if</span> (orderId &gt; bean.orderId) &#123;</span><br><span class="line">        result = <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (orderId &lt; bean.orderId) &#123;</span><br><span class="line">        result = -<span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        result = price &gt; bean.price ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeInt(orderId);</span><br><span class="line">    out.writeDouble(price);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    orderId = in.readInt();</span><br><span class="line">    price = in.readDouble();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderGroupingComparator类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(OrderBean<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 要求只要id相同,就认为是相同的key</span></span><br><span class="line">    OrderBean aBean = (OrderBean) a;</span><br><span class="line">    OrderBean bBean = (OrderBean) b;</span><br><span class="line">    <span class="keyword">if</span> (aBean.getOrderId() == bBean.getOrderId()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> aBean.getOrderId() &gt; bBean.getOrderId() ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderMapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> OrderBean orderBean = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、封装对象</span></span><br><span class="line">    orderBean.setOrderId(Integer.parseInt(fields[<span class="number">0</span>]));</span><br><span class="line">    orderBean.setPrice(Double.parseDouble(fields[<span class="number">2</span>]));</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(orderBean, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderReducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 输出第一个</span></span><br><span class="line">    context.write(key, NullWritable.get());</span><br><span class="line">    <span class="comment">// 循环几次输出前几</span></span><br><span class="line">    <span class="comment">//for (NullWritable value : values) &#123;</span></span><br><span class="line">    <span class="comment">//    context.write(key, NullWritable.get());</span></span><br><span class="line">    <span class="comment">//&#125;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>编写OrderDriver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(OrderDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(OrderMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(OrderReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setGroupingComparatorClass(OrderGroupingComparator<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>MapTask工作机制<br><img src="MapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="MapTask工作机制"></p><ul><li>Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value</li><li>Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value</li><li>Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区(调用Partitioner)，并写入一个环形内存缓冲区中</li><li>Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作<ul><li>溢写步骤1：利用快速排序算法对缓存区内的数据进行排序。排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序</li><li>溢写步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out(N表示当前溢写次数)中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作</li><li>溢写步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中</li></ul></li><li>Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件<br>当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。<br>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor(默认10)个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。<br>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销</li></ul></li><li><p>ReduceTask</p><ul><li><p>ReduceTask工作机制<br><img src="ReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="ReduceTask工作机制"></p><ul><li>Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中</li><li>Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多</li><li>Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可</li><li>Reduce阶段：reduce()函数将计算结果写到HDFS上</li></ul></li><li><p>设置ReduceTask并行度(个数)<br>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置(默认值为1)：job.setNumReduceTasks(4);</p></li><li><p>一个测试ReduceTask数目的实验</p><ul><li>实验环境：1个Master节点，16个Slave节点；CPU：8GHZ，内存: 2G</li><li>实验结论(数据量为1G)</li></ul><table><thead><tr><th>ReduceTask数目</th><th>总时间</th></tr></thead><tbody><tr><td>1</td><td>892</td></tr><tr><td>5</td><td>146</td></tr><tr><td>10</td><td>110</td></tr><tr><td>15</td><td>92</td></tr><tr><td>16</td><td>88</td></tr><tr><td>20</td><td>100</td></tr><tr><td>25</td><td>128</td></tr><tr><td>30</td><td>101</td></tr><tr><td>45</td><td>145</td></tr><tr><td>60</td><td>104</td></tr></tbody></table></li><li><p>注意事项</p><ul><li>ReduceTask = 0，表示没有Reduce阶段，输出文件个数和Map个数一致</li><li>ReduceTask默认值就是1，所以输出文件个数为一个</li><li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜(一些节点很忙,其余空闲)</li><li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask</li><li>具体多少个ReduceTask，需要根据集群性能而定</li><li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程？答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行</li></ul></li></ul></li><li><p>OutputFormat数据输出</p><ul><li><p>OutputFormat接口实现类<br>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。以下是几种常见的OutputFormat实现类</p><ul><li><p>文本输出TextOutputFormat<br>默认的输出格式是TextOutputFormat，<strong>它把每条记录写为文本行</strong>。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串</p></li><li><p>SequenceFileOutputFormat<br>将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的<strong>格式紧凑，很容易被压缩</strong></p></li><li><p>自定义OutputFormat：根据用户需求，自定义实现输出</p><ul><li><p>使用场景<br>为了实现<strong>控制最终文件的输出路径和输出格式</strong>，可以自定义OutputFormat<br>例如：要在一个MapReduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义OutputFormat来实现</p></li><li><p>自定义OutputFormat步骤</p><ul><li>自定义一个类继承FileOutputFormat</li><li>改写RecordWriter，具体改写输出数据的方法write()</li></ul></li><li><p>自定义OutputFormat案例实操</p><ul><li><p>需求：过滤输入的log日志，指定包含某特定字段的记录输出到一个文件，其他则输出到另一个文件</p></li><li><p>案例分析<br><img src="%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="自定义OutputFormat案例分析"></p></li><li><p>代码编写</p><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// http://www.baidu.com</span></span><br><span class="line">    context.write(value, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    String line = key.toString();</span><br><span class="line">    line += <span class="string">"\r\n"</span>;</span><br><span class="line">    k.set(line);</span><br><span class="line">    <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">        context.write(k, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>自定义OutputFormat、RecordWriter类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> FRecordWriter(job);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FSDataOutputStream fosSobxiong;</span><br><span class="line">  <span class="keyword">private</span> FSDataOutputStream fosOther;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、获取文件系统</span></span><br><span class="line">      FileSystem fs = FileSystem.get(job.getConfiguration());</span><br><span class="line">      <span class="comment">// 2、创建输出到sobxiong.log的输出流</span></span><br><span class="line">      fosSobxiong = fs.create(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Downloads/sobxiong.log"</span>));</span><br><span class="line">      <span class="comment">// 3、创建输出到other.log的输出流</span></span><br><span class="line">      fosOther = fs.create(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Downloads/other.log"</span>));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 判断key中是否有sobxiong,如果有写出到sobxiong,否则输出到other</span></span><br><span class="line">    <span class="keyword">if</span> (key.toString().contains(<span class="string">"sobxiong"</span>)) &#123;</span><br><span class="line">        fosSobxiong.write(key.toString().getBytes());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fosOther.write(key.toString().getBytes());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    IOUtils.closeStream(fosOther);</span><br><span class="line">    IOUtils.closeStream(fosSobxiong);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">  job.setJarByClass(FilterDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapperClass(FilterMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(FilterReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">  job.setOutputFormatClass(FilterOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 虽然我们自定义了outputFormat,但是因为我们的outputFormat继承自fileOutputFormat</span></span><br><span class="line">  <span class="comment">// 而fileOutputFormat要输出一个_SUCCESS文件,所以,在这还得指定一个输出目录</span></span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li></ul></li><li><p>Join多种应用</p><ul><li><p>Reduce Join</p><ul><li><p>工作原理<br>Map端的主要工作：为来自不同表或文件的key/value对，<strong>打标签以区别不同来源的记录</strong>。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出<br>Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些<strong>来源于不同文件的记录(在Map阶段已经打标志)分开</strong>，最后进行合并</p></li><li><p>案例实操</p><ul><li><p>需求：将商品信息表中数据根据商品pid合并到订单数据表中</p></li><li><p>案例分析：通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联<br><img src="Reduce%E7%AB%AF%E8%A1%A8%E5%90%88%E5%B9%B6.png" alt="Reduce端表合并"></p></li><li><p>代码编写</p><ul><li>合并后的Bean类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 略去空参/全参构造器、getter/setter方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 订单id</span></span><br><span class="line">  <span class="keyword">private</span> String id;</span><br><span class="line">  <span class="comment">// 产品id</span></span><br><span class="line">  <span class="keyword">private</span> String pid;</span><br><span class="line">  <span class="comment">// 数量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> amount;</span><br><span class="line">  <span class="comment">// 产品名称</span></span><br><span class="line">  <span class="keyword">private</span> String pName;</span><br><span class="line">  <span class="comment">// 标记: 产品/订单</span></span><br><span class="line">  <span class="keyword">private</span> String flag;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> id + <span class="string">'\t'</span> + amount + <span class="string">'\t'</span> + pName;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeUTF(id);</span><br><span class="line">    out.writeUTF(pid);</span><br><span class="line">    out.writeInt(amount);</span><br><span class="line">    out.writeUTF(pName);</span><br><span class="line">    out.writeUTF(flag);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    id = in.readUTF();</span><br><span class="line">    pid = in.readUTF();</span><br><span class="line">    amount = in.readInt();</span><br><span class="line">    pName = in.readUTF();</span><br><span class="line">    flag = in.readUTF();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String fileName;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TableBean tableBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Text key = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取文件的名称</span></span><br><span class="line">    FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">    fileName = inputSplit.getPath().getName();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// id pid amount</span></span><br><span class="line">  <span class="comment">// 1001 01 1</span></span><br><span class="line">  <span class="comment">// pid pname</span></span><br><span class="line">  <span class="comment">// 01 小米</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="keyword">if</span> (fileName.startsWith(<span class="string">"order"</span>)) &#123; <span class="comment">// 订单表</span></span><br><span class="line">        <span class="comment">// 封装kv</span></span><br><span class="line">        tableBean.setId(fields[<span class="number">0</span>]);</span><br><span class="line">        tableBean.setPid(fields[<span class="number">1</span>]);</span><br><span class="line">        tableBean.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">        <span class="comment">// 属性不能为空,不然会序列化会出错</span></span><br><span class="line">        tableBean.setpName(<span class="string">""</span>);</span><br><span class="line">        tableBean.setFlag(<span class="string">"order"</span>);</span><br><span class="line">        <span class="keyword">this</span>.key.set(fields[<span class="number">1</span>]);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// 产品表</span></span><br><span class="line">        <span class="comment">// 封装kv</span></span><br><span class="line">        tableBean.setId(<span class="string">""</span>);</span><br><span class="line">        tableBean.setPid(fields[<span class="number">0</span>]);</span><br><span class="line">        tableBean.setAmount(<span class="number">0</span>);</span><br><span class="line">        tableBean.setpName(fields[<span class="number">1</span>]);</span><br><span class="line">        tableBean.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">        <span class="keyword">this</span>.key.set(fields[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 写出</span></span><br><span class="line">    context.write(<span class="keyword">this</span>.key, tableBean);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Reducer类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 存储所有订单集合</span></span><br><span class="line">    List&lt;TableBean&gt; beans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="comment">// 存储产品信息</span></span><br><span class="line">    TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">    <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"order"</span>.equals(value.getFlag())) &#123;</span><br><span class="line">            TableBean tmpBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// value是引用,tmpBean是实实在在的对象</span></span><br><span class="line">                BeanUtils.copyProperties(tmpBean, value);</span><br><span class="line">                beans.add(tmpBean);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                BeanUtils.copyProperties(pdBean, value);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 拼接表,设置商品名称</span></span><br><span class="line">    <span class="keyword">for</span> (TableBean bean : beans) &#123;</span><br><span class="line">        bean.setpName(pdBean.getpName());</span><br><span class="line">        context.write(bean, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取配置信息,创建job对象实例</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定本程序的jar包所在的本地路径</span></span><br><span class="line">  job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定本业务job要使用的Mapper/Reducer业务类</span></span><br><span class="line">  job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定Mapper输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始文件所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>测试结果：</li></ul><table><thead><tr><th>pid</th><th>pname</th><th>amount</th></tr></thead><tbody><tr><td>1001</td><td>小米</td><td>1</td></tr><tr><td>1001</td><td>小米</td><td>1</td></tr><tr><td>1002</td><td>华为</td><td>2</td></tr><tr><td>1002</td><td>华为</td><td>2</td></tr><tr><td>1003</td><td>格力</td><td>3</td></tr><tr><td>1003</td><td>格力</td><td>3</td></tr></tbody></table><ul><li>总结<ul><li>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜</li><li>解决方案：Map端实现数据合并</li></ul></li></ul></li></ul></li></ul></li><li><p>Map Join</p><ul><li><p>使用场景：适用于一张表十分小、一张表很大的场景</p></li><li><p>优点<br>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？<br>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜</p></li><li><p>具体方法：采用DistributedCache</p><ul><li>在Mapper的setup阶段，将文件读取到缓存集合中</li><li>在驱动函数中加载缓存(缓存普通文件到Task运行节点)：job.addCacheFile(new URI(“file:///Users/sobxiong/Downloads/testInput3/pd.txt”))</li></ul></li><li><p>案例实操</p><ul><li><p>需求同Reduce Join</p></li><li><p>案例分析<br><img src="Map%E7%AB%AF%E8%A1%A8%E5%90%88%E5%B9%B6.png" alt="Map端表合并"></p></li><li><p>代码编写</p><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job信息</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置加载jar包路径</span></span><br><span class="line">  job.setJarByClass(DistributedCacheDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联map</span></span><br><span class="line">  job.setMapperClass(DistributedCacheMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 没有reduce阶段,map阶段输出即为最终输出</span></span><br><span class="line">  <span class="comment">// 4、设置最终输出数据类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 6、加载缓存数据</span></span><br><span class="line">  job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///Users/sobxiong/Downloads/testInput3/pd.txt"</span>));</span><br><span class="line">  <span class="comment">// 7、Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span></span><br><span class="line">  job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 8、提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">5</span>);</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 缓存小表</span></span><br><span class="line">    String cachePath = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">    BufferedReader bufferedReader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(cachePath), StandardCharsets.UTF_8));</span><br><span class="line">    String lineStr;</span><br><span class="line">    <span class="keyword">while</span> (StringUtils.isNotEmpty(lineStr = bufferedReader.readLine())) &#123;</span><br><span class="line">        <span class="comment">// 1、切割</span></span><br><span class="line">        <span class="comment">// pid pname</span></span><br><span class="line">        <span class="comment">// 01 小米</span></span><br><span class="line">        String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">        <span class="comment">// 2、封装到集合去</span></span><br><span class="line">        pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(bufferedReader);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// id pid amount</span></span><br><span class="line">    <span class="comment">// 1001 01 1</span></span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、获取pid</span></span><br><span class="line">    String pid = fields[<span class="number">1</span>];</span><br><span class="line">    <span class="comment">// 4、取出pname</span></span><br><span class="line">    String pName = pdMap.get(pid);</span><br><span class="line">    <span class="comment">// 5、拼接</span></span><br><span class="line">    lineStr = lineStr.replace(<span class="string">';'</span>, <span class="string">'\t'</span>) + <span class="string">'\t'</span> + pName;</span><br><span class="line">    k.set(lineStr);</span><br><span class="line">    <span class="comment">// 6、写出</span></span><br><span class="line">    context.write(k, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>计数器应用<br>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量</p><ul><li><p>计数器API</p><ul><li>采用枚举的方式统计计数</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> MyCounter&#123;MALFORORMED,NORMAL&#125;</span><br><span class="line"><span class="comment">//对枚举定义的自定义计数器加1</span></span><br><span class="line">context.getCounter(MyCounter.MALFORORMED).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><ul><li>采用计数器组、计数器名称的方式统计</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 组名和计数器名称随便起,但最好有意义</span></span><br><span class="line">context.getCounter(<span class="string">"counterGroup"</span>, <span class="string">"counter"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><ul><li>计数结果在程序运行后的控制台上查看</li></ul></li></ul></li><li><p>数据清洗(ETL)<br>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序</p><ul><li><p>案例实操(简单解析版——运用计数器)——复杂版(字段多,过滤的需求多,思路与下面无差)</p><ul><li><p>需求：去除日志中字段长度小于等于11的日志</p></li><li><p>代码编写</p><ul><li>Mapper类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    <span class="comment">// 2、解析数据</span></span><br><span class="line">    <span class="keyword">boolean</span> isDirty = parseLog(line, context);</span><br><span class="line">    <span class="keyword">if</span> (!isDirty) &#123;</span><br><span class="line">        <span class="comment">// 3、解析通过,写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line">    String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line">    <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"clean"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"dirty"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Driver类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job信息</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 2、加载jar包</span></span><br><span class="line">  job.setJarByClass(LogDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联map</span></span><br><span class="line">  job.setMapperClass(LogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置最终输出类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 设置reduceTask个数为0</span></span><br><span class="line">  job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 5、设置输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 6、提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>MapReduce开发总结<br>编写MapReduce程序时，需要考虑如下方面</p><ul><li><p>输入数据接口：InputFormat</p><ul><li>默认使用的实现类是：TextInputFormat</li><li>TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</li><li>KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key/value。默认分隔符是tab(\t)</li><li>NlineInputFormat按照指定的行数N来划分切片</li><li>CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率</li><li>自定义InputFormat</li></ul></li><li><p>逻辑处理接口：Mapper<br>用户根据业务需求实现其中三个方法：map()、setup()、cleanup()</p></li><li><p>Partitioner分区</p><ul><li>默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号(分区数大于1时)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key.hashCode() &amp; Integer.MAXVALUE % numReduces</span><br></pre></td></tr></table></figure><ul><li>如果业务上有特别的需求，可以自定义分区</li></ul></li><li><p>Comparable排序</p><ul><li>当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法</li><li>部分排序：对最终输出的每一个文件进行内部排序</li><li>全排序：对所有数据进行排序，通常只有一个Reduce</li><li>二次排序：排序的条件有两个</li></ul></li><li><p>Combiner合并<br>Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果</p></li><li><p>Reduce端分组：GroupingComparator<br>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序</p></li><li><p>逻辑处理接口：Reduce<br>用户根据业务需求实现其中三个方法：reduce()、setup()、cleanup()</p></li><li><p>输出数据接口：OutputFormat</p><ul><li>默认实现类是TextOutputFormat，功能逻辑是：将每一个kv对向目标文本文件输出一行</li><li>将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩</li><li>自定义OutputFormat</li></ul></li></ul></li></ul><h2 id="Hadoop数据压缩"><a href="#Hadoop数据压缩" class="headerlink" title="Hadoop数据压缩"></a>Hadoop数据压缩</h2><ul><li><p>概述<br>压缩技术能够有效减少底层存储系统(HDFS)读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要<br>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，<strong>数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩</strong>。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价</p><ul><li>压缩策略和原则<br>压缩是提高Hadoop运行效率的一种优化策略<br>通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度</li></ul><p><strong>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能</strong><br>压缩基本原则</p><ul><li>运算密集型的job，少用压缩</li><li>IO密集型的job，多用压缩</li></ul></li><li><p>MR支持的压缩编码</p></li></ul><table><thead><tr><th>压缩格式</th><th>是否hadoop自带</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th><th>换成压缩格式后,原来程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是,直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样,不需要修改</td></tr><tr><td>Gzip</td><td>是,直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样,不需要修改</td></tr><tr><td>bzip2</td><td>是,直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样,不需要修改</td></tr><tr><td>LZO</td><td>否,需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引,还需要指定输入格式</td></tr><tr><td>Snappy</td><td>否,需要安装</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样,不需要修改</td></tr></tbody></table><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p><table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>Gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较</p><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr><tr><td>Snappy</td><td>8.3GB</td><td>较大</td><td>最快</td><td>最快</td></tr></tbody></table><ul><li><p>压缩方式选择</p><ul><li>Gzip压缩<ul><li>优点<br>压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便</li><li>缺点：不支持Split(切片)</li><li>应用场景</li></ul><strong>当每个文件压缩之后在130M以内的(1个块大小内)，都可以考虑用Gzip压缩格式</strong>。例如说一天或者一个小时的日志压缩成一个Gzip文件</li><li>Bzip2压缩<ul><li>优点：支持Split(切片)；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便</li><li>缺点：压缩/解压速度慢</li><li>应用场景<br>适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split(切片)，而且兼容之前的应用程序的情况</li></ul></li><li>Lzo压缩<ul><li>优点：压缩/解压速度也比较快，合理的压缩率；支持Split(切片)，是Hadoop中最流行的压缩格式之一；可以在Linux系统下安装lzop命令，使用方便</li><li>缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建索引，还需要指定InputFormat为Lzo格式)</li><li>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显</li></ul></li><li>Snappy压缩<ul><li>优点：高速压缩速度和合理的压缩率</li><li>缺点：不支持Split(切片)；压缩率比Gzip要低；Hadoop本身不支持，需要安装</li><li>应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入</li></ul></li></ul></li><li><p>压缩位置选择<br>压缩可以在MapReduce作用的任意阶段启用<br><img src="MapReduce%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE.png" alt="MapReduce压缩位置"></p></li><li><p>压缩参数配置<br>要在Hadoop中启用压缩，可配置如下参数：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs(在core-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress(在mapred-site.xml中配置)</td><td>false</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec(在mapred-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置)</td><td>false</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置)</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置)</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table></li><li><p>压缩实操</p><ul><li>数据流的压缩和解压缩<br>CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据：<ul><li>要想对正在被写入一个输出流的数据进行<strong>压缩</strong>，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流</li><li>相反，要想对从输入流读取而来的数据进行<strong>解压缩</strong>，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据<br>测试如下的压缩方式：<table><thead><tr><th>压缩格式</th><th>编解码类</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr></tbody></table></li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  compress(<span class="string">"/Users/sobxiong/Downloads/test.txt"</span>, <span class="string">"org.apache.hadoop.io.compress.BZip2Codec"</span>);</span><br><span class="line">  decompress(<span class="string">"/Users/sobxiong/Downloads/test.txt.bz2"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 解压缩</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、压缩方式检查</span></span><br><span class="line">  CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line">  CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(filePath));</span><br><span class="line">  <span class="keyword">if</span> (codec == <span class="keyword">null</span>) &#123;</span><br><span class="line">      System.out.println(<span class="string">"Can't process!"</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 2、获取输入流</span></span><br><span class="line">  FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filePath));</span><br><span class="line">  CompressionInputStream cis = codec.createInputStream(fis);</span><br><span class="line">  <span class="comment">// 3、获取输出流</span></span><br><span class="line">  FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filePath + <span class="string">".decode"</span>));</span><br><span class="line">  <span class="comment">// 4、流的对拷</span></span><br><span class="line">  IOUtils.copyBytes(cis, fos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">10</span>, <span class="keyword">false</span>);</span><br><span class="line">  <span class="comment">// 5、关闭资源</span></span><br><span class="line">  IOUtils.closeStream(fos);</span><br><span class="line">  IOUtils.closeStream(cis);</span><br><span class="line">  IOUtils.closeStream(fis);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 压缩</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String filePath, String compressTypeClassName)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取输入流</span></span><br><span class="line">  FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filePath));</span><br><span class="line">  <span class="comment">// 2、获取输出流</span></span><br><span class="line">  Class&lt;?&gt; classCodec = Class.forName(compressTypeClassName);</span><br><span class="line">  CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(classCodec, <span class="keyword">new</span> Configuration());</span><br><span class="line">  FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filePath + codec.getDefaultExtension()));</span><br><span class="line">  CompressionOutputStream cos = codec.createOutputStream(fos);</span><br><span class="line">  <span class="comment">// 3、流的对拷</span></span><br><span class="line">  IOUtils.copyBytes(fis, cos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">10</span>, <span class="keyword">false</span>);</span><br><span class="line">  <span class="comment">// 4、关闭资源</span></span><br><span class="line">  IOUtils.closeStream(cos);</span><br><span class="line">  IOUtils.closeStream(fos);</span><br><span class="line">  IOUtils.closeStream(fis);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Map输出端采用压缩(以万能的WordCount案例为例)<br>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可<br>具体实现(只修改Driver部分代码,Mapper和Reducer不变)：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、获取Job对象</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec<span class="class">.<span class="keyword">class</span>, <span class="title">CompressionCodec</span>.<span class="title">class</span>)</span>;</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 之后代码保持原样</span></span><br></pre></td></tr></table></figure><ul><li>Reduce输出端采用压缩(以万能的WordCount案例为例)<br>具体实现(只修改Driver部分代码,Mapper和Reducer不变)：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 之前代码保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, GzipCodec<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 之后代码保持不变</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="Yarn资源调度器"><a href="#Yarn资源调度器" class="headerlink" title="Yarn资源调度器"></a>Yarn资源调度器</h2><p>Yarn是一个资源调度平台，<strong>负责为运算程序提供服务器运算资源</strong>，相当于一个<strong>分布式的操作系统平台</strong>，而MapReduce等运算程序则相当于<strong>运行于操作系统之上的应用程序</strong></p><ul><li><p>Yarn基本架构<br>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成<br><img src="Yarn%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png" alt="Yarn基本架构"></p></li><li><p>Yarn工作机制</p><ul><li>Yarn工作机制图解<br><img src="Yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="Yarn工作机制"></li><li>工作机制详解<ul><li>MR程序提交到客户端所在的节点</li><li>YarnRunner向ResourceManager申请一个Application</li><li>RM将该应用程序的资源路径返回给YarnRunner</li><li>该程序将运行所需资源提交到HDFS上</li><li>程序资源提交完毕后，申请运行MrAppMaster</li><li>RM将用户的请求初始化成一个Task</li><li>其中一个NodeManager领取到Task任务</li><li>该NodeManager创建容器Container，并产生MrAppmaster</li><li>Container从HDFS上拷贝资源到本地</li><li>MrAppmaster向RM申请运行MapTask的资源</li><li>RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器</li><li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序</li><li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器运行ReduceTask</li><li>ReduceTask向MapTask获取相应分区的数据</li><li>程序运行完毕后，MR会向RM申请注销自己</li></ul></li></ul></li><li><p>作业提交全过程</p><ul><li>作业提交过程之Yarn图解<br><img src="%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BYarn.png" alt="作业提交过程之Yarn"></li><li>作业提交过程之MapReduce图解<br><img src="%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BMapReduce.png" alt="作业提交过程之MapReduce"></li><li>作业提交过程详解<ul><li>作业提交<ul><li>Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业</li><li>Client向RM申请一个作业id</li><li>RM给Client返回该job资源的提交路径和作业id</li><li>Client提交jar包、切片信息和配置文件到指定的资源提交路径</li><li>Client提交完资源后，向RM申请运行MrAppMaster</li></ul></li><li>作业初始化<ul><li>当RM收到Client的请求后，将该job添加到容量调度器中</li><li>某一个空闲的NM领取到该Job</li><li>该NM创建Container，并产生MrAppmaster</li><li>下载Client提交的资源到本地</li></ul></li><li>任务分配<ul><li>MrAppMaster向RM申请运行多个MapTask任务资源</li><li>RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器</li></ul></li><li>任务运行<ul><li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序</li><li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask</li><li>ReduceTask向MapTask获取相应分区的数据</li><li>程序运行完毕后，MR会向RM申请注销自己</li></ul></li><li>进度和状态更新<br>YARN中的任务将其进度和状态(包括counter)返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新，展示给用户</li><li>作业完成<br>除了向应用管理器请求作业进度外，客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后，应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查</li></ul></li></ul></li><li><p>资源调度器<br>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。</p><ul><li>先进先出调度器(FIFO)<br><img src="FIFO%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="FIFO调度器"></li><li>容量调度器(Capacity Scheduler)<br><img src="%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="容量调度器"></li><li>公平调度器(Fair Scheduler)<br><img src="%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="公平调度器"></li></ul><p>Hadoop3.1.3默认的资源调度器是Capacity Scheduler。具体设置详见yarn-default.xml文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>任务的推测执行</p><ul><li><p>作业完成时间取决于最慢的任务完成时间<br>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。<br>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p></li><li><p>推测执行机制<br>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果</p></li><li><p>执行推测任务的前提条件</p><ul><li>每个Task只能有一个备份任务</li><li>当前Job已完成的Task必须不小于0.05(5%)</li><li>开启推测执行参数设置。mapred-site.xml文件中默认是打开的</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>不能启用推测执行机制情况<ul><li>任务间存在严重的负载倾斜</li><li>特殊任务，比如任务向数据库中写数据</li></ul></li><li>算法原理<br><img src="%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86.png" alt="推测执行算法原理"></li></ul></li></ul></li></ul><h2 id="Hadoop企业优化"><a href="#Hadoop企业优化" class="headerlink" title="Hadoop企业优化"></a>Hadoop企业优化</h2><ul><li><p>MapReduce跑的慢的原因<br>MapReduce程序效率的瓶颈在于两点：</p><ul><li>计算机性能：CPU、内存、磁盘健康、网络</li><li>I/O操作优化<ul><li>I/O操作优化</li><li>Map和Reduce数设置不合理</li><li>Map运行时间太长，导致Reduce等待过久</li><li>小文件过多</li><li>大量的不可分块的超大文件</li><li>Spill次数过多</li><li>Merge次数过多等</li></ul></li></ul></li><li><p>MapReduce优化方法<br>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数</p><ul><li><p>数据输入</p><ul><li>合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢</li><li>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景</li></ul></li><li><p>Map阶段</p><ul><li><strong>减少溢写(Spill)次数</strong>：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO</li><li><strong>减少合并(Merge)次数</strong>：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间</li><li>在Map之后，<strong>不影响业务逻辑前提下，先进行Combine处理</strong>，减少I/O</li></ul></li><li><p>Reduce阶段</p><ul><li><strong>合理设置Map和Reduce数</strong>：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误</li><li><strong>设置Map、Reduce共存</strong>：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</li><li><strong>规避使用Reduce</strong>：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗</li><li><strong>合理设置Reduce端的Buffer</strong>：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：<strong>mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整</strong></li></ul></li><li><p>I/O传输</p><ul><li><strong>采用数据压缩的方式</strong>，减少网络IO的的时间。安装Snappy和LZO压缩编码器</li><li><strong>使用SequenceFile二进制文件</strong></li></ul></li><li><p>数据倾斜问题</p><ul><li>数据倾斜现象<ul><li>数据频率倾斜：某一个区域的数据量要远远大于其他区域</li><li>数据大小倾斜：部分记录的大小远远大于平均值</li></ul></li><li>减少数据倾斜的方法<ul><li><strong>抽样和范围分区</strong>：可以通过对原始数据进行抽样得到的结果集来预设分区边界值</li><li><strong>自定义分区</strong>：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例</li><li><strong>Combine</strong>：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据</li><li><strong>采用Map Join，尽量避免Reduce Join</strong></li></ul></li></ul></li><li><p>常用的调优参数</p><ul><li><p>资源相关参数</p><ul><li>以下参数是在用户自己的MR应用程序中配置就可以生效(mapred-default.xml)</li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.memory.mb</td><td>一个MapTask可使用的资源上限(单位:MB)，默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死</td></tr><tr><td>mapreduce.reduce.memory.mb</td><td>一个ReduceTask可使用的资源上限(单位:MB)，默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死</td></tr><tr><td>mapreduce.map.cpu.vcores</td><td>每个MapTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.cpu.vcores</td><td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td></tr><tr><td>mapreduce.reduce.shuffle.parallelcopies</td><td>每个Reduce去Map中取数据的并行数。默认值是5</td></tr><tr><td>mapreduce.reduce.shuffle.merge.percent</td><td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td></tr><tr><td>mapreduce.reduce.shuffle.input.buffer.percent</td><td>Buffer大小占Reduce可用内存的比例。默认值0.7</td></tr><tr><td>mapreduce.reduce.input.buffer.percent</td><td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td></tr></tbody></table><ul><li>应该在YARN启动之前就配置在服务器的配置文件中才能生效(yarn-default.xml)</li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>给应用程序Container分配的最小内存，默认值：1024</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>给应用程序Container分配的最大内存，默认值：8192</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container申请的最小CPU核数，默认值：1</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container申请的最大CPU核数，默认值：32</td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>给Containers分配的最大物理内存，默认值：8192</td></tr></tbody></table><ul><li>Shuffle性能优化的关键参数，应在YARN启动之前就配置好(mapred-default.xml)</li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.task.io.sort.mb</td><td>Shuffle的环形缓冲区大小，默认100m</td></tr><tr><td>mapreduce.map.sort.spill.percent</td><td>环形缓冲区溢出的阈值，默认80%</td></tr></tbody></table></li><li><p>容错相关参数(MapReduce性能优化)</p></li></ul><table><thead><tr><th>配置参数</th><th>参数说明</th></tr></thead><tbody><tr><td>mapreduce.map.maxattempts</td><td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4</td></tr><tr><td>mapreduce.reduce.maxattempts</td><td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4</td></tr><tr><td>mapreduce.task.timeout</td><td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间(单位毫秒)，默认是600000。如果你的程序对每条输入数据的处理时间过长(比如会访问数据库，通过网络拉取数据等)，建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”</td></tr></tbody></table></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
