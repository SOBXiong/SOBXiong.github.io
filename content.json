{"meta":{"title":"SOBXiong的博客","subtitle":"","description":"一只编程菜鸡，对Android、Java后端、大数据和Vue雨露均沾","author":"SOBXiong","url":"https://sobxiong.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-06-23T14:47:17.665Z","updated":"2020-06-23T14:47:17.656Z","comments":true,"path":"404.html","permalink":"https://sobxiong.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2020-06-23T14:45:30.095Z","updated":"2020-06-23T14:45:30.085Z","comments":true,"path":"about/index.html","permalink":"https://sobxiong.github.io/about/index.html","excerpt":"","text":"一只编程菜鸡，对Android、Java后端、大数据和Vue雨露均沾"},{"title":"所有分类","date":"2020-06-23T14:46:03.448Z","updated":"2020-06-23T14:46:03.438Z","comments":true,"path":"categories/index.html","permalink":"https://sobxiong.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-06-23T14:46:49.268Z","updated":"2020-06-23T14:46:49.258Z","comments":true,"path":"tags/index.html","permalink":"https://sobxiong.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hive","slug":"大数据/Hive","date":"2020-07-06T12:57:06.000Z","updated":"2020-07-12T10:32:11.096Z","comments":true,"path":"2020/07/06/大数据/Hive/","link":"","permalink":"https://sobxiong.github.io/2020/07/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/","excerpt":"内容 Hive基本概念 Hive安装 Hive数据类型 DDL数据定义 DML数据操作 查询 函数","text":"内容 Hive基本概念 Hive安装 Hive数据类型 DDL数据定义 DML数据操作 查询 函数 Hive基本概念 什么是Hive 由Facebook开源用于解决海量结构化日志的数据统计 基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能 Hive处理的数据存储在HDFS Hive分析数据底层的实现是MapReduce 执行程序运行在Yarn上 本质是将HQL(Hive Query Language)转化成MapReduce程序 Hive的优缺点 优点： 操作接口采用类SQL语法，提供快速开发的能力(简单、容易上手) 避免了去写MapReduce，减少开发人员的学习成本 Hive的执行延迟比较高，因此Hive常用于数据分析和对实时性要求不高的场合 Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数 缺点： Hive的HQL表达能力有限： 迭代式算法无法表达 数据挖掘方面不擅长 Hive的效率比较低： Hive自动生成的MapReduce作业，通常情况下不够智能化 Hive调优比较困难，粒度较粗 Hive架构原理 用户接口：ClientCLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive) 元数据：Metastore元数据包括：表名、表所属的数据库(默认是default)、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在目录等； 默认存储在自带的derby数据库中(存在bug)，推荐使用MySQL存储Metastore Hadoop：使用HDFS进行存储，使用MapReduce进行计算 驱动器：Driver 解析器(SQL Parser)：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误 编译器(Physical Plan)：将AST编译生成逻辑执行计划 优化器(Query Optimizer)：对逻辑执行计划进行优化 执行器(Execution)：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/SparkHive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将 执行返回的结果输出到用户交互接口 Hive和数据库比较由于Hive采用了类似SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。下面将从多个方面来阐述Hive和数据库的差异。数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性 查询语言由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言 HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发 数据存储位置Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中 数据更新由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的 索引Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于MapReduce的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive不适合在线数据查询 执行Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的。而数据库通常有自己的执行引擎 执行延迟Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小。当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势 可扩展性由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的(世界上最大的Hadoop集群在Yahoo，2009年的规模在4000台节点左右)。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右 数据规模由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小 Hive安装 安装地址 Hive官网地址：http://hive.apache.org 文档查看地址：https://cwiki.apache.org/confluence/display/Hive/GettingStarted 下载地址：http://archive.apache.org/dist/hive github地址：https://github.com/apache/hive Hive安装部署 Hive安装及配置 上传：把apache-hive-3.1.2-bin.tar.gz上传到/opt/software目录下 解压：tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/ 修改目录名：mv apache-hive-3.1.2-bin hive-3.1.2 修改配置文件(conf目录下)： 备份一份配置文件：cp hive-env.sh.template hive-env.sh.template.copy 修改配置文件后缀：mv hive-env.sh.template hive-env.sh 配置hive-env.sh文件(底部加入)： 12export HADOOP_HOME=/opt/module/hadoop-3.1.3export HIVE_CONF_DIR=/opt/module/hive-3.1.2/conf Hadoop集群启动(hadoop1启动hdfs——sbin/start-dfs.sh,hadoop2启动yarn——sbin/start-yarn.sh) Hive基本操作 初始化默认的derby数据库：bin/schematool -dbType derby -initSchema(初始化后在hive根目录会产生derby.log和metastore目录) 启动hive：bin/hive 启动时会发生Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V错误 这是因为hive内依赖的guava和hadoop内的版本不一致 分别查看hive(lib目录下)和hadoop(share/hadoop/common/lib目录下)的guava依赖版本：guava-19.0.jar和guava-27.0-jre.jar 删除hive的低版本guava-19.0.jar，将hadoop的高版本guava-27.0-jre.jar复制到hive的lib目录下 重启启动hive 查看数据库：show databases; 打开默认数据库：use default; 显示default数据库中的表：show tables; 创建一张表(数据类型为java中类型)：create table student(id int,name string); 查看表的结构：desc student; 向表中插入数据：insert into student values(1,”SOBXiong”); 查询表中数据：select * from student; 退出hive：quit; 本地文件导入Hive需求：将本地/opt/module/data/hive/student.txt的数据导入到hive的student表中 数据准备(tab键隔开) 1231 xixi2 haha3 hehe Hive操作 导入student.txt的数据到之前创建的student表中：load data local inpath ‘/opt/module/data/hive/student.txt’ into table student; 查询结果(发现都是NULL NULL,因为格式不对)：select * from student; 删除已创建的student表：drop table student; 创建新的student表(声明文件分隔符’\\t’)：create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’; 重新导入数据并重新查询结果 查看http://hadoop1:9870中的HDFS文件，发现/user/hive/warehouse/student下就有数据 第二种插入数据的方式：直接将文件上传至HDFS服务器 上传本地文件(相当于cp复制)：hadoop fs -put stu1.txt /user/hive/warehouse/student 上传HDFS文件(相当于mv移动)：hadoop fs -put /stu2.txt /user/hive/warehouse/student derby存储元数据的问题(推荐使用mysql)： 只能开启一个hive客户端 在不同的目录开启hive客户端会在当前目录下创建derby.log和metastore文件，相当于数据不共享 MySql安装 安装包准备： 查看yum中历史的mysql或者mariadb的依赖：rpm -qa | grep mysql/mariadb 如有历史依赖，删除：yum remove mysql-libs/mariadb-libs 下载mysql的rpm包：前往https://dev.mysql.com/downloads/mysql/下载5.7.30的Red Hat Enterprise Linux7版本(CentOS7)的RPM Bundle包 安装MySql 解压tar包：tar -xvf mysql-5.7.30-1.el7.x86_64.rpm-bundle.tar 使用rpm命令安装MySql组件 12345# 依赖关系为common→libs→client→serverrpm -ivh commonrpm -ivh libsrpm -ivh clientrpm -ivh server 启动MySql：systemctl start mysqld.service 查看MySql状态：systemctl status mysqld.service 查看初始化的随机密码：grep ‘temporary password’ /var/log/mysqld.log 登录MySql：mysql -u root -p 修改密码校验策略(不然设置新密码会提示密码错误)：set global validate_password_policy=0; 修改密码：alter user root@localhost identified by ‘your password’; 授权root用户远程访问权限 12grant all privileges on *.* to 'root' @'%' identified by 'your password';flush privileges; 设置MySql完毕，退出：quit; Hive元数据配置到MySql 拷贝mysql-connector JDBC驱动文件 前往https://dev.mysql.com/downloads/connector/j/下载驱动文件5.1.49版本 解压文件mysql-connector-java-5.1.49.tar.gz，拷贝mysql-connector-java-5.1.49-bin.jar到hive的lib目录下 配置metastore到MySql 在conf目录下创建hive-site.xml配置文件：touch hive-site.xml 修改配置文件： 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- xml下的&amp;需要转义为&amp;amp; --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop1:3306/metastore?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;your password&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 初始化Hive的MySql元数据数据库：bin/schematool -dbType mysql -initSchema 启动hive，MySql中新增了metastore数据库(表DBS和TBS比较重要) HiveJDBC访问 停止hadoop： 1234# hadoop1stop-dfs.sh# hadoop2stop-yarn.sh 修改hadoop配置： 12345678910111213141516171819202122&lt;!-- hdfs-site.xml 启用webhdfs --&gt;&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- core-site.xml 设置hadoop的代理用户 hadoop.proxyuser.xxx.hosts xxx是操作的用户 org.apache.hadoop.security.authorize.AuthorizationException: User: sobxiong is not allowed to impersonate root(state=08S01,code=0) User:xxx即为下面该填入的用户--&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.sobxiong.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.sobxiong.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 修改hive配置： 1234567891011&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;description&gt;Bind host on which to run the HiveServer2 Thrift service.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;11000&lt;/value&gt; &lt;description&gt;Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.&lt;/description&gt;&lt;/property&gt; 启动hiveserver2服务：bin/hiveserver2 启动beeline：bin/beeline 连接hiveserver2： 1234beeline&gt; !connect jdbc:hive2://hadoop1:11000Enter username for jdbc:hive2://hadoop102:10000: sobxiongEnter password for jdbc:hive2://hadoop102:10000: your password(数据库的密码)# 接下来的操作就跟Hive Cli使用类似SQL Hive常用交互命令 -e &lt;quoted-query-string&gt;：不进入hive的交互窗口执行sql语句，例如：bin/hive -e “select * from student;” -f &lt;filename&gt;：执行脚本中sql语句 结果打印在terminal上：bin/hive -f /opt/module/data/hive/hive.hql 结果打印在指定文件中：bin/hive -f /opt/module/data/hive/hive.hql &gt; ./hive_result.txt Hive其他命令操作 在Hive Cli命令窗口中查看hdfs文件系统：dfs -ls / 在Hive Cli命令窗口中查看本地文件系统：! ls / 查看在hive中输入的所有历史命令：cat ~/.hivehistory Hive常见属性配置 Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse 在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹 修改default数据仓库原始位置(hive-default.xml.template -&gt; hive-site.xml) 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 查询后信息显示配置 在hive-site.xml加入如下配置： 12345678910&lt;!-- 表列名显示 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 当前使用数据库显示 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 重启Hive Cli Hive运行日志信息配置 Hive的日志信息默认存放在/tmp/{current_user}目录下 修改Hive的日志信息存放在hive安装目录的logs文件夹下修改conf/hive-log4j.properties配置文件(hive-log4j.properties.template -&gt; hive-log4j.properties)：hive.log.dir=/opt/module/hive-3.1.2/logs 参数配置方式 查看当前所有的配置信息(Hive Cli命令窗口下)：set; 参数配置的三种方式 配置文件方式默认配置文件：hive-default.xml用户自定义配置文件：hive-site.xml注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效 命令行参数方式启动Hive时，可以在命令行添加-hiveconf param=value来设定参数例如：bin/hive -hiveconf mapred.reduce.tasks=10;(注意：仅对本次hive启动有效)查看参数设置：set mapred.reduce.tasks; 参数声明方式在HQL中使用SET关键字设定参数(注意：仅对本次hive启动有效)例如：set mapred.reduce.tasks=100;上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了 Hive数据类型 基本数据类型(Hive数据类型大小写不敏感) Hive数据类型 Java数据类型 长度 TINYINT byte 1byte有符号整数 SMALLINT short 2byte有符号整数 INT int 4byte有符号整数 BIGINT long 8byte有符号整数 BOOLEAN boolean 布尔类型，true或false FLOAT float 单精度浮点数 DOUBLE double 双精度浮点数 STRING string 字符系列，可以指定字符集，可以使用单引号或者双引号 TIMESTAMP - 时间类型 BINARY - 字节数组 Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过他不能声明其最多能存储多少个字符，理论上它可以存储2GB的字符数 集合数据类型 数据类型 描述 语法示例 STRUCT 和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用 struct() MAP MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取键last对应的值数据 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用 Array() Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套 集合数据类型案例实操 假设JSON为原始数据，具体如下： 123456789101112&#123; \"name\": \"songsong\", \"friends\": [\"bingbing\" , \"lili\"], \"children\": &#123; \"xiao song\": 18 , \"xiaoxiao song\": 19 &#125;, \"address\":&#123; \"street\": \"hui long guan\" , \"city\": \"beijing\" &#125;&#125; 基于上述数据结构，建立本地测试文件test.txt，具体格式如下： 12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 注意：MAP、STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用’_’ Hive上创建测试表test 1234567891011121314create table test(name string,friends array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)/* 设置列分隔符为',' */row format delimited fields terminated by ','/* 设置map、struct和array的分隔符(数据分割符号)为'_' */collection items terminated by '_'/* 设置map中的key/value的分隔符为',' */map keys terminated by ':'/* 设置行分隔符为'\\n'(也是默认值) */lines terminated by '\\n'; 导入文本数据到测试表中 1load data local inpath '/opt/module/data/hive/test.txt' into table test; 访问三种集合列里的数据 1select friends[1],children['xiao song'],address.city from test; 类型转换Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化。例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作 隐式类型转换规则 任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT 所有整数类型、FLOAT和STRING(符合数字)类型都可以隐式地转换成DOUBLE TINYINT、SMALLINT、INT都可以转换为FLOAT BOOLEAN类型不可以转换为任何其它的类型 使用CAST操作显示进行数据类型转换例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值NULL DDL数据定义 创建数据库 创建一个数据库，默认在HDFS上的存储路径式/user/hive/warehouse/*.db：create database if not exists db_hive;(if not exists避免要创建的数据库已存在) 创建一个数据库，指定在HDFS上存放的路径 1create database db_hive2 location '/db_hive2.db' 查询数据库 显示数据库 显示数据库：show databases; 过滤查询显示的数据库 1show databases like 'db_hive'; 查看数据库 显示数据库信息：desc database db_hive; 显示数据库详细信息(extended)：desc database extended db_hive; 切换当前数据库：use db_hive; 修改数据库用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置修改数据库属性值： 1alter database db_hive set dbproperties('createtime'='20200708') 查看修改结果：desc database extended db_hive; 删除数据库 删除空数据库：drop database if exists db_hive;(if exists避免要删除的数据库不存在) 如果数据库中表不为空，可以采用cascade命令集联强制删除：drop database db_hive cascade; 创建表 建表语法 123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)][CLUSTERED BY (col_name, col_name, ...)[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS][ROW FORMAT row_format][STORED AS file_format][LOCATION hdfs_path] 字段解释说明 CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径(LOCATION)，Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据 COMMENT：为表和列添加注释 PARTITIONED BY创建分区表 CLUSTERED BY创建分桶表 SORTED BY不常用 ROW FORMATDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIE (property_name=property_value, property_name=property_value, …)]用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化 STORED AS指定存储文件类型常用的存储文件类型：SEQUENCEFILE(二进制序列文件)、TEXTFILE(文本)、RCFILE(列式存储格式文件)如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE LOCATION：指定表在HDFS上的存储位置 LIKE允许用户复制现有的表结构，但是不复制数据 管理表 介绍默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会(或多或少地)控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据 实际操作 创建普通表 123456create table if not exists student(id int, name string)row format delimited fields terminated by '\\t'stored as textfilelocation '/user/hive/warehouse/student2'; 根据查询结果创建表(查询的结果会添加到新创建的表中)：create table if not exists student2 as select id, name from student; 根据已存在的表结构创建表：create table if not exists student3 like student; 查询表的类型：desc formatted student; 外部表 介绍：因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉 实际操作(创建表,其余操作与管理表类似) 123456create external table if not exists default.dept(deptno int,dname string,loc int)row format delimited fields terminated by '\\t'; 管理表与外部表 相互转换 12345-- 修改内部表为外部表：alter table student2 set tblproperties('EXTERNAL'='TRUE');-- 修改外部表为内部表：alter table student2 set tblproperties('EXTERNAL'='FALSE');-- 注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写 使用场景每天将收集到的网站日志定期流入HDFS文本文件。在外部表(原始日志表)的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表 分区表分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多 分区表基本操作 创建分区表语法 12345create table dept_partition(deptno int, dname string, loc string)partitioned by (month string)row format delimited fields terminated by '\\t'; 加载数据到分区表中 123hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201709');hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201708');hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201707’); 查询分区表中数据 单分区查询 1select * from dept_partition where month='201709'; 多分区联合查询 12345678-- 查询多个分区select * from dept_partition where month='201709'unionselect * from dept_partition where month='201708'unionselect * from dept_partition where month='201707';-- 查询全部select * from dept_partition; 增加分区 创建单个分区 1alter table dept_partition add partition(month='201706'); 同时创建多个分区 1alter table dept_partition add partition(month='201705') partition(month='201704'); 删除分区： 删除单个分区 1alter table dept_partition drop partition (month='201704'); 同时删除多个分区 1alter table dept_partition drop partition (month='201705'), partition (month='201706'); 查看分区表有多少分区：show partitions dept_partition; 查看分区表结构：desc formatted dept_partition; 分区表扩展用法 创建二级分区表 12345create table dept_partition2(deptno int, dname string, loc string)partitioned by (month string, day string)row format delimited fields terminated by '\\t'; 加载二级分区数据 1load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709', day='13'); 查询二级分区数据 1select * from dept_partition2 where month='201709' and day='13'; 将数据上传到分区目录后，让分区表和数据产生关联的方式 上传数据后修复(适用于数据较多的情况) 上传数据： 123# Hive Cli命令环境下dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;dfs -put /opt/module/data/hive/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; 查询数据(查询不到) 1select * from dept_partition2 where month='201709' and day='12'; 执行修复命令：msck repair table dept_partition2; 上传数据后添加分区 上传数据(同上) 执行添加分区 1alter table dept_partition2 add partition(month='201709',day='11'); 创建文件夹后load数据到分区 创建目录：dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; 上传数据 1load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709',day='10'); 修改表 重命名表 语法：ALTER TABLE table_name RENAME TO new_table_name 实例：alter table dept_partition2 rename to dept_partition3; 添加、修改和删除表分区(同上) 增加、修改、替换列信息 语法： 更新列：ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 添加和替换列：ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …) 注意：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段 实操 查询表结构(用于查看修改结果)：desc dept_partition; 添加列：alter table dept_partition add columns(deptdesc string); 更新列：alter table dept_partition change column deptdesc desc int;(貌似需要符合隐式转换规则) 替换列：alter table dept_partition replace columns(deptno string, dname string, loc string); 删除表：drop table dept_partition; DML数据操作 数据导入 向表中装载数据(Load) 语法 1load data [local] inpath 'path_name' [overwrite] into table table_name [partition (partcol1=val1,...)]; 参数解释 load data：表示加载数据 local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表 inpath：表示加载数据的路径 overwrite：表示覆盖表中已有数据，否则表示追加 into table：表示加载到哪张表 table_name：表示具体的表 partition：表示上传到指定分区 实际操作 加载本地文件到Hive： 1load data local inpath '/opt/module/data/hive/student.txt' into table default.student; 加载(覆盖)HDFS文件数据到Hive中 123# Hive Cli命令环境下dfs -put /opt/module/data/hive/student.txt /user/sobxiong/hive;load data inpath '/user/sobxiong/hive/student.txt' (overwrite)into table default.student; 通过查询语句向表中插入数据(Insert) 基本插入数据 1insert into table student partition(month='201709') values(1,'wangwu'); 根据单表查询结果插入数据 12insert overwrite table student partition(month='201708')select id, name from student where month='201709'; 根据多表查询结果插入数据 12345from studentinsert overwrite table student partition(month='201707')select id, name where month='201709'insert overwrite table student partition(month='201706')select id, name where month='201709'; 查询语句中创建表并加载数据(As Select,查询的结果会添加到新创建的表中)：create table if not exists student3 as select id, name from student; 创建表时通过Location指定加载数据路径 指定在HDFS上的位置创建表 12345create table if not exists student5(id int, name string)row format delimited fields terminated by '\\t'location '/user/hive/warehouse/student5'; 上传数据到HDFS上 查询数据 Import数据到指定Hive表中(注意：先用export导出后,才能导入) 1import table student2 partition(month='201709') from '/user/hive/warehouse/export/student'; 数据导出 Insert导出 将查询的结果导出到本地 1insert overwrite local directory '/opt/module/data/hive/export/student' select * from student; 将查询的结果格式化导出到本地 1insert overwrite local directory '/opt/module/data/hive/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' select * from student; 将查询结果导出到HDFS上(取消local) Hadoop命令导出到本地(Hive Cli命令环境下)：dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/data/hive/export/student3.txt; Hive Shell命令导出：基本语法(hive -f/-e 执行语句或脚本 &gt; file_name) 1bin/hive -e 'select * from default.student;' &gt; /opt/module/data/hive/export/student4.txt; Export导出到HDFS上(导出数据包括表数据和元数据) 1export table default.student to '/user/hive/warehouse/export/student'; Sqoop导出(敬请期待) 清除表中数据(注意：只能删除管理表,不能删除外部表中数据)：truncate table student; 查询 查询语句语法官方wiki文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select 12345678910[WITH CommonTableExpression (, CommonTableExpression)*]SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [ORDER BY col_list] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] ] [LIMIT number] 基本查询 全表和特定列查询注意： SQL大小写不敏感 SQL可以写在一行/多行 关键字不能被缩写也不能分行 各子句一般要分行写 使用缩进提高语句的可读性 列别名 重命名一个列 便于计算 紧跟列名(或在列名和别名之间加入关键字AS) 算术运算符 +、-、*、/、%、&amp;、|、^、- 常用函数 求总行数：count() 求最大值/最小值：max()/min() 求总和：sum() 求平均值：avg() limit语句：典型的查询会返回多行数据。LIMIT子句用于限制返回的行数 Where语句：使用Where子句可以过滤掉不满足条件的行，需要紧跟From子句 比较运算符(同样可以用于Join… on和Having语句)以下只介绍除=、&gt;和&lt;等简单的运算符 操作符 支持的数据类型 描述 A&lt;=&gt;B 基本数据类型 如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL A&lt;&gt;B, A!=B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE A [NOT] BETWEEN B AND C 基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果 A IS [NOT] NULL 所有数据类型 如果A(不)等于NULL，则返回TRUE(FALSE)，反之返回FALSE(TRUE) [NOT] IN(数值1, 数值2) 所有数据类型 (不)使用IN运算显示列表中的值 A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果 A RLIKE B, A REGEXP B STRING 类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配 Like和RLike 使用LIKE运算选择类似的值 选择条件可以包含字符或数字： % 代表零个或多个字符(任意个字符) _ 代表一个字符 RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件 案例 查找以2开头薪水的员工信息 1select * from emp where sal LIKE '2%'; 查找第二个数值为2的薪水的员工信息 1select * from emp where sal LIKE '_2%'; 查找薪水中含有2的员工信息 1select * from emp where String(sal) RLIKE '[2]'; 逻辑运算符(And/Or/Not) 分组 Group By语句GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作 Having语句与where不同点： where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据 where后面不能写分组函数，而having后面可以使用分组函数 having只用于group by分组统计语句 Join语句 等值Join 12select e.empno, e.ename, d.deptno, d.dname from emp e join dept don e.deptno = d.deptno; 表的别名好处：(1)简化查询；(2)有限地提高执行效率 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来 12select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno= d.deptno; 左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回 12select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno= d.deptno; 右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回 12select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno= d.deptno; 满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代 12select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno= d.deptno; 多表连接：连接n个表，一般至少需要n-1个连接条件 123456SELECT e.ename, d.deptno, l. loc_nameFROM emp eJOIN dept dON d.deptno = e.deptnoJOIN location lON d.loc = l.loc; 多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的 笛卡尔积一般会在下面情况下出现： 省略连接条件 连接条件无效 所有表中的所有行互相连接一般会设置禁止出现笛卡尔积，如果有特殊情况，需要在单独在命令执行前设置一次性环境 连接谓词在新版中支持or 123-- 无实际意义(ename = dname),只做可行性试验select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno= d.deptno or e.ename=d.dname; 排序 全局排序(Order By:一个Reducer) 排序方式：ASC(ascend升序,默认)、DESC(descend降序) Order By子句在Select语句的结尾 多个列排序(同MySQL) 内部排序(Sort By:每个Reduce内部进行排序,对全局结果集来说不是排序) 注意：如果使用sort by不使用distribute by(即没有指定分区字段)，那么就采用一种产生随机数的函数分配分区(避免数据倾斜) 设置reduce数：set mapreduce.job.reduce=3; 按照部门编号降序排序： 123-- 三个结果文件insert overwrite local directory '/opt/module/data/hive/sortby-result'select * from emp sort by deptno desc; 分区排序(Distribute By:类似MR中partition,进行分区,结合sort by使用) DISTRIBUTE BY语句要写在SORT BY语句之前。distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果 设置reduce数：set mapreduce.job.reduces=3; 先按照部门编号分区，再按照员工编号降序排序 1insert overwrite local directory '/opt/module/data/hive/distribute-result' select * from emp distribute by deptno sort by empno desc; Clubster By 当distribute by和sorts by字段相同时，可以使用cluster by方式。 cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC 具体实操 123-- 以下两种写法等价select * from emp cluster by deptno;select * from emp distribute by deptno sort by deptno; 注意：按照部门编号分区，不一定就是固定死的数值(要看具体数据表中的字段不同的数目以及reduce设置的数目)，可以是20号和30号部门分到一个分区里面去 分桶及抽样查询 分桶表数据存储 介绍：分区针对的是数据的存储路径；分桶针对的是数据文件。分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。分桶是将数据集分解成更容易管理的若干部分的另一个技术 通过导入数据文件方式创建分桶表 创建表 1234create table stu_buck(id int, name string)clustered by(id)into 4 bucketsrow format delimited fields terminated by '\\t'; 查看表结构：desc formatted stu_back;(Num Buckets: 4) 导入数据到分桶表 1load data local inpath '/opt/module/data/hive/student.txt' into table stu_buck; 在浏览器上查看创建的分桶表是否分成4个桶(4个文件)：在新版本中分成四个桶 通过子查询导入数据方式创建分桶表 先创建普通的stu表 1create table stu(id int, name string) row format delimited fields terminated by '\\t'; 向普通stu表中导入数据 1load data local inpath '/opt/module/data/hive/student.txt' into table stu; 清空stu_buck表中数据：truncate table stu_buck; 子查询方式导入数据到分桶表：insert into table stu_buck select id, name from stu; 浏览器查看：新版本中有4个分桶(文件) 需要设置Hive的属性(老版本) 1234567# 设置启用分桶set hive.enforce.bucketing=true;# 设置reduce数目为-1,会自动使用分桶数作为reduce的数目set mapreduce.job.reduces=-1;# 清空分桶表,重新导入数据,再去查看浏览器中的分桶truncate table stu_back;insert into table stu_buck select id, name from stu; 分桶抽样查询 介绍：对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求 实操：select * from stu_buck tablesample(bucket 1 out of 4 on id); 语法：tablesample是抽样语句，语法：tablesample(bucket x out of y) 参数解释 y：y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2)2个bucket的数据，当y=8时，抽取(4/8)1/2个bucket的数据 x：x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取(4/2)2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据 注意：x的值必须小于等于y的值 其他常用查询函数 空字段赋值 函数说明：NVL：给值为NULL的数据赋值，它的格式是NVL(string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL，则返回NULL(replace_with可以是常量也可以是同表的另一个列) 查询(常量)：select nvl(comm,-1) from emp; 查询(另一列)：select nvl(comm,mgr) from emp; 时间类 date_format(格式化时间,第一个变量时间串只能是以’-‘分割) 123select date_format('2020-07-11','yyyy:MM:dd');-- 时间不以'-'分割,可以通过regex正则表达式替换/自定义函数select regexp_replace('2020/07/11','/','-'); date_add/sub(时间跟天数相加/相减) 1select date_add('2020-07-11',-5/5); datediff(时间相差的间隔,前者-后者) 1select datediff('2020-07-11','2020-07-08'); CASE WHEN 数据准备 123456悟空 A 男大海 A 男宋宋 B 男凤姐 A 女婷姐 B 女婷婷 B 女 需求：求出不同部门的男女各多少人 创建emp_set.txt，复制数据 创建Hive表并导入数据 12345678-- 建表create table emp_sex(name string,dept_id string,sex string)row format delimited fields terminated by \"\\t\";-- 导入本地数据load data local inpath '/opt/module/data/hive/emp_sex.txt' into table emp_sex; 查询数据 12345678select dept_id, sum(case sex when '男' then 1 else 0 end) male_count, sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id; 行转列 相关函数说明CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段 数据准备 12345孙悟空 白羊座 A大海 射手座 A宋宋 白羊座 B猪八戒 白羊座 A凤姐 射手座 A 需求：把星座和血型一样的人归类到一起。结果如下 person_info.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table person_info(name string,constellation string,blood_type string)row format delimited fields terminated by \"\\t\";-- 导入数据load data local inpath '/opt/module/data/hive/person_info.txt' into table person_info; 查询数据 123456789101112131415161718192021222324252627-- 总查询语句select t1.constellation_blood_type, concat_ws('|', collect_set(t1.name)) namefrom (select name, concat(constellation, \",\", blood_type) constellation_blood_type from person_info ) t1group by t1.constellation_blood_type;-- 第一步,查询出'射手座,A' '大海'select concat(constellation, \",\", blood_type) constellation_blood_type, namefrom person_info;-- 第二步,连接相同星座和血型的nameselect constellation_blood_type, concat_ws('|', collect_set(t1.name)) namefrom t1group by t1.constellation_blood_type;-- 最后一步,替换from后的t1为第一步中的临时表 列转行 函数说明EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行LateRal View： 用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias 解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合 数据准备 123《疑犯追踪》 悬疑,动作,科幻,剧情《Lie to me》 悬疑,警匪,动作,心理,剧情《战狼2》 战争,动作,灾难 需求：将电影分类中的数组数据展开，结果如下所示 123456789101112《疑犯追踪》 悬疑《疑犯追踪》 动作《疑犯追踪》 科幻《疑犯追踪》 剧情《Lie to me》 悬疑《Lie to me》 警匪《Lie to me》 动作《Lie to me》 心理《Lie to me》 剧情《战狼2》 战争《战狼2》 动作《战狼2》 灾难 创建本地movie.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table movie_info(movie string,category array&lt;string&gt;)row format delimited fields terminated by \"\\t\"collection items terminated by \",\";-- 导入数据load data local inpath \"/opt/module/data/hive/movie.txt\" into table movie_info; 按需查询数据 12345select movie, category_namefrom movie_info lateral view explode(category) table_tmp as category_name; 窗口函数 函数说明OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化 参数说明 Over()内 CURRENT ROW：当前行 n PRECEDING：往前n行数据 n FOLLOWING：往后n行数据 UNBOUNDED：起点，UNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点 Over()外 LAG(col,n)：往前第n行数据 LEAD(col,n)：往后第n行数据 NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型 数据准备 123456789101112131415&#x2F;&#x2F; name,orderdate,costjack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94 需求 查询在2017年4月份购买过的顾客及总人数 查询顾客的购买明细及月购买总额 上述的场景，要将cost按照日期进行累加 查询顾客上次的购买时间 查询前20%时间的订单信息 创建本地business.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table business(name string,orderdate string,cost int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';-- 导入数据load data local inpath \"/opt/module/data/hive/business.txt\" into table business; 按需查询数据 查询在2017年4月份购买过的顾客及总人数 1234select name,count(*) over()from businesswhere substring(orderdate,1,7) = '2017-04'group by name; 查询顾客的购买明细及月购买总额 12select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) frombusiness; 上述的场景,要将cost按照日期进行累加 12345678910-- partition by ... order by和distribute by ... sort by效果相同,可替换select name,orderdate,cost, sum(cost) over() as sample1,--所有行相加 sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行from business; 查看顾客上次的购买时间 1select name,orderdate,cost,lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate) as last_time from business; 查询前20%时间的订单信息 12345select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) ntile_id from business) bwhere ntile_id = 1; Rank排名函数 函数说明： Rank()：排序相同时会重复，总数不会变 DENSE_RANK()：排序相同时会重复，总数会减少 ROW_NUMBER()：会根据顺序计算 数据准备 12345678910111213&#x2F;&#x2F; name subject score孙悟空 语文 87孙悟空 数学 95孙悟空 英语 68大海 语文 94大海 数学 56大海 英语 84宋宋 语文 64宋宋 数学 86宋宋 英语 84婷婷 语文 65婷婷 数学 85婷婷 英语 78 需求：计算各学科成绩排名 创建本地score.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table score(name string,subject string,score int)row format delimited fields terminated by \"\\t\";-- 导入数据load data local inpath '/opt/module/data/hive/score.txt' into table score; 按需查询 12345678select name, subject, score, rank() over(partition by subject order by score desc) rank, dense_rank() over(partition by subject order by score desc) dense_rank, row_number() over(partition by subject order by score desc) row_numberfrom score; 函数","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://sobxiong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hive","slug":"Hive","permalink":"https://sobxiong.github.io/tags/Hive/"}]},{"title":"MFC","slug":"MFC","date":"2020-07-04T03:07:21.000Z","updated":"2020-07-04T07:31:15.768Z","comments":true,"path":"2020/07/04/MFC/","link":"","permalink":"https://sobxiong.github.io/2020/07/04/MFC/","excerpt":"内容 MFC入门 基于对话框编程 常用控件 综合案例","text":"内容 MFC入门 基于对话框编程 常用控件 综合案例 MFC入门 MFC介绍：MFC用于在Windows平台上做GUI开发，MFC是微软基础类库的缩写 为什么要学MFC：向现实低头 Windows消息机制 基本概念解释我们在编写标准C程序的时候,经常会调用各种库函数来辅助完成某些功能。初学者使用得最多的C库函数就是printf了，这些库函数是由你所使用的编译器厂商提供的。在Windows平台下，也有类似的函数可供调用：不同的是，这些函数是由Windows操作系统本身提供的 SDK软件开发工具包(Software Development Kit)，一般都是一些被软件工程师用于为特定的软件包、软件框架、硬件平台、操作系统等建立应用软件的开发工具的集合 API函数提供给应用程序编程的接口(Application Programming Interface)Windows应用程序API函数是通过C语言实现的，所有主要的Windows函数都在Windows.h头文件中进行了声明 窗口 窗口是Windows应用程序中一个非常重要的元素，一个Windows应用程序至少要有一个窗口，称为主窗口 窗口是屏幕上的一块矩形区域，是Windows应用程序与用户进行交互的接口。利用窗口可以接收用户的输入、以及显示输出 一个应用程序窗口通常都包含标题栏、菜单栏、系统菜单、最小化框、最大化框、 可调边框，有的还有滚动条。典型的窗口如下图所示： 窗口可以分为客户区和非客户区，如上图。客户区是窗口的一部分，应用程序通常在客户区中显示文字或者绘制图形 标题栏、菜单栏、系统菜单、最小化框和最大化框、可调边框统称为窗口的非客户区，它们由Windows系统来管理，而应用程序则主要管理客户区的外观及操作 窗口可以有一个父窗口，有父窗口的窗口称为子窗口。除了上图所示类型的窗口外，对话框和消息框也是一种窗口。在对话框上通常还包含许多子窗口，这些子窗口的形式有按钮、单选按钮、复选框、组框、文本编辑框等 在Windows应用程序中，窗口是通过窗口句柄(HWND)来标识的。我们要对某个窗口进行操作，首先就要得到这个窗口的句柄 句柄句柄(HANDLE)是Windows程序中一个重要的概念，使用也非常频繁。在Windows程序中，有各种各样的资源(窗口、图标、光标、画刷等)，系统在创建这些资源时会为它们分配内存，并返回标识这些资源的标识号，即句柄。在后面的内容中我们还会看到图标句柄(HICON)、光标句柄(HCURSOR)和画刷句柄(HBRUSH) 消息与消息队列 Windows程序设计是一种完全不同于传统的DOS方式的程序设计方法。它是一种事件驱动方式的程序设计模式，主要是基于消息的(类似Android的Handle机制?) 每一个Windows应用程序开始执行后，系统都会为该程序创建一个消息队列，这个消息队列用来存放该程序创建的窗口的消息 例如，当用户在窗口中画图的时候，按下鼠标左键，此时，操作系统会感知到这一事件，于是将这个事件包装成一个消息，投递到应用程序的消息队列中，等待应用程序的处理 然后应用程序通过一个消息循环不断地从消息队列中取出消息，并进行响应 在这个处理过程中，操作系统也会给应用程序“发送消息”。所谓“发送消息”，实际上是操作系统调用程序中一个专门负责处理消息的函数，这个函数称为窗口过程 WinMain函数当Windows操作系统启动一个程序时，它调用的就是该程序的WinMain函数(实际是由插入到可执行文件中的启动代码调用的)。WinMain是Windows程序的入口点函数，与DOS程序的入口点函数main的作用相同，当WinMain函数结束或返回时，Windows应用程序结束 Windows编程模型一个完整的Win32程序(#include &lt;windows.h&gt;)实现的功能是创建一个窗口，并在该窗口中响应键盘及鼠标消息，程序的实现步骤为： WinMain函数的定义 创建一个窗口 进行消息循环 编写窗口过程函数具体案例实操(基于VS2017)： 创建项目：文件 -&gt; 新建 -&gt; 项目 -&gt; Visual C++ -&gt; 空项目 创建主函数文件：解决方案资源管理器 -&gt; 源文件 -&gt; 添加 -&gt; 新建项 -&gt; Vistual C++ -&gt; C++文件(.cpp) -&gt; 在名称中打入xxx.c(使用C语言编写) WinMain函数介绍 123456789101112131415161718192021222324// 程序入口函数// WINAPI 代表__stdcall 参数的传递顺序：从右到左依次入栈,并且在函数返回前清空堆栈int WINAPI WinMain( // 应用程序实例句柄 HINSTANCE hInstance, // 上一个应用程序句柄,在win32环境下,参数一般为NULL,不起作用了 HINSTANCE hPrevInstance, // char * argv[],命令行参数 LPSTR lpCmdLine, // 显示命令 最大化、最小化、正常 int nShowCmd)/*具体解释：WINAPI：是一个宏，它代表的是__stdcall(两个下划线)，表示的是参数传递的顺序：从右往左入栈，同时在函数返回前自动清空堆栈hInstance：表示该程序当前运行的实例的句柄，这是一个数值。当程序在Windows下运行时，它唯一标识运行中的实例(注意，只有运行中的程序实例，才有实例句柄)。一个应用程序可以运行多个实例，每运行一个实例，系统都会给该实例分配一个句柄值，并通过hInstance参数传递给WinMain函数hPrevInstance：表示当前实例的前一个实例的句柄。在Win32环境下，这个参数总是NULL，即在Win32环境下，这个参数不再起作用lpCmdLine：是一个以空终止的字符串，指定传递给应用程序的命令行参数，相当于C或C++中的main函数中的参数char *argv[]nShowCmd：表示一个窗口的显示，表示它是要最大化显示、最小化显示、正常大小显示还是隐藏显示*/ 创建一个窗口创建一个完整的窗口，需要经过下面几个步骤： 设计一个窗口类一个完整的窗口具有许多特征，包括光标(鼠标进入该窗口时的形状)、图标、背景色等在创建一个窗口前，要对该类型的窗口进行设计，指定窗口的特征。在Windows中，窗口的特征就是由WNDCLASS结构体来定义的，我们只需给WNDCLASS结构体对应的成员赋值，即可完成窗口类的设计以下是WNDCLASS结构体相应成员变量的解释 style：指定窗口的样式(风格)，常用的样式如下 类型 含义 CS_HREDRAW 当窗口水平方向上的宽度发生变化时，将重新绘制整个窗口。当窗口发生重绘时，窗口中的文字和图形将被擦除。如果没有指定这一样式，那么在水平方向上调整窗口宽度时，将不会重绘窗口 CS_VREDRAW 当窗口垂直方向上的高度发生变化时，将重新绘制整个窗口。如果没有指定这一样式，那么在垂直方向上调整窗口高度时，将不会重绘窗口 CS_NOCLOSE 禁用系统菜单的Close命令，这将导致窗口没有关闭按钮 CS_DBLCLKS 当用户在窗口中双击鼠标时，向窗口过程发送鼠标双击消息 12345678910111213// WNDCLASS结构体typedef struct _WNDCLASS&#123; UINT style; WNDPROC lpfnWndProc; int cbClsExtra; int cbWndExtra; HINSTANCE hInstance; HICON hIcon; HCURSOR hCursor; HBRUSH hbrBackground; LPCWSTR lpszMenuName; LPCWSTR lpszClassName;&#125; WNDCLASS; 注册窗口类 创建窗口","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"MFC","slug":"MFC","permalink":"https://sobxiong.github.io/tags/MFC/"}]},{"title":"动态规划","slug":"数据结构/LeetCode/动态规划","date":"2020-06-26T15:10:58.000Z","updated":"2020-06-26T15:20:49.550Z","comments":true,"path":"2020/06/26/数据结构/LeetCode/动态规划/","link":"","permalink":"https://sobxiong.github.io/2020/06/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","excerpt":"内容 背景与介绍 10-正则表达式匹配 139-单词拆分","text":"内容 背景与介绍 10-正则表达式匹配 139-单词拆分 背景与介绍 背景：经常在题目中get不到这道题是要用动态规划求解，而且当意会是动态规划题目后也经常很难一下子得出状态转移方程等解题要素… 介绍：动态规划算法与分治法类似，其基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。若用分治法来解这类问题，则分解得到的子问题数目太多，有些子问题被重复计算了很多次。如果我们能够保存已解决的子问题的答案，而在需要时再找出已求得的答案，这样就可以避免大量的重复计算，节省时间。我们可以用一个表来记录所有已解的子问题的答案。不管该子问题以后是否被用到，只要它被计算过，就将其结果填入表中。这就是动态规划法的基本思路。具体的动态规划算法多种多样，但它们具有相同的填表格式。 10-正则表达式匹配 题目描述： 139-单词拆分 题目描述：","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://sobxiong.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"}]},{"title":"双指针","slug":"数据结构/LeetCode/双指针","date":"2020-06-26T15:02:31.000Z","updated":"2020-06-26T15:19:49.369Z","comments":true,"path":"2020/06/26/数据结构/LeetCode/双指针/","link":"","permalink":"https://sobxiong.github.io/2020/06/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/%E5%8F%8C%E6%8C%87%E9%92%88/","excerpt":"内容 背景与介绍 16-最接近的三数之和","text":"内容 背景与介绍 16-最接近的三数之和 背景与介绍 背景：因为写的时候想不到，以及不会做 介绍：双指针法有时也叫快慢指针，在数组里是用两个整型值代表下标，在链表里是两个指针，一般能实现O（n）的时间解决问题，两个指针的位置一般在第一个元素和第二个元素或者第一个元素和最后一个元素，快指针在前“探路”，当符合某种条件时慢指针向前挪 16-最接近的三数之和 题目描述：","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://sobxiong.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"}]},{"title":"Zookeeper入门","slug":"大数据/Zookeeper","date":"2020-06-26T09:26:19.000Z","updated":"2020-07-06T14:21:35.077Z","comments":true,"path":"2020/06/26/大数据/Zookeeper/","link":"","permalink":"https://sobxiong.github.io/2020/06/26/%E5%A4%A7%E6%95%B0%E6%8D%AE/Zookeeper/","excerpt":"内容 Zookeeper入门 Zookeeper安装 Zookeeper实战 Zookeeper内部原理 面试真题","text":"内容 Zookeeper入门 Zookeeper安装 Zookeeper实战 Zookeeper内部原理 面试真题 Zookeeper入门 概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目 特点 数据结构 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等 统一命名服务 统一配置管理 统一集群管理 服务器节点动态上下线 软负载均衡 下载地址：官网地址——https://zookeeper.apache.org Zookeeper安装 本地模式安装部署 安装前准备 安装jdk 拷贝Zookeeper安装包到Linux系统下 解压到指定目录：tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/ 配置修改 修改配置文件(conf目录下)：mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件，修改dataDir路径：dataDir=/opt/module/zookeeper-3.6.1/zkData 新建zkData目录(不同于Hadoop目录不能存在)：mkdir zkData 操作Zookeeper 启动Zookeeper Server(服务端)：bin/zkServer.sh start 查看进程是否启动：jps(正常会有一个QuorumPeerMain) 查看状态：bin/zkServer.sh status 启动Zookeeper Client(客户端)：bin/zkCli.sh 查看文件列表：ls /(一开始只有[zookeeper]) 退出Zookeeper Client：quit 停止Zookeeper Server：bin/zkServer.sh stop 配置参数解读Zookeeper中的配置文件zoo.cfg中参数含义解读如下： tickTime = 2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) initLimit = 10：LF初始通信时限集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数(tickTime的数量)，用它来限定集群中的Zookeeper服务器连接到Leader的时限 syncLimit = 5：LF同步通信时限集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer dataDir：数据文件目录 + 数据持久化路径主要用于保存Zookeeper中的数据 clientPort = 2181：客户端连接端口监听客户端连接的端口 Zookeeper实战 分布式安装部署 集群规划：在hadoop1、hadoop2、hadoop3三个节点上部署Zookeeper形成集群 安装：分发zookeeper到hadoop2、hadoop3：xsync zookeeper-3.6.1/ 配置服务器编号： 在zookeeper-3.6.1目录下创建zkData目录：mkdir zkData 在zkData目录下创建myid文件：touch myid 编辑myid文件(设置当前server编号)：1 同步zkData到hadoop2、hadoop3上：xsync zkData/ 在hadoop2、hadoop3上修改myid中的内容为2、3 配置zoo.cfg文件 新增集群节点配置 1234# clusterserver.1=hadoop1:2888:3888server.2=hadoop2:2888:3888server.3=hadoop3:2888:3888 同步zoo.cfg文件：xsync zoo.cfg 配置参数解读：server.A=B:C:D A是一个数，表示这是第几号服务器。集群模式下配置文件myid中的数字就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server B是这个服务器的地址 C是这个服务器Follower与集群中的Leader服务器交换信息的端口 D是用来执行选举时服务器相互通信的端口：万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader 集群操作 分别启动Zookeeper(启动前需要关闭Linux防火墙,使得各节点能够相互通信)：bin/zkServer.sh start 查看状态：bin/zkServer.sh status 客户端命令行操作(启动命令行：bin/zkCli.sh) 命令基本语法 功能描述 help(打错也是一个效果,当前无help命令) 显示所有操作命令 ls [-s] [-w] [-R] path 使用ls命令来查看当前path下znode中所包含的内容(-s：查看更新次数等详细数据,替代ls2;-w：设置watcher监听器,只有效一次;-R：递归查看节点) create [-s] [-e] path [data] 创建节点(-s：含有序列;-e：临时(重启或者超时消失);data：写入path的内容,如果没有data创建不出节点) get [-s] [-w] path 获得节点的值(-s：获取更加详细的节点数据;-w：设置watcher监听器,只有效一次) set path data 设置节点的具体值 stat path 查看节点状态 delete path 删除节点 deleteall path 递归删除节点 API应用 IDEA环境搭建 创建空maven项目 添加pom文件： 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.13.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在resources目录下新建一个日志配置文件log4j.properties 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建ZooKeeper客户端 123456789101112131415// 访问的ipprivate String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\";// 会话超时时间private int sessionTimeout = 2000;// zookeeper客户端private ZooKeeper zkClient;@Beforepublic void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; &#125; &#125;);&#125; 创建子节点 1234567// 1、创建节点@Testpublic void createNode() throws IOException, KeeperException, InterruptedException &#123; String path = zkClient.create(\"/sobxiong\", \"sobxiong,xixixihahaha\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(\"path = \" + path);&#125; 获取子节点并监听节点变化 1234567891011121314151617181920212223242526272829// 2、获取子节点,并监控节点的变化@Testpublic void getDataAndWatch() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(\"child = \" + child); &#125; System.out.println(\"------------\"); Thread.sleep(Long.MAX_VALUE);&#125;// 设置watcher中的process方法,使其继续调用自身继续监听@Beforepublic void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(\"child = \" + child); &#125; System.out.println(\"------------\"); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;);&#125; 判断Znode是否存在 123456// 3、判断节点是否存在@Testpublic void judgeNodeExist() throws KeeperException, InterruptedException &#123; Stat stat = zkClient.exists(\"/sobxiong\", false); System.out.println(\"stat = \" + stat);&#125; 监听服务器节点动态上下线案例 需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线 案例分析： 具体实现 先在集群上创建/servers节点：create /servers “servers” 服务端向Zookeeper注册： 12345678910111213141516171819202122232425262728293031323334public class DistributeServer &#123; // 访问的ip private String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\"; // 会话超时时间 private int sessionTimeout = 2000; // zookeeper客户端 private ZooKeeper zkClient; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeServer server = new DistributeServer(); // 1、连接zookeeper集群 server.getConnect(); // 2、注册节点 server.register(args[0]); // 3、业务逻辑 server.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void register(String hostName) throws KeeperException, InterruptedException &#123; String path = zkClient.create(\"/servers/server\", hostName.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostName + \" is online...\"); &#125; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123;&#125; &#125;); &#125;&#125; 客户端注册监听： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class DistributeClient &#123; // 访问的ip private String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\"; // 会话超时时间 private int sessionTimeout = 2000; // zookeeper客户端 private ZooKeeper zkClient; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeClient client = new DistributeClient(); // 1、获取zookeeper集群连接 client.getConnect(); // 2、注册监听 client.getChildren(); // 3、业务逻辑处理 client.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void getChildren() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/servers\", true); // 存储服务器节点主机名称集合 List&lt;String&gt; hostNames = new ArrayList(); for (String child : children) &#123; byte[] data = zkClient.getData(\"/servers/\" + child, false, null); hostNames.add(new String(data)); &#125; // 将所有在线主机名称打印 System.out.println(\"hostNames = \" + hostNames); &#125; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; getChildren(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;&#125; Zookeeper内部原理 节点类型 Stat结构体 cZxid：创建节点的事务zxid——每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生 ctime：znode被创建的毫秒数(从1970年开始) mzxid：znode最后更新的事务zxid mtime：znode最后修改的毫秒数(从1970年开始) pZxid：znode最后更新的子节点zxid cversion：znode子节点变化号，znode子节点修改次数 dataversion：znode数据变化号 aclVersion：znode访问控制列表的变化号 ephemeralOwner：如果是临时节点，这个是znode拥有者的session id；如果不是临时节点则是0 dataLength：znode的数据长度 numChildren：znode子节点数量 监听器原理 选举机制 半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器 Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的 选举过程的举例(假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的——没有历史数据，在存放数据量这一点上，都是一样的)： 服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上(3票)，选举无法完成，服务器1状态保持为LOOKING 服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的(服务器1)大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1、2状态保持LOOKING 服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING 服务器4启动，发起一次选举。此时服务器1、2、3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING 服务器5启动，同4一样当小弟 写数据流程 面试真题 请简述ZooKeeper的选举机制？参见4.4 ZooKeeper的监听原理是什么？参见4.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？(1)部署方式：单机模式、集群模式；(2)角色：Leader和Follower；(3)集群最少需要机器数：3 ZooKeeper的常用命令有哪些？ls create get delete set","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://sobxiong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://sobxiong.github.io/tags/Zookeeper/"}]},{"title":"SpringCloud","slug":"Spring/SpringCloud","date":"2020-06-21T13:46:45.000Z","updated":"2020-06-25T13:49:50.187Z","comments":true,"path":"2020/06/21/Spring/SpringCloud/","link":"","permalink":"https://sobxiong.github.io/2020/06/21/Spring/SpringCloud/","excerpt":"内容 前期准备 Eureka介绍","text":"内容 前期准备 Eureka介绍 前期准备 SpringCloud：分布式微服务架构的一站式解决方案，是多种微服务架构落地技术的集合体，俗称微服务全家桶；SpringCloud已成为微服务开发的主流技术栈 版本选择： SpringBoot：2.2.2.RELEASE git源码地址：https://github.com/spring-projects/spring-boot/releases/ 官方文档：https://docs.spring.io/spring-boot/docs/2.2.2.RELEASE/reference/htmlsingle/ SpringCloud：Hoxton.SR1、Alibaba 2.1.0.RELEASE(boot和cloud一起运用时,boot要照顾cloud) git源码地址：https://github.com/spring-projects/spring-cloud/wiki 官网：https://spring.io/projects/spring-cloud 官方文档：https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/htmlsingle/ 中文文档：https://www.bookstack.cn/read/spring-cloud-docs/docs-index.md cloud与boot之间的依赖关系 https://spring.io/projects/spring-cloud#overview(大版本) https://start.spring.io/actuator/info(具体版本) 架构编码构建 重要规矩：约定 &gt; 配置 &gt; 编码 IDEA创建project工作空间 父工程project(pom项目) 设置项目字符编码：Editor -&gt; File Encodings -&gt; Global Encoding、Project Encoding、Default encoding for properties files设置UTF-8，勾选上Transparent native-to-ascii conversion 设置注解生效激活：Build,… -&gt; Complier -&gt; Annotatoin Processors -&gt; 勾选Enable annotation processing java编译版本选择8：Build,… -&gt; Complier -&gt; Java Compiler -&gt; 设置父工程java编译版本为1.8 父工程project的pom文件设置： 1234567891011121314151617181920...&lt;packaging&gt;pom&lt;/packaging&gt;&lt;!-- 统一管理jar包版本 --&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; ...&lt;/properties&gt;&lt;!-- 子模块继承之后,提供作用：锁定版本 + 子modlue不用写groupId和version 父工程声明后不会直接引入,在子工程pom文件声明后才会正式引用--&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; ... &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 子模块构建 五个步骤：建module、改pom、写yml、主启动、业务类 设置热部署Devtools： 添加devtools的pom依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 添加plugin插件的pom依赖： 12345678910&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;addResources&gt;true&lt;/addResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 设置启用自动build：Compiler -&gt; ADBC四个选项(首字母)打勾 更新idea属性值：command+shift+A调出搜索action，键入Registry，compiler.automake.allow.when.app.running和actionSystem.assertFocusAccessFromEdt打勾 重启idea RestTemplate介绍：提供了多种便捷访问远程Http服务的方法，是一种简单便捷的访问restful服务模版类，是Spring提供的用于访问Rest服务的客户端模板工具集 官网地址：https://docs.spring.io/spring-framework/docs/5.2.2.RELEASE/javadoc-api/org/springframework/web/client/RestTemplate.html 使用：使用restTemplate访问restful接口非常地简单粗暴。(url、requestMap、ResponseBean.class)这三个参数分别代表REST请求地址、请求参数、HTTP响应需转换成的对象类型 Eureka介绍 什么是服务注册与发现Eureka采用了CS的设计架构，Eureka Server作为服务注册功能的服务器，它是服务注册中心，而系统中的其他微服务，使用 Eureka的客户端连接到Eureka Server并维持心跳连接。这样系统维护人员就可以通过Eureka Server来监控各个微服务是否正常运行在服务注册与发现中有一个注册中心。当服务器启动时，会把当前自己的服务器信息比如自己服务地址、通信地址等以别名方式注册到注册中心上，另一方(消费者｜服务提供者)以别名的方式去注册中心上获取实际的服务器通讯地址，然后再实现本地RPC调用RPC远程调用。框架核心设计思想在于注册中心，因为使用注册中心管理每个服务与服务之间的一个依赖关系(服务治理概念)。在任何RPC远程框架中，都会有一个注册中心(存放服务地址相关信息(接口地址)) 两个组件 Eureka Server提供服务注册服务各个微服务节点通过配置启动后，会在EurekaServer中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息 EurekaClient通过注册中心进行访问是一个Java客户端，用于简化与Eureka Server的交互，客户端也同时具备一个内置的，使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会向Eureka Server发送心跳(默认周期30秒)。如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，Eureka Server将会从服务注册表中将这个服务节点移除(默认90秒) 单机Eureka 服务端创建 创建module：cloud-eureka-server7001 改pom： 12345&lt;!-- 主要加入eureka-server依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 写yml 1234567891011121314server: port: 7001eureka: instance: # eureka服务端实例名称 hostname: localhost client: # false表示不向注册中心注册自己 register-with-eureka: false # false表示自己就是注册中心，职责是维护服务实例，并不需要去检索服务 fetch-registry: false service-url: # 设置与 eureka server交互的地址查询服务和注册服务都需要依赖这个地址 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka 主启动 12345678@SpringBootApplication// 声明自己是eureka的服务端@EnableEurekaServerpublic class EurekaMainApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaMainApplication.class,args); &#125;&#125; 测试：http://localhost:7001查看eureka的服务页面 客户端设置Eureka Client端cloud-provider-payment8001将注册进EurekaServer成为服务提供者 provider，cloud-consumer-order80同理 改pom 12345&lt;!-- eureka client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 写yml 12345678eureka: client: # 表示是否将自己注册进Eureka Server,默认为true register-with-eureka: true # 是否从Eureka Server抓取已有的注册信息，默认为true。单节点无所谓，集群必须设置为true才能配合Ribbon使用负载均衡 fetch-registry: false service-url: defaultZone: http://localhost:7001/eureka 添加注解：在SpringBootApplication启动类上添加@EnableEurekaClient注解 测试：http://localhost:7001查看eureka的服务页面，看看Application Instances 集群Eureka构建 集群目的：高可用，如果注册中心只有一个，出了故障就会导致整个服务环境不可用(解决方法：搭建Eureka注册中心集群，实现负载均衡+故障排错) 集群注册原理：互相注册，相互守望(多个eureka server相互测住,保障信息共享) 搭建集群 新建7002模块 复制7001的pom、yml和主启动文件 修改hosts(新增) 127.0.0.1 eureka7001.com127.0.0.1 eureka7002.com 修改yml(相互注册) 12345678eureka: instance: # eureka服务端实例名称(为区分,取不同) hostname: eureka7001.com client: service-url: # 两台集群所以相互注册 defaultZone: http://eureka7002.com:7002/eureka 测试：访问http://eureka7001.com:7001以及http://eureka7002.com:7002，查看DS Replicas 部署微服务模块到集群 修改两个模块(payment与order)的yml：defaultZone: http://eureka7002.com:7002/eureka,http://eureka7001.com:7001/eureka 启动微服务测试 支付模块微服务的配置 新建另一支付模块payment8002，复制payment8001的pom、yml(更改端口8002)和主启动类 获取端口号 123// 获取yml设置的端口号@Value(\"$&#123;server.port&#125;\")private String serverPort; 修改consumer80中controller中的支付url(改为动态) 之前单机eurake中写死为8001，集群中有8001和8002 查看eurake页面http://eureka7001.com:7001中8001和8002对应的application名称(就是yml设置的application.name) 将url设置为：http + application名(例：http://CLOUD-PAYMENT-SERVICE) 此时未开启负载均衡不能访问页面：将consumer80模块下的配置类ApplicationContextConfig声明的RestTemplate的bean方法上添加注解@LoadBalanced开启负载均衡 访问http://localhost/consumer/payment/get/1查看json串，如在payment的controller方法中返回方法中设置了port字符串，则可以看到8001和8002回来变换(线性负载均衡) actuator微服务信息完善修改主机名与暴露ip地址，可在http://eureka7002.com:7002查看自定义主机名的变化(鼠标在主机名上方，浏览器左下角会出现完整访问地址) 123456eureka: instance: # 自定义主机名 instance-id: payment8002 # 设置暴露ip地址 prefer-ip-address: true 服务发现Discovery payment8001的controller添加代码 1234567891011121314151617@Resourceprivate DiscoveryClient discoveryClient;@GetMapping(\"/discovery\")public Object discovery() &#123; // 获取服务列表清单 List&lt;String&gt; services = discoveryClient.getServices(); for (String service : services) &#123; log.info(\"service: &#123;&#125;\", service); &#125; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"CLOUD-PAYMENT-SERVICE\"); for (ServiceInstance instance : instances) &#123; log.info(\"instance: id = &#123;&#125;, host = &#123;&#125;, port = &#123;&#125;, uri = &#123;&#125;\", instance.getInstanceId(), instance.getHost(), instance.getPort(), instance.getUri()); &#125; return discoveryClient;&#125; payment8001主类添加注解@EnableDiscoveryClient 等待热重启，观察日志 eureka自我保护(宁可保留错误的服务注册信息，也不盲目注销任何可能健康的服务实例——好死不如赖活着) 概述：保护模式主要用于一组客户端和Eureka Server之间存在网络分区场景下的保护。一旦进入保护模式。Eureka Server将会尝试保护其服务注册表中的信息，不再删除服务注册表中的数据，也就是不会注销任何微服务 是否开启保护模式：默认开始，如果在Eureka Server页面看到提示(EMERGENCY!……JUST TO BE SAFE)，则说明开启 为什么会产生自我保护机制：为了防止EurekaClient可以正常运行，但是在EurekaServer网络不通的情况下，EurekaServer不会立刻将EurekaClient服务剔除 什么是自我保护模式：默认情况下，如果EurekaServer在一定时间内没有接收到某个微服务实例的心跳，EurekaServer将会注销该实例(默认90s)。但是当网络分区故障发生(延时、卡顿、拥挤)时，微服务与EurekaServer之间无法正常通信，以上行为可能变得非常危险——因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka通过“自我保护模式”来解决这个问题——当EurekaServer节点在短时间内丢失过多客户端时(可能发生了网络分区故障)，那么这个节点就会进入自我保护模式 关闭自我保护(访问eureka发现红字消息，关闭8001服务页面服务立马消失) 123456789101112131415# eureka7001eureka: server: # 关闭自我保护机制 enable-self-preservation: false # 心跳时间默认90s，改为2000ms，即2s eviction-interval-timer-in-ms: 2000# payment8001eureka: instance: # eureka客户端发送心跳的时间间隔，默认30s lease-renewal-interval-in-seconds: 1 # eureka服务端在收到最后一次心跳等待的时间上线，默认90s lease-expiration-duration-in-seconds: 2","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"}]},{"title":"Spring注解驱动开发","slug":"Spring/Spring注解驱动开发","date":"2020-06-19T06:57:26.000Z","updated":"2020-06-26T08:58:59.887Z","comments":true,"path":"2020/06/19/Spring/Spring注解驱动开发/","link":"","permalink":"https://sobxiong.github.io/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/","excerpt":"内容 容器 扩展原理 Web","text":"内容 容器 扩展原理 Web 容器 @Configuration：类注解(配置类=配置文件,注解告诉Spring这是一个配置类) @Bean：方法注解，在类方法中给出返回Bean的方法，并在方法上添加@Bean注解(给容器中注册一个Bean,类型为返回值的类型,id默认为方法名,可复写注解的value属性复写id)。 @Scope：方法注解，设置作用域。常用值为： prototype：多实例，ioc容器诶懂并不会去调用方法创建对象放在容器中。每次获取的时候才会调用方法创建对象 singleton(默认单实例)：ioc容器启动会调用方法创建对象放到ioc容器中，以后每次获取就是直接从容器中(可看作使用map.get())拿 @Lazy：懒加载，只有在singleton单实例下才生效，且需要在返回bean的方法上加上@Lazy注解。单实例bean默认在容器启动的时候创建对象；懒加载在容器启动时不创建对象，第一次使用(获取)Bean创建对象并初始化 123456@Scope(\"singleton\")@Lazy@Bean(\"person\")public Person person() &#123; return new Person(\"SOBXiong\", 22);&#125; @ComponentScans：指定扫描规则组(value为ComponentScan集合) @ComponentScan：类注解，指定组件扫描规则 value：指定包名，这样Spring会扫描包下的所有组件(SpringBoot情况可能不同,不需要) excludeFilters：指定排除的过滤器，filter可根据注解排除(排除规则)，classed指定注解的类 includeFilters：指定只需要包含的过滤器 useDefaultFilters：是否适用缺省的过滤器，默认true；如果要使includeFilters生效，则必须设置为false FilterType： FilterType.ANNOTATION：按照注解方式 FilterType.ASSIGNABLE_TYPE：按照指定的类型(具体的类,包括子类和实现类) FilterType.REGEX：适用正则表达式 FilterType.CUSTOM：使用自定义规则，需要自定义实现TypeFilter接口的类 123456789101112131415161718192021222324252627282930313233@Configuration@ComponentScan(value = \"packageName\",excludeFilters = &#123; @Filter(type=FilterType.ANNOTATION,classes=&#123;Controller.class,Service.class&#125;)&#125;)public class MainConfig &#123; @Bean public Person person() &#123; return new Person(\"SOBXiong\", 22); &#125;&#125;public class TestTypeFilter implements TypeFilter &#123; /** * * @param metadataReader 读取到的当前正在扫描的类的信息 * @param metadataReaderFactory 可以获取到其他任何类信息的工厂 * @return boolean * @throws IOException */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; // 获取当前类注解的信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); // 获取当前正在扫描的类的类信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); // 获取当前类的资源信息(类路径等) Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println(\"className = \" + className); return className.contains(\"test\"); // return false; &#125;&#125; @Conditional：按照一定的条件进行判断，满足条件给容器中注册bean(Spring底层大量用到);可以设置在类上，也可以设置在方法上。设置在返回bean的方法上：只根据条件解决是否注册bean。设置在类上：类中注册统一设置，满足条件时，这个类中配置的所有bean注册才能生效 1234567891011121314151617181920212223242526272829303132@Conditional(TestCondition.class)@Bean(\"person2\")public Person person2() &#123; return new Person(\"SOBXiong\", 22);&#125;public class TestCondition implements Condition &#123; /** * @param conditionContext 判断条件能使用的上下文(环境) * @param annotatedTypeMetadata 注释信息 * @return boolean */ @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) &#123; // 1、能获取到ioc使用的beanFactory ConfigurableListableBeanFactory beanFactory = conditionContext.getBeanFactory(); // 2、获取类加载器 ClassLoader classLoader = conditionContext.getClassLoader(); // 3、获取当前环境信息 Environment environment = conditionContext.getEnvironment(); // 4、获取到bean定义的注册类 BeanDefinitionRegistry registry = conditionContext.getRegistry(); // 可以判断容器中的bean注册情况,也可以给容器中注册bean boolean isDefinition = registry.containsBeanDefinition(\"person\"); // 获取运行系统的名称 String osName = environment.getProperty(\"os.name\"); if (osName.contains(\"Windows\")) &#123; return true; &#125; return false; &#125;&#125; @Import：导入组件，id默认是组件的全类名 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 给容器中注册组件： * 1、包扫描+组件标注注解(@Controller、@Service、@Repository、@Component) * 2、@Bean[导入第三方包里面的组件] * 3、@Import[快速给容器中导入一个组件] * 1、容器会自动注册这个组件,id默认是全类名 * 2、ImportSelector：返回需要导入的组件的全类名数组(SpringBoot源码中许多地方用到); * 3、ImportBeanDefinitionRegistrar：手动注册bean到容器中 * 4、使用Spring提供的FactoryBean(工厂Bean),其他与Spring整合的框架使用的特别多 * 1、默认获取的是工厂bean调用getObject创建的对象 * 2、要获取工厂bean本身,需要给id前面加一个&amp; */@Configuration@Import(&#123;TestImportSelector.class, TestImportBeanDefinitionRegistrar.class&#125;)public class MainConfig &#123;&#125;// 自定义逻辑返回需要导入的组件public class TestImportSelector implements ImportSelector &#123; /** * @param annotationMetadata 当前标注@Import注解的类的所有注解信息 * @return String[] 导入到容器中的组件全类名数组 */ @Override public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; return new String[]&#123;\"com.xiong.test.Animal\", \"com.xiong.test.Person\"&#125;; &#125;&#125;public class TestImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; /** * 可以把所有需要添加到容器中的bean通过调用BeanDefinitionRegistry.registerBeanDefinition手工注册 * @param importingClassMetadata 当前类的注解信息 * @param registry BeanDefinition注册类 */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; boolean isWorldExist = registry.containsBeanDefinition(\"World\"); if (!isWorldExist) &#123; // 注册一个bean,指定bean的名称和bean的定义信息(bean的类型,bean的Scope...) registry.registerBeanDefinition(\"world\", new RootBeanDefinition(World.class)); &#125; &#125;&#125; FactoryBean(工厂Bean)： 1234567891011121314151617181920212223242526// 创建一个Spring定义的FactoryBeanpublic class TestFactoryBean implements FactoryBean&lt;Animal&gt; &#123; // 返回一个Animal对象,这个对象会添加到容器中 // 调用此方法得到对象 @Override public Animal getObject() throws Exception &#123; return new Animal(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Animal.class; &#125; // 是否是单实例 @Override public boolean isSingleton() &#123; return true; &#125;&#125;@Configurationpublic class MainConfig &#123; @Bean public TestFactoryBean testFactoryBean()&#123; return new TestFactoryBean(); &#125;&#125; @Bean指定初始化和销毁方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class Car &#123; public Car()&#123; System.out.println(\"Car 构造方法\"); &#125; public void init()&#123; System.out.println(\"Car init\"); &#125; public void destroy()&#123; System.out.println(\"Car destroy\"); &#125;&#125;/** * bean的生命周期： * bean创建 --&gt; 初始化 --&gt; 销毁 * 容器管理bean的生命周期; * 我们可以自定义初始化和销毁方法;容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法 * * 构造(对象创建)： * 单实例：在容器启动的时候创建对象 * 多实例：在每次获取的时候创建对象 * 初始化：对象创建完成,并赋值结束,调用初始化方法 * 销毁： * 单实例：容器关闭的时候 * 多实例：容器不会管理这个bean,容器不会调用销毁方法 * * 1、指定初始化和销毁方法(通过@Bean注解指定init-method和destroy-method) * 2、通过让Bean实现InitializingBean(定义初始化方法逻辑),DisposableBean(定义销毁逻辑) * 3、可以使用JSR250： * @PostConstruct：在bean创建完成并且属性赋值完毕再执行初始化方法 * @PreDestroy：在容器销毁bean之前通知进行清理工作 * 4、BeanPostProcessor：bean的后置处理器(在bean初始化前后进行一些工作) * postProcessBeforeInitialization：在初始化之前工作 * postProcessAfterInitialization：在初始化之后工作 */@Configurationpublic class MainConfigLifecycle &#123; // @Scope(\"prototype\") @Bean(initMethod = \"init\", destroyMethod = \"destroy\") public Car car() &#123; return new Car(); &#125;&#125;@ComponentScan(\"com.xiong.test\")@Componentpublic class Cat implements InitializingBean, DisposableBean &#123; public Cat() &#123; System.out.println(\"Cat构造函数...\"); &#125; @Override public void destroy() throws Exception &#123; System.out.println(\"Cat destroy...\"); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println(\"Cat init...\"); &#125;&#125;/** * 后置处理器：在bean初始化前后进行处理工作 */@Componentpublic class TestBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessBeforeInitialization: \" + beanName + \" , \" + bean); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessAfterInitialization: \" + beanName + \" , \" + bean); return bean; &#125;&#125; BeanPostProcessor原理 1234567891011// 遍历得到容器中所有的BeanPostProcessor;挨个执行beforeInitialization,// 一旦返回null,跳出for循环,不追执行后面的BeanPostProcessor.postProcessBeforeInitialization()// 给bean进行属性赋值populateBean(beanName, mbd, instanceWrapper);initializeBean(beanName, exposedObject, mbd);&#123; // 以下就是initializeBean的粗略内容 applyBeanPostProcessorsBeforeInitialization(bean, beanName); invokeInitMethods(beanName, wrappedBean, mbd); applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);&#125; Spring底层对BeanPostProcessor的使用 ApplicationContextAwareProcessor：可让bean获取容器对象context BeanValidationPostProcessor：Web表单校验的处理器 InitDestroyAnnotationBeanPostProcessor：@PostConstruct和@Bean的init-method等方法的具体实现 AutowiredAnnotationBeanPostProcessor：@Autowired自动注入功能的具体实现 属性赋值 使用@Value赋值： 基本数值 SpEL：#{} ${}：取出配置文件(properties或yaml)中的值(在运行环境变量里面的值) 使用@PropertySource加载外部配置文件 1234567891011121314151617181920212223// 使用@PropertySource读取外部配置文件中的k/v保存到运行的环境变量中// 加载完外部的配置文件以后使用$&#123;&#125;取出配置文件的值// 当前只能加载properties文件,yaml不能,是采用的加载器问题@PropertySource(value = &#123;\"classpath:application.properties\"&#125;, encoding = \"utf-8\")@Configurationpublic class MainConfigPropertyValues &#123; @Bean public Person person() &#123; return new Person(); &#125;&#125;public class Person &#123; @Value(\"SOBXiong\") private String name; @Value(\"#&#123;22+5&#125;\") private int age; @Value(\"$&#123;person.nickName&#125;\") private String nickName;&#125;// application.properties// person.nickName=熊哈哈 自动装配 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * 自动装配： * Spring利用依赖注入(DI),完成对IOC容器中各个组件的依赖关系赋值 * @Autowired：自动注入(Spring定义的) * TestService&#123; * @Autowired TestDao testDao; * &#125; * 1、默认优先按照类型去容器中找对应的组件：context.getBean(TestDao.class); * 2、如果找到多个相同类型的组件,将属性名作为组件的id去容器中查找 * 3、@Qualifier(\"testDao\")：使用@Qualifier指定需要装配的组件id,而不是使用属性名 * 4、自动装配默认一定要将属性赋值好,没有就会报错(可以使用@Autowired注解中的required=false避免报错) * 5、@Primary：让Spring进行自动装配的时候默认使用首选的bean(此时@Qualifier不能使用);也可以使用@Qualifier指定需要装配的具体bean * 6、Spring还支持使用@Resource(JSR250)和@Inject(JSR330)[java规范的注解] * @Resource：可以和@Autowired一样实现自动装配功能,但默认是按照组件名称进行装配的(也可以通过name属性进行指定id);不能支持@Qualifier和required=false * @Inject：需要导入javax.inject的包,和@Autowired的功能一样,但没有required属性 * 7、@Autowired可以在构造器、参数、方法和属性上标注,都是从容器中获取组件的值 * 1、[标注在方法位置]：@Bean标注方法的方法参数;参数从容器中获取;默认不写@Autowired效果是一样的;都能自动装配 * 2、[标注在构造器位置]：如果组件只有一个有参构造器,这个有参构造器的@Autowired可以省略,参数位置的组件还是可以自动从容器中获取; * 但如有既有有参又有无参,会优先调用无参构造器,这使得boss的car属性和容器中的car不是同一个 * 3、[标注在参数位置] * 8、自定义组件想要使用Spring容器底层的的一些组件(ApplicationContext、BeanFactory等) * 自定义组件实现xxxAware接口：在创建对象的时候,会调用接口规定的方法注入相关组件; * xxxAware使用xxxProcessor：applicationContextAware =&gt; applicationContextAwareProcessor(BeanPostProcessor的实现类) */@Configuration@ComponentScan(\"com.xiong.test2\")public class MainConfigAutowired &#123; @Primary @Bean(\"testDao2\") public TestDao testDao() &#123; return new TestDao(\"2\"); &#125; // @Bean标注的方法创建对象的时候,方法参数的值从容器中获取 @Bean public Boss boss(Car car)&#123; Boss boss = new Boss(); boss.setCar(car); return boss; &#125;&#125;@Componentpublic class Car &#123;&#125;// 默认加在ioc容器中的组件，容器启动会调用无参构造器创建对象，在进行初始化赋值等操作// @Componentpublic class Boss &#123; private Car car; public Boss() &#123; &#125; // 构造器要用的组件，都是从容器中获取 // @Autowired public Boss(@Autowired Car car) &#123; this.car = car; System.out.println(\"Boss constructor with one parameter!\"); &#125; public Car getCar() &#123; return car; &#125; // 标注在方法上，Spring容器创建当前对象，就会调用方法完成赋值 // 方法使用的参数，自定义类型的值从ioc容器中获取 // @Autowired public void setCar(Car car) &#123; this.car = car; &#125; @Override public String toString() &#123; return \"Boss&#123;\" + \"car=\" + car + '&#125;'; &#125;&#125;@Servicepublic class TestService &#123; @Qualifier(\"testDao2\") @Autowired private TestDao testDao; @Override public String toString() &#123; return \"TestService&#123;\" + \"testDao=\" + testDao + '&#125;'; &#125;&#125;@Repositorypublic class TestDao &#123; private String label = \"1\";&#125; @Profile：Spring为我们提供的可以根据当前环境,动态地激活和切换一系列组件的共功能;指定组件在哪个环境的情况下才能被注册到容器中,不指定,任何环境下都能注册这个组件(开发、测试/生产环境,数据源(A/B/C)) 加了环境标志性的bean,只有这个环境被激活的时候才能注册到容器中(默认环境是default) 123@Profile(\"test\")@Bean(\"testDataSource\")public DataSource dataSourceTest() throws Exception &#123; ... &#125; 写在配置类上,只有是指定的环境的时候,整个配置类里面的内容才能生效 没有标注环境标识的bean在任何环境下都是加载的 环境的激活： 使用命令行动态参数：虚拟机参数位置加载 -Dspring.profiles.active=test 代码的方式激活某种环境 1234567891011121314151617@Testvoid contextLoads() &#123; // 1、创建ioc容器 AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); // 2、设置需要激活的环境 context.getEnvironment().setActiveProfiles(\"test\"); // 3、注册主配置类 context.register(MainConfigProfile.class); // 4、启动刷新容器 context.refresh(); ... // 关闭容器 context.close();&#125; AOP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220/** * AOP：指在程序运行期间动态地将某段代码切入到指定方法指定位置进行运行地编程方式 * 1、导入AOP模块：Spring AOP(SpringBoot导入MVC模块会连着导入AOP模块) * 2、定义一个业务逻辑类(MathCalculator)：在业务逻辑运行地时候将日志进行打印(方法之前、方法运行结束、方法出现异常...) * 3、定义一个日志切面类(LogAspect)：切面类里面地方法需要动态感知MathCalculator.div运行到哪里然后执行 * 通知方法： * 前置通知@Before：logStart(在目标方法div运行之前运行) * 后置通知@After：logEnd(在目标方法div运行结束之后运行——无论方法是正常结束还是异常结束) * 返回通知@AfterReturning：logReturn(在目标方法div正常返回之后运行) * 异常通知@AfterThrowing：logException(在目标方法div出现异常以后运行) * 环绕通知@Around：动态代理,手动推进目标方法div运行(jointPoint.proceed()) * 4、给切面类的目标方法标注何时何地运行(通知注解) * 5、将切面类和业务逻辑类(目标方法所在类)都加入到容器中 * 6、必须告诉Spring哪个类是切面类(给切面类上加一个注解@Aspect) * 7、给配置类中加@EnableAspectJAutoProxy(开启基于注解的AOP模式);在Spring中有很多的@EnableXXX注解,替代以前的xml配置 * * 三步： * 1、将业务逻辑组件和切面类都加入到容器中;告诉Spring哪个是切面类(@Aspect) * 2、在切面类上的每一个通知方法上标注通知注解,告诉Spring何时何地运行(切入点表达式) * 3、开启基于注解的AOP模式：@EnableAspectJAutoProxy * * AOP原理：[看给容器中注册了什么组件,这个组件什么时候工作以及这个组件的功能是什么?] * @EnableAspectJAutoProxy： * 1、@EnableAspectJAutoProxy是什么？ * @Import(AspectJAutoProxyRegistrar.class)：给容器中导入AspectJAutoProxyRegistrar * 利用AspectJAutoProxyRegistrar自定义给容器中注册bean：BeanDefinition * internalAutoProxyCreator = AnnotationAwareAspectJAutoProxyCreator * 给容器中注册一个AnnotationAwareAspectJAutoProxyCreator,id为internalAutoProxyCreator * * 2、AnnotationAwareAspectJAutoProxyCreator： * AbstractAutoProxyCreator实现了SmartInstantiationAwareBeanPostProcessor,BeanFactoryAware接口 * AbstractAdvisorAutoProxyCreator * AspectJAwareAdvisorAutoProxyCreator * AnnotationAwareAspectJAutoProxyCreator * 关注后置处理器(bean初始化完成前后做事情)、自动装配beanFactory * * 阅读线索： * AbstractAutoProxyCreator.setBeanFactory() * AbstractAutoProxyCreator.postProcessBeforeInstantiation() * AbstractAutoProxyCreator.postProcessAfterInitialization() * * AbstractAdvisorAutoProxyCreator.setBeanFactory()复写父类方法 * 方法内还执行了initBeanFactory() * * AnnotationAwareAspectJAutoProxyCreator.initBeanFactory()复写父类方法 * * 1、传入配置类,创建ioc容器 * 2、调用配置类,调用refresh()刷新容器 * 3、registerBeanPostProcessors(beanFactory)：注册bean的后置处理器来方便拦截bean的创建 * 1、先获取ioc容器已定义了的需要创建的所有BeanPostProcessor(第一步传入配置类带入的)： * String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); * 2、给容器中加入别的BeanPostProcessor * 3、优先注册实现了PriorityOrdered接口的BeanPostProcessor * 4、再给容器中注册实现了Ordered接口的BeanPostProcessor * 5、最后注册没实现优先级接口的BeanPostProcessor * 6、注册BeanPostProcessor,实际上就是创建BeanPostProcessor对象并保存在容器中 * 创建internalAutoProxyCreator的BeanPostProcessor[AnnotationAwareAspectJAutoProxyCreator对象] * 1、创建Bean的实例 * 2、populateBean()：给bean的各种属性赋值 * 3、initializeBean()：初始化bean * 1、invokeAwareMethods()：处理Aware接口的方法回调 * 2、applyBeanPostProcessorsBeforeInitialization()：应用后置处理器的postProcessBeforeInitialization()方法 * 3、invokeInitMethods()：执行自定义的初始化方法 * 4、applyBeanPostProcessorsAfterInitialization()：应用后置处理器的postProcessAfterInitialization()方法 * 4、BeanPostProcessor(AnnotationAwareAspectJAutoProxyCreator)创建成功 -&gt; aspectJAdvisorsBuilder * 7、把BeanPostProcessor注册到BeanFactory中：beanFactory.addBeanPostProcessor(postProcessor); * -----------------------------------------创建和注册AnnotationAwareAspectJAutoProxyCreator的过程---------------------------- * 4、finishBeanFactoryInitialization(beanFactory)：完成BeanFactory初始化工作,创建剩下来的单实例bean * 1、遍历获取容器中所有的Bean,依次创建对象getBean(beanName) * getBean -&gt; doGetBean() -&gt; getSingleton() * 2、创建bean[AnnotationAwareAspectJAutoProxyCreator在所有bean创建之前会有一个拦截, * InstantiationAwareBeanPostProcessor,会调用postProcessBeforeInstantiation()方法] * 1、先从缓存中获取当前bean,如果能获取到,说明bean是被创建过的,直接使用;否则再创建 * 只要被创建好的bean都会被缓存起来 * 2、createBean()：创建bean——AnnotationAwareAspectJAutoProxyCreator会在任何bean创建之前先尝试返回bean的实例 * [BeanPostProcessor是在Bean对象创建完成初始化前后调用的] * [InstantiationAwareBeanPostProcessor是在创建Bean实例之前先尝试用后置处理器返回对象的] * 1、resolveBeforeInstantiation()：希望后置处理器在此能返回一个代理对象,如果能返回 * 代理对象就使用; * 1、后置处理器先尝试返回对象： * bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); * 拿到所有后置处理器,如果是InstantiationAwareBeanPostProcessor * 就执行postProcessBeforeInstantiation()方法 * if (bean != null) &#123; * bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); * &#125; * 否则就进行第二步 * 2、doCreateBean()：真正地去创建bean实例,和3.6流程一样 * 3、 * * AnnotationAwareAspectJAutoProxyCreator[InstantiationAwareBeanPostProcessor]的作用 * 1、每一个bean创建之前,调用postProcessBeforeInstantiation() * 关心MathCalculator和LogAspect的创建 * 1、判断当前bean是否在advisedBeans中(保存了需要增强的bean——需要切面) * 2、判断当前bean是否是基础类型的(Advice、Pointcut、Advisor、AopInfrastructureBean) * 或者是否是切面(@Aspect) * 3、判断是否需要跳过 * 1、获取候选的增强器(切面里面的通知方法),每一个封装的通知方法的增强器是InstantiationModelAwarePointcutAdvisor * 判断每一个增强器是否是AspectJPointcutAdvisor类型(返回true) * 2、返回false * 2、创建对象 * postProcessAfterInitialization * // 在需要的时候包装 * return wrapIfNecessary() * * 1、获取当前bean的所有增强器(通知方法) * 1、找到候选的所有增强器(找哪些通知方法是需要切入当前bean方法的) * 2、获取到能在bean使用的增强器 * 3、给增强器排序 * 2、保存当前bean到advisedBeans * 3、如果当前bean需要增强,创建当前bean的代理对象 * 1、获取所有增强器(通知方法) * 2、保存到proxyFactory * 3、创建代理对象：Spring自动决定 * JdkDynamicAopProxy()：jdk动态代理 * ObjenesisCglibAopProxy()：cglib的动态代理 * 4、给容器中返回当前组件使用cglib增强了的代理对象 * 5、以后容器中获取到的就是这个组件的代理对象，执行目标方法的时候，代理对象就会执行通知方法的流程 * * 3、目标方法执行: * 容器中保存了组件的代理对象(cglib增强后的对象)，这个对象里面保存了详细信息(比如增强器、目标对象...) * 1、CglibAopProxy.intercept()拦截目标方法的执行 * 2、根据ProxyFactory对象获取将要执行的目标方法的拦截器链 * List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); * 1、List&lt;Object&gt; interceptorList保存所有拦截器(长度为5) * 一个默认的ExposeInvocationInterceptor和四个增强器 * 2、遍历所有的增强器,将其转为Interceptor * 3、将增强器转为List&lt;MethodInterceptor&gt; * 如果是MethodInterceptor,直接加入集合 * 如果不是,使用AdvisorAdapter转为MethodInterceptor * 转换完成返回MethodInterceptor数组 * 3、如果没有拦截器链,直接执行目标方法 * 拦截器链(每一个通知方法又被包装为方法拦截器,利用MethodInterceptor机制) * 4、如果有拦截器链，把需要执行的目标对象、目标方法、拦截器链等信息传入创建一个CglibMethodInvocation对象， * 并调用它的proceed()方法 * 5、拦截器的触发过程 * 1、如果没有拦截器(或者拦截器的索引为拦截器数组大小-1——到了最后一个拦截器),直接执行目标方法 * 2、链式获取每一个拦截器,拦截器执行invoke()方法,每一个拦截器等待下一个拦截器执行完成返回以后再来执行; * 拦截器链的机制,保证通知方法与目标方法的执行顺序 * * 总结： * 1、@EnableAspectJAutoProxy开始AOP功能 * 2、@EnableAspectJAutoProxy会给容器中注册一个组件AnnotationAwareAspectJAutoProxyCreator * 3、AnnotationAwareAspectJAutoProxyCreator是一个后置处理器 * 4、容器的创建流程： * 1、refresh()容器刷新后registerBeanPostProcessors()注册后置处理器：创建AnnotationAwareAspectJAutoProxyCreator对象 * 2、finishBeanFactoryInitialization()初始化剩下的单实例bean * 1、创建业务逻辑组件和切面组件 * 2、AnnotationAwareAspectJAutoProxyCreator拦截组件的创建过程 * 3、组件创建完之后,判断组件是否需要增强 * 是：把切面的通知方法包装成增强器(Advisor),给业务逻辑组件创建一个代理对象(cglib) * 5、执行目标方法： * 1、代理对象执行目标方法 * 2、CglibAopProxy.intercept()进行拦截： * 1、得到目标方法的拦截器链(增强器包装成拦截器MethodInterceptor) * 2、利用拦截器的链式机制,依次进入每一个拦截器进行执行 * 3、效果： * 正常执行：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 返回通知 * 出现异常：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 异常通知 */@EnableAspectJAutoProxy@Configurationpublic class MainConfigAOP &#123; // 业务逻辑类加入容器中 @Bean public MathCalculator calculator()&#123; return new MathCalculator(); &#125; // 切面类加入到容器中 @Bean public LogAspect logAspect()&#123; return new LogAspect(); &#125;&#125;public class MathCalculator &#123; public int div(int i, int j) &#123; return i / j; &#125;&#125;/** * 切面类,方法中joinPoint必须写在参数表的第一位(否则报错) */@Aspectpublic class LogAspect &#123; // 抽取公共的切入点表达式(参考Spring官方文档) // 1、本类引用(方法名()) // 2、其他的切面引用(全类名方法名()) @Pointcut(\"execution(public int com.xiong.test3.MathCalculator.div(int, int))\") public void pointCut() &#123;&#125; // @Before在目标方法之前切入：切入点表达式(指定在哪个方法切入) @Before(\"pointCut()\") public void logStart(JoinPoint joinPoint) &#123; Object[] args = joinPoint.getArgs(); System.out.println(joinPoint.getSignature().getName() + \"运行开始... 参数列表是: &#123;\" + Arrays.asList(args) + \"&#125;\"); &#125; @After(\"pointCut()\") public void logEnd(JoinPoint joinPoint) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行结束...\"); &#125; @AfterReturning(value = \"pointCut()\", returning = \"result\") public void logReturn(JoinPoint joinPoint, Object result) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行正常返回... 结果: &#123;\" + result + \"&#125;\"); &#125; @AfterThrowing(value = \"pointCut()\", throwing = \"exception\") public void logException(JoinPoint joinPoint, Exception exception) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行出现异常... 异常信息: &#123;\" + exception.getMessage() + \"&#125;\"); &#125;&#125;@Testvoid contextLoads() &#123; // 1、创建ioc容器 AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(MainConfigAOP.class); // 必须使用Spring容器中的组件 MathCalculator calculator = context.getBean(MathCalculator.class); calculator.div(1, 0); context.close();&#125;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"}]},{"title":"Hadoop入门","slug":"大数据/Hadoop","date":"2020-06-03T13:48:10.000Z","updated":"2020-07-12T10:49:43.660Z","comments":true,"path":"2020/06/03/大数据/Hadoop/","link":"","permalink":"https://sobxiong.github.io/2020/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/","excerpt":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode HDFS2.X新特性 MapReduce概述 Hadoop序列化 MapReduce框架原理 Hadoop数据压缩 Yarn资源调度器 Hadoop企业优化","text":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode HDFS2.X新特性 MapReduce概述 Hadoop序列化 MapReduce框架原理 Hadoop数据压缩 Yarn资源调度器 Hadoop企业优化 概论 概念：大数据指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。需要解决的问题：海量数据的存储和海量数据的分析计算问题。 大数据特点(4V)： Volume(大量) Velocity(高速) Variety(多样)：结构化/非结构化数据，结构化数据以数据库/文本为主，非结构化数据包括网络日志、音频、视频、图片和地理位置信息等。 Value(低价值密度)：价值密度的高度与数据总量的大小成反比，如何快速对有价值数据“提纯”称为目前大数据背景下待解决的难题。 大数据应用场景：物流仓储、零售、旅游、商品广告推荐、保险、金融、房产、人工智能 大数据部门业务流程：产品人员提需求(统计总用户数、日活跃用户数、回流用户数等) =&gt; 数据部门搭建数据平台、分析数据指标 =&gt; 数据可视化(报表展示、邮件发送、大屏幕展示等) 大数据部门组织结构： Hadoop介绍 Hadoop是什么： 是一个由Apache基金会开发的分布式系统基础架构。 主要解决海量数据的存储和海量数据的分析计算问题。 广义上来说，Hadoop通常指更广泛的概念——Hadoop生态圈。 Hadoop发展历史 Lucene框架是Doug Cutting开创的开源软件，用Java编写，实现与Goole类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询和索引引擎。 2001年年底Lucene称为Apache基金会的一个子项目。 对于海量数据的场景，Lucene面对与Google同样的困难，存储数据困难，检索速度慢。 学习和模仿Google解决这些问题的办法：微型版Nutch。 Google是Hadoop的思想之源(其在大数据方面的三篇论文)GFS -&gt; HDFS Map-Reduce -&gt; MR BigTable -&gt; HBase 2003年~04年，Google公开部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。 2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。 2006年3月份，Map-Reduce和Nutch Distributed File System(NDFS)分别被纳入到Hadoop项目中，Hadoop就正式诞生，标志着大数据十代来临。 Hadoop名字来源于Doug Cutting儿子的玩具大象 Hadoop三大发行版本 Apache：最原始(基础)的版本，对于入门学习最好 Cloudera：在大型互联网企业中用的较多，产品主要为CDH，Cloudera Manager，Cloudera Support： CDH是Cloudera的Hadoop发行版，完全开源，比Apache版本在兼容性、安全性、稳定性上有所增强。 Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。 Cloudera Support即是对Hadoop的技术支持。 Hortonworks：文档较好 Hortonworks的主打产品是Hortonworks Data Platform(HDP)，也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari——一款开源的安装和管理系统。 HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 Hadoop的优势(4高) 高可靠性：Hadoop底层维护多个数据副本，即使某个计算元素或存储出现故障，也不会导致数据的丢失。 高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点。 高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。 高容错性：能够自动将失败的任务重新分配。 Hadoop组成 1.x：Common(辅助工具)、HDFS(数据存储)、MapReduce(计算+资源调度) 2.x：Common(辅助工具)、HDFS(数据存储)、Yarn(资源调度)、MapReduce(计算) HDFS架构概述： HDFS全名——Hadoop Distributed File System 组成： NameNode(nn)：存储文件的元数据，如文件名、文件目录结构、文件属性(生成时间、副本数、文件权限)以及每个文件的块列表和块所在的DataNode等——类似书的目录(索引) DataNode(dn)：在本地文件系统存储文件块数据以及块数据的校验和——具体的书章节内容 Secondary NameNode(2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照——辅助NameNode工作 Yarn架构概述 MapReduce架构概述 将计算分为两个阶段：Map和Reduce Map阶段并行处理输入数据 Reduce阶段对Map结果进行汇总 大数据技术生态体系 环境搭建 配置Java环境变量 123456789&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##JAVA_HOMEexport JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_251export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Java版本java -version 配置Hadoop环境变量 12345678910&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##HADOOP_HOMEexport HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;binexport PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Hadoop版本hadoop version Hadoop目录说明 bin目录：存放对Hadoop相关服务(HDFS,YARN)进行操作的脚本 etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 lib目录：存放Hadoop的本地库(对数据进行压缩解压缩功能) sbin目录：存放启动或停止Hadoop相关服务的脚本 share目录：存放Hadoop的依赖jar包、文档、和官方案例 Hadoop运行模式 本地模式 官方WordCount案例(统计单词数目)： 1234567891011&#x2F;&#x2F; 创建wcinput文件夹mkdir wcinput&#x2F;&#x2F; 创建wc.input文件cd wcinputtouch wc.input&#x2F;&#x2F; 编辑wc.input随意输入字符vim wc.input&#x2F;&#x2F; 回到Hadoop目录执行程序hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput&#x2F; wcouput&#x2F;&#x2F; 查看结果cat wcoutput&#x2F;part-r-00000 伪分布式模式 配置集群 设置hadoop-env.sh：在文件中设置JAVA_HOME为本地JDK地址 设置core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; 设置hdfs-site.xml： 12345&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 启动集群 格式化NameNode：bin/hdfs namenode -format 启动NameNode和DataNode：sbin/start-dfs.sh(关闭stop) 查看集群 查看是否启动成功：jps(JDK中的命令,不是Linux命令,类似ps) web端查看HDFS文件系统：http://192.168.232.100:9870(需要CentOS主机上设置关闭防火墙,在3.x版本端口号默认为9870) 查看产生的log日志：cd /hadoop/logs 注意：不能一直格式化NameNode，格式化NameNode会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。(最好关闭NameNode和DataNode) 操作集群(所有命令类似于在Linux Terminal的命令行操作,需要加上固定前缀bin/hdfs dfs -) 在HDFS文件系统上创建一个input文件夹：bin/hdfs dfs -mkdir -p /user/sobxiong/input 将测试文件内容上传到文件系统上：bin/hdfs dfs -put wcinput/wc.input /user/sobxiong/input/ 查看上传的文件是否正确： 12bin/hdfs dfs -ls /user/sobxiong/input/bin/hdfs dfs -cat /user/sobxiong/input/wc.input 运行MapReduce程序：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/sobxiong/input/ /user/sobxiong/output 查看输出结果：bin/hdfs dfs -cat /user/sobxiong/output/* 也可以在浏览器的文件系统中查看 将测试文件内容下载到本地：bin/hdfs dfs -get /user/sobxiong/output/part-r-00000 ./wcoutput/ 删除输出结果：bin/hdfs dfs -rm -r /user/sobxiong/output 启动Yarn并运行MapReduce程序 配置集群 配置yarn-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置yarn-site.xml 1234567891011121314&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置Yarn应用的classPath --&gt;&lt;!-- 不配置出现：错误: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster --&gt;&lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;命令行下输入hadoop classpath的一长串环境&lt;/value&gt;&lt;/property&gt;&lt;!-- 不需要设置yarn.resourcemanager.hostname，可以直接通过ip+端口号的方式访问 --&gt; 配置mapred-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置mapred-site.xml： 123456&lt;!-- 指定MR运行在YARN上 --&gt;&lt;!-- 默认是local，本地文件 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 启动集群 启动前必须保证NameNode和DataNode已启动 启动ResourceManager和NodeManager：sbin/start-yarn.sh(关闭stop) 集群操作 yarn浏览器页面查看：8088端口 删除文件系统上的output文件：bin/hdfs dfs -rm -r /user/sobxiong/output 执行MapReduce程序：同上hadoop操作 查看结果：同上cat操作，也可以在浏览器端查看 配置历史服务器 配置mapred-site.xml： 1234567891011&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;172.16.85.130:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;172.16.85.130:19888&lt;/value&gt;&lt;/property&gt; 启动历史服务器：bin/mapred –daemon start historyserver(stop关闭) 查看历史服务器是否启动：jps 查看JobHistory：http://172.16.85.130:19888/jobhistory 配置日志的聚集： 概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上 好处：可以方便的查看到程序运行详情，方便开发调试 注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryManager 配置yarn-site.xml： 1234567891011&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 配置文件说明 默认配置文件： core-defalut.xml - hadoop-common-3.1.3.jar/core-default.xml hdfs-default.xml - hadoop-hdfs-3.1.3.jar/hdfs-default.xml yarn-default.xml - hadoop-yarn-common-3.1.3.jar/yarn-default.xml mapred-default.xml - hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml 自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置(优先级高) 完全分布式运行模式 虚拟机准备(3台，完全复制) 编写集群分发脚本xsync scp(secure copy)安全拷贝 定义：scp可以实现服务器与服务器之间的数据拷贝 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 scp -r(递归) $pdir/$fname $user@$host:$pdir/$fname rsync远程同步工具 作用：主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点 rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 rsync -r(递归)v(显示复制过程)l(拷贝符号连接) $pdir/$fname $user@$host:$pdir/$fname xsync集群分发脚本 需求：循环复制文件到所有节点的相同目录下 需求分析： rsync命令原始：rsync -rvl /opt/module root@hadoop2:/opt/ 期望脚本：xsync 需同步的文件名 说明：在home/sobxiong/bin这个目录下存放的脚本，sobxiong用户在系统任何地方都可以直接执行 脚本实现 在/home/sobxiong目录下创建bin目录，并在bin目录下创建xsync文件 在xsync中键入如下代码： 12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=2; host&lt;4; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 修改脚本xsync具有执行权限：chmod 777 xsync 调用脚本形式：xsync 文件名 集群配置 集群部署规划： 类型 hadoop1 hadoop2 hadoop3 HDFS NameNode、DataNode DataNode SecondaryNameNode、DataNode YARN NodeManager ResourceManager、NodeManager NodeManager 配置集群 核心配置文件core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; HDFS配置文件hdfs-site.xml 1234567891011&lt;!-- 副本数目 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop3:50090&lt;/value&gt;&lt;/property&gt; YARN配置文件yarn-site.xml 1234567891011&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop2&lt;/value&gt;&lt;/property&gt; MapReduce配置文件mapred-site.xml 12345&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 在集群上分发配置好的hadoop配置文件：xsync /opt/module/hadoop-3.1.3/etc 查看文件分发情况 集群单点启动 集群第一次启动，需要格式化NameNode：hadoop namenode -format(把log和data文件删除) 在hadoop1上启动NameNode：hadoop-daemon.sh start namenode 在hadoop1、2、3上分别启动DataNode：hadoop-daemon.sh start datanode SSH免密登陆配置 配置ssh 基本语法：ssh ip 无密钥配置 免密登录原理： 生成公钥和私钥：ssh-keygen -t rsa(生成文件id_rsa-私钥,id_rsa.pub-公钥) 将公钥拷贝到要免密登录的目标机器上：ssh-copy-id hadoop2(只是当前用户,root还需要另外配置) .ssh文件下(~/.ssh)的文件功能 known_hosts：记录ssh访问过的计算机的公钥 id_rsa：生成的私钥 id_rsa.pub：生成的公钥 authorized_keys：存放授权过的无密登录服务器公钥 群起集群 配置workers：vim etc/hadoop/workers；加入hadoop1、hadoop2、hadoop3(不能有空行和空格)；同步所有节点配置文件 - xsync etc/hadoop/workers 启动集群 集群第一次启动，需要格式化NameNode(格式化前关闭启动的所有namenode和datanode进程,然后再删除data和log数据)：bin/hdfs namenode -format 启动HDFS：sbin/start-dfs.sh(在hadoop1上启动,这样hadoo1、2、3均会启动对应的进程) 启动YARN：sbin/start-yarn.sh(在hadoop2上启动,在ResourceManager所在机器hadoop2上启动Yarn) 查看NameNode：hadoop1:9870 集群基本测试 上传文件到集群：bin/hdfs dfs -put xx xx 查看上传文件存储位置 查看HDFS文件存储路径：/opt/module/hadoop-3.1.3/data/tmp/dfs/data/current/BP-1002151198-172.16.85.130-1591848799222/current/finalized/subdir0/subdir0 查看HDFS在磁盘存储文件的内容：cat blk_xxx(文本文件) 拼接大文件：cat blk_xxx &gt;&gt; temp，最后temp就是初始的文件 集群启动/停止方式总结 各个服务组件逐一启动/停止 分别启动/停止HDFS组件：hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode 启动/停止YARN：yarn-daemon.sh start/stop resourcemanager/nodemanager 各个模块分开启动/停止(配置ssh是前提)常用 整体启动/停止HDFS：start-dfs.sh/stop-dfs.sh 整体启动/停止YARN：start-yarn.sh/stop-yarn.sh 集群时间同步 crontab定时任务： 基本语法：crontab[选项] 选项说明 -e：编辑crontab定时任务 -l：查询crontab任务 -r：删除当前用户所有的crontab任务 参数说明：***** [任务] *的含义： 第一个：一小时当中的第几分钟(0~59) 第二个：一天当中的第几个小时(0~23) 第三个：一个月当中的第几天(1~31) 第四个：一年当中的第几月(1~12) 第五个：一周当中的星期几(0~7,0和7均代表星期日) 特殊符号： ：代表任何时间。比如第一个“”代表一小时中每分钟都执行一次 ,：代表不连续的时间。如“0 8,12,16 * * *”命令，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 -：代表连续的时间范围。比如“0 5 * * 1-6”命令，代表在周一到周六的凌晨5点0分执行命令 /n：代表每隔多久执行一次。比如“/10 * * * *”命令，代表每隔10分钟就执行一遍命令 ntp方式进行同步 具体思路：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 具体实操 时间服务器配置： 检查ntp是否安装：rpm -qa&#124;grep ntp，有ntp、fontpackages-filesystem以及ntpdate 修改ntp配置文件 123456789101112# 授权1172.16.85.0-172.16.85.255网段上的所有机器可以从这台机器上查询和同步时间restrict 172.16.85.0 mask 172.16.85.130.0 nomodify notrap# 修改集群在局域网中,不使用其他互联网上的时间#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst# 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步server 127.127.1.0fudge 127.127.1.0 stratum 10 修改/etc/sysconfig/ntpd文件：SYNC_HWCLOCK=yes(让硬件时间与系统时间一起同步) 重新启动ntpd服务： 12service ntpd statusservice ntpd start 设置ntpd服务开机自启动：chkconfig ntpd on 其他机器配置(root用户)： 配置10分钟与时间服务器同步一次： 12crontab -e*/10 * * * * /usr/sbin/ntpdate hadoop1 修改任意机器时间：date -s “2020-11-11 11:11:11” 十分钟后查看机器是否与时间服务器同步：date Hadoop编译源码 前期准备 jar包准备(hadoop源码、JDK8、Maven、Ant、Protobuf)：Protobuf在Google的github中的Release页面(3.1.3Hadoop对应2.5.0版本) jar包安装 安装JDK 12345678tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module/# JAVA_HOME(/etc/profile)export JAVA_HOME=/opt/module/jdk1.8.0_251export PATH=$PATH:$JAVA_HOME/binsource /etc/profilejava -version 安装Maven 12345678910111213141516tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/# MAVEN_HOME(/etc/profile)export MAVEN_HOME=/opt/module/apache-maven-3.6.3export PATH=$PATH:$MAVEN_HOME/binsource /etc/profilemvn -version# 修改maven仓库镜像&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 安装Ant 12345678tar -zxvf apache-ant-1.10.8-bin.tar.gz -C /opt/module/# ANT_HOME(/etc/profile)export ANT_HOME=/opt/module/apache-ant-1.10.8export PATH=$PATH:$ANT_HOME/binsource /etc/profileant -version 安装glibc-headers和g++：yum install glibc-headers、yum install gcc-c++ 安装make和cmake：yum install make 安装cmake(要装3.x版本,低版本编译不通过) 1234567891011tar -zxvf cmake-3.17.3.tar.gz -C /opt/modulecd /opt/module/cmake-3.17.3./configuremakemake install# CMAKE_HOME(/etc/profile)export CMAKE_HOME=/opt/module/cmake-3.17.3export PATH=$PATH:$CMAKE_HOME/binsource /etc/profilecmake --version 安装protobuf： 12345678910111213tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/cd /opt/module/protobuf-2.5.0/./configuremakemake checkmake installldconfig# LD_LIBRARY_PATH(/etc/profile)export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0export PATH=$PATH:$LD_LIBRARY_PATHprotoc --version 安装openssl库：yum install openssl-devel 安装ncurses-devel库：yum install ncurses-devel 编译源码 解压源码到/opt目录 进入hadoop源码主目录 通过maven执行编译命令：mvn package -Pdist,native -DskipTests -Dtar HDFS概述 HDFS产出背景及定义 产生背景：随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种 定义：HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色 使用背景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用 HDFS优缺点 优点： 高容错性 数据自动保存多个副本。它通过增加副本的形式，提高容错性 某一个副本丢失以后，它可以自动恢复(通过将副本复制到另一个可用的节点) 适合处理大数据 数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据 文件规模：能够处理百万规模以上的文件数量，数量相当之大 可构建在廉价机器上，通过多副本机制，提高可靠性 缺点： 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的 无法高效的对大量小文件进行存储： 存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的 小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标 不支持并发写入、文件随机修改： 一个文件只能有一个写，不允许多个线程同时写 仅支持数据appen(追加)，不支持文件的随机修改 HDFS组成架构 NameNode(nn)：Master，一个主管、管理者 管理HDFS的名称空间 配置副本策略 管理数据块(Block)映射信息 处理客户端读写请求 DataNode：Slave。NameNode下达命令，DataNode执行实际的操作 存储实际的数据块 执行数据块的读/写操作 Client：客户端 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传 与NameNode交互，获取文件的位置信息 与DataNode交互，读取或者写入数据 Client提供一些命令来管理HDFS，比如NameNode格式化 Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作 Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务 辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode 在紧急情况下，可辅助恢复NameNode HDFS文件块大小HDFS中的文件在物理上是分块存储(Block)，块的大小可以通过配置参数(dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M为什么文件块的大小不能设置太小，也不能设置太大？ HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢 总结：HDFS块的大小设置主要取决于磁盘传输速率 HDFS的Shell操作 基本语法bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令其中dfs是fs的实现类 命令大全：bin/hadoop fs 使用命令： -help：输出命令的帮助(hadoop fs -help rm) -ls：显示目录信息(hadoop fs -ls /) -mkdir：在HDFS上创建目录[-p递归](hdoop fs -mkdir -p /sobxiong/test) -moveFromLocal：从本地剪切粘贴到HDFS[前路径为本地,后路径为HDFS](hadoop fs -moveFromLocal ./test.txt /sobxiong/test/) -appendToFile：追加一个文件到已经存在的文件末尾[前路径为本地,后路径为HDFS](hadoop fs -appendToFile ./test.txt /sobxiong/test/test.txt) -cat：显示文件内容(hadoop fs -cat /sobxiong/test/test.txt) -chgrp、-chmod、-chown：修改文件所属的权限，同Linux文件系统中的用法 -copyFromLocal：从本地文件系统拷贝文件到HDFS中，同-moveFromLocal -copyToLocal：从HDFS拷贝文件到本地[前路径为HDFS,后路径为本地](hadoop fs -copyToLocal /sobxiong/test/test.txt ./) -cp：把文件从HDFS的一个路径拷贝到HDFS的另一个路径 -mv：把文件从HDFS的一个路径移动到HDFS的另一个路径 -get：等同于copyToLocal(用法同copyToLocal)，从HDFS下载文件到本地 -getmerge：合并下载多个文件(hadoop fs -getmerge /sobxiong/test/* ./all.txt) -put：等同于copyFromLocal(用法同copyFromLocal) -tail：显示一个文件的末尾(hadoop fs -tail /sobxiong/test/test.txt) -rm：删除文件或文件夹[-r递归删除目录] -rmdir：删除空目录 -du：统计文件夹的大小信息[-h显示单位,-s总和](hadoop fs -du -h -s /) -setrep：设置HDFS中文件的副本数目[这里设置的副本数只是记录在NameNode的元数据中,是否真的会有这么多副本还得看DataNode的数量.因为目前只有3台设备,最多也就3个副本,只有节点数的增加到10台时,副本数才能达到10;只要加入一台设备,就会把副本复制到设备上,直到加到10台](hadoop fs -setrep 10 /sobxiong/test/test.txt) HDFS客户端操作 客户端环境准备 将Hadoop安装到mac上，并设置环境变量 123456# HADOOP_HOME(~/.bash_profile)export HADOOP_HOME=\"/Users/sobxiong/module/hadoop-3.1.3\"export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource ~/.bash_profilehadoop version 创建Maven工程测试：idea创建quickstart项目 导入依赖： 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt; 创建测试类 12345678910111213141516public class HDFSClient &#123; public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException &#123; Configuration configuration = new Configuration(); // configuration.set(\"fs.defaultFS\", \"hdfs://hadoop1:9000\"); // 1、获取hdfs客户端对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、在hdfs上创建路径 fileSystem.mkdirs(new Path(\"/sobxiong2/test\")); // 3、关闭资源 fileSystem.close(); System.out.println(\"finish\"); &#125;&#125; HDFS的API操作 文件上传 12345678910111213141516171819202122232425262728293031/** * 参数优先级： * 1、客户端代码中设置的值 * 2、ClassPath(resources)下的用户自定义配置文件(hdfs-site.xml) * 3、服务器的默认配置 * @throws Exception */// 1、文件上传@Testpublic void testCopyFromLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); configuration.set(\"dfs.replication\", \"2\"); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行上传API fileSystem.copyFromLocalFile(new Path(\"/Users/sobxiong/Documents/文件块大小大致计算.png\"), new Path(\"/sobxiong/test2.png\")); // 3、关闭资源 fileSystem.close();&#125;// 将hdfs-site.xml拷贝至项目的根目录resources资源文件夹下&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 文件下载 123456789101112131415// 2、文件下载@Testpublic void testCopyToLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行下载操作 // fileSystem.copyToLocalFile(new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test.png\")); // 本地模式,true,不会产生crc文件 fileSystem.copyToLocalFile(false, new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test1.png\"), true); // 3、关闭资源 fileSystem.close();&#125; 文件删除 12345678910111213// 3、文件删除@Testpublic void testDelete() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、文件删除(第二个参数,是否递归删除,文件夹时有效) fileSystem.delete(new Path(\"/sobxiong/test2.png\"), false); // 3、关闭资源 fileSystem.close();&#125; 文件更名 12345678910111213// 4、文件更名@Testpublic void testRename() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行更名操作 fileSystem.rename(new Path(\"/sobxiong/test1.png\"), new Path(\"/sobxiong/1tset.png\")); // 3、关闭资源 fileSystem.close();&#125; 文件详情查看 1234567891011121314151617181920212223242526272829303132// 5、文件详情查看@Testpublic void testListFiles() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、查看文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(new Path(\"/\"), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); // 查看文件名称、权限、长度 System.out.println(\"name: \" + fileStatus.getPath().getName()); System.out.println(\"permission: \" + fileStatus.getPermission()); System.out.println(\"length: \" + fileStatus.getLen()); // 查看块信息 BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(\"host = \" + host); &#125; System.out.println(\"----------------\"); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; 判断是文件还是文件夹 1234567891011121314151617181920// 6、判断是文件还是文件夹@Testpublic void testListStatus() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、判断操作 FileStatus[] fileStatuses = fileSystem.listStatus(new Path(\"/\")); for (FileStatus fileStatus : fileStatuses) &#123; if (fileStatus.isFile()) &#123; System.out.println(\"file = \" + fileStatus.getPath().getName()); &#125; else &#123; System.out.println(\"dir = \" + fileStatus.getPath().getName()); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; HDFS的I/O流操作 HDFS文件上传 1234567891011121314151617// 把本地文件上传到HDFS根目录@Testpublic void upload() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FileInputStream fileInputStream = new FileInputStream(new File(\"/Users/sobxiong/Downloads/课件.rar\")); // 3、获取输出流 FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path(\"/test.rar\")); // 4、流的对拷 IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fsDataOutputStream); IOUtils.closeStream(fileInputStream); fileSystem.close();&#125; HDFS文件下载 1234567891011121314151617// 从HDFS下载文件到本地磁盘@Testpublic void download() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/test.rar\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/test1.rar\")); // 4、流的对拷 IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125; 定位文件获取 12345678910111213141516171819202122232425262728293031323334353637383940414243// 下载第一块@Testpublic void readFileSeek1() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part1\")); // 4、流的对拷(只拷贝第一个块128MB) byte[] buf = new byte[1024]; for (int i = 0; i &lt; 1024 * 128; i++) &#123; fsDataInputStream.read(buf); fileOutputStream.write(buf); &#125; // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载第二块@Testpublic void readFileSeek2() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、设置指定读取的起点 fsDataInputStream.seek(1024 * 1024 * 128); // 4、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part2\")); // 5、流的对拷(拷贝剩下的两个Block块) IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 6、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载完第一块和剩余的部分后,可通过cat hadoop-3.1.3.tar.gz.part2 &gt;&gt; hadoop-3.1.3.tar.gz.part1将剩余部分追加到第一块上,修改文件名(删去.part1),就得到完整的文件 HDFS的数据流 HDFS写数据流程 剖析文件写入： 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在 NameNode返回是否可以上传 客户端请求第一个Block上传到哪几个DataNode服务器上 NameNode返回3个DataNode节点，分别为dn1、dn2、dn3 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成 dn1、dn2、dn3逐级应答客户端 客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器(此后重复执行3-7步) 网络拓扑-节点距离计算在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。节点距离：两个节点到达最近的共同祖先的距离总和 机架感知(2.7.2版本副本节点选择,性能和安全的综合考量) 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个 第二个副本和第一个副本位于相同机架，随机节点 第三个副本位于不同机架，随机节点 HDFS读数据流程 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址 挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据 DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验) 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件 NameNode和SecondaryNameNode NN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并 第一阶段：NameNode启动 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存 客户端对元数据进行增删改的请求 NameNode记录操作日志，更新滚动日志(先记日志,类似数据库) NameNode在内存中对数据进行增删改 第二阶段：Secondary NameNode工作 Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果 Secondary NameNode请求执行CheckPoint NameNode滚动正在写的Edits日志 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode Secondary NameNode加载编辑日志和镜像文件到内存，并合并 生成新的镜像文件fsimage.chkpoint 拷贝fsimage.chkpoint到NameNode NameNode将fsimage.chkpoint重新命名成fsimage 补充：Fsimage：NameNode内存中元数据序列化后形成的文件。Edits：记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)。NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息)，如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并(所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage)。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。SecondaryNameNode首先会询问NameNode是否需要CheckPoint(触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了)。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中 Fsimage和Edits解析 概念 NameNode被格式化之后，将在/data/tmp/dfs/name/current目录中产生如下文件fsimage_0000000000000000000fsimage_0000000000000000000.md5seen_txidVERSION Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息 Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中 seen_txid文件保存的是一个数字，就是最后一个edits_的数字 每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并 查看Fsimage文件：hdfs oiv -p 文件类型(XML) -i 镜像文件 -o 转换后文件输出路径例：hdfs oiv -p XML -i fsimage_0000000000000000025 o fsimage.xmlFsimage中没有记录块所对应的DataNode，为什么？在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报 查看Edits文件：hdfs oev -p 文件类型(XML) -i 编辑日志 -o 转换后文件输出路径例：hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o edits.xmlNameNode如何确定下次开机启动的时候合并那些Edits？通过seen_txid查看 CheckPoint时间设置 通常情况下，SecondaryNameNode每隔一小时执行一次 一分钟检查一次操作次数 当操作次数达到1百万时，SecondaryNameNode执行一次 123456789101112131415&lt;!-- hdfs-default.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt;1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; NameNode故障处理 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录 kiil -9 NameNode进程编号(用jps查看NameNode的进程编号) 删除NameNode存储的数据(data/tmp/dfs/name)：rm -rf /data/tmp/dfs/name/* 拷贝SecondaryNameNode(hadoop2)中数据到原NameNode(hadoop1)存储数据目录：scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary/* ./name/ 重新启动NameNode(hadoop1)：sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中 修改hdfs-site.xml(加入下述内容)： 123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; kill -9 NameNode进程 删除NameNode存储的数据(同方法一) 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 123scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary /data/tmp/dfs/cd /data/tmp/dfs/namesecondaryrm -rf in_use.lock 导入检查点数据(等待一会ctrl+c结束掉) 启动NameNode：sbin/hadoop-daemon.sh start namenode 集群安全模式 概述 NameNode启动NameNode启动时，首先将镜像文件(Fsimage)载入内存，并执行编辑日志(Edits)中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的 DataNode启动 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统 安全模式退出判断如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值：dfs.replication.min=1)。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式 基本语法集群处于安全模式，不能执行重要操作(写操作)。集群启动完成后，自动退出安全模式 查看安全模式状态：bin/hdfs dfsadmin -safemode get 进入安全模式状态：bin/hdfs dfsadmin -safemode enter 离开安全模式状态：bin/hdfs dfsadmin -safemode leave 等待安全模式状态：bin/hdfs dfsadmin -safemode wait 案例模拟等待安全模式 查看当前模式：bin/hdfs dfsadmin -safemode get 先进入安全模式：bin/hdfs dfsadmin -safemode enter 创建并执行下面的脚本 12345678910touch safemode.shvim safemode.sh# safemode.sh#!/bin/bashhdfs dfsadmin -safemode waithdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /chmod 777 safemode.sh./safemode.sh 再打开一个窗口，执行：hdfs dfsadmin -safemode leave 安全模式退出，HDFS集群上已经有上传的数据了 NameNode多目录配置 NameNode的本地目录可以配置成多个，但每个目录存放内容相同(相当于备份)，增加了可靠性 具体配置如下 在hdfs-site.xml文件中增加如下内容 1234&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 停止集群，删除data和logs中所有数据 12345hadoop2：sbin/stop-yarn.shhadoop2：rm -rf data/ logs/hadoop1：sbin/stop-dfs.shhadoop2：rm -rf data/ logs/shadoop3：rm -rf data/ logs/s 格式化集群并启动 123hadoop1：bin/hdfs namenode -formathadoop1：sbin/start-dfs.shhadoop2：sbin/start-yarn.sh 查看结果：dfs目录下出现两个目录name1和name2 DataNode DataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳 DataNode启动后向NameNode注册，通过后，周期性(1小时)的向NameNode上报所有的块信息 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用 集群运行中可以安全加入和退出一些机器 数据完整性DataNode节点保证数据完整性的方法： 当DataNode读取Block的时候，它会计算CheckSum(类似crc校验位) 如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏 Client读取其他DataNode上的Block DataNode在其文件创建后周期验证CheckSum 掉线时限参数设置\bhdfs-default.xml： 12345678910&lt;!-- heartbeat.recheck.interval单位为毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;!-- dfs.heartbeat.interval单位为秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 服役新数据节点(hadoop4未服役) 需求：随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点 环境准备 利用hadoop3主机再克隆一台hadoop4主机 修改hadoop4主机IP地址和主机名称 在hadoop1主机上将/etc/hosts下添加hadoop4的ip地址映射条目，并分发到hadoop2-4 hadoop4主机删除原来HDFS文件系统留存的文件(data和log目录)——不然会发生3和4轮换出现的问题,因为3和4有着一样的data和log reboot重启加载配置 服役新节点具体步骤 hadoop1-3按之前步骤已启动 在hadoop4主机上单独启动： 12hdfs --daemon start datanodeyarn --daemon start nodemanager 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 刷新http://hadoop1:9870web页面，等待 在hadoop4上上传文件 如果数据不均衡，可以使用命令实现集群的在平衡：sbin/start-balancer.sh 结束后在workers文件中加入hadoop4，之后直接start-dfs.sh和start-yarn.sh即可启动 退役旧数据节点(hadoop4未退役) 添加白名单(hadoop4未退役)添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出 在NameNode的hadoop-3.1.3/etc/hadoop目录下创建dfs.hosts文件 1234567touch dfs.hostsvim dfs.hosts# dfs.hosts(不添加hadoop4,不允许有空行和空格)hadoop1hadoop2hadoop3 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 配置文件分发：xsync hdfs-site.xml;xsync dfs.hosts 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 在web页面刷新等待 黑名单退役(hadoop4未退役)在黑名单上面的主机都会被强制退出。注意：不允许白名单和黑名单中同时出现同一个主机名称 在hadoop-3.1.3/etc/hadoop下创建dfs.hosts.exclude文件，并加入要退役节点 12345touch dfs.hosts.excludevim dfs.hosts.exclude# dfs.hosts.excludehadoop4 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 刷新web页面等待 DataNode多目录配置 DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 需要在hdfs-site.xml上修改配置 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; 关闭当前运行的hadoop节点 删除各节点的data和log目录 格式化dataNode 重启部署hadoop节点 HDFS2.X新特性 集群间数据拷贝 scp实现两个远程主机之间的文件复制 123456# 从当前主机向目的主机 推 pushscp -r hello.txt root@hadoop2:/user/sobxiong/hello.txt# 从目的主机向当前主机 拉 pullscp -r root@hadoop2:/user/sobxiong/hello.txt hello.txt# 通过本主机中转实现两个远程主机的文件复制scp -r root@hadoop2:/user/sobxiong/hello.txt root@hadoop3:/user/sobxiong 采用distcp命令实现两个Hadoop集群之间的递归数据复制 1hadoop distcp hdfs://haoop1:9000/user/sobxiong/hello.txt hdfs://hadoop2:9000/user/sobxiong/hello.txt 小文件存档 HDFS存储小文件弊端每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB 解决存储小文件办法之一HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存 实际操作 启动YARN进程：start-yarn.sh(hadoop2) 归档文件(归档后的路径不得实现存在)把/sobxiong目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/sobxiongOutput路径下：hadoop archive -archiveName input.har -p /sobxiong /sobxiongOutput 查看归档： 123456789101112# 普通查看文件命令hadoop fs -ls R /sobxiongOutput/input.har# -rw-r--r-- 3 sobxiong supergroup 0 2020-06-24 16:06 /sobxiongOutput/input.har/_SUCCESS# -rw-r--r-- 3 sobxiong supergroup 305 2020-06-24 16:06 /sobxiongOutput/input.har/_index# -rw-r--r-- 3 sobxiong supergroup 23 2020-06-24 16:06 /sobxiongOutput/input.har/_masterindex# -rw-r--r-- 3 sobxiong supergroup 317029 2020-06-24 16:06 /sobxiongOutput/input.har/part-0# 采用har格式查看文件(可以像以往一样操作内部文件,也需要har格式)hadoop fs -ls R har:///sobxiongOutput/input.har# -rw-r--r-- 3 sobxiong supergroup 84942 2020-06-21 11:38 har:///sobxiongOutput/input.har/1tset.png# -rw-r--r-- 3 sobxiong supergroup 84942 2020-06-21 11:33 har:///sobxiongOutput/input.har/test.png# -rw-r--r-- 3 sobxiong supergroup 147145 2020-06-23 13:52 har:///sobxiongOutput/input.har/test6.txt 解归档文件：hadoop fs -cp har:///sobxiongOutput/input.har/* / 回收站开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用 开启回收站功能参数说明： 默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间(分钟为单位) 默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。该值设置和fs.trash.interval的参数值相同(要求fs.trash.checkpoint.interval &lt;= fs.trash.interval) 回收站工作机制 启动回收站：修改core-site.xml，配置垃圾回收时间为1分钟 1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 查看回收站：回收站在集群中的路径：/user/sobxiong/.Trash/ 修改访问回收站用户名称：进入垃圾回收站用户名称，默认是dr.who，修改为sobxiong(同样是core-site.xml) 1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt; 恢复回收站数据：hadoop fs -mv /user/sobxiong/.Trash/Current/user/sobxiong/input / 清空回收站：hadoop fs -expunge 快照管理 命令介绍 实际操作 开启/禁用指定目录的快照功能：hdfs dfsadmin -allowSnapshot(-disallowSnapshot) /user/sobxiong/input 对目录创建快照 1234567hdfs dfs -createSnapshot /user/sobxiong/input# 快照和源文件使用相同数据hdfs dfs -ls R /user/sobxiong/input/.snapshot/# 指定名称创建快照hdfs dfs -createSnapshot /user/sobxiong/input test 重命名快照：hdfs dfs -renameSnapshot /user/sobxiong/input test test01 列出当前用户所有可快照目录：hdfs lsSnapshottableDir 比较两个快照目录的不同之处(可以是源文件和快照,使用’.’,此时”.snapshot/name”用于指定具体快照)：hdfs snapshotDiff /user/sobxiong/input . .snapshot/test01 恢复快照：hdfs dfs -cp /user/sobxiong/input/.snapshot/s20200624-134303.027 / MapReduce概述 MapReduce定义 MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上 MapReduce优缺点 优点： MapReduce易于编程它简单地实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行 良好的扩展性(hadoop)当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力 高容错性MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的 适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力 缺点： 不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果 擅长流式计算流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的 不擅长DAG(有向图)计算多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下 核心思想 MapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调 MapTask：负责Map阶段的整个数据处理流程 ReuceTask：负责Reduce阶段的整个数据处理流程 常用数据序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable MapReduce编程规范用户编写的程序分成三个部分：Mapper、Reducer和Driver Mapper 用户自定义的Mapper要继承自己的父类 Mapper的输入数据是KV对的形式(KV的类型可自定义) Mapper中的业务逻辑写在map()方法中 Mapper的输出数据是KV对的形式(KV的类型可自定义) map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次 Reducer 用户自定义的Reducer要继承自己的父类 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 Driver：相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象 WordCount案例实操 需求：在给定的文本文件中统计输出每一个单词出现的总次数 需求分析 环境准备 创建maven空项目 改pom 123456789101112131415161718192021222324252627282930&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.13.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt; 在resources资源文件夹下新建log4j.properties文件 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 编写MapReduce程序 编写mapper类 12345678910111213141516171819202122232425262728/*** map阶段* KEYIN：输入数据的key类型(默认写LongWritable:偏移量)* VALUEIN：输入数据的value类型* KEYOUT：输出数据的key类型* VALUEOUT：输出数据的value类型* &lt;sobxiong,1&gt;输出数据* 输出作为reduce阶段的输入*/public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private Text k = new Text(); private IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // sobxiong sobxiong // 1、获取一行 String lineStr = value.toString(); // 2、切割单词 String[] words = lineStr.split(\" \"); // 3、循环写出 for (String word : words) &#123; // &lt;sobxiong,1&gt; k.set(word); context.write(k, v); &#125; &#125;&#125; 编写Reducer类 1234567891011121314151617181920/*** reduce阶段* KEYIN,VALUEIN：map阶段输出的kv* KEYOUT,VALUEOUT：reduce输出的kv*/public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; // 1、累加求和 for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 &lt;sobxiong,2&gt; v.set(sum); context.write(key, v); &#125;&#125; 编写Driver驱动类 123456789101112131415161718192021222324252627public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设置jar存储位置 job.setJarByClass(WordCountDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job // job.submit(); // true：打印一些信息 boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1); &#125;&#125; 本地测试：启动旁边的下箭头Edit Configuration，新增Application，选择Main Class并在Program arguments中加入两个参数中间用空格隔开，前者是input文件所在文件夹，后者是输出文件夹，要求不能存在(不然会出错)。例如：/Users/sobxiong/Downloads/input /Users/sobxiong/Downloads/ouputTest 在集群上测试 用maven打jar包，添加打包插件依赖 12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.xiong.hadoop.WordCountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 将程序打成jar包，maven install即可 将获取的两个jar包(一个不带依赖,另一带)，将不带依赖的jar上传到hadoop1主机上 启动Hadoop集群 执行WordCount程序：hadoop jar WordCount.jar com.xiong.hadoop.WordCountDriver /sobxiong /outputTest(第四个参数为启动的主类名,第五个为输入文件所在文件夹,第六个为输出文件夹——不能事先存在) Hadoop序列化 序列化概述 什么是序列化：序列化就是把内存中的对象，转换成字节序列(或其他数据传输协议)以便于存储到磁盘(持久化)和网络传输；反序列化就是将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象 为什么要序列化：一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机 为什么不用Java的序列化：Java的序列化是一个重量级序列化框架(Serializable)，一个对象被序列化后，会附带很多额外的信息(各种校验信息，Header，继承体系等)，不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制(Writable) Hadoop序列化特点： 紧凑：高效使用存储空间 快速：读写数据的额外开销小 可扩展：随着通信协议的升级而可升级 互操作：支持多语言的交互 自定义bean对象实现序列化接口(Writable)在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。具体实现bean对象序列化步骤如下7步： 实现Writable接口 反序列化时，需要反射调用空参构造函数，所以必须有空参构造 重写序列化方法 重写反序列化方法(注意反序列化的顺序和序列化的顺序完全一致) 要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用 如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框架中的Shuffle过程要求对key必须能排序 序列化案例实操 需求：统计每一个手机号耗费的总上行流量、下行流量、总流量 案例分析 编写MapReduce程序 编写流量统计的Bean对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*测试数据1;13736230513;192.196.100.1;www.atguigu.com;2481;24681;2002;13846544121;192.196.100.2;264;0;2003;13956435636;192.196.100.3;132;1512;2004;13966251146;192.168.100.1;240;0;4045;18271575951;192.168.100.2;www.atguigu.com;1527;2106;2006;84188413;192.168.100.3;www.atguigu.com;4116;1432;2007;13590439668;192.168.100.4;1116;954;2008;15910133277;192.168.100.5;wwww.haol23.com;3156;2936;2009;13729199489;192.168.100.6;240;0;20010;13630577991;192.168.100.7;www.shouhu.com;6960;690;20011;15043685818;192.168.100.8;www.baidu.com;3659;3538;20012;15959002129;192.168.100.9;www.atguigu.com;1938;180;50013;13560439638;192.168.100.10;918;4938;20014;13470253144;192.168.100.11;180;180;20015;13682846555;192.168.100.12;wwww.qq.com;1938;2910;20016;13992314666;192.168.100.13;www.gaga.com;3008;3720;20017;13509468723;192.168.100.14;www.qinghua.com;7335;110349;40418;18390173782;192.168.100.15;www.sogou.com;9531;2412;20019;13975057813;192.168.100.16;www.baidu.com;11058;48243;20020;13768778790;192.168.100.17;120;120;20021;13568436656;192.168.100.18;www.alibaba.com;2481;24681;20022;13568436656;192.168.100.19;1116;954;200*/public class FlowBean implements Writable &#123; // 上行流量 private long upFlow; // 下行流量 private long downFlow; // 总流量 private long sumFlow; // 空参构造,为了后续反射 public FlowBean() &#123; &#125; // 序列化方法 @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeLong(upFlow); dataOutput.writeLong(downFlow); dataOutput.writeLong(sumFlow); &#125; // 反序列化方法 @Override public void readFields(DataInput dataInput) throws IOException &#123; // 必须要求和序列化方法顺序一致 upFlow = dataInput.readLong(); downFlow = dataInput.readLong(); sumFlow = dataInput.readLong(); &#125; @Override public String toString() &#123; return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125;&#125; 编写mapper类 123456789101112131415161718public class FlowMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; private Text k = new Text(); private FlowBean flowBean = new FlowBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割\\t String[] fields = lineStr.split(\"\\t\"); // 3、封装对象 k.set(fields[1]); long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); flowBean.set(upFlow, downFlow); // 4、写出 context.write(k, flowBean); &#125;&#125; 编写Reducer类 12345678910111213141516public class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; private FlowBean v = new FlowBean(); @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 long sumUpFlow = 0; long sumDownFlow = 0; for (FlowBean value : values) &#123; sumUpFlow += value.getUpFlow(); sumDownFlow += value.getDownFlow(); &#125; // 2、写出 v.set(sumUpFlow, sumDownFlow); context.write(key, v); &#125;&#125; 编写Driver驱动类 12345678910111213141516171819202122public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设计jar路径 job.setJarByClass(FlowDriver.class); // 3、关联mapper和reducer job.setMapperClass(FlowMapper.class); job.setReducerClass(FlowReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce框架原理 InputFormat数据输入 切片与MapTask并行度决定机制 问题引出：MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ MapTask并行度决定机制 数据块：Block是HDFS物理上把数据分成一块一块数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储 Job提交流程源码和切片源码详解 Job提交流程源码详解 1234567891011121314151617181920212223242526waitForCompletion();submit(); // 1、建立连接 connect(); // 1)创建提交Job的代理 new Cluster(getConfiguration()); // 判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2、提交job submitter.submitJobInternal(Job.this, cluster); // 1)创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2)获取jobid,并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3)拷贝jar包到集群 copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // 4)计算切片,生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job); // 5)向Stag路径写XML配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 6)提交Job,返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); FileInputFormat切片源码解析(input.getSplits(job)) 程序先找到你数据存储的目录 开始遍历处理(规划切片)目录下的每一个文件 遍历第一个文件ss.txt 获取文件大小fs.sizeOf(ss.txt) 计算切片大小：computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M(YARN集群默认128M——2.x,62M-1.x;本地运行默认32M) 默认情况下，切片大小=blocksize 开始切，形成第1个切片：ss.txt—0:128M、第2个切片ss.txt—128:256M、第3个切片ss.txt—256M:300M(每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片) 将切片信息写到一个切片规划文件中 整个切片的核心过程在getSplit()方法中完成 InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等 提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数 FileInputFormat切片机制 切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于Block大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 案例分析 输入数据有两个文件：file1.txt - 320M;file2.txt - 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： 文件 切片区间 file1.txt.split1 0~128 file1.txt.split2 129~256 file1.txt.split3 257~320 file2.txt.split1 0~10 切片大小参数配置 源码中计算切片大小的公式：Math.max(minSize, Math.min(maxSize, blockSize));mapreduce.input.fileinputformat.split.minsize=1 默认值为1mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值Long.MAXValue 默认情况下，切片大小=blocksize 切片大小设置maxsize(切片最大值)：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值minsize(切片最小值)：参数调的比blockSize大，则可以让切片变得比blockSize还大 获取切片信息API 1234// 获取切片的文件名称String name = inputSplit.getPath().getName();// 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) context.getInputSplit(); CombineTextInputFormat切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下 应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理 虚拟存储切片最大值设置：CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);(注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值) 切片机制切片机制：生成切片过程包括虚拟存储过程和切片过程二部分 虚拟存储过程将输入目录下所有文件的大小依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块(防止出现太小切片)例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成(2.01M和2.01M)两个文件 切片过程 判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片 如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片 测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M、(2.55M、2.55M)、3.4M、(3.4M、3.4M)。最终会形成3个切片，大小分别为：(1.7M + 2.55M)，(2.55M + 3.4M)，(3.4M + 3.4M) CombineTextInputFormat案例实操 需求：将输入的大量小文件合并成一个切片统一处理 输入数据：准备4个小文件 期望：期望一个切片处理4个文件 实现过程 不做任何处理，运行之前的WordCount案例程序，观察切片个数为4——(number of splits:4) 在WordcountDriver中增加如下代码(设置切片最大值为4m)，运行程序，观察运行的切片个数为3 1234// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304); 在WordcountDriver中增加如下代码(设置切片最大值为20m)，运行程序，观察运行的切片个数为1(代码同上,值改为20971520) FileInputFormat实现类思考：在运行MapReduce程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢?FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等 TextInputFormatTextInputFormat是默认的FileInputFormat实现类。按行读取每条记录，键是存储该行在整个文件中的起始字节偏移量——LongWritable类型；值是这行的内容，不包括任何行终止符(换行符和回车符)——Text类型 1234567891011&#x2F;&#x2F; 示例：Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对：(0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为&lt;key,value&gt;对。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “\\t”)来设定分隔符——默认分隔符是tab(\\t) 1234567891011&#x2F;&#x2F; 示例(其中——&gt;表示一个水平方向的制表符)：line1 ——&gt;Rich learning formline2 ——&gt;Intelligent learning engineline3 ——&gt;Learning more convenientline4 ——&gt;From the real demand for more close to the enterprise&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对(键是每行排在制表符之前的Text序列)：(line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) NLineInputFormat如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按Block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数 / N = 切片数，如果不整除，切片数 = 商 + 1 123456789101112&#x2F;&#x2F; 示例：Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise&#x2F;&#x2F; 如果N是2,则每个输入分片包含两行。开启2个MapTask(键和值与TextInputFormat生成的一样)：(0,Rich learning form)(19,Intelligent learning engine)&#x2F;&#x2F; 另一个mapper则收到后两行：(47,Learning more convenient)(72,From the real demand for more close to the enterprise) KeyValueTextInputFormat使用案例 需求：统计输入文件中每一行的第一个单词相同的行数 案例分析 代码实现 Mapper类 123456789public class KVTextMapper extends Mapper&lt;Text, Text, Text, IntWritable&gt; &#123; private IntWritable intWritable = new IntWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、封装对象 // 2、写出 context.write(key, intWritable); &#125;&#125; Reducer类 1234567891011121314public class KVTextReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable intWritable = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 intWritable.set(sum); context.write(key, intWritable); &#125;&#125; Driver类 12345678910111213141516171819202122232425public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration configuration = new Configuration(); configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \"); Job job = Job.getInstance(configuration); // 2、设置jar存储位置 job.setJarByClass(KVTextDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setInputFormatClass(KeyValueTextInputFormat.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; NLineInputFormat使用案例 需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中 案例分析 代码实现 Mapper和Reducer类参照WordCount Driver类 123456789101112131415161718192021222324252627public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 设置切片InputSplit中划分三条记录 NLineInputFormat.setNumLinesPerSplit(job, 3); // 使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class); // 2、设置jar存储位置 job.setJarByClass(NLineDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(NLineMapper.class); job.setReducerClass(NLineReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; 观察控制台打印的number of splits 自定义InputFormat在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题自定义InputFormat步骤如下： 自定义一个类继承FileInputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件 自定义InputFormat案例实操 需求：将多个小文件合并成一个SequenceFile文件(SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式)，SequenceFile里面存储着多个文件，存储的形式为key——文件路径 + 名称，value——文件内容 案例分析 代码实现 自定义InputFormat 12345678public class WholeFileInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt; &#123; @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; 自定义RecordReader类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt; &#123; private FileSplit split; private Configuration configuration; private Text k = new Text(); private BytesWritable v = new BytesWritable(); boolean isProgress = true; @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; // 初始化 this.split = (FileSplit) split; configuration = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; // 核心业务逻辑 // 每个文件创建一次reader if (isProgress) &#123; // 1、获取fs对象 Path path = split.getPath(); FileSystem fileSystem = path.getFileSystem(configuration); // 2、获取输入流 FSDataInputStream fis = fileSystem.open(path); // 3、拷贝 byte[] buffer = new byte[(int) split.getLength()]; IOUtils.readFully(fis, buffer, 0, buffer.length); // 4、封装v v.set(buffer, 0, buffer.length); // 5、封装key k.set(path.toString()); // 6、关闭资源 IOUtils.closeStream(fis); isProgress = false; return true; &#125; return false; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return k; &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return v; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123;&#125;&#125; Mapper类 123456public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; Reducer类 123456789public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 循环写出 for (BytesWritable value : values) &#123; context.write(key, value); &#125; &#125;&#125; Driver类 123456789101112131415161718192021222324public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setInputFormatClass(WholeFileInputFormat.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); // 2、设计jar路径 job.setJarByClass(SequenceFileDriver.class); // 3、关联mapper和reducer job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce工作流程 流程示意图 流程详解上面的流程是整个MapReduce的全部工作流程，Shuffle过程是从第7步开始到第16步结束，具体Shuffle过程详解，如下： MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序 ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据 ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并(归并排序) 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程——从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法 注意Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M Shuffle Shuffle机制：Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle Partition分区 问题引出：要求将统计结果按照条件输出到不同文件中(分区)。比如：将统计结果按照手机归属地不同省份输出到不同文件中(分区) 默认Partitionr分区：默认分区是根据key的hashCode对ReduceTasks个数取模得到的(如果分区数大于1)。用户没法控制哪个key存储到哪个分区 1234567public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; &#123; public void configure(JobConf job) &#123;&#125; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 自定义Partitioner步骤 自定义类继承Partitioner，重写getPartition()方法 在Job驱动中，设置自定义Partitioner：job.setPartitionerClass(CustomPartitioner.class); 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask：job.setNumReduceTasks(5); 分区总结 如果ReduceTask的数量 &gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx 如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，会抛出异常 如果ReduceTask的数量 = 1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000 分区号必须从零开始，逐一累加 案例分析：假设自定义分区数为5，则 job.setNumReduceTasks(1)：会正常运行，只不过会产生一个输出文件 job.setNumReduceTasks(2)：会报错 job.setNumReduceTasks(6)：大于5，程序会正常运行，会产生空文件part-r-00005 Partition分区案例实操 需求：将统计结果按照手机归属地不同省份输出到不同文件中(分区) 案例分析 实操 在FlowBean案例基础上，增加一个分区类 12345678910111213141516171819public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text text, FlowBean flowBean, int numPartitions) &#123; // key是手机号,value是流量信息 // 获取手机号前三位 String prePhoneNum = text.toString().substring(0, 3); int partition = 4; if (\"136\".equals(prePhoneNum)) &#123; partition = 0; &#125; else if (\"137\".equals(prePhoneNum)) &#123; partition = 1; &#125; else if (\"138\".equals(prePhoneNum)) &#123; partition = 2; &#125; else if (\"139\".equals(prePhoneNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在Driver驱动主函数中添加自定义数据分区设置和ReduceTask设置 1234// 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 指定ReduceTask的数目job.setNumReduceTasks(6); WritableComparable排序 排序概述：排序是MapReduce框架中最重要的操作之一。MapTask和ReduceTask均会对数据按照key进行排序，该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 排序的分类 部分排序：MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序 全排序：最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构 辅助排序(GroupingComparator分组)：在Reduce端对key进行分组。应用——在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序 二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序 自定义排序WritableComparable 原理分析：bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序 案例实操(全排序) 需求：对FlowBean案例产生的结果再次对总流量进行排序 案例分析 代码实现 FlowBean对象在之前案例基础上实现WritableComparable接口，实现compareTo()方法 12345@Overridepublic int compareTo(FlowBean bean) &#123; // 按总流量倒序 return this.sumFlow &gt; bean.sumFlow ? -1 : 1;&#125; 编写Mapper类 12345678910111213141516171819public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; private Text v = new Text(); private FlowBean flowBean = new FlowBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、封装对象 v.set(fields[1]); long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); flowBean.set(upFlow, downFlow); // 4、写出 context.write(flowBean, v); &#125;&#125; 编写Reducer类 12345678public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; context.write(value, key); &#125; &#125;&#125; 编写Driver类 1234567891011121314151617181920212223public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设计jar路径 job.setJarByClass(FlowCountSortDriver.class); // 3、关联mapper和reducer job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Combiner合并 Combiner介绍 自定义Combiner实现步骤 自定义一个Combiner继承Reducer，重写reduce方法 在Driver驱动类中设置：job.setCombinerClass(xxCombiner.class); 自定义Combiner实操 需求：统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能 案例分析 代码实现 方案一 12345678910111213141516171819// 新建WordCountCombiner类public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 v.set(sum); context.write(key, v); &#125;&#125;// 在WordCountDriver驱动类中指定Combinerjob.setCombinerClass(WordcountCombiner.class); 方案二：将WordCountReducer作为Combiner 结果查看：控制台打印中的Map-Reduce Framework中的Combine记录 GroupingComparator分组(辅助排序) 简要介绍：对Reduce阶段的数据根据某一个或几个字段进行分组 分组排序步骤 自定义类继承WritableComparator 重写compare()方法 创建一个构造将比较对象的类传给父类 GroupingComparator分组实操 需求：求出每一个订单中最贵的商品 案例分析 利用“订单id和成交金额price”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序。Reduce将从Map中获取排序好的数据 在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品 代码实现 编写订单信息OrderBean类 12345678910111213141516171819202122232425262728// 忽略了空参/全参构造器、getter/setter和toString方法public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int orderId; private double price; @Override public int compareTo(OrderBean bean) &#123; int result; if (orderId &gt; bean.orderId) &#123; result = 1; &#125; else if (orderId &lt; bean.orderId) &#123; result = -1; &#125; else &#123; result = price &gt; bean.price ? -1 : 1; &#125; return result; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(orderId); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; orderId = in.readInt(); price = in.readDouble(); &#125;&#125; 编写OrderGroupingComparator类 123456789101112131415public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; // 要求只要id相同,就认为是相同的key OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; if (aBean.getOrderId() == bBean.getOrderId()) &#123; return 0; &#125; return aBean.getOrderId() &gt; bBean.getOrderId() ? 1 : -1; &#125;&#125; 编写OrderMapper类 12345678910111213141516public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; private OrderBean orderBean = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、封装对象 orderBean.setOrderId(Integer.parseInt(fields[0])); orderBean.setPrice(Double.parseDouble(fields[2])); // 4、写出 context.write(orderBean, NullWritable.get()); &#125;&#125; 编写OrderReducer类 1234567891011public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 输出第一个 context.write(key, NullWritable.get()); // 循环几次输出前几 //for (NullWritable value : values) &#123; // context.write(key, NullWritable.get()); //&#125; &#125;&#125; 编写OrderDriver类 123456789101112131415161718192021222324public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、设置jar存储位置 job.setJarByClass(OrderDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); job.setGroupingComparatorClass(OrderGroupingComparator.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; MapTask工作机制 Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区(调用Partitioner)，并写入一个环形内存缓冲区中 Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作 溢写步骤1：利用快速排序算法对缓存区内的数据进行排序。排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序 溢写步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out(N表示当前溢写次数)中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作 溢写步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中 Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor(默认10)个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销 ReduceTask ReduceTask工作机制 Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中 Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多 Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可 Reduce阶段：reduce()函数将计算结果写到HDFS上 设置ReduceTask并行度(个数)ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置(默认值为1)：job.setNumReduceTasks(4); 一个测试ReduceTask数目的实验 实验环境：1个Master节点，16个Slave节点；CPU：8GHZ，内存: 2G 实验结论(数据量为1G) ReduceTask数目 总时间 1 892 5 146 10 110 15 92 16 88 20 100 25 128 30 101 45 145 60 104 注意事项 ReduceTask = 0，表示没有Reduce阶段，输出文件个数和Map个数一致 ReduceTask默认值就是1，所以输出文件个数为一个 如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜(一些节点很忙,其余空闲) ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask 具体多少个ReduceTask，需要根据集群性能而定 如果分区数不是1，但是ReduceTask为1，是否执行分区过程？答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行 OutputFormat数据输出 OutputFormat接口实现类OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。以下是几种常见的OutputFormat实现类 文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串 SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩 自定义OutputFormat：根据用户需求，自定义实现输出 使用场景为了实现控制最终文件的输出路径和输出格式，可以自定义OutputFormat例如：要在一个MapReduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义OutputFormat来实现 自定义OutputFormat步骤 自定义一个类继承FileOutputFormat 改写RecordWriter，具体改写输出数据的方法write() 自定义OutputFormat案例实操 需求：过滤输入的log日志，指定包含某特定字段的记录输出到一个文件，其他则输出到另一个文件 案例分析 代码编写 Mapper类 1234567public class FilterMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // http://www.baidu.com context.write(value, NullWritable.get()); &#125;&#125; Reducer类 12345678910111213public class FilterReducer extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; &#123; private Text k = new Text(); @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String line = key.toString(); line += \"\\r\\n\"; k.set(line); for (NullWritable value : values) &#123; context.write(k, NullWritable.get()); &#125; &#125;&#125; 自定义OutputFormat、RecordWriter类 12345678910111213141516171819202122232425262728293031323334353637383940public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; return new FRecordWriter(job); &#125;&#125;public class FRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; private FSDataOutputStream fosSobxiong; private FSDataOutputStream fosOther; public FRecordWriter(TaskAttemptContext job) &#123; try &#123; // 1、获取文件系统 FileSystem fs = FileSystem.get(job.getConfiguration()); // 2、创建输出到sobxiong.log的输出流 fosSobxiong = fs.create(new Path(\"/Users/sobxiong/Downloads/sobxiong.log\")); // 3、创建输出到other.log的输出流 fosOther = fs.create(new Path(\"/Users/sobxiong/Downloads/other.log\")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断key中是否有sobxiong,如果有写出到sobxiong,否则输出到other if (key.toString().contains(\"sobxiong\")) &#123; fosSobxiong.write(key.toString().getBytes()); &#125; else &#123; fosOther.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; IOUtils.closeStream(fosOther); IOUtils.closeStream(fosSobxiong); &#125;&#125; Driver类 1234567891011121314151617181920212223242526public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FilterDriver.class); job.setMapperClass(FilterMapper.class); job.setReducerClass(FilterReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要将自定义的输出格式组件设置到job中 job.setOutputFormatClass(FilterOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 虽然我们自定义了outputFormat,但是因为我们的outputFormat继承自fileOutputFormat // 而fileOutputFormat要输出一个_SUCCESS文件,所以,在这还得指定一个输出目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Join多种应用 Reduce Join 工作原理Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在Map阶段已经打标志)分开，最后进行合并 案例实操 需求：将商品信息表中数据根据商品pid合并到订单数据表中 案例分析：通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联 代码编写 合并后的Bean类 123456789101112131415161718192021222324252627282930313233343536// 略去空参/全参构造器、getter/setter方法public class TableBean implements Writable &#123; // 订单id private String id; // 产品id private String pid; // 数量 private int amount; // 产品名称 private String pName; // 标记: 产品/订单 private String flag; @Override public String toString() &#123; return id + '\\t' + amount + '\\t' + pName; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(id); out.writeUTF(pid); out.writeInt(amount); out.writeUTF(pName); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; id = in.readUTF(); pid = in.readUTF(); amount = in.readInt(); pName = in.readUTF(); flag = in.readUTF(); &#125;&#125; Mapper类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt; &#123; private String fileName; private final TableBean tableBean = new TableBean(); private final Text key = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取文件的名称 FileSplit inputSplit = (FileSplit) context.getInputSplit(); fileName = inputSplit.getPath().getName(); &#125; // id pid amount // 1001 01 1 // pid pname // 01 小米 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); String[] fields = lineStr.split(\";\"); if (fileName.startsWith(\"order\")) &#123; // 订单表 // 封装kv tableBean.setId(fields[0]); tableBean.setPid(fields[1]); tableBean.setAmount(Integer.parseInt(fields[2])); // 属性不能为空,不然会序列化会出错 tableBean.setpName(\"\"); tableBean.setFlag(\"order\"); this.key.set(fields[1]); &#125; else &#123; // 产品表 // 封装kv tableBean.setId(\"\"); tableBean.setPid(fields[0]); tableBean.setAmount(0); tableBean.setpName(fields[1]); tableBean.setFlag(\"pd\"); this.key.set(fields[0]); &#125; // 写出 context.write(this.key, tableBean); &#125;&#125; Reducer类 1234567891011121314151617181920212223242526272829303132public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 存储所有订单集合 List&lt;TableBean&gt; beans = new ArrayList&lt;&gt;(); // 存储产品信息 TableBean pdBean = new TableBean(); for (TableBean value : values) &#123; if (\"order\".equals(value.getFlag())) &#123; TableBean tmpBean = new TableBean(); try &#123; // value是引用,tmpBean是实实在在的对象 BeanUtils.copyProperties(tmpBean, value); beans.add(tmpBean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; else &#123; try &#123; BeanUtils.copyProperties(pdBean, value); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 拼接表,设置商品名称 for (TableBean bean : beans) &#123; bean.setpName(pdBean.getpName()); context.write(bean, NullWritable.get()); &#125; &#125;&#125; Driver类 12345678910111213141516171819202122public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取配置信息,创建job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定本程序的jar包所在的本地路径 job.setJarByClass(TableDriver.class); // 3、指定本业务job要使用的Mapper/Reducer业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 4、指定Mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 5、指定最终输出的数据的kv类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 6、指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 测试结果： pid pname amount 1001 小米 1 1001 小米 1 1002 华为 2 1002 华为 2 1003 格力 3 1003 格力 3 总结 缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜 解决方案：Map端实现数据合并 Map Join 使用场景：适用于一张表十分小、一张表很大的场景 优点思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜 具体方法：采用DistributedCache 在Mapper的setup阶段，将文件读取到缓存集合中 在驱动函数中加载缓存(缓存普通文件到Task运行节点)：job.addCacheFile(new URI(“file:///Users/sobxiong/Downloads/testInput3/pd.txt”)) 案例实操 需求同Reduce Join 案例分析 代码编写 Driver类 1234567891011121314151617181920212223public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException &#123; // 1、获取job信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、设置加载jar包路径 job.setJarByClass(DistributedCacheDriver.class); // 3、关联map job.setMapperClass(DistributedCacheMapper.class); // 没有reduce阶段,map阶段输出即为最终输出 // 4、设置最终输出数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、加载缓存数据 job.addCacheFile(new URI(\"file:///Users/sobxiong/Downloads/testInput3/pd.txt\")); // 7、Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0 job.setNumReduceTasks(0); // 8、提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Mapper类 1234567891011121314151617181920212223242526272829303132333435363738394041public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; private Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(5); private Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 缓存小表 String cachePath = context.getCacheFiles()[0].getPath(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(new FileInputStream(cachePath), StandardCharsets.UTF_8)); String lineStr; while (StringUtils.isNotEmpty(lineStr = bufferedReader.readLine())) &#123; // 1、切割 // pid pname // 01 小米 String[] fields = lineStr.split(\";\"); // 2、封装到集合去 pdMap.put(fields[0], fields[1]); &#125; // 2、关闭资源 IOUtils.closeStream(bufferedReader); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // id pid amount // 1001 01 1 // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、获取pid String pid = fields[1]; // 4、取出pname String pName = pdMap.get(pid); // 5、拼接 lineStr = lineStr.replace(';', '\\t') + '\\t' + pName; k.set(lineStr); // 6、写出 context.write(k, NullWritable.get()); &#125;&#125; 计数器应用Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量 计数器API 采用枚举的方式统计计数 123enum MyCounter&#123;MALFORORMED,NORMAL&#125;//对枚举定义的自定义计数器加1context.getCounter(MyCounter.MALFORORMED).increment(1); 采用计数器组、计数器名称的方式统计 12// 组名和计数器名称随便起,但最好有意义context.getCounter(\"counterGroup\", \"counter\").increment(1); 计数结果在程序运行后的控制台上查看 数据清洗(ETL)在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序 案例实操(简单解析版——运用计数器)——复杂版(字段多,过滤的需求多,思路与下面无差) 需求：去除日志中字段长度小于等于11的日志 代码编写 Mapper类 123456789101112131415161718192021222324public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String line = value.toString(); // 2、解析数据 boolean isDirty = parseLog(line, context); if (!isDirty) &#123; // 3、解析通过,写出 context.write(value, NullWritable.get()); &#125; &#125; private boolean parseLog(String line, Context context) &#123; String[] fields = line.split(\" \"); if (fields.length &gt; 11) &#123; context.getCounter(\"map\", \"clean\").increment(1); return false; &#125; else &#123; context.getCounter(\"map\", \"dirty\").increment(1); return true; &#125; &#125;&#125; Driver类 1234567891011121314151617181920public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、加载jar包 job.setJarByClass(LogDriver.class); // 3、关联map job.setMapperClass(LogMapper.class); // 4、设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置reduceTask个数为0 job.setNumReduceTasks(0); // 5、设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce开发总结编写MapReduce程序时，需要考虑如下方面 输入数据接口：InputFormat 默认使用的实现类是：TextInputFormat TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回 KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key/value。默认分隔符是tab(\\t) NlineInputFormat按照指定的行数N来划分切片 CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率 自定义InputFormat 逻辑处理接口：Mapper用户根据业务需求实现其中三个方法：map()、setup()、cleanup() Partitioner分区 默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号(分区数大于1时) 1key.hashCode() &amp; Integer.MAXVALUE % numReduces 如果业务上有特别的需求，可以自定义分区 Comparable排序 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法 部分排序：对最终输出的每一个文件进行内部排序 全排序：对所有数据进行排序，通常只有一个Reduce 二次排序：排序的条件有两个 Combiner合并Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果 Reduce端分组：GroupingComparator在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序 逻辑处理接口：Reduce用户根据业务需求实现其中三个方法：reduce()、setup()、cleanup() 输出数据接口：OutputFormat 默认实现类是TextOutputFormat，功能逻辑是：将每一个kv对向目标文本文件输出一行 将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩 自定义OutputFormat Hadoop数据压缩 概述压缩技术能够有效减少底层存储系统(HDFS)读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价 压缩策略和原则压缩是提高Hadoop运行效率的一种优化策略通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度 注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能压缩基本原则 运算密集型的job，少用压缩 IO密集型的job，多用压缩 MR支持的压缩编码 压缩格式 是否hadoop自带 算法 文件扩展名 是否可切分 换成压缩格式后,原来程序是否需要修改 DEFLATE 是,直接使用 DEFLATE .deflate 否 和文本处理一样,不需要修改 Gzip 是,直接使用 DEFLATE .gz 否 和文本处理一样,不需要修改 bzip2 是,直接使用 bzip2 .bz2 是 和文本处理一样,不需要修改 LZO 否,需要安装 LZO .lzo 是 需要建索引,还需要指定输入格式 Snappy 否,需要安装 Snappy .snappy 否 和文本处理一样,不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec Gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s Snappy 8.3GB 较大 最快 最快 压缩方式选择 Gzip压缩 优点压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便 缺点：不支持Split(切片) 应用场景 当每个文件压缩之后在130M以内的(1个块大小内)，都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件 Bzip2压缩 优点：支持Split(切片)；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便 缺点：压缩/解压速度慢 应用场景适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split(切片)，而且兼容之前的应用程序的情况 Lzo压缩 优点：压缩/解压速度也比较快，合理的压缩率；支持Split(切片)，是Hadoop中最流行的压缩格式之一；可以在Linux系统下安装lzop命令，使用方便 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建索引，还需要指定InputFormat为Lzo格式) 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显 Snappy压缩 优点：高速压缩速度和合理的压缩率 缺点：不支持Split(切片)；压缩率比Gzip要低；Hadoop本身不支持，需要安装 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入 压缩位置选择压缩可以在MapReduce作用的任意阶段启用 压缩参数配置要在Hadoop中启用压缩，可配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs(在core-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress(在mapred-site.xml中配置) false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置) false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置) RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 压缩实操 数据流的压缩和解压缩CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据： 要想对正在被写入一个输出流的数据进行压缩，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流 相反，要想对从输入流读取而来的数据进行解压缩，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据测试如下的压缩方式： 压缩格式 编解码类 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec 12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args) throws Exception &#123; compress(\"/Users/sobxiong/Downloads/test.txt\", \"org.apache.hadoop.io.compress.BZip2Codec\"); decompress(\"/Users/sobxiong/Downloads/test.txt.bz2\");&#125;// 解压缩private static void decompress(String filePath) throws IOException &#123; // 1、压缩方式检查 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filePath)); if (codec == null) &#123; System.out.println(\"Can't process!\"); return; &#125; // 2、获取输入流 FileInputStream fis = new FileInputStream(new File(filePath)); CompressionInputStream cis = codec.createInputStream(fis); // 3、获取输出流 FileOutputStream fos = new FileOutputStream(new File(filePath + \".decode\")); // 4、流的对拷 IOUtils.copyBytes(cis, fos, 1024 * 1024 * 10, false); // 5、关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(cis); IOUtils.closeStream(fis);&#125;// 压缩private static void compress(String filePath, String compressTypeClassName) throws IOException, ClassNotFoundException &#123; // 1、获取输入流 FileInputStream fis = new FileInputStream(new File(filePath)); // 2、获取输出流 Class&lt;?&gt; classCodec = Class.forName(compressTypeClassName); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(classCodec, new Configuration()); FileOutputStream fos = new FileOutputStream(new File(filePath + codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // 3、流的对拷 IOUtils.copyBytes(fis, cos, 1024 * 1024 * 10, false); // 4、关闭资源 IOUtils.closeStream(cos); IOUtils.closeStream(fos); IOUtils.closeStream(fis);&#125; Map输出端采用压缩(以万能的WordCount案例为例)即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可具体实现(只修改Driver部分代码,Mapper和Reducer不变)： 123456789// 1、获取Job对象Configuration conf = new Configuration();// 开启map端输出压缩conf.setBoolean(\"mapreduce.map.output.compress\", true);// 设置map端输出压缩方式conf.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class, CompressionCodec.class);Job job = Job.getInstance(conf);// 之后代码保持原样 Reduce输出端采用压缩(以万能的WordCount案例为例)具体实现(只修改Driver部分代码,Mapper和Reducer不变)： 1234567891011// 之前代码保持不变// 设置reduce端输出压缩开启FileOutputFormat.setCompressOutput(job, true);// 设置压缩的方式FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);// 6、设置程序运行的输入和输出路径FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1]));// 之后代码保持不变 Yarn资源调度器Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序 Yarn基本架构YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成 Yarn工作机制 Yarn工作机制图解 工作机制详解 MR程序提交到客户端所在的节点 YarnRunner向ResourceManager申请一个Application RM将该应用程序的资源路径返回给YarnRunner 该程序将运行所需资源提交到HDFS上 程序资源提交完毕后，申请运行MrAppMaster RM将用户的请求初始化成一个Task 其中一个NodeManager领取到Task任务 该NodeManager创建容器Container，并产生MrAppmaster Container从HDFS上拷贝资源到本地 MrAppmaster向RM申请运行MapTask的资源 RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器 MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序 MrAppMaster等待所有MapTask运行完毕后，向RM申请容器运行ReduceTask ReduceTask向MapTask获取相应分区的数据 程序运行完毕后，MR会向RM申请注销自己 作业提交全过程 作业提交过程之Yarn图解 作业提交过程之MapReduce图解 作业提交过程详解 作业提交 Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业 Client向RM申请一个作业id RM给Client返回该job资源的提交路径和作业id Client提交jar包、切片信息和配置文件到指定的资源提交路径 Client提交完资源后，向RM申请运行MrAppMaster 作业初始化 当RM收到Client的请求后，将该job添加到容量调度器中 某一个空闲的NM领取到该Job 该NM创建Container，并产生MrAppmaster 下载Client提交的资源到本地 任务分配 MrAppMaster向RM申请运行多个MapTask任务资源 RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器 任务运行 MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序 MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask ReduceTask向MapTask获取相应分区的数据 程序运行完毕后，MR会向RM申请注销自己 进度和状态更新YARN中的任务将其进度和状态(包括counter)返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新，展示给用户 作业完成除了向应用管理器请求作业进度外，客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后，应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。 先进先出调度器(FIFO) 容量调度器(Capacity Scheduler) 公平调度器(Fair Scheduler) Hadoop3.1.3默认的资源调度器是Capacity Scheduler。具体设置详见yarn-default.xml文件 12345&lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 任务的推测执行 作业完成时间取决于最慢的任务完成时间一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？ 推测执行机制发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果 执行推测任务的前提条件 每个Task只能有一个备份任务 当前Job已完成的Task必须不小于0.05(5%) 开启推测执行参数设置。mapred-site.xml文件中默认是打开的 1234567891011&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; 不能启用推测执行机制情况 任务间存在严重的负载倾斜 特殊任务，比如任务向数据库中写数据 算法原理 Hadoop企业优化 MapReduce跑的慢的原因MapReduce程序效率的瓶颈在于两点： 计算机性能：CPU、内存、磁盘健康、网络 I/O操作优化 I/O操作优化 Map和Reduce数设置不合理 Map运行时间太长，导致Reduce等待过久 小文件过多 大量的不可分块的超大文件 Spill次数过多 Merge次数过多等 MapReduce优化方法MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数 数据输入 合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢 采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景 Map阶段 减少溢写(Spill)次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO 减少合并(Merge)次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间 在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O Reduce阶段 合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误 设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间 规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗 合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整 I/O传输 采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器 使用SequenceFile二进制文件 数据倾斜问题 数据倾斜现象 数据频率倾斜：某一个区域的数据量要远远大于其他区域 数据大小倾斜：部分记录的大小远远大于平均值 减少数据倾斜的方法 抽样和范围分区：可以通过对原始数据进行抽样得到的结果集来预设分区边界值 自定义分区：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例 Combine：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据 采用Map Join，尽量避免Reduce Join 常用的调优参数 资源相关参数 以下参数是在用户自己的MR应用程序中配置就可以生效(mapred-default.xml) 配置参数 参数说明 mapreduce.map.memory.mb 一个MapTask可使用的资源上限(单位:MB)，默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死 mapreduce.reduce.memory.mb 一个ReduceTask可使用的资源上限(单位:MB)，默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死 mapreduce.map.cpu.vcores 每个MapTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.cpu.vcores 每个ReduceTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.shuffle.parallelcopies 每个Reduce去Map中取数据的并行数。默认值是5 mapreduce.reduce.shuffle.merge.percent Buffer中的数据达到多少比例开始写入磁盘。默认值0.66 mapreduce.reduce.shuffle.input.buffer.percent Buffer大小占Reduce可用内存的比例。默认值0.7 mapreduce.reduce.input.buffer.percent 指定多少比例的内存用来存放Buffer中的数据，默认值是0.0 应该在YARN启动之前就配置在服务器的配置文件中才能生效(yarn-default.xml) 配置参数 参数说明 yarn.scheduler.minimum-allocation-mb 给应用程序Container分配的最小内存，默认值：1024 yarn.scheduler.maximum-allocation-mb 给应用程序Container分配的最大内存，默认值：8192 yarn.scheduler.minimum-allocation-vcores 每个Container申请的最小CPU核数，默认值：1 yarn.scheduler.maximum-allocation-vcores 每个Container申请的最大CPU核数，默认值：32 yarn.nodemanager.resource.memory-mb 给Containers分配的最大物理内存，默认值：8192 Shuffle性能优化的关键参数，应在YARN启动之前就配置好(mapred-default.xml) 配置参数 参数说明 mapreduce.task.io.sort.mb Shuffle的环形缓冲区大小，默认100m mapreduce.map.sort.spill.percent 环形缓冲区溢出的阈值，默认80% 容错相关参数(MapReduce性能优化) 配置参数 参数说明 mapreduce.map.maxattempts 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4 mapreduce.reduce.maxattempts 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4 mapreduce.task.timeout Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间(单位毫秒)，默认是600000。如果你的程序对每条输入数据的处理时间过长(比如会访问数据库，通过网络拉取数据等)，建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://sobxiong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://sobxiong.github.io/tags/Hadoop/"}]}],"categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://sobxiong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hive","slug":"Hive","permalink":"https://sobxiong.github.io/tags/Hive/"},{"name":"MFC","slug":"MFC","permalink":"https://sobxiong.github.io/tags/MFC/"},{"name":"数据结构","slug":"数据结构","permalink":"https://sobxiong.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://sobxiong.github.io/tags/Zookeeper/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"},{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://sobxiong.github.io/tags/Hadoop/"}]}