{"meta":{"title":"SOBXiong的博客","subtitle":"","description":"一只编程菜鸡，对Android、Java后端、大数据和Vue雨露均沾","author":"SOBXiong","url":"https://sobxiong.github.io","root":"/"},"pages":[],"posts":[{"title":"10-正则表达式匹配","slug":"数据结构/LeetCode/10-正则表达式匹配","date":"2020-06-23T10:41:07.000Z","updated":"2020-06-23T10:43:53.202Z","comments":true,"path":"2020/06/23/数据结构/LeetCode/10-正则表达式匹配/","link":"","permalink":"https://sobxiong.github.io/2020/06/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/10-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D/","excerpt":"","text":"","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://sobxiong.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"}]},{"title":"SpringCloud","slug":"Spring/SpringCloud","date":"2020-06-21T13:46:45.000Z","updated":"2020-06-23T10:43:50.450Z","comments":true,"path":"2020/06/21/Spring/SpringCloud/","link":"","permalink":"https://sobxiong.github.io/2020/06/21/Spring/SpringCloud/","excerpt":"内容 理论入门 扩展原理 Web","text":"内容 理论入门 扩展原理 Web 理论入门 SpringCloud：分布式微服务架构的一站式解决方案，是多种微服务架构落地技术的集合体，俗称微服务全家桶；SpringCloud已成为微服务开发的主流技术栈 版本选择： SpringBoot：2.2.2.RELEASE git源码地址：https://github.com/spring-projects/spring-boot/releases/ 官方文档：https://docs.spring.io/spring-boot/docs/2.2.2.RELEASE/reference/htmlsingle/ SpringCloud：Hoxton.SR1、Alibaba 2.1.0.RELEASE(boot和cloud一起运用时,boot要照顾cloud) git源码地址：https://github.com/spring-projects/spring-cloud/wiki 官网：https://spring.io/projects/spring-cloud 官方文档：https://cloud.spring.io/spring-cloud-static/Hoxton.SR1/reference/htmlsingle/ 中文文档：https://www.bookstack.cn/read/spring-cloud-docs/docs-index.md cloud与boot之间的依赖关系 https://spring.io/projects/spring-cloud#overview(大版本) https://start.spring.io/actuator/info(具体版本) 架构编码构建 重要规矩：约定 &gt; 配置 &gt; 编码 IDEA创建project工作空间 父工程project(pom项目) 设置项目字符编码：Editor -&gt; File Encodings -&gt; Global Encoding、Project Encoding、Default encoding for properties files设置UTF-8，勾选上Transparent native-to-ascii conversion 设置注解生效激活：Build,… -&gt; Complier -&gt; Annotatoin Processors -&gt; 勾选Enable annotation processing java编译版本选择8：Build,… -&gt; Complier -&gt; Java Compiler -&gt; 设置父工程java编译版本为1.8 父工程project的pom文件设置： 1234567891011121314151617181920...&lt;packaging&gt;pom&lt;/packaging&gt;&lt;!-- 统一管理jar包版本 --&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; ...&lt;/properties&gt;&lt;!-- 子模块继承之后,提供作用：锁定版本 + 子modlue不用写groupId和version 父工程声明后不会直接引入,在子工程pom文件声明后才会正式引用--&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; ... &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 子模块构建 五个步骤：建module、改pom、写yml、主启动、业务类 设置热部署Devtools： 添加devtools的pom依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 添加plugin插件的pom依赖： 12345678910&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;addResources&gt;true&lt;/addResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 设置启用自动build：Compiler -&gt; ADBC四个选项打勾 更新idea属性值：command+shift+A调出搜索action，键入Registry，compiler.automake.allow.when.app.running和actionSystem.assertFocusAccessFromEdt打勾 重启idea","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"}]},{"title":"Spring注解驱动开发","slug":"Spring/Spring注解驱动开发","date":"2020-06-19T06:57:26.000Z","updated":"2020-06-22T14:32:57.030Z","comments":true,"path":"2020/06/19/Spring/Spring注解驱动开发/","link":"","permalink":"https://sobxiong.github.io/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/","excerpt":"内容 容器 扩展原理 Web","text":"内容 容器 扩展原理 Web 容器 @Configuration：类注解(配置类=配置文件,注解告诉Spring这是一个配置类) @Bean：方法注解，在类方法中给出返回Bean的方法，并在方法上添加@Bean注解(给容器中注册一个Bean,类型为返回值的类型,id默认为方法名,可复写注解的value属性复写id)。 @Scope：方法注解，设置作用域。常用值为： prototype：多实例，ioc容器诶懂并不会去调用方法创建对象放在容器中。每次获取的时候才会调用方法创建对象 singleton(默认单实例)：ioc容器启动会调用方法创建对象放到ioc容器中，以后每次获取就是直接从容器中(可看作使用map.get())拿 @Lazy：懒加载，只有在singleton单实例下才生效，且需要在返回bean的方法上加上@Lazy注解。单实例bean默认在容器启动的时候创建对象；懒加载在容器启动时不创建对象，第一次使用(获取)Bean创建对象并初始化 123456@Scope(\"singleton\")@Lazy@Bean(\"person\")public Person person() &#123; return new Person(\"SOBXiong\", 22);&#125; @ComponentScans：指定扫描规则组(value为ComponentScan集合) @ComponentScan：类注解，指定组件扫描规则 value：指定包名，这样Spring会扫描包下的所有组件(SpringBoot情况可能不同,不需要) excludeFilters：指定排除的过滤器，filter可根据注解排除(排除规则)，classed指定注解的类 includeFilters：指定只需要包含的过滤器 useDefaultFilters：是否适用缺省的过滤器，默认true；如果要使includeFilters生效，则必须设置为false FilterType： FilterType.ANNOTATION：按照注解方式 FilterType.ASSIGNABLE_TYPE：按照指定的类型(具体的类,包括子类和实现类) FilterType.REGEX：适用正则表达式 FilterType.CUSTOM：使用自定义规则，需要自定义实现TypeFilter接口的类 123456789101112131415161718192021222324252627282930313233@Configuration@ComponentScan(value = \"packageName\",excludeFilters = &#123; @Filter(type=FilterType.ANNOTATION,classes=&#123;Controller.class,Service.class&#125;)&#125;)public class MainConfig &#123; @Bean public Person person() &#123; return new Person(\"SOBXiong\", 22); &#125;&#125;public class TestTypeFilter implements TypeFilter &#123; /** * * @param metadataReader 读取到的当前正在扫描的类的信息 * @param metadataReaderFactory 可以获取到其他任何类信息的工厂 * @return boolean * @throws IOException */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; // 获取当前类注解的信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); // 获取当前正在扫描的类的类信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); // 获取当前类的资源信息(类路径等) Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println(\"className = \" + className); return className.contains(\"test\"); // return false; &#125;&#125; @Conditional：按照一定的条件进行判断，满足条件给容器中注册bean(Spring底层大量用到);可以设置在类上，也可以设置在方法上。设置在返回bean的方法上：只根据条件解决是否注册bean。设置在类上：类中注册统一设置，满足条件时，这个类中配置的所有bean注册才能生效 1234567891011121314151617181920212223242526272829303132@Conditional(TestCondition.class)@Bean(\"person2\")public Person person2() &#123; return new Person(\"SOBXiong\", 22);&#125;public class TestCondition implements Condition &#123; /** * @param conditionContext 判断条件能使用的上下文(环境) * @param annotatedTypeMetadata 注释信息 * @return boolean */ @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) &#123; // 1、能获取到ioc使用的beanFactory ConfigurableListableBeanFactory beanFactory = conditionContext.getBeanFactory(); // 2、获取类加载器 ClassLoader classLoader = conditionContext.getClassLoader(); // 3、获取当前环境信息 Environment environment = conditionContext.getEnvironment(); // 4、获取到bean定义的注册类 BeanDefinitionRegistry registry = conditionContext.getRegistry(); // 可以判断容器中的bean注册情况,也可以给容器中注册bean boolean isDefinition = registry.containsBeanDefinition(\"person\"); // 获取运行系统的名称 String osName = environment.getProperty(\"os.name\"); if (osName.contains(\"Windows\")) &#123; return true; &#125; return false; &#125;&#125; @Import：导入组件，id默认是组件的全类名 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 给容器中注册组件： * 1、包扫描+组件标注注解(@Controller、@Service、@Repository、@Component) * 2、@Bean[导入第三方包里面的组件] * 3、@Import[快速给容器中导入一个组件] * 1、容器会自动注册这个组件,id默认是全类名 * 2、ImportSelector：返回需要导入的组件的全类名数组(SpringBoot源码中许多地方用到); * 3、ImportBeanDefinitionRegistrar：手动注册bean到容器中 * 4、使用Spring提供的FactoryBean(工厂Bean),其他与Spring整合的框架使用的特别多 * 1、默认获取的是工厂bean调用getObject创建的对象 * 2、要获取工厂bean本身,需要给id前面加一个&amp; */@Configuration@Import(&#123;TestImportSelector.class, TestImportBeanDefinitionRegistrar.class&#125;)public class MainConfig &#123;&#125;// 自定义逻辑返回需要导入的组件public class TestImportSelector implements ImportSelector &#123; /** * @param annotationMetadata 当前标注@Import注解的类的所有注解信息 * @return String[] 导入到容器中的组件全类名数组 */ @Override public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; return new String[]&#123;\"com.xiong.test.Animal\", \"com.xiong.test.Person\"&#125;; &#125;&#125;public class TestImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; /** * 可以把所有需要添加到容器中的bean通过调用BeanDefinitionRegistry.registerBeanDefinition手工注册 * @param importingClassMetadata 当前类的注解信息 * @param registry BeanDefinition注册类 */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; boolean isWorldExist = registry.containsBeanDefinition(\"World\"); if (!isWorldExist) &#123; // 注册一个bean,指定bean的名称和bean的定义信息(bean的类型,bean的Scope...) registry.registerBeanDefinition(\"world\", new RootBeanDefinition(World.class)); &#125; &#125;&#125; FactoryBean(工厂Bean)： 1234567891011121314151617181920212223242526// 创建一个Spring定义的FactoryBeanpublic class TestFactoryBean implements FactoryBean&lt;Animal&gt; &#123; // 返回一个Animal对象,这个对象会添加到容器中 // 调用此方法得到对象 @Override public Animal getObject() throws Exception &#123; return new Animal(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Animal.class; &#125; // 是否是单实例 @Override public boolean isSingleton() &#123; return true; &#125;&#125;@Configurationpublic class MainConfig &#123; @Bean public TestFactoryBean testFactoryBean()&#123; return new TestFactoryBean(); &#125;&#125; @Bean指定初始化和销毁方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class Car &#123; public Car()&#123; System.out.println(\"Car 构造方法\"); &#125; public void init()&#123; System.out.println(\"Car init\"); &#125; public void destroy()&#123; System.out.println(\"Car destroy\"); &#125;&#125;/** * bean的生命周期： * bean创建 --&gt; 初始化 --&gt; 销毁 * 容器管理bean的生命周期; * 我们可以自定义初始化和销毁方法;容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法 * * 构造(对象创建)： * 单实例：在容器启动的时候创建对象 * 多实例：在每次获取的时候创建对象 * 初始化：对象创建完成,并赋值结束,调用初始化方法 * 销毁： * 单实例：容器关闭的时候 * 多实例：容器不会管理这个bean,容器不会调用销毁方法 * * 1、指定初始化和销毁方法(通过@Bean注解指定init-method和destroy-method) * 2、通过让Bean实现InitializingBean(定义初始化方法逻辑),DisposableBean(定义销毁逻辑) * 3、可以使用JSR250： * @PostConstruct：在bean创建完成并且属性赋值完毕再执行初始化方法 * @PreDestroy：在容器销毁bean之前通知进行清理工作 * 4、BeanPostProcessor：bean的后置处理器(在bean初始化前后进行一些工作) * postProcessBeforeInitialization：在初始化之前工作 * postProcessAfterInitialization：在初始化之后工作 */@Configurationpublic class MainConfigLifecycle &#123; // @Scope(\"prototype\") @Bean(initMethod = \"init\", destroyMethod = \"destroy\") public Car car() &#123; return new Car(); &#125;&#125;@ComponentScan(\"com.xiong.test\")@Componentpublic class Cat implements InitializingBean, DisposableBean &#123; public Cat() &#123; System.out.println(\"Cat构造函数...\"); &#125; @Override public void destroy() throws Exception &#123; System.out.println(\"Cat destroy...\"); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println(\"Cat init...\"); &#125;&#125;/** * 后置处理器：在bean初始化前后进行处理工作 */@Componentpublic class TestBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessBeforeInitialization: \" + beanName + \" , \" + bean); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessAfterInitialization: \" + beanName + \" , \" + bean); return bean; &#125;&#125; BeanPostProcessor原理 1234567891011// 遍历得到容器中所有的BeanPostProcessor;挨个执行beforeInitialization,// 一旦返回null,跳出for循环,不追执行后面的BeanPostProcessor.postProcessBeforeInitialization()// 给bean进行属性赋值populateBean(beanName, mbd, instanceWrapper);initializeBean(beanName, exposedObject, mbd);&#123; // 以下就是initializeBean的粗略内容 applyBeanPostProcessorsBeforeInitialization(bean, beanName); invokeInitMethods(beanName, wrappedBean, mbd); applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);&#125; Spring底层对BeanPostProcessor的使用 ApplicationContextAwareProcessor：可让bean获取容器对象context BeanValidationPostProcessor：Web表单校验的处理器 InitDestroyAnnotationBeanPostProcessor：@PostConstruct和@Bean的init-method等方法的具体实现 AutowiredAnnotationBeanPostProcessor：@Autowired自动注入功能的具体实现 属性赋值 使用@Value赋值： 基本数值 SpEL：#{} ${}：取出配置文件(properties或yaml)中的值(在运行环境变量里面的值) 使用@PropertySource加载外部配置文件 1234567891011121314151617181920212223// 使用@PropertySource读取外部配置文件中的k/v保存到运行的环境变量中// 加载完外部的配置文件以后使用$&#123;&#125;取出配置文件的值// 当前只能加载properties文件,yaml不能,是采用的加载器问题@PropertySource(value = &#123;\"classpath:application.properties\"&#125;, encoding = \"utf-8\")@Configurationpublic class MainConfigPropertyValues &#123; @Bean public Person person() &#123; return new Person(); &#125;&#125;public class Person &#123; @Value(\"SOBXiong\") private String name; @Value(\"#&#123;22+5&#125;\") private int age; @Value(\"$&#123;person.nickName&#125;\") private String nickName;&#125;// application.properties// person.nickName=熊哈哈 自动装配 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * 自动装配： * Spring利用依赖注入(DI),完成对IOC容器中各个组件的依赖关系赋值 * @Autowired：自动注入(Spring定义的) * TestService&#123; * @Autowired TestDao testDao; * &#125; * 1、默认优先按照类型去容器中找对应的组件：context.getBean(TestDao.class); * 2、如果找到多个相同类型的组件,将属性名作为组件的id去容器中查找 * 3、@Qualifier(\"testDao\")：使用@Qualifier指定需要装配的组件id,而不是使用属性名 * 4、自动装配默认一定要将属性赋值好,没有就会报错(可以使用@Autowired注解中的required=false避免报错) * 5、@Primary：让Spring进行自动装配的时候默认使用首选的bean(此时@Qualifier不能使用);也可以使用@Qualifier指定需要装配的具体bean * 6、Spring还支持使用@Resource(JSR250)和@Inject(JSR330)[java规范的注解] * @Resource：可以和@Autowired一样实现自动装配功能,但默认是按照组件名称进行装配的(也可以通过name属性进行指定id);不能支持@Qualifier和required=false * @Inject：需要导入javax.inject的包,和@Autowired的功能一样,但没有required属性 * 7、@Autowired可以在构造器、参数、方法和属性上标注,都是从容器中获取组件的值 * 1、[标注在方法位置]：@Bean标注方法的方法参数;参数从容器中获取;默认不写@Autowired效果是一样的;都能自动装配 * 2、[标注在构造器位置]：如果组件只有一个有参构造器,这个有参构造器的@Autowired可以省略,参数位置的组件还是可以自动从容器中获取; * 但如有既有有参又有无参,会优先调用无参构造器,这使得boss的car属性和容器中的car不是同一个 * 3、[标注在参数位置] * 8、自定义组件想要使用Spring容器底层的的一些组件(ApplicationContext、BeanFactory等) * 自定义组件实现xxxAware接口：在创建对象的时候,会调用接口规定的方法注入相关组件; * xxxAware使用xxxProcessor：applicationContextAware =&gt; applicationContextAwareProcessor(BeanPostProcessor的实现类) */@Configuration@ComponentScan(\"com.xiong.test2\")public class MainConfigAutowired &#123; @Primary @Bean(\"testDao2\") public TestDao testDao() &#123; return new TestDao(\"2\"); &#125; // @Bean标注的方法创建对象的时候,方法参数的值从容器中获取 @Bean public Boss boss(Car car)&#123; Boss boss = new Boss(); boss.setCar(car); return boss; &#125;&#125;@Componentpublic class Car &#123;&#125;// 默认加在ioc容器中的组件，容器启动会调用无参构造器创建对象，在进行初始化赋值等操作// @Componentpublic class Boss &#123; private Car car; public Boss() &#123; &#125; // 构造器要用的组件，都是从容器中获取 // @Autowired public Boss(@Autowired Car car) &#123; this.car = car; System.out.println(\"Boss constructor with one parameter!\"); &#125; public Car getCar() &#123; return car; &#125; // 标注在方法上，Spring容器创建当前对象，就会调用方法完成赋值 // 方法使用的参数，自定义类型的值从ioc容器中获取 // @Autowired public void setCar(Car car) &#123; this.car = car; &#125; @Override public String toString() &#123; return \"Boss&#123;\" + \"car=\" + car + '&#125;'; &#125;&#125;@Servicepublic class TestService &#123; @Qualifier(\"testDao2\") @Autowired private TestDao testDao; @Override public String toString() &#123; return \"TestService&#123;\" + \"testDao=\" + testDao + '&#125;'; &#125;&#125;@Repositorypublic class TestDao &#123; private String label = \"1\";&#125; @Profile：Spring为我们提供的可以根据当前环境,动态地激活和切换一系列组件的共功能;指定组件在哪个环境的情况下才能被注册到容器中,不指定,任何环境下都能注册这个组件(开发、测试/生产环境,数据源(A/B/C)) 加了环境标志性的bean,只有这个环境被激活的时候才能注册到容器中(默认环境是default) 123@Profile(\"test\")@Bean(\"testDataSource\")public DataSource dataSourceTest() throws Exception &#123; ... &#125; 写在配置类上,只有是指定的环境的时候,整个配置类里面的内容才能生效 没有标注环境标识的bean在任何环境下都是加载的 环境的激活： 使用命令行动态参数：虚拟机参数位置加载 -Dspring.profiles.active=test 代码的方式激活某种环境 1234567891011121314151617@Testvoid contextLoads() &#123; // 1、创建ioc容器 AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); // 2、设置需要激活的环境 context.getEnvironment().setActiveProfiles(\"test\"); // 3、注册主配置类 context.register(MainConfigProfile.class); // 4、启动刷新容器 context.refresh(); ... // 关闭容器 context.close();&#125; AOP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * AOP：指在程序运行期间动态地将某段代码切入到指定方法指定位置进行运行地编程方式 * 1、导入AOP模块：Spring AOP(SpringBoot导入MVC模块会连着导入AOP模块) * 2、定义一个业务逻辑类(MathCalculator)：在业务逻辑运行地时候将日志进行打印(方法之前、方法运行结束、方法出现异常...) * 3、定义一个日志切面类(LogAspect)：切面类里面地方法需要动态感知MathCalculator.div运行到哪里然后执行 * 通知方法： * 前置通知@Before：logStart(在目标方法div运行之前运行) * 后置通知@After：logEnd(在目标方法div运行结束之后运行——无论方法是正常结束还是异常结束) * 返回通知@AfterReturning：logReturn(在目标方法div正常返回之后运行) * 异常通知@AfterThrowing：logException(在目标方法div出现异常以后运行) * 环绕通知@Around：动态代理,手动推进目标方法div运行(jointPoint.proceed()) * 4、给切面类的目标方法标注何时何地运行(通知注解) * 5、将切面类和业务逻辑类(目标方法所在类)都加入到容器中 * 6、必须告诉Spring哪个类是切面类(给切面类上加一个注解@Aspect) * 7、给配置类中加@EnableAspectJAutoProxy(开启基于注解的AOP模式);在Spring中有很多的@EnableXXX注解,替代以前的xml配置 * * 三步： * 1、将业务逻辑组件和切面类都加入到容器中;告诉Spring哪个是切面类(@Aspect) * 2、在切面类上的每一个通知方法上标注通知注解,告诉Spring何时何地运行(切入点表达式) * 3、开启基于注解的AOP模式：@EnableAspectJAutoProxy */@EnableAspectJAutoProxy@Configurationpublic class MainConfigAOP &#123; // 业务逻辑类加入容器中 @Bean public MathCalculator calculator()&#123; return new MathCalculator(); &#125; // 切面类加入到容器中 @Bean public LogAspect logAspect()&#123; return new LogAspect(); &#125;&#125;public class MathCalculator &#123; public int div(int i, int j) &#123; return i / j; &#125;&#125;/** * 切面类,方法中joinPoint必须写在参数表的第一位(否则报错) */@Aspectpublic class LogAspect &#123; // 抽取公共的切入点表达式(参考Spring官方文档) // 1、本类引用(方法名()) // 2、其他的切面引用(全类名方法名()) @Pointcut(\"execution(public int com.xiong.test3.MathCalculator.div(int, int))\") public void pointCut() &#123;&#125; // @Before在目标方法之前切入：切入点表达式(指定在哪个方法切入) @Before(\"pointCut()\") public void logStart(JoinPoint joinPoint) &#123; Object[] args = joinPoint.getArgs(); System.out.println(joinPoint.getSignature().getName() + \"运行开始... 参数列表是: &#123;\" + Arrays.asList(args) + \"&#125;\"); &#125; @After(\"pointCut()\") public void logEnd(JoinPoint joinPoint) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行结束...\"); &#125; @AfterReturning(value = \"pointCut()\", returning = \"result\") public void logReturn(JoinPoint joinPoint, Object result) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行正常返回... 结果: &#123;\" + result + \"&#125;\"); &#125; @AfterThrowing(value = \"pointCut()\", throwing = \"exception\") public void logException(JoinPoint joinPoint, Exception exception) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行出现异常... 异常信息: &#123;\" + exception.getMessage() + \"&#125;\"); &#125;&#125;@Testvoid contextLoads() &#123; // 1、创建ioc容器 AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(MainConfigAOP.class); // 必须使用Spring容器中的组件 MathCalculator calculator = context.getBean(MathCalculator.class); calculator.div(1, 0); context.close();&#125;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"}]},{"title":"Hadoop入门","slug":"大数据/hadoop","date":"2020-06-03T13:48:10.000Z","updated":"2020-06-23T06:08:20.674Z","comments":true,"path":"2020/06/03/大数据/hadoop/","link":"","permalink":"https://sobxiong.github.io/2020/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop/","excerpt":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode","text":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode 概论 概念：大数据指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。需要解决的问题：海量数据的存储和海量数据的分析计算问题。 大数据特点(4V)： Volume(大量) Velocity(高速) Variety(多样)：结构化/非结构化数据，结构化数据以数据库/文本为主，非结构化数据包括网络日志、音频、视频、图片和地理位置信息等。 Value(低价值密度)：价值密度的高度与数据总量的大小成反比，如何快速对有价值数据“提纯”称为目前大数据背景下待解决的难题。 大数据应用场景：物流仓储、零售、旅游、商品广告推荐、保险、金融、房产、人工智能 大数据部门业务流程：产品人员提需求(统计总用户数、日活跃用户数、回流用户数等) =&gt; 数据部门搭建数据平台、分析数据指标 =&gt; 数据可视化(报表展示、邮件发送、大屏幕展示等) 大数据部门组织结构： Hadoop介绍 Hadoop是什么： 是一个由Apache基金会开发的分布式系统基础架构。 主要解决海量数据的存储和海量数据的分析计算问题。 广义上来说，Hadoop通常指更广泛的概念——Hadoop生态圈。 Hadoop发展历史 Lucene框架是Doug Cutting开创的开源软件，用Java编写，实现与Goole类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询和索引引擎。 2001年年底Lucene称为Apache基金会的一个子项目。 对于海量数据的场景，Lucene面对与Google同样的困难，存储数据困难，检索速度慢。 学习和模仿Google解决这些问题的办法：微型版Nutch。 Google是Hadoop的思想之源(其在大数据方面的三篇论文)GFS -&gt; HDFS Map-Reduce -&gt; MR BigTable -&gt; HBase 2003年~04年，Google公开部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。 2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。 2006年3月份，Map-Reduce和Nutch Distributed File System(NDFS)分别被纳入到Hadoop项目中，Hadoop就正式诞生，标志着大数据十代来临。 Hadoop名字来源于Doug Cutting儿子的玩具大象 Hadoop三大发行版本 Apache：最原始(基础)的版本，对于入门学习最好 Cloudera：在大型互联网企业中用的较多，产品主要为CDH，Cloudera Manager，Cloudera Support： CDH是Cloudera的Hadoop发行版，完全开源，比Apache版本在兼容性、安全性、稳定性上有所增强。 Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。 Cloudera Support即是对Hadoop的技术支持。 Hortonworks：文档较好 Hortonworks的主打产品是Hortonworks Data Platform(HDP)，也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari——一款开源的安装和管理系统。 HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 Hadoop的优势(4高) 高可靠性：Hadoop底层维护多个数据副本，即使某个计算元素或存储出现故障，也不会导致数据的丢失。 高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点。 高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。 高容错性：能够自动将失败的任务重新分配。 Hadoop组成 1.x：Common(辅助工具)、HDFS(数据存储)、MapReduce(计算+资源调度) 2.x：Common(辅助工具)、HDFS(数据存储)、Yarn(资源调度)、MapReduce(计算) HDFS架构概述： HDFS全名——Hadoop Distributed File System 组成： NameNode(nn)：存储文件的元数据，如文件名、文件目录结构、文件属性(生成时间、副本数、文件权限)以及每个文件的块列表和块所在的DataNode等——类似书的目录(索引) DataNode(dn)：在本地文件系统存储文件块数据以及块数据的校验和——具体的书章节内容 Secondary NameNode(2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照——辅助NameNode工作 Yarn架构概述 MapReduce架构概述 将计算分为两个阶段：Map和Reduce Map阶段并行处理输入数据 Reduce阶段对Map结果进行汇总 大数据技术生态体系 环境搭建 配置Java环境变量 123456789&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##JAVA_HOMEexport JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_251export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Java版本java -version 配置Hadoop环境变量 12345678910&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##HADOOP_HOMEexport HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;binexport PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Hadoop版本hadoop version Hadoop目录说明 bin目录：存放对Hadoop相关服务(HDFS,YARN)进行操作的脚本 etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 lib目录：存放Hadoop的本地库(对数据进行压缩解压缩功能) sbin目录：存放启动或停止Hadoop相关服务的脚本 share目录：存放Hadoop的依赖jar包、文档、和官方案例 Hadoop运行模式 本地模式 官方WordCount案例(统计单词数目)： 1234567891011&#x2F;&#x2F; 创建wcinput文件夹mkdir wcinput&#x2F;&#x2F; 创建wc.input文件cd wcinputtouch wc.input&#x2F;&#x2F; 编辑wc.input随意输入字符vim wc.input&#x2F;&#x2F; 回到Hadoop目录执行程序hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput&#x2F; wcouput&#x2F;&#x2F; 查看结果cat wcoutput&#x2F;part-r-00000 伪分布式模式 配置集群 设置hadoop-env.sh：在文件中设置JAVA_HOME为本地JDK地址 设置core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; 设置hdfs-site.xml： 12345&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 启动集群 格式化NameNode：bin/hdfs namenode -format 启动NameNode和DataNode：sbin/start-dfs.sh(关闭stop) 查看集群 查看是否启动成功：jps(JDK中的命令,不是Linux命令,类似ps) web端查看HDFS文件系统：http://192.168.232.100:9870(需要CentOS主机上设置关闭防火墙,在3.x版本端口号默认为9870) 查看产生的log日志：cd /hadoop/logs 注意：不能一直格式化NameNode，格式化NameNode会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。(最好关闭NameNode和DataNode) 操作集群(所有命令类似于在Linux Terminal的命令行操作,需要加上固定前缀bin/hdfs dfs -) 在HDFS文件系统上创建一个input文件夹：bin/hdfs dfs -mkdir -p /user/sobxiong/input 将测试文件内容上传到文件系统上：bin/hdfs dfs -put wcinput/wc.input /user/sobxiong/input/ 查看上传的文件是否正确： 12bin/hdfs dfs -ls /user/sobxiong/input/bin/hdfs dfs -cat /user/sobxiong/input/wc.input 运行MapReduce程序：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/sobxiong/input/ /user/sobxiong/output 查看输出结果：bin/hdfs dfs -cat /user/sobxiong/output/* 也可以在浏览器的文件系统中查看 将测试文件内容下载到本地：bin/hdfs dfs -get /user/sobxiong/output/part-r-00000 ./wcoutput/ 删除输出结果：bin/hdfs dfs -rm -r /user/sobxiong/output 启动Yarn并运行MapReduce程序 配置集群 配置yarn-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置yarn-site.xml 1234567891011121314&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置Yarn应用的classPath --&gt;&lt;!-- 不配置出现：错误: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster --&gt;&lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;命令行下输入hadoop classpath的一长串环境&lt;/value&gt;&lt;/property&gt;&lt;!-- 不需要设置yarn.resourcemanager.hostname，可以直接通过ip+端口号的方式访问 --&gt; 配置mapred-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置mapred-site.xml： 123456&lt;!-- 指定MR运行在YARN上 --&gt;&lt;!-- 默认是local，本地文件 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 启动集群 启动前必须保证NameNode和DataNode已启动 启动ResourceManager和NodeManager：sbin/start-yarn.sh(关闭stop) 集群操作 yarn浏览器页面查看：8088端口 删除文件系统上的output文件：bin/hdfs dfs -rm -r /user/sobxiong/output 执行MapReduce程序：同上hadoop操作 查看结果：同上cat操作，也可以在浏览器端查看 配置历史服务器 配置mapred-site.xml： 1234567891011&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;172.16.85.130:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;172.16.85.130:19888&lt;/value&gt;&lt;/property&gt; 启动历史服务器：bin/mapred –daemon start historyserver(stop关闭) 查看历史服务器是否启动：jps 查看JobHistory：http://172.16.85.130:19888/jobhistory 配置日志的聚集： 概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上 好处：可以方便的查看到程序运行详情，方便开发调试 注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryManager 配置yarn-site.xml： 1234567891011&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 配置文件说明 默认配置文件： core-defalut.xml - hadoop-common-3.1.3.jar/core-default.xml hdfs-default.xml - hadoop-hdfs-3.1.3.jar/hdfs-default.xml yarn-default.xml - hadoop-yarn-common-3.1.3.jar/yarn-default.xml mapred-default.xml - hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml 自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置(优先级高) 完全分布式运行模式 虚拟机准备(3台，完全复制) 编写集群分发脚本xsync scp(secure copy)安全拷贝 定义：scp可以实现服务器与服务器之间的数据拷贝 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 scp -r(递归) $pdir/$fname $user@$host:$pdir/$fname rsync远程同步工具 作用：主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点 rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 rsync -r(递归)v(显示复制过程)l(拷贝符号连接) $pdir/$fname $user@$host:$pdir/$fname xsync集群分发脚本 需求：循环复制文件到所有节点的相同目录下 需求分析： rsync命令原始：rsync -rvl /opt/module root@hadoop2:/opt/ 期望脚本：xsync 需同步的文件名 说明：在home/sobxiong/bin这个目录下存放的脚本，sobxiong用户在系统任何地方都可以直接执行 脚本实现 在/home/sobxiong目录下创建bin目录，并在bin目录下创建xsync文件 在xsync中键入如下代码： 12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=2; host&lt;4; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 修改脚本xsync具有执行权限：chmod 777 xsync 调用脚本形式：xsync 文件名 集群配置 集群部署规划： 类型 hadoop1 hadoop2 hadoop3 HDFS NameNode、DataNode DataNode SecondaryNameNode、DataNode YARN NodeManager ResourceManager、NodeManager NodeManager 配置集群 核心配置文件core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; HDFS配置文件hdfs-site.xml 1234567891011&lt;!-- 副本数目 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop3:50090&lt;/value&gt;&lt;/property&gt; YARN配置文件yarn-site.xml 1234567891011&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop2&lt;/value&gt;&lt;/property&gt; MapReduce配置文件mapred-site.xml 12345&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 在集群上分发配置好的hadoop配置文件：xsync /opt/module/hadoop-3.1.3/etc 查看文件分发情况 集群单点启动 集群第一次启动，需要格式化NameNode：hadoop namenode -format(把log和data文件删除) 在hadoop1上启动NameNode：hadoop-daemon.sh start namenode 在hadoop1、2、3上分别启动DataNode：hadoop-daemon.sh start datanode SSH免密登陆配置 配置ssh 基本语法：ssh ip 无密钥配置 免密登录原理： 生成公钥和私钥：ssh-keygen -t rsa(生成文件id_rsa-私钥,id_rsa.pub-公钥) 将公钥拷贝到要免密登录的目标机器上：ssh-copy-id hadoop2(只是当前用户,root还需要另外配置) .ssh文件下(~/.ssh)的文件功能 known_hosts：记录ssh访问过的计算机的公钥 id_rsa：生成的私钥 id_rsa.pub：生成的公钥 authorized_keys：存放授权过的无密登录服务器公钥 群起集群 配置workers：vim etc/hadoop/workers；加入hadoop1、hadoop2、hadoop3(不能有空行和空格)；同步所有节点配置文件 - xsync etc/hadoop/workers 启动集群 集群第一次启动，需要格式化NameNode(格式化前关闭启动的所有namenode和datanode进程,然后再删除data和log数据)：bin/hdfs namenode -format 启动HDFS：sbin/start-dfs.sh(在hadoop1上启动,这样hadoo1、2、3均会启动对应的进程) 启动YARN：sbin/start-yarn.sh(在hadoop2上启动,在ResourceManager所在机器hadoop2上启动Yarn) 查看NameNode：hadoop1:9870 集群基本测试 上传文件到集群：bin/hdfs dfs -put xx xx 查看上传文件存储位置 查看HDFS文件存储路径：/opt/module/hadoop-3.1.3/data/tmp/dfs/data/current/BP-1002151198-172.16.85.130-1591848799222/current/finalized/subdir0/subdir0 查看HDFS在磁盘存储文件的内容：cat blk_xxx(文本文件) 拼接大文件：cat blk_xxx &gt;&gt; temp，最后temp就是初始的文件 集群启动/停止方式总结 各个服务组件逐一启动/停止 分别启动/停止HDFS组件：hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode 启动/停止YARN：yarn-daemon.sh start/stop resourcemanager/nodemanager 各个模块分开启动/停止(配置ssh是前提)常用 整体启动/停止HDFS：start-dfs.sh/stop-dfs.sh 整体启动/停止YARN：start-yarn.sh/stop-yarn.sh 集群时间同步 crontab定时任务： 基本语法：crontab[选项] 选项说明 -e：编辑crontab定时任务 -l：查询crontab任务 -r：删除当前用户所有的crontab任务 参数说明：***** [任务] *的含义： 第一个：一小时当中的第几分钟(0~59) 第二个：一天当中的第几个小时(0~23) 第三个：一个月当中的第几天(1~31) 第四个：一年当中的第几月(1~12) 第五个：一周当中的星期几(0~7,0和7均代表星期日) 特殊符号： ：代表任何时间。比如第一个“”代表一小时中每分钟都执行一次 ,：代表不连续的时间。如“0 8,12,16 * * *”命令，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 -：代表连续的时间范围。比如“0 5 * * 1-6”命令，代表在周一到周六的凌晨5点0分执行命令 /n：代表每隔多久执行一次。比如“/10 * * * *”命令，代表每隔10分钟就执行一遍命令 ntp方式进行同步 具体思路：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 具体实操 时间服务器配置： 检查ntp是否安装：rpm -qa&#124;grep ntp，有ntp、fontpackages-filesystem以及ntpdate 修改ntp配置文件 123456789101112# 授权1172.16.85.0-172.16.85.255网段上的所有机器可以从这台机器上查询和同步时间restrict 172.16.85.0 mask 172.16.85.130.0 nomodify notrap# 修改集群在局域网中,不使用其他互联网上的时间#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst# 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步server 127.127.1.0fudge 127.127.1.0 stratum 10 修改/etc/sysconfig/ntpd文件：SYNC_HWCLOCK=yes(让硬件时间与系统时间一起同步) 重新启动ntpd服务： 12service ntpd statusservice ntpd start 设置ntpd服务开机自启动：chkconfig ntpd on 其他机器配置(root用户)： 配置10分钟与时间服务器同步一次： 12crontab -e*/10 * * * * /usr/sbin/ntpdate hadoop1 修改任意机器时间：date -s “2020-11-11 11:11:11” 十分钟后查看机器是否与时间服务器同步：date Hadoop编译源码 前期准备 jar包准备(hadoop源码、JDK8、Maven、Ant、Protobuf)：Protobuf在Google的github中的Release页面(3.1.3Hadoop对应2.5.0版本) jar包安装 安装JDK 12345678tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module/# JAVA_HOME(/etc/profile)export JAVA_HOME=/opt/module/jdk1.8.0_251export PATH=$PATH:$JAVA_HOME/binsource /etc/profilejava -version 安装Maven 12345678910111213141516tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/# MAVEN_HOME(/etc/profile)export MAVEN_HOME=/opt/module/apache-maven-3.6.3export PATH=$PATH:$MAVEN_HOME/binsource /etc/profilemvn -version# 修改maven仓库镜像&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 安装Ant 12345678tar -zxvf apache-ant-1.10.8-bin.tar.gz -C /opt/module/# ANT_HOME(/etc/profile)export ANT_HOME=/opt/module/apache-ant-1.10.8export PATH=$PATH:$ANT_HOME/binsource /etc/profileant -version 安装glibc-headers和g++：yum install glibc-headers、yum install gcc-c++ 安装make和cmake：yum install make 安装cmake(要装3.x版本,低版本编译不通过) 1234567891011tar -zxvf cmake-3.17.3.tar.gz -C /opt/modulecd /opt/module/cmake-3.17.3./configuremakemake install# CMAKE_HOME(/etc/profile)export CMAKE_HOME=/opt/module/cmake-3.17.3export PATH=$PATH:$CMAKE_HOME/binsource /etc/profilecmake --version 安装protobuf： 12345678910111213tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/cd /opt/module/protobuf-2.5.0/./configuremakemake checkmake installldconfig# LD_LIBRARY_PATH(/etc/profile)export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0export PATH=$PATH:$LD_LIBRARY_PATHprotoc --version 安装openssl库：yum install openssl-devel 安装ncurses-devel库：yum install ncurses-devel 编译源码 解压源码到/opt目录 进入hadoop源码主目录 通过maven执行编译命令：mvn package -Pdist,native -DskipTests -Dtar HDFS概述 HDFS产出背景及定义 产生背景：随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种 定义：HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色 使用背景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用 HDFS优缺点 优点： 高容错性 数据自动保存多个副本。它通过增加副本的形式，提高容错性 某一个副本丢失以后，它可以自动恢复(通过将副本复制到另一个可用的节点) 适合处理大数据 数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据 文件规模：能够处理百万规模以上的文件数量，数量相当之大 可构建在廉价机器上，通过多副本机制，提高可靠性 缺点： 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的 无法高效的对大量小文件进行存储： 存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的 小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标 不支持并发写入、文件随机修改： 一个文件只能有一个写，不允许多个线程同时写 仅支持数据appen(追加)，不支持文件的随机修改 HDFS组成架构 NameNode(nn)：Master，一个主管、管理者 管理HDFS的名称空间 配置副本策略 管理数据块(Block)映射信息 处理客户端读写请求 DataNode：Slave。NameNode下达命令，DataNode执行实际的操作 存储实际的数据块 执行数据块的读/写操作 Client：客户端 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传 与NameNode交互，获取文件的位置信息 与DataNode交互，读取或者写入数据 Client提供一些命令来管理HDFS，比如NameNode格式化 Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作 Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务 辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode 在紧急情况下，可辅助恢复NameNode HDFS文件块大小HDFS中的文件在物理上是分块存储(Block)，块的大小可以通过配置参数(dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M为什么文件块的大小不能设置太小，也不能设置太大？ HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢 总结：HDFS块的大小设置主要取决于磁盘传输速率 HDFS的Shell操作 基本语法bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令其中dfs是fs的实现类 命令大全：bin/hadoop fs 使用命令： -help：输出命令的帮助(hadoop fs -help rm) -ls：显示目录信息(hadoop fs -ls /) -mkdir：在HDFS上创建目录[-p递归](hdoop fs -mkdir -p /sobxiong/test) -moveFromLocal：从本地剪切粘贴到HDFS[前路径为本地,后路径为HDFS](hadoop fs -moveFromLocal ./test.txt /sobxiong/test/) -appendToFile：追加一个文件到已经存在的文件末尾[前路径为本地,后路径为HDFS](hadoop fs -appendToFile ./test.txt /sobxiong/test/test.txt) -cat：显示文件内容(hadoop fs -cat /sobxiong/test/test.txt) -chgrp、-chmod、-chown：修改文件所属的权限，同Linux文件系统中的用法 -copyFromLocal：从本地文件系统拷贝文件到HDFS中，同-moveFromLocal -copyToLocal：从HDFS拷贝文件到本地[前路径为HDFS,后路径为本地](hadoop fs -copyToLocal /sobxiong/test/test.txt ./) -cp：把文件从HDFS的一个路径拷贝到HDFS的另一个路径 -mv：把文件从HDFS的一个路径移动到HDFS的另一个路径 -get：等同于copyToLocal(用法同copyToLocal)，从HDFS下载文件到本地 -getmerge：合并下载多个文件(hadoop fs -getmerge /sobxiong/test/* ./all.txt) -put：等同于copyFromLocal(用法同copyFromLocal) -tail：显示一个文件的末尾(hadoop fs -tail /sobxiong/test/test.txt) -rm：删除文件或文件夹[-r递归删除目录] -rmdir：删除空目录 -du：统计文件夹的大小信息[-h显示单位,-s总和](hadoop fs -du -h -s /) -setrep：设置HDFS中文件的副本数目[这里设置的副本数只是记录在NameNode的元数据中,是否真的会有这么多副本还得看DataNode的数量.因为目前只有3台设备,最多也就3个副本,只有节点数的增加到10台时,副本数才能达到10;只要加入一台设备,就会把副本复制到设备上,直到加到10台](hadoop fs -setrep 10 /sobxiong/test/test.txt) HDFS客户端操作 客户端环境准备 将Hadoop安装到mac上，并设置环境变量 123456# HADOOP_HOME(~/.bash_profile)export HADOOP_HOME=\"/Users/sobxiong/module/hadoop-3.1.3\"export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource ~/.bash_profilehadoop version 创建Maven工程测试：idea创建quickstart项目 导入依赖： 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt; 创建测试类 12345678910111213141516public class HDFSClient &#123; public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException &#123; Configuration configuration = new Configuration(); // configuration.set(\"fs.defaultFS\", \"hdfs://hadoop1:9000\"); // 1、获取hdfs客户端对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、在hdfs上创建路径 fileSystem.mkdirs(new Path(\"/sobxiong2/test\")); // 3、关闭资源 fileSystem.close(); System.out.println(\"finish\"); &#125;&#125; HDFS的API操作 文件上传 12345678910111213141516171819202122232425262728293031/** * 参数优先级： * 1、客户端代码中设置的值 * 2、ClassPath(resources)下的用户自定义配置文件(hdfs-site.xml) * 3、服务器的默认配置 * @throws Exception */// 1、文件上传@Testpublic void testCopyFromLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); configuration.set(\"dfs.replication\", \"2\"); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行上传API fileSystem.copyFromLocalFile(new Path(\"/Users/sobxiong/Documents/文件块大小大致计算.png\"), new Path(\"/sobxiong/test2.png\")); // 3、关闭资源 fileSystem.close();&#125;// 将hdfs-site.xml拷贝至项目的根目录resources资源文件夹下&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 文件下载 123456789101112131415// 2、文件下载@Testpublic void testCopyToLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行下载操作 // fileSystem.copyToLocalFile(new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test.png\")); // 本地模式,true,不会产生crc文件 fileSystem.copyToLocalFile(false, new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test1.png\"), true); // 3、关闭资源 fileSystem.close();&#125; 文件删除 12345678910111213// 3、文件删除@Testpublic void testDelete() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、文件删除(第二个参数,是否递归删除,文件夹时有效) fileSystem.delete(new Path(\"/sobxiong/test2.png\"), false); // 3、关闭资源 fileSystem.close();&#125; 文件更名 12345678910111213// 4、文件更名@Testpublic void testRename() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行更名操作 fileSystem.rename(new Path(\"/sobxiong/test1.png\"), new Path(\"/sobxiong/1tset.png\")); // 3、关闭资源 fileSystem.close();&#125; 文件详情查看 1234567891011121314151617181920212223242526272829303132// 5、文件详情查看@Testpublic void testListFiles() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、查看文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(new Path(\"/\"), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); // 查看文件名称、权限、长度 System.out.println(\"name: \" + fileStatus.getPath().getName()); System.out.println(\"permission: \" + fileStatus.getPermission()); System.out.println(\"length: \" + fileStatus.getLen()); // 查看块信息 BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(\"host = \" + host); &#125; System.out.println(\"----------------\"); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; 判断是文件还是文件夹 1234567891011121314151617181920// 6、判断是文件还是文件夹@Testpublic void testListStatus() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、判断操作 FileStatus[] fileStatuses = fileSystem.listStatus(new Path(\"/\")); for (FileStatus fileStatus : fileStatuses) &#123; if (fileStatus.isFile()) &#123; System.out.println(\"file = \" + fileStatus.getPath().getName()); &#125; else &#123; System.out.println(\"dir = \" + fileStatus.getPath().getName()); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; HDFS的I/O流操作 HDFS文件上传 1234567891011121314151617// 把本地文件上传到HDFS根目录@Testpublic void upload() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FileInputStream fileInputStream = new FileInputStream(new File(\"/Users/sobxiong/Downloads/课件.rar\")); // 3、获取输出流 FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path(\"/test.rar\")); // 4、流的对拷 IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fsDataOutputStream); IOUtils.closeStream(fileInputStream); fileSystem.close();&#125; HDFS文件下载 1234567891011121314151617// 从HDFS下载文件到本地磁盘@Testpublic void download() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/test.rar\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/test1.rar\")); // 4、流的对拷 IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125; 定位文件获取 12345678910111213141516171819202122232425262728293031323334353637383940414243// 下载第一块@Testpublic void readFileSeek1() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part1\")); // 4、流的对拷(只拷贝第一个块128MB) byte[] buf = new byte[1024]; for (int i = 0; i &lt; 1024 * 128; i++) &#123; fsDataInputStream.read(buf); fileOutputStream.write(buf); &#125; // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载第二块@Testpublic void readFileSeek2() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、设置指定读取的起点 fsDataInputStream.seek(1024 * 1024 * 128); // 4、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part2\")); // 5、流的对拷(拷贝剩下的两个Block块) IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 6、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载完第一块和剩余的部分后,可通过cat hadoop-3.1.3.tar.gz.part2 &gt;&gt; hadoop-3.1.3.tar.gz.part1将剩余部分追加到第一块上,修改文件名(删去.part1),就得到完整的文件 HDFS的数据流 HDFS写数据流程 剖析文件写入： 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在 NameNode返回是否可以上传 客户端请求第一个Block上传到哪几个DataNode服务器上 NameNode返回3个DataNode节点，分别为dn1、dn2、dn3 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成 dn1、dn2、dn3逐级应答客户端 客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器(此后重复执行3-7步) 网络拓扑-节点距离计算在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。节点距离：两个节点到达最近的共同祖先的距离总和 机架感知(2.7.2版本副本节点选择,性能和安全的综合考量) 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个 第二个副本和第一个副本位于相同机架，随机节点 第三个副本位于不同机架，随机节点 HDFS读数据流程 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址 挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据 DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验) 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件 NameNode和SecondaryNameNode NN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并 第一阶段：NameNode启动 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存 客户端对元数据进行增删改的请求 NameNode记录操作日志，更新滚动日志(先记日志,类似数据库) NameNode在内存中对数据进行增删改 第二阶段：Secondary NameNode工作 Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果 Secondary NameNode请求执行CheckPoint NameNode滚动正在写的Edits日志 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode Secondary NameNode加载编辑日志和镜像文件到内存，并合并 生成新的镜像文件fsimage.chkpoint 拷贝fsimage.chkpoint到NameNode NameNode将fsimage.chkpoint重新命名成fsimage 补充：Fsimage：NameNode内存中元数据序列化后形成的文件。Edits：记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)。NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息)，如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并(所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage)。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。SecondaryNameNode首先会询问NameNode是否需要CheckPoint(触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了)。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中 Fsimage和Edits解析 概念 NameNode被格式化之后，将在/data/tmp/dfs/name/current目录中产生如下文件fsimage_0000000000000000000fsimage_0000000000000000000.md5seen_txidVERSION Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息 Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中 seen_txid文件保存的是一个数字，就是最后一个edits_的数字 每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并 查看Fsimage文件：hdfs oiv -p 文件类型(XML) -i 镜像文件 -o 转换后文件输出路径例：hdfs oiv -p XML -i fsimage_0000000000000000025 o fsimage.xmlFsimage中没有记录块所对应的DataNode，为什么？在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报 查看Edits文件：hdfs oev -p 文件类型(XML) -i 编辑日志 -o 转换后文件输出路径例：hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o edits.xmlNameNode如何确定下次开机启动的时候合并那些Edits？通过seen_txid查看 CheckPoint时间设置 通常情况下，SecondaryNameNode每隔一小时执行一次 一分钟检查一次操作次数 当操作次数达到1百万时，SecondaryNameNode执行一次 123456789101112131415&lt;!-- hdfs-default.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt;1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; NameNode故障处理 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录 kiil -9 NameNode进程编号(用jps查看NameNode的进程编号) 删除NameNode存储的数据(data/tmp/dfs/name)：rm -rf /data/tmp/dfs/name/* 拷贝SecondaryNameNode(hadoop2)中数据到原NameNode(hadoop1)存储数据目录：scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary/* ./name/ 重新启动NameNode(hadoop1)：sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中 修改hdfs-site.xml(加入下述内容)： 123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; kill -9 NameNode进程 删除NameNode存储的数据(同方法一) 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 123scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary /data/tmp/dfs/cd /data/tmp/dfs/namesecondaryrm -rf in_use.lock 导入检查点数据(等待一会ctrl+c结束掉) 启动NameNode：sbin/hadoop-daemon.sh start namenode 集群安全模式 概述 NameNode启动NameNode启动时，首先将镜像文件(Fsimage)载入内存，并执行编辑日志(Edits)中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的 DataNode启动 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统 安全模式退出判断如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值：dfs.replication.min=1)。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式 基本语法集群处于安全模式，不能执行重要操作(写操作)。集群启动完成后，自动退出安全模式 查看安全模式状态：bin/hdfs dfsadmin -safemode get 进入安全模式状态：bin/hdfs dfsadmin -safemode enter 离开安全模式状态：bin/hdfs dfsadmin -safemode leave 等待安全模式状态：bin/hdfs dfsadmin -safemode wait 案例模拟等待安全模式 查看当前模式：bin/hdfs dfsadmin -safemode get 先进入安全模式：bin/hdfs dfsadmin -safemode enter 创建并执行下面的脚本 12345678910touch safemode.shvim safemode.sh# safemode.sh#!/bin/bashhdfs dfsadmin -safemode waithdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /chmod 777 safemode.sh./safemode.sh 再打开一个窗口，执行：hdfs dfsadmin -safemode leave 安全模式退出，HDFS集群上已经有上传的数据了 NameNode多目录配置 NameNode的本地目录可以配置成多个，但每个目录存放内容相同(相当于备份)，增加了可靠性 具体配置如下 在hdfs-site.xml文件中增加如下内容 1234&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 停止集群，删除data和logs中所有数据 12345hadoop2：sbin/stop-yarn.shhadoop2：rm -rf data/ logs/hadoop1：sbin/stop-dfs.shhadoop2：rm -rf data/ logs/shadoop3：rm -rf data/ logs/s 格式化集群并启动 123hadoop1：bin/hdfs namenode -formathadoop1：sbin/start-dfs.shhadoop2：sbin/start-yarn.sh 查看结果：dfs目录下出现两个目录name1和name2 DataNode DataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳 DataNode启动后向NameNode注册，通过后，周期性(1小时)的向NameNode上报所有的块信息 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用 集群运行中可以安全加入和退出一些机器 数据完整性DataNode节点保证数据完整性的方法： 当DataNode读取Block的时候，它会计算CheckSum(类似crc校验位) 如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏 Client读取其他DataNode上的Block DataNode在其文件创建后周期验证CheckSum 掉线时限参数设置\bhdfs-default.xml： 12345678910&lt;!-- heartbeat.recheck.interval单位为毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;!-- dfs.heartbeat.interval单位为秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 服役新数据节点 需求：随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点 环境准备 利用hadoop3主机再克隆一台hadoop4主机 修改hadoop4主机IP地址和主机名称 在hadoop1主机上将/etc/hosts下添加hadoop4的ip地址映射条目，并分发到hadoop2-4 在hadoop1主机上修改hadoop-3.1.3/etc/hadoop/workers通信节点，添加hadoop4，并分发到hadoop2-4 hadoop4主机删除原来HDFS文件系统留存的文件(data和log目录)——不然会发生3和4轮换出现的问题,因为3和4有着一样的data和log reboot重启加载配置 服役新节点具体步骤 直接启动DataNode，即可关联到集群 12bin/hdfs --daemon start datanodebin/yarn --daemon start nodemanager 在hadoop4上上传文件 如果数据不均衡，可以使用命令实现集群的在平衡：sbin/start-balancer.sh 退役旧数据节点(此方法貌似不管用) 添加白名单：添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出 在NameNode的hadoop-3.1.3/etc/hadoop目录下创建dfs.hosts文件 1234567touch dfs.hostsvim dfs.hosts# dfs.hosts(不添加hadoop4,不允许有空行和空格)hadoop1hadoop2hadoop3 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 配置文件分发：xsync hdfs-site.xml;xsync dfs.hosts 刷新NameNode：hdfs dfsadmin -refreshNodes 更新ResourceManager：yarn rmadmin -refreshNodes","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://sobxiong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://sobxiong.github.io/tags/Hadoop/"}]}],"categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://sobxiong.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"},{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"},{"name":"大数据","slug":"大数据","permalink":"https://sobxiong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://sobxiong.github.io/tags/Hadoop/"}]}