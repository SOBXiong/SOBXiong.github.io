{"meta":{"title":"SOBXiong的博客","subtitle":"","description":"一只编程菜鸡，对Android、Java后端、大数据和Vue雨露均沾","author":"SOBXiong","url":"https://sobxiong.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-06-23T14:47:17.665Z","updated":"2020-06-23T14:47:17.656Z","comments":true,"path":"404.html","permalink":"https://sobxiong.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2020-06-23T14:45:30.095Z","updated":"2020-06-23T14:45:30.085Z","comments":true,"path":"about/index.html","permalink":"https://sobxiong.github.io/about/index.html","excerpt":"","text":"一只编程菜鸡，对Android、Java后端、大数据和Vue雨露均沾"},{"title":"所有标签","date":"2020-06-23T14:46:49.268Z","updated":"2020-06-23T14:46:49.258Z","comments":true,"path":"tags/index.html","permalink":"https://sobxiong.github.io/tags/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2020-06-23T14:46:03.448Z","updated":"2020-06-23T14:46:03.438Z","comments":true,"path":"categories/index.html","permalink":"https://sobxiong.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"动态规划","slug":"Algorithm/LeetCode/动态规划","date":"2020-09-21T13:35:06.000Z","updated":"2020-09-21T13:37:45.331Z","comments":true,"path":"2020/09/21/Algorithm/LeetCode/动态规划/","link":"","permalink":"https://sobxiong.github.io/2020/09/21/Algorithm/LeetCode/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","excerpt":"内容 动态规划介绍 方法总结 LeetCode题集","text":"内容 动态规划介绍 方法总结 LeetCode题集 动态规划介绍方法总结LeetCode题集 Q10：正则表达式匹配","categories":[],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://sobxiong.github.io/tags/Algorithm/"}]},{"title":"滑动窗口","slug":"Algorithm/LeetCode/滑动窗口","date":"2020-09-20T14:21:20.000Z","updated":"2020-09-21T13:37:29.443Z","comments":true,"path":"2020/09/20/Algorithm/LeetCode/滑动窗口/","link":"","permalink":"https://sobxiong.github.io/2020/09/20/Algorithm/LeetCode/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","excerpt":"内容 滑动窗口介绍 方法总结 LeetCode题集","text":"内容 滑动窗口介绍 方法总结 LeetCode题集 滑动窗口介绍方法总结LeetCode题集 Q3：无重复字符的最长字串题解：","categories":[],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://sobxiong.github.io/tags/Algorithm/"}]},{"title":"JUC","slug":"Language/Java/JUC","date":"2020-09-18T11:28:07.000Z","updated":"2020-09-20T14:26:22.455Z","comments":true,"path":"2020/09/18/Language/Java/JUC/","link":"","permalink":"https://sobxiong.github.io/2020/09/18/Language/Java/JUC/","excerpt":"内容 JUC是什么 Lock接口 线程间通信 线程间定制化调用通信 线程八锁 线程不安全集合 Callable接口 JUC辅助类","text":"内容 JUC是什么 Lock接口 线程间通信 线程间定制化调用通信 线程八锁 线程不安全集合 Callable接口 JUC辅助类 JUC是什么 JUC介绍：JDK1.5时Java引入的并发编程工具包——java.util.concurrent 基础知识回顾： 进程/线程是什么： 进程：进程是一个具有一定独立功能的程序关于某个数据集合的一次运行活动。它是操作系统动态执行的基本单元，在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元 线程：通常在一个进程中可以包含若干个线程，当然一个进程中至少有一个线程，不然没有存在的意义。线程可以利用进程所拥有的资源，在引入线程的操作系统中，通常都是把进程作为分配资源的基本单位，而把线程作为独立运行和独立调度的基本单位，由于线程比进程更小，基本上不拥有系统资源，故对它的调度所付出的开销就会小得多，能更高效的提高系统多个程序间并发执行的程度 进程/线程例子： 进程：QQ.ext、word.exe 线程：word检查拼写、word容灾备份 线程的状态 123456789// Thread.javapublic enum State &#123; NEW, // 创建 RUNNABLE, // 准备就绪(还需等待OS),Thread实例start()后并不是马上运行,只是进入就绪状态,等待OS BLOCKED, // 阻塞 WAITING, // 等待(一直等下去——不见不散) TIMED_WAITING, // 等待(有时限的等待——过时不候) TERMINATED; // 终止&#125; wait/sleep的区别： wait/sleep都可以使当前线程暂停 wait放开手睡眠，放开手里的锁 sleep握紧手睡眠，唤醒后手里还有锁 并发/并行各自都是什么： 并发：同一时刻多个线程在访问同一个资源(例子：抢车票) 并行：多项工作同时执行，之后在汇合(例子：泡脚玩手机) Lock接口 复习Synchronized： 多线程口诀1、2： 高内聚低耦合 线程、操作、资源类 实现步骤： 创建资源类 资源类里创建同步方法(代码块) 创建线程，访问资源 卖票实例： 1234567891011121314151617181920212223242526272829303132333435/*** 题目：三个售票员,卖100张票* 多线程编程的企业级套路 + 模版* 1、高内聚低耦合* 2、线程 操作(对外暴露的调用方法) 资源类*/public class SaleTicket &#123; public static void main(String[] args) &#123; final Ticket ticket = new Ticket(); // 创建线程不能直接继承Thread类,因为Java是单继承,资源宝贵,要使用接口方式 // 如果方法体简单,可以不用继承Runnable接口,而直接采用匿名内部类/lambda表达式 // 创建线程要使用两个参数Thread(runnable, name)的方式 new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"C\").start(); &#125;&#125;// 资源类class Ticket &#123; private int number = 100; public synchronized void saleTicket() &#123; if (number &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + \"\\t卖出第\" + (number--) + \"张票\\t,还剩下\" + number + \"张票\"); &#125; &#125;&#125; Lock接口： Lock介绍(摘录自JDK1.8)：Lock implementations provide more extensive locking operations than can be obtained using synchronized methods and statements. They allow more flexible structuring, may have quite different properties, and may support multiple associated Condition objects —— 锁实现提供了比使用同步方法和语句可以获得的更广泛的锁操作。它们允许更灵活的结构，可能具有非常不同的属性，并且可能支持多个关联的条件对象 Lock的常用实现类ReentrantLock(可重入锁) 1234567891011121314151617181920212223// Lock使用模版class SharedResource &#123; private final ReentrantLock lock = new ReentrantLock(); /** * synchronized与Lock的区别 * 1、首先synchronized是java内置关键字,在jvm层面;Lock是个java类 * 2、synchronized无法判断是否获取锁的状态,Lock可以判断是否获取到锁 * 3、synchronized会自动释放锁(a:线程执行完同步代码会释放锁;b:线程执行过程中发生异常会释放锁);Lock需在finally中手动释放锁(unlock()方法释放锁),否则容易造成线程死锁 * 4、用synchronized关键字的两个线程1和线程2,如果当前线程1获得锁,线程2线程等待。如果线程1阻塞,线程2则会一直等待下去;而Lock锁就不一定会等待下去,如果尝试获取不到锁,线程可以不用一直等待就结束了 * 5.synchronized的锁可重入、不可中断、非公平,而Lock锁可重入、可判断、可公平(默认非公平,二者皆可) * 6.Lock锁适合大量同步的代码的同步问题,synchronized锁适合代码少量的同步问题 */ public void competitionMethod() &#123; // block until condition holds lock.lock(); try &#123; // ... method body &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; Lock方式卖票实例： 123456789101112131415161718192021222324252627282930313233public class SaleTicket &#123; public static void main(String[] args) &#123; final Ticket ticket = new Ticket(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"C\").start(); &#125;&#125;// 资源类class Ticket &#123; private int number = 100; private final Lock lock = new ReentrantLock(); public void saleTicket() &#123; lock.lock(); try &#123; if (number &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + \"\\t卖出第\" + (number--) + \"张票\\t,还剩下\" + number + \"张票\"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 线程间通信 题目：两个线程来操作初始值为零的一个变量，实现一个线程对该变量加1,另一个线程对该变量减1。实现交替10个轮次,变量初始值为0 线程间通信： 生产者/消费者模型 通知等待唤醒机制 多线程口诀3： 判断 干活 通知 老版本synchronized实现： 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Cake &#123; private int number = 0; public synchronized void increment() throws InterruptedException &#123; // 1、判断 if (number != 0) &#123; this.wait(); &#125; // 2、干活 number++; System.out.println(Thread.currentThread().getName() + \"\\t生产,剩余\" + number); // 3、通知 this.notifyAll(); &#125; public synchronized void decrement() throws InterruptedException &#123; // 1、判断 if (number == 0) &#123; this.wait(); &#125; // 2、干活 number--; System.out.println(Thread.currentThread().getName() + \"\\t消费,剩余\" + number); // 3、通知 this.notifyAll(); &#125;&#125;/*** 题目：两个线程来操作初始值为零的一个变量,实现一个线程对该变量加1,另一个线程对该变量减1;实现交替10个轮次,变量初始值为0* 1、高聚合低耦合前提下,线程操作资源类* 2、判断/干活/通知*/public class ThreadWaitNotify &#123; public static void main(String[] args) &#123; Cake cake = new Cake(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.increment(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ProducerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.decrement(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ConsumerA\").start(); &#125;&#125; 结果：符合要求 如果换成4个线程(2消费者,2生产者) 12345678910111213141516171819202122232425262728293031323334353637383940414243// 只改变mainpublic static void main(String[] args) &#123; Cake cake = new Cake(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.increment(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ProducerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.decrement(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ConsumerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.increment(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ProducerB\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.decrement(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ConsumerB\").start();&#125; 结果：出现错误，有可能生产出大于1的cake来 原因：换成4个线程会导致错误——虚假唤醒，因为在Java多线程判断时，不能用if。错误出在了判断上面：如果突然有一个增加cake的线程进入到if里面了，但突然中断了并交出控制权。等到唤醒后由于是if，不需要再次进行验证，而是直接走下去了，所以进行了错误的增加 解决方法：把所有的资源类的increment()和decrement()方法中的if判断变为while判断 多线程口诀4：注意多线程之间的虚假唤醒 新版本Lock实现： 新老版本对标： synchronized - Lock wait - await notify - signal Lock示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Cake1 &#123; private int number = 0; private final Lock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); public void increment() &#123; lock.lock(); try &#123; // 1、判断 while (number != 0) &#123; condition.await(); System.out.println(Thread.currentThread().getName() + \"\\t生产,剩余\" + number); &#125; // 2、干活 number++; // 3、通知 condition.signalAll(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void decrement() &#123; lock.lock(); try &#123; // 1、判断 while (number == 0) &#123; condition.await(); System.out.println(Thread.currentThread().getName() + \"\\t消费,剩余\" + number); &#125; // 2、干活 number--; // 3、通知 condition.signalAll(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;/*** 题目：两个线程来操作初始值为零的一个变量,实现一个线程对该变量加1,另一个线程对该变量减1;实现交替10个轮次,变量初始值为0* 1、高聚合低耦合前提下,线程操作资源类* 2、判断/干活/通知* 3、多线程交互中,必须要防止多线程的虚假唤醒,也即(判断只能用while,不能用if)*/public class ThreadAwaitSignal &#123; public static void main(String[] args) &#123; Cake1 cake = new Cake1(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.increment(); &#125; &#125;, \"ProducerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.decrement(); &#125; &#125;, \"ConsumerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.increment(); &#125; &#125;, \"ProducerB\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.decrement(); &#125; &#125;, \"ConsumerB\").start(); &#125;&#125; 线程间定制化调用通信 题目：多线程之间按顺序调用，实现A -&gt; B -&gt; C，三个线程启动,要求如下：AAAAA打印5次，BBBBB打印10次，CCCCC打印15次，以上操作进行10轮 多线程口诀5：标志位 实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class ShareResource &#123; private int number = 1; // 1 -&gt; A, 2 -&gt; B, 3 -&gt; C // 一把锁lock private final Lock lock = new ReentrantLock(); // 三把钥匙condition private final Condition conditionA = lock.newCondition(); private final Condition conditionB = lock.newCondition(); private final Condition conditionC = lock.newCondition(); public void printFromA() &#123; lock.lock(); try &#123; // 1、判断 while (number != 1) &#123; conditionA.await(); &#125; // 2、干活 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"AAAAA ~~~\"); &#125; // 3、通知(修改标志位,通知下一个) number = 2; conditionB.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printFromB() &#123; lock.lock(); try &#123; // 1、判断 while (number != 2) &#123; conditionB.await(); &#125; // 2、干活 for (int i = 0; i &lt; 10; i++) &#123; System.out.println(\"BBBBB ~~~\"); &#125; // 3、通知(修改标志位,通知下一个) number = 3; conditionC.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printFromC() &#123; lock.lock(); try &#123; // 1、判断 while (number != 3) &#123; conditionC.await(); &#125; // 2、干活 for (int i = 0; i &lt; 15; i++) &#123; System.out.println(\"CCCCC ~~~\"); &#125; // 3、通知(修改标志位,通知下一个) number = 1; conditionA.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;/*** 题目：多线程之间按顺序调用,实现A -&gt; B -&gt; C,三个线程启动,要求如下：AAAAA打印5次,BBBBB打印10次,CCCCC打印15次,以上操作进行10轮* 1、高聚合低耦合前提下,线程操作资源类* 2、判断/干活/通知* 3、多线程交互中,必须要防止多线程的虚假唤醒,也即(判断只能用while,不能用if)* 4、标志位*/public class ThreadOrderAccess &#123; public static void main(String[] args) &#123; ShareResource shareResource = new ShareResource(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) shareResource.printFromA(); &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) shareResource.printFromB(); &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) shareResource.printFromC(); &#125;, \"C\").start(); &#125;&#125; 线程八锁 八锁示例： 情况1(标准访问) 12345678910111213141516171819202122232425262728293031class Phone &#123; public synchronized void sendEmail() throws Exception &#123; System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 1、标准访问,请问先打印邮件还是短信? 邮件public class Lock8 &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况2(其一线程sleep) 1234567891011121314151617181920212223242526272829303132class Phone &#123; public synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 2、邮件方法暂停4秒,请问先打印邮件还是短信? 邮件public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况3(新增一个普通方法) 123456789101112131415161718192021222324252627282930313233343536class Phone &#123; public synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125; public void hello() &#123; System.out.println(\"Hello ~~~\"); &#125;&#125;// 3、在2的基础上,新增并使用一个普通方法hello(),请问先打印邮件还是hello? hellopublic class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.hello(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况4(两个对象) 123456789101112131415161718192021222324252627282930313233class Phone &#123; public synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 4、两部手机,请问先打印邮件还是短信? 短信public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); Phone phone1 = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone1.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况5(改为静态同步方法) 1234567891011121314151617181920212223242526272829303132class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public static synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 5、两个静态同步方法,同一部手机,请问先打印邮件还是短信? 邮件public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况6(两个对象调用静态同步方法) 123456789101112131415161718192021222324252627282930313233class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public static synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 6、两个静态同步方法,两部手机,请问先打印邮件还是短信? 邮件public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); Phone phone1 = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone1.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况7(一个静态同步方法,一个普通同步方法) 1234567891011121314151617181920212223242526272829303132class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 7、一个普通同步方法,一个静态同步方法,一部手机,请问先打印邮件还是短信? 短信public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况8(一个静态同步方法,一个普通同步方法,两个对) 123456789101112131415161718192021222324252627282930313233class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 8、一个普通同步方法,一个静态同步方法,两部手机,请问先打印邮件还是短信? 短信public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); Phone phone1 = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone1.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 八锁分析： 情况1、2：如果一个对象有多个synchronized方法，某个时刻内只要有一个线程去调用当前对象的一个synchronized方法，那么其它线程只能等待。换句话说，某个时刻内只能有唯一一个线程去访问这些synchronized方法。锁作用的是当前对象this，this被锁定后其它的线程都不能进入到当前对象的其它的synchronized方法(情况1、2都进入了sendEmail()方法,因此不响应sendMessage()方法) 情况3、4：普通方法和同步锁无关，一个线程调用了synchronized方法，另一个线程可以同时调用普通方法；当换成两个对象后，synchronized锁的是对象实例，而当前有两个实例，锁的就不是同一把锁了，因此sendMessage先打印 情况5、6：对于静态同步方法，锁是当前类的Class对象，对于同一个Phone类锁是相同的(Phone.class)，因此进入sendEmail()方法后不会响应sendMessage()方法，而是等待sendEmail()方法执行完毕 情况7、8：对于普通同步方法和静态同步方法，他们锁的对象不同，普通同步方法锁的是当前对象实例(Phone的一个实例对象)，而静态同步方法锁的是当前类的Class对象(Phone.class)。他们锁的对象不同，不会相互影响，因此先打印sendMessage 线程八锁总结： synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现为以下3种形式： 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的Class对象 对于同步方法块，锁是Synchonized括号里配置的对象 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。也就是说如果一个实例对象的普通同步方法获取锁后，该实例对象的其他普通同步方法必须等待已获取锁的方法释放锁后才能获取锁 其他实例对象的普通同步方法跟当前实例对象的普通同步方法用的是不同的锁(不同的实例对象)，所以无须等待当前实例对象已获取锁的普通同步方法释放锁就可以获取他们自己的锁 所有静态同步方法用的是同一把锁——类对象本身，普通同步方法的锁和静态同步方法的锁是两个不同的对象，所以静态同步方法与普通同步方法之间是不会有竞态条件的。但一旦一个静态同步方法获取锁后，其他静态同步方法都必须等待该方法释放锁后才能获取锁(而不管是同一个实例对象的静态同步方法之间，还是不同实例对象的静态同步方法之间，只要它们是同一个类的实例对象) 线程不安全集合 请举例说明集合类是不安全的 示例： List集合 情况1：3个线程同时读写ArrayList(结果：运行基本不报错,但是会出现List中有时内容为null或者集合元素个数不等于3的情况) 情况2：30个线程同时读写ArrayList(结果：运行报错——java.util.ConcurrentModificationException并发修改异常) 123456789public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 3 or 30; i++) &#123; new Thread(() -&gt; &#123; list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(list); &#125;, \"list\" + i).start(); &#125;&#125; 出错原因：ArrayList本身就是线程不安全的(为了性能考虑,不加锁性能提升但会出错误) 123456// ArrayList.javapublic boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 解决方案： Vector(线程安全,加了synchronized,加锁数据一致但性能下降;性能较差,不要使用) 1234567// Vector.javapublic synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; Collections工具类(少量数据可以使用)：Collections.synchronizedList(new ArrayList&lt;&gt;()) CopyOnWriteArrayList(推荐使用) 123456789101112131415161718192021// CopyOnWriteArrayList.java// CopyOnWrite容器即写时复制的容器。往一个容器添加元素的时候,不直接往当前容器Object[]添加,// 而是先将当前容器Object[]进行Copy,复制出一个新的容器Object[] newElements,// 然后向新的容器Object[] newElements里添加元素// 添加元素后,再将原容器的引用指向新的容器setArray(newElements)。// 这样做的好处是可以对CopyOnWrite容器进行并发的读,而不需要加锁,因为当前容器不会添加任何元素。// 所以CopyOnWrite容器是一种读写分离的思想,读和写不同的容器 public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; Set集合： 示例： 123456789101112public static void main(String[] args) &#123; // new HashSet&lt;&gt;() // Collections.synchronizedSet(new HashSet&lt;&gt;()) // new CopyOnWriteArraySet&lt;&gt;() Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(() -&gt; &#123; set.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(set); &#125;, \"set\" + i).start(); &#125;&#125; 解决方案： Collections工具类：Collections.synchronizedSet(new HashSet&lt;&gt;()) CopyOnWriteArraySet Map集合： 示例： 123456789101112public static void main(String[] args) &#123; // new HashMap&lt;&gt;() // Collections.synchronizedMap(new HashMap&lt;&gt;()) // new ConcurrentHashMap&lt;&gt;() Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); for (int i = 0; i &lt; 300; i++) &#123; new Thread(() -&gt; &#123; map.put(Thread.currentThread().getName(), UUID.randomUUID().toString().substring(0, 8)); System.out.println(map); &#125;, \"map\" + i).start(); &#125;&#125; 解决方案： Collections工具类：Collections.synchronizedMap(new HashMap&lt;&gt;()) ConcurrentHashMap Callable接口 获得多线程的方法有几种? 继承Thread类(不建议使用) 实现Runnale接口 实现Callable接口 从线程池获取 Callable是什么：是一个JDK1.5推出的线程接口，比Runnable更强大。是一个函数式接口，可用作lambda表达式 与Runnable的区别： 是否有返回值 是否会抛出异常 落地方法不同(run()/call()) 123456789class MyThread implements Runnable &#123; @Override public void run() &#123;&#125;&#125;class MyThread2 implements Callable&lt;String&gt; &#123; @Override public String call() throws Exception &#123; return null; &#125;&#125; 怎么使用： 直接替换runnable：不可行，Thread的构造方法传参都是Runnable接口，没有Callable接口 找中间人FutureTask： 123456789101112131415161718192021222324252627282930313233343536373839// FutureTask.classpublic class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; // ... public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable &#125; // ...&#125;class MyThread2 implements Callable&lt;String&gt; &#123; @Override public String call() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"~~~\" + Thread.currentThread().getName() + \" Come in call() ~~~\"); return \"1024\"; &#125;&#125;/*** 多线程第3种创建多线程的方式* get()方法一般请放在最后一行,get()方法会阻塞线程*/public class CallableDemo &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(new MyThread2()); new Thread(futureTask, \"A\").start(); new Thread(futureTask, \"B\").start(); // System.out.println(Thread.currentThread().getName() + \"计算中~~~\"); while (!futureTask.isDone())&#123; TimeUnit.MILLISECONDS.sleep(500); System.out.println(Thread.currentThread().getName() + \"计算中~~~\"); &#125; System.out.println(futureTask.get()); System.out.println(futureTask.get()); System.out.println(Thread.currentThread().getName() + \"计算完成~~~\"); &#125;&#125; FutureTask/Callable应用场景：在主线程中需要执行比较耗时的操作但又不想阻塞主线程时，可把这些操作交给FutureTask对象在后台完成。当主线程将来需要操作结果时可以通过FutureTask对象获得后台作业的计算结果或者执行状态。一般FutureTask多用于耗时的计算任务，主线程可在完成自己的任务后再去获取结果。仅在计算完成时才能检索结果；如果计算尚未完成，则会阻塞get()方法。get()方法获取结果只有在计算完成时获取，否则会阻塞直到任务转入完成状态，再会返回结果或者抛出异常。一旦计算完成，就不会再重新开始或取消计算，如果再次调用结果方法，会将缓存的结果直接返回。 JUC辅助类 CountDownLatch(减少计数) 例子： 12345678910111213141516171819202122232425262728// 要求：所有子线程完成后(所有同学离开教室),主线程退出(班长离开教室)public class CountDownLatchDemo &#123; public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0; i &lt; 6; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(finalI); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \"离开教室~~~\"); countDownLatch.countDown(); &#125;, UUID.randomUUID().toString().substring(0, 8)).start(); &#125; countDownLatch.await(); System.out.println(Thread.currentThread().getName() + \"班长关门走人~~~\"); &#125; // 常规方法无法完成,会有乱序 private static void closeDoor() &#123; for (int i = 0; i &lt; 6; i++) &#123; new Thread(() -&gt; System.out.println(Thread.currentThread().getName() + \"离开教室~~~\"), i + \"\").start(); &#125; System.out.println(Thread.currentThread().getName() + \"班长关门走人~~~\"); &#125;&#125; 原理：CountDownLatch主要有两个方法(countDown()以及await())。当一个或多个线程调用await()方法时，这些线程会阻塞。其它线程调用countDown()方法会将计数器减1(调用countDown()方法的线程不会阻塞)，当计数器的值变为0时，因await()方法阻塞的线程会被唤醒，继续执行 CyclicBarrier(循环栅栏) 例子： 123456789101112131415161718192021222324// 要求：子线程全部完成后再运行指定方法(集齐七棵龙珠召唤神龙)public class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; System.out.println(Thread.currentThread().getName() + \"召唤神龙~~~\")); for (int i = 1; i &lt;= 7; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(finalI); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \"收集到第\" + finalI + \"颗龙珠~~~\"); try &#123; cyclicBarrier.await(); // 在7个线程中最后一个线程到达await()屏障，之后下面的语句和cyclicBarrier中设定的动作才会被调度执行 System.out.println(Thread.currentThread().getName() + \"收集第\" + finalI + \"颗龙珠完毕~~~\"); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;, i + \"\").start(); &#125; &#125;&#125; 原理：CyclicBarrier的字面意思是可循环(Cyclic)使用的屏障(Barrier)。它要做的事情是让一组线程到达一个屏障(也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。线程进入屏障通过CyclicBarrier的await()方法 SemaphoreDemo 例子： 12345678910111213141516171819202122// 要求：只有三个线程，但是希望六个线程都能够运行(有3个空闲车位,共有6辆车,一开始3辆车抢到,之后开走1辆另外的车占1个车位)public class SemaphoreDemo &#123; public static void main(String[] args) &#123; // 模拟资源类,有3个空车位 Semaphore semaphore = new Semaphore(3); for (int i = 0; i &lt; 6; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; try &#123; semaphore.acquire(); System.out.println(Thread.currentThread().getName() + \" 抢到了车位 ~~~\"); TimeUnit.SECONDS.sleep(finalI); System.out.println(Thread.currentThread().getName() + \" 离开了车位 ~~~\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;, i + \"号车\").start(); &#125; &#125;&#125; 原理：在信号量上我们定义两种操作：acquire()——获取，当一个线程调用acquire()操作时，它要么成功并获取信号量(信号量减1)，要么一直等下去直到有线程释放信号量或超时；release()——释放，实际上会将信号量的值加1，然后唤醒等待的线程。信号量主要用于两个目的，一个是用于多个共享资源的互斥使用，另一个用于并发线程数的控制","categories":[],"tags":[{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"}]},{"title":"C++ Primer","slug":"Language/C_C++/C++_Primer","date":"2020-09-15T11:04:58.000Z","updated":"2020-09-16T07:36:23.146Z","comments":true,"path":"2020/09/15/Language/C_C++/C++_Primer/","link":"","permalink":"https://sobxiong.github.io/2020/09/15/Language/C_C++/C++_Primer/","excerpt":"内容 开始 变量和基本类型","text":"内容 开始 变量和基本类型 开始 标准输入输出对象： 标准库定义了另外两个ostream对象：cerr和clog，通常使用cerr来输出警告和错误消息，而使用clog来输出程序运行的一般性信息 1std::cout &lt;&lt; \"test\" &lt;&lt; std::endl; ::是作用域运算符 输出(输入)运算符(&lt;&lt;或&gt;&gt;)接受两个运算对象：左侧的运算对象必须是一个ostream(istream)对象，右侧的运算对象是要打印(读入)的值。该运算符将给定的值写到给定的ostream(istream)对象中，计算结果就是其左侧运算对象(每次都返回std::cout/std::cin) 运算符打印endl是一个被称为操纵值的特殊值。写入endl的效果是结束当前行，并将与设备关联的缓冲区(buffer)中的内容刷到设备中。缓冲刷新操作可以保证到目前为止程序所产生的所有输出都真正写入输入流中，而不是仅停留在内存中等待写入流(我们常在调试时打印语句,这类语句应该保证”一直”刷新流。否则,如果程序崩溃,输出可能还留在缓冲区中,从而导致关于程序崩溃位置的错误推断) 1while(std::cin &gt;&gt; value) 当使用一个istream对象作为条件时，其效果是检测流的状态。如果流是有效的(流未遇到错误)，那么检测成功。当遇到文件结束符，或遇到一个无效输入时(与value定义的类型不符)，istream对象的状态会变为无效(此时会使条件变为假) 包含来自标准库的头文件应该使用(&lt;&gt;)包围文件名。对于不属于标准库的头文件则应该使用(“”)包围 变量和基本类型 类型转换： 当把一个非布尔类型的算术值赋给布尔类型时，初始值为0则结果为false；否则结果为true 当把一个布尔值赋值给非布尔类型时，初始值为false则结果为0；否则结果为1 当把一个浮点数赋给整数类型时进行了近似处理，结果值将仅保留整数部分 当把一个整数值赋给浮点类型时，小数部分记为0。如果该整数所占控件超过了浮点类型的容量，精度有可能丢失 当赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数去模后的余数 当赋给带符号类型一个超出它表示范围的值时，结果是未定义的 当无符号和带符号相互赋值时，可能会导致数值的偏差(符号位) 字面值 默认情况下，十进制字面值是带符号数，八进制和十六进制字面值既可能时带符号也可能时无符号 如果一个字面值连与之关联的最大的数据类型都放不下，将产生错误 初始化 C++11定义了许多初始化的方式，C++11后花括号得到全面应用 1234int test = 0;int test = &#123;0&#125;;int test&#123;0&#125;;int test(0); 默认初始化 内置类型：如果是内置类型的变量未被显式初始化，它的值由定义的位置决定。定义于任何函数体之外的变量被初始化为0。一种例外情况是，定义在函数体内部的内置类型变量将不被初始化。一个未被初始化的内置类型变量的值是未定义的，如果试图拷贝或以其他方式访问此类值将引发错误 类类型：每个类各自决定其初始化对象的方式。而且是否允许不经初始化就定义对象也由类自己决定。如果类允许这种行为，它将决定对象的初始值到底是什么 变量声明和定义 为了允许把程序拆分成多个逻辑部分来编写，C++语言支持分离式编译机制，该机制允许将程序分割为若干个文件，每个文件可被独立编译。如果将程序分为多个文件，则需要有在文件间共享代码的方法(例如std::cout定义于标准库,却能被我们使用)。为了支持分离式编译，C++将声明和定义区分开。声明使得名字为程序所致，一个文件如果想使用别处定义的名字则必须包含对那个名字的声明。定义负责创建与名字关联的实体 变量声明规定了变量的类型和名字，这一点与定义相同。此外，定义还申请存储空间，也可能会为变量赋一个初始值 如果想声明一个变量而非定义它，就需要在变量名前添加关键字extern，并且不要显式地初始化变量 任何包含了显式初始化的声明即成为定义。我们能给由extern关键字标记的变量赋一个初始值，但这么做抵消了extern的作用，就变成定义了 在函数体内部，如果试图初始化一个由extern关键字标记的变量，将引发错误 变量能且只能被定义一次，但可以被多次声明 引用 引用为对象起了另外一个名字，引用类型引用另外一种类型 引用并非对象，相反的，它只是为一个已经存在的对象所起的另外一个名字 定义引用时，程序把引用和它的初始值绑定在一起，而不是将初始值拷贝给引用。一旦初始化完成，引用将和它的初始值对象一直绑定在一起。因为无法令引用重新绑定到另一个对象，因此引用必须初始化 引用本身不是对象，因此不能定义引用的引用 引用只能绑定在对象上，而不能与字面值或某个表达式的计算结果绑定在一起 指针 指针是”指向”另外一种类型的符合类型。与引用类似，指针也实现了对其他对象的间接访问 指针本身就是一个对象，允许对指针赋值和拷贝，而且在指针的生命周期内它可以先后指向几个不同的对象 指针无须在定义时赋初值。和其他内置类型一样，在块作用域内定义的指针如果没有被初始化，也将拥有一个不确定的值(野指针) 指针存放某个对象的地址。引用不是对象，没有实际地址，因此不能定义指向引用的指针 绝大部分情况下指针的类型都要和它所指向的对象严格匹配(void*传参、多态等除外) ‘*’为解引用符，对指针解引用会得到所指的对象。如果给解引用的结果赋值，实际上就是给指针所指的对象赋值 void*是一种特殊的指针类型，可用于存放任意对象的地址 利用void*指针能做的事比较有限：拿它和其他指针比较、作为函数的输入或输出、赋值给另外一个void*指针。不能直接操作void*所指的对象，因为并不知道这个对象的类型，也就无法确定能在这个对象上能做哪些操作(如果能够确认该对象的类型,可以进行强制转换——cast) 复合类型 12// p1是指向int的指针,p2是一个int型变量int* p1, p2; 这种写法可能会产生误导：int*放在一起好像是这条赋值语句中所有变量共同的类型一样。恰恰相反，基本数据类型是int而非int*。*仅仅是修饰了p而且，对该声明语句中的其他变量并不产生任何作用(因此最好把&amp;和*这俩类型修饰符写在变量名前,与数据类型以空格间隔) 12int *p; // p是一个int型指针int *&amp;r = p; // p是一个对指针p的引用 引用本身不是一个对象，因此不能定义指向引用的指针。但指针是对象，所以存在对指针的引用 要理解r的类型的简单方法是从右向左阅读r的定义。离变量名最近的符号(&amp;)对变量类型有最直接的影响，因此r是一个引用。声明符的其余部分用以确定r引用的类型是什么，*说明r引用的是一个指针。最后由声明的基本类型int指出r引用的是一个int指针 const限定符","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"https://sobxiong.github.io/tags/C-C/"}]},{"title":"Nginx","slug":"Middleware/Nginx","date":"2020-09-06T03:39:08.000Z","updated":"2020-09-20T14:25:53.943Z","comments":true,"path":"2020/09/06/Middleware/Nginx/","link":"","permalink":"https://sobxiong.github.io/2020/09/06/Middleware/Nginx/","excerpt":"内容 Nginx简介 Nginx安装 Nginx常用的命令和配置文件 Nginx配置实例-反向代理 Nginx配置实例-负载均衡 Nginx配置实例-动静分离 Nginx搭建高可用集群 Nginx原理简述","text":"内容 Nginx简介 Nginx安装 Nginx常用的命令和配置文件 Nginx配置实例-反向代理 Nginx配置实例-负载均衡 Nginx配置实例-动静分离 Nginx搭建高可用集群 Nginx原理简述 Nginx简介 Nginx概述：是一个高性能的HTTP和反向代理服务器,特点是占有内存少，并发能力强，事实上Nginx的并发能力确实在同类型的网页服务器中表现较好 Nginx的作用： 作为Web服务器：Nginx可以作为静态页面的web服务器，同时还支持CGI协议的动态语言，比如perl、php等。但是不支持java。Java程序只能通过与tomcat配合完成 正向代理：如果把局域网外的Internet想象成一个巨大的资源库，则局域网中的客户端要访问Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理(需要设置代理地址) 反向代理：客户端对代理是无感知的，因为客户端不需要任何配置就可以访问。我们只需要将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后再返回给客户端。此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器地址，隐藏了真实服务器IP地址 负载均衡：单个服务器解决不了并发需求，可以增加服务器的数量，然后将请求分发到各个服务器上，将原先请求集中到单个服务器上的情况变为将请求分发到多个服务器上，将负载分发到不同的服务器。这就是所说的负载均衡 动静分离：为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速度。降低原来单个服务器的压力 Nginx安装 进入Nginx官网http://nginx.org/的download板块下载 具体安装Nginx 安装所需的第三方库：pcre、openssl、zlib 1yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 安装nginx 1234567# 解压缩nginx的tar.gz包tar -zxvf nginx-1.18.0.tar.gz -C /opt/module# 进入目录,执行./configurecd /opt/module/nginx-1.18.0sudo ./configure# make安装sudo make &amp;&amp; make install 测试(注意Linux的防火墙设置) 123456# 进入/usr/local/nginx/sbin目录cd /usr/local/nginx/sbin# 启动nginxsudo ./nginx# 输入当前虚拟机的地址的80端口,查看是否能看到Welcome to nginx# http://sobxiong.com Nginx常用的命令和配置文件 Nginx常用的命令(当前路径:/usr/local/nginx/sbin) 查看nginx版本号：./nginx -v 启动命令：./nginx 关闭命令：./nginx -s stop 重新加载命令(重新加载配置文件)：./nginx - s reload nginx.conf配置文件 介绍：Nginx默认的配置文件都放在主目录下的conf目录，主配置文件nginx.conf也在其中，后续对nginx的使用基本上都是对此配置文件进行相应的修改 配置文件示例： 1234567891011121314151617181920212223242526272829worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application&#x2F;octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name localhost; location &#x2F; &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 &#x2F;50x.html; location &#x3D; &#x2F;50x.html &#123; root html; &#125; &#125;&#125; 配置文件解释：根据上述文件，可以明显地讲nginx.conf配置文件分为三部分 全局块： 123# Nginx服务器并发处理服务的关键配置,worker_processes值越大,可以支持的并发处理量也越多# 但是会受到硬件、软件等设备的制约,一般设置为当前计算机的CPU核心数worker_processes 1; 从配置文件开始到events块之间的内容，主要会设置一些影响nginx服务器整体运行的配置指令，主要包括配置运行Nginx服务器的用户(组)、允许生成的worker process数、进程pid存放路径、日志存放路径和类型以及配置文件的引入等 events块： 1234events &#123; # 每个work process支持的最大连接数为1024 worker_connections 1024;&#125; events块涉及的指令主要影响Nginx服务器与用户的网络连接。常用的设置包括是否开启对多work process下的网络连接进行序列化、是否允许同时接收多个网络连接、选取哪种事件驱动模型来处理连接请求、每个work process可以同时支持的最大连接数等。这部分的配置对Nginx的性能影响较大，在实际中应该灵活配置 http块： 1234567891011121314151617181920212223http &#123; include mime.types; default_type application&#x2F;octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name localhost; location &#x2F; &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 &#x2F;50x.html; location &#x3D; &#x2F;50x.html &#123; root html; &#125; &#125;&#125; 这算是Nginx服务器配置中修改最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里http块还可以包括http全局块、server块 http全局块：配置的指令包括文件引入、MIME-TYPE定义、日志自定义、连接超时时间、单链接请求数上限等 server块：server块和虚拟主机有密切关系。虚拟主机从用户角度看和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器硬件成本每个http块可以包括多个server块，而每个server块就相当于一个虚拟主机而每个server块也分为全局server块以及可以同时包含多个locaton块 全局server块：最常见的配置是本虚拟机主机的监听配置和本虚拟主机的名称或IP配置 location块：一个server块可以配置多个location块该块的主要作用是基于Nginx服务器接收到的请求字符串(例如server_name/uri-string)，对虚拟主机名称(也可以是IP别名)之外的字符串(例如前面的/uri-string)进行匹配，对特定的请求进行处理。地址定向、数据缓存和应答控制等功能，还有许多第三方模块的配置也在这里进行 Nginx配置实例-反向代理 反向代理实例1 最终需求：使用nginx反向代理，访问 www.123.com 跳转到虚拟机的8080端口 实现步骤： 测试端口8080准备： 虚拟机安装Tomcat并启动(启动命令bin/startup.sh) 在mac中通过浏览器访问虚拟机Tomcat主页：http://172.16.85.201:8080 修改host文件：在mac中修改hosts文件，将 www.123.com 映射到虚拟机地址172.16.85.201。此时可以通过 www.123.com:8080 访问到测试端口 修改Nginx配置： 12345678server &#123; listen 80; server_name 172.16.85.201; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:8080; &#125;&#125; 最终测试 反向代理实例2 最终需求：使用Nginx反向代理，根据访问的路径跳转到不同端口的服务中。其中Nginx监听端口为 9001，要求访问http://172.16.85.201:9001/edu/直接跳转到172.16.85.201:8080，访问http://172.16.85.201:9001/vod/直接跳转到172.16.85.201:8081 实现步骤： 准备两个Tomcat服务器，一个8080端口，一个8081端口；创建文件夹和测试页面(在8080的Tomcat目录下创建edu目录，其内创建一个a.html测试页面;同理在8081的Tomcat目录下创建vod目录，其内创建一个b.html测试页面) 修改Nginx配置： 123456789101112server &#123; listen 9001; server_name 172.16.85.201; location ~ &#x2F;edu&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:8080; &#125; location ~ &#x2F;vod&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:8081; &#125;&#125; 分别测试网址&lt;172.16.85.201:9001/edu/a.html&gt;和&lt;172.16.85.201:9001/vod/b.html&gt;，验证结果 Location指令说明 用途：用于匹配URL 语法： 123location [ &#x3D; | ~ | ~* | ^~] uri &#123;&#125; 参数说明： = ：用于不含正则表达式的uri前，要求请求字符串与uri严格匹配，如果匹配成功，就停止继续向下搜索并立即处理该请求 ~：用于表示uri包含正则表达式，并且区分大小写 ~*：用于表示uri包含正则表达式，并且不区分大小写 ^~：用于不含正则表达式的uri前，要求Nginx服务器找到标识uri和请求字符串匹配度最高的 location后，立即使用此location处理请求，而不再使用location块中的正则uri和请求字符串做匹配 如果uri包含正则表达式，则必须要有或者*标识 Nginx配置实例-负载均衡 负载均衡示例 最终效果：浏览器访问http://172.16.85.201/edu.a.html，能够将请求负载均衡到8080端口和8081端口 准备工作： 准备一台虚拟机，装上两个Tomcat服务器，端口为8080和8081 在两个Tomcat服务器的webapps目录中，创建edu文件夹和其中的a.html用于测试 在Nginx的配置文件中进行负载均衡的配置 1234567891011121314upstream myserver&#123; server 172.16.85.201:8080; server 172.16.85.201:8081;&#125;server &#123; listen 80; server_name 172.16.85.201; location / &#123; proxy_pass http://myserver; root html; index index.html index.htm;&#125; Nginx分配服务器策略 轮询(默认)：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除 weight(权重)：weight代表权重，默认为1，权重越高被分配的客户端越多 1234upstream myserver&#123; server 172.16.85.201:8080 weight=10; server 172.16.85.201:8081 weight=20;&#125; ip_hash(适用于session)：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器 12345upstream myserver&#123; ip_hash; server 172.16.85.201:8080; server 172.16.85.201:8081;&#125; fair(第三方,公平)：按后端服务器的响应时间来分配请求，响应时间短的优先分配 12345upstream myserver&#123; server 172.16.85.201:8080; server 172.16.85.201:8081; fair;&#125; Nginx配置实例-动静分离 基本介绍：Nginx动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用Nginx处理静态页面，Tomcat处理动态页面。动静分离从目前实现角度来讲大致分为两种：一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案；另外一种方法就是动态跟静态文件混合在一起发布，通过nginx来分开 动静分离示例 准备工作：在虚拟机linux系统的本地文件系统中准备静态资源，用于进行访问 Nginx进行动静分离的配置 123456789101112131415server &#123; listen 80; server_name 172.16.85.201; location /text/ &#123; root /opt/data/; index index.html index.htm; &#125; location /image/ &#123; root /opt/data/; # 开启该配置后,访问/image/页面会展示当前目录下的文件基本信息列表 autoindex on; &#125;&#125; Nginx搭建高可用集群 准备工作： 准备两台Linux虚拟机，各自都装上Nginx 在两台服务器安装keepalived：yum install keepalived -y 修改/etc/keepalived下的keepalived.conf配置： 1234567891011121314151617181920212223242526272829303132333435363738global_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 172.16.85.201 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_script chk_http_port &#123; script \"/opt/data/nginx_check.sh\" #(检测脚本执行的间隔) interval 2 weight 2&#125;vrrp_instance VI_1 &#123; # 备份服务器上将MASTER改为BACKUP state MASTER # 网卡 interface ens33 # 主、备机的virtual_router_id必须相同 virtual_router_id 51 # 主、备机取不同的优先级,主机值较大,备份机值较小 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; # VRRP H 虚拟地址 172.16.85.250 &#125;&#125; 在/opt/data下添加检测脚本文件nginx_check.sh： 123456789#!/bin/bashA=`ps -C nginx –no-header |wc -l`if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then killall keepalived fifi 启动两台Linux虚拟机上的nginx和keepalived 1234# 启动nginx(/usr/local/nginx/sbin)./nginx# 启动keepalived服务systemctl start keepalived.service 输入ifconfig可以查看到虚拟ip——172.16.85.250 测试 在主机上访问172.16.85.250，nginx主页显示正常 把主服务器(172.16.85.250)的nginx和keepalived停止 再次访问172.16.85.250，主页依旧正常 Nginx原理简述 Nginx主要采用master-worker模式 一个master和多个worker的好处： 可以使用nginx –s reload热部署 每个worker是独立的进程，如果有其中的一个worker出现问题，其他worker可继续进行争抢，实现请求过程，不会造成服务中断 worker个数：和服务器cpu数相等 发送请求，占用了多少worker的连接数：2/4 Nginx有一个master，有四个worker，每个worker支持最大的连接数1024，那么支持的最大并发数是多少?普通的静态访问最大并发数是worker_connections * worker_processes / 2；如果作为反向代理，最大并发数量应该是worker_connections * worker_processes / 4","categories":[],"tags":[{"name":"Middleware","slug":"Middleware","permalink":"https://sobxiong.github.io/tags/Middleware/"}]},{"title":"SpringCloud第一季","slug":"Spring/SpringCloud/SpringCloud第一季","date":"2020-09-06T02:53:12.000Z","updated":"2020-09-06T03:30:12.631Z","comments":true,"path":"2020/09/06/Spring/SpringCloud/SpringCloud第一季/","link":"","permalink":"https://sobxiong.github.io/2020/09/06/Spring/SpringCloud/SpringCloud%E7%AC%AC%E4%B8%80%E5%AD%A3/","excerpt":"内容 微服务概述","text":"内容 微服务概述 微服务概述 微服务是什么：微服务的核心就是将传统的一站式应用根据业务拆分成一个一个的服务，彻底地去耦合。每一个微服务提供单个业务功能的服务，一个服务做一件事。从技术角度看就是一种小而独立的处理过程，类似进程概念，能够自行单独启动或销毁，还可以拥有自己独立的数据库","categories":[],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"}]},{"title":"Spark","slug":"BigData/Spark","date":"2020-09-04T04:22:21.000Z","updated":"2020-09-20T15:13:04.926Z","comments":true,"path":"2020/09/04/BigData/Spark/","link":"","permalink":"https://sobxiong.github.io/2020/09/04/BigData/Spark/","excerpt":"内容 Spark概述 Spark快速上手 Spark运行环境 Spark核心编程","text":"内容 Spark概述 Spark快速上手 Spark运行环境 Spark核心编程 Spark概述 Spark是什么：一种基于内存的快速、通用、可扩展的大数据分析计算引擎(unified analytics engine for large-scale data processing) Spark And Hadoop 从时间节点上看： Hadoop 2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发 2008年1月，Hadoop成为Apache顶级项目 2011年1.0正式发布 2012年3月稳定版发布 2013年10月发布2.X(Yarn)版本 Spark 2009年，Spark诞生于伯克利大学的AMPLab实验室 2010年，伯克利大学正式开源了Spark项目 2013年6月，Spark成为了Apache基金会下的项目 2014年2月，Spark以飞快的速度成为了Apache的顶级项目 2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark 从功能上看： Hadoop Hadoop是由Java编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架 作为Hadoop分布式文件系统，HDFS处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的The Google File System这篇论文，是GFS的开源实现 MapReduce是一种编程模型，Hadoop根据Google的MapReduce论文将其实现。作为Hadoop的分布式计算模型，MapReduce是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易 HBase是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop中非常重要的组件 Spark Spark是一种由Scala开发的快速、通用、可扩展的大数据分析引擎 Spark Core中提供了Spark最基础与最核心的功能 Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据 Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API 综合以上，Spark出现的时间相对较晚，并且主要功能主要是用于数据计算。因此Spark一直被认为是Hadoop MapReduce框架的升级版 Spark Or Hadoop Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景(如：机器学习、图挖掘算法、交互式数据挖掘算法)中存在诸多计算效率等问题。因此Spark应运而生，Spark就是在传统的MapReduce计算框架的基础上，优化其计算过程，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型 机器学习中ALS、凸优化梯度下降等都需要基于数据集或者数据集的衍生数据反复查询、反复操作。MR模式不太合适，即使多MR串行处理，性能和时间也是一个问题，而且数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的Scala语言恰恰擅长函数的处理 Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集(Resilient Distributed Datasets)，它提供了比MapReduce更丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法 Spark和Hadoop的根本差异是多个作业之间的数据通信问题：Spark多个作业之间的数据通信是基于内存的，而Hadoop是基于磁盘的 Spark Task的启动时间快。Spark采用fork线程的方式，而Hadoop采用创建新的进程的方式 Spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都要依赖于磁盘交互 Spark的缓存机制比HDFS的缓存机制高效 综上所述，在绝大多数的数据计算场景中，Spark确实会比MapReduce更有优势。但Spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够而导致Job执行失败，此时MapReduce是一个更好的选择，所以Spark并不能完全替代MR Spark核心模块 Spark Core：Spark Core中提供了Spark最基础与最核心的功能。Spark其他的功能如Spark SQL、Spark Streaming、GraphX以及MLlib都是在Spark Core的基础上进行扩展的 Spark SQL：Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据 Spark Streaming：Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API Spark MLlib：MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语 Spark GraphX：GraphX是Spark面向图计算提供的框架与算法库 Spark快速上手 在IDEA上初体验Spark API 创建Maven项目(IDEA中最简单的maven项目,不采用任何模版项目) IDEA安装Scala插件 pom添加依赖关系 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- 该插件用于将Scala代码编译成class文件 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;!-- 声明绑定到maven的compile阶段 --&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; WordCount案例 配置log4j日志输出(过滤Spark框架的执行日志)——在resource目录下创建log4j.properties文件 12345678910111213141516171819log4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n# Set the default spark-shell log level to ERROR. When running the spark-shell, the# log level for this class is used to overwrite the root logger's log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=ERROR# Settings to quiet third party logs that are too verboselog4j.logger.org.spark_project.jetty=ERRORlog4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERRORlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERRORlog4j.logger.org.apache.parquet=ERRORlog4j.logger.parquet=ERROR# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive supportlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATALlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR 案例代码： 12345678910111213141516171819202122232425262728293031323334353637383940object Spark02_WordCount &#123; def main(args: Array[String]): Unit = &#123; // Spark是一个计算框架 // 开发人员使用Spark框架的Api实现计算 // 1、准备Spark环境 // setMaster：设定Spark环境的位置 val sparkConfig = new SparkConf() .setMaster(\"local\") .setAppName(\"wordCount\") // 2、建立和Spark的连接 // jdbc：connection val sparkContext = new SparkContext(sparkConfig) // 3、实现业务操作 // 3.1、读取指定目录下的数据文件(多个) // 参数path可以指向单一的文件/文件目录 // RDD: 更适合并行计算的数据模型 val fileRDD: RDD[String] = sparkContext.textFile(path = \"./src/main/resources/input\") // 3.2、将读取的内容进行扁平化操作,切分单词 val wordRDD: RDD[String] = fileRDD.flatMap(_.split(\" \")) // 3.3、将分词后的数据进行结构的转换 // word -&gt; (word,1) val mapRDD: RDD[(String, Int)] = wordRDD.map(word =&gt; (word, 1)) // 3.4、将转换结构后的数据根据单词进行分组聚合 // reduceByKey: 根据数据key进行分组,然后对value进行统计聚合 val wordSumRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_ + _) // 打印 val wordCountArray: Array[(String, Int)] = wordSumRDD.collect() println(wordCountArray.mkString(\",\")) // 4、释放连接 sparkContext.stop() &#125;&#125; Spark运行环境 基本介绍：Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行，在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来 Local本地模式 介绍：所谓的Local模式就是不需要其他任何节点资源就可以在本地执行Spark代码的环境，一般用于教学、调试、演示等。而之前在IDEA中运行代码的环境我们称之为开发环境 环境准备： 解压缩spark文件 引入hadoop等Jar包 启动Local环境 进入解压缩的目录，执行：bin/spark-shell –master local[*] 启动后，可以使用当前主机的4040端口进行Web UI监控 命令行工具 准备：在spark根目录下的data目录中，添加word.txt文件，准备一些英文单词 在Local环境中输入 1sc.textFile(\"data/word.txt\").flatMap(_.split(\"\")).map((_,1)).reduceByKey(_+_).collect 回车后会实时输出结果，sc是Spark Context的简写，该变量由命令行工具提供 退出：:quit(Scala)或者Ctrl + C 提交应用 12345bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master local[2] \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 提交应用参数解释 –class：表示要执行程序的主类 –master local[2]：部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量 spark-examples_2.12-2.4.5.jar：运行的应用类所在的jar包 10：表示程序的入口参数，用于设定当前应用的任务数量 Standalone模式 介绍：local本地模式只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行。Spark自身节点运行的集群模式叫做独立部署(Standalone)模式。Spark的Standalone模式体现了经典的master-slave模式 集群规划： hadoop101 hadoop102 hadoop103 Worker Master Worker Worker 环境准备 解压缩spark文件 引入hadoop等Jar包 修改配置(conf目录) 修改slaves.template文件名改为slaves 修改slaves文件，添加work节点：hadoop101、hadoop102、hadoop103(用回车分割,不能其他空格空行) 修改spark-env.sh.template文件名为spark-env.sh 修改spark-env.sh文件： 123export JAVA_HOME=/opt/module/jdk1.8.0_251SPARK_MASTER_HOST=hadoop101SPARK_MASTER_PORT=7077 分发配置文件：xsync spark-env.sh 启动集群 执行脚本命令：sbin/start-all.sh 查看Master资源监控Web UI界面：http://hadoop101:8080 提交应用(–master spark://hadoop101:7077：独立部署模式,连接到Spark集群) 12345bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master spark://hadoop101:7077 \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 提交参数说明 123456bin/spark-submit \\--class &lt;main-class&gt;--master &lt;master-url&gt; \\... # other options&lt;application-jar&gt; \\[application-arguments] 参数 解释 可选值举例 –class Spark程序中包含主函数的类 / –master Spark程序运行的模式 local模式(local[*])、standalone模式(spark://hadoop101:7077)、Yarn模式(Yarn) –executor-memory 1G 指定每个executor可用内存为1G 符合集群内存配置即可，具体情况具体分析 –total-executor-cores 2 指定所有executor使用的cpu核数为2个 同上 –executor-cores 指定每个executor使用的cpu核数 同上 application-jar 打包好的应用jar(包含依赖)。该URL在集群中全局可见。比如hdfs://共享存储系统；如果是file://path，那么所有的节点的path都要包含同样的jar 同上 application-arguments 传给main()方法的参数 同上 配置历史服务器 介绍：由于spark-shell停止或spark任务结束后，集群监控的4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况 具体配置步骤： 修改spark-defaults.conf.template文件名为spark-defaults.conf 修改spark-default.conf文件： 1234# 设置开启日志记录spark.eventLog.enabled true# 设置日志存储路径spark.eventLog.dir hdfs://hadoop101:9000/spark_log 启动hadoop集群，hdfs上的spark_log目录需要提前存在 12sbin/start-dfs.shhadoop dfs -mkdir /spark_log 修改spark-env.sh文件，添加日志配置： 12345678# 设置历史日志选项# 参数1：Web UI访问端口号# 参数2：指定历史服务器日志存储路径# 参数3: 指定保存Application历史记录的个数,如果超过这个值,旧的应用程序信息将被删除(是内存中的应用数,而不是页面上显示的应用数)export SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=18080-Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/spark_log-Dspark.history.retainedApplications=30\" 分发配置文件：xsync conf 重启集群和历史服务 12sbin/start-all.shsbin/start-history-server.sh 重新执行任务 查看历史服务情况：http://hadoop101:18080 配置高可用(HA) 介绍：所谓的高可用是因为当前集群中的Master节点只有一个，因此会存在单点故障问题。为了解决单点故障问题，需要在集群中配置多个Master节点，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置 集群规划： hadoop101 hadoop102 hadoop103 Master、Zookeeper、Worker Master、Zookeeper、Worker Zookeeper、Worker 停止集群：sbin/stop-all.sh 启动Zookeeper：bin/zkServer.sh start 修改spark-env.sh配置： 12345678910# 注释master的host和port,不能把master固定# SPARK_MASTER_HOST=hadoop101# SPARK_MASTER_PORT=7077SPARK_MASTER_WEBUI_PORT=8989# 设置zookeeper配置export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER-Dspark.deploy.zookeeper.url=hadoop101,hadoop102,hadoop103-Dspark.deploy.zookeeper.dir=/spark\" 分发配置文件：xsync spark-env.sh 重启集群：sbin/start-all.sh 启动hadoop102的单独master节点(使hadoop102节点的master状态处于备用状态)：sbin/start-master.sh 提交应用到高可用集群： 123456bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master spark://hadoop101:7077,hadoop102:7077 \\--deploy-mode cluster \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 停止hadoop101的master进程：kill -9 xxx(Master的进程号) 查看hadoop102的Master资源监控Web UI(8989端口)，经过一段时间，hadoop102节点master状态提升为活动状态 Yarn模式 基本介绍：独立部署(Standalone)模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是Spark主要是计算框架，而不是资源调度框架，所以资源调度并不是它的强项，因此还是和其他专业的资源调度框架集成会更靠谱一些。其中，在国内工作中，Yarn使用的非常多 环境准备 解压缩spark文件 引入hadoop等Jar包 修改配置文件 hadoop的配置文件yarn-site.xml，并分发 1234567891011121314151617&lt;!-- 是否启动一个线程检查每个任务正使用的物理内存量,如果任务超出分配值, 则直接将其杀掉,默认是true--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否启动一个线程检查每个任务正使用的虚拟内存量, 如果任务超出分配值,则直接将其杀掉,默认是true--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; spark的配置文件spark-env.sh，并分发 123export JAVA_HOME=/opt/module/jdk1.8.0_251# 设置yarn配置目录YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop 启动HDFS和YARN集群 提交应用 12345bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 之后便可以在hadoop102节点的8088的Web UI上查看到跑的Spark应用 配置历史服务器 参照Standalone模式的spark-env.sh配置 修改spark-defaults.conf配置 12spark.yarn.historyServer.address=hadoop101:18080spark.history.ui.port=18080 k8s以及Mesos模式 Mesos介绍：Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核，在Twitter得到广泛使用，管理着Twitter超过300000台服务器上的应用部署。但国内依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但原理其实都差不多 k8s模式：容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes(k8s)，Spark也在最近的版本中支持了k8s部署模式。具体介绍网址如下：https://spark.apache.org/docs/latest/running-on-kubernetes.html 部署模式对比 模式 机器数 需启动的进程 应用场景 Local 1 无 Spark Standalone 3 Master及Worker 单独部署 Yarn 1 Yarn以及HDFS 混合部署 端口号总结 Spark查看当前Spark-shell运行任务情况端口号：4040(计算) Spark Master内部通信服务端口号：7077 Standalone模式下，Spark Master Web端口号：8080(资源) Spark历史服务器端口号：18080 Hadoop YARN任务运行情况查看端口号：8088 Spark核心编程 基本介绍：Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是： RDD : 弹性分布式数据集 累加器：分布式共享只写变量 广播变量：分布式共享只读变量 RDD 基本介绍：RDD(Resilient Distributed Dataset)弹性分布式数据集，是Spark中最基本的数据处理模型。Scala代码中是一个抽象类，它代表一个弹性的、不可变、可分区并且其中元素可并行计算的集合 重点： 弹性： 存储的弹性：内存与磁盘的自动切换 容错的弹性：数据丢失可以自动恢复 计算的弹性：计算出错重试机制 分片的弹性：可根据需要重新分片 分布式：数据存储在大数据集群(hadoop的HDFS集群)不同节点上 数据集：RDD封装了计算逻辑，并不保存数据 数据抽象：RDD是一个抽象类，需要子类具体实现 不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑 可分区、并行计算 基础编程 RDD创建 从集合(内存)中创建RDD：两个方法parallelize()和makeRDD()，其中后者只是包装了前者 1234567891011val sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从内存中创建RDD// 1、parallelize:并行val list = List(1, 2, 3, 4)val rdd: RDD[Int] = sparkContext.parallelize(list)println(rdd.collect().mkString(\",\"))// makeRDD底层代码就是调用了parallelize,只是为了方便理解val rdd1: RDD[Int] = sparkContext.makeRDD(list)println(rdd1.collect().mkString(\",\"))sparkContext.stop() 从外部存储(文件)创建RDD：包括本地文件系统、所有Hadoop支持的数据集(HDFS、HBase等) 1234567891011121314151617181920212223val sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从磁盘File中创建RDD// path：读取文件(目录)的路径// 相对路径,如果是IDEA,那么是从项目根开始查找// path路径根据环境的不同自动发生改变// Spark读取文件时,默认采用Hadoop读取文件的规则——一行一行读取// 指向文件目录,目录的文本文件都会被读取// 读取目录// val fileRDD: RDD[String] = sparkContext.textFile(path = \"input\")// 读取指定文件// val fileRDD: RDD[String] = sparkContext.textFile(path = \"input/w.txt\")// 读取通配符文件val fileRDD: RDD[String] = sparkContext.textFile(path = \"input/word*.txt\")// 文件路径还可以指向第三方存储系统：HDFS// val fileRDD: RDD[String] = sparkContext.textFile(path = \"hdfs://input/word*.txt\")println(fileRDD.collect().mkString(\",\"))sparkContext.stop() 从其他RDD创建：通过一个RDD运算完后，再产生新的RDD 直接创建RDD(new)：使用new的方式直接构造RDD，一般由Spark框架自身使用 RDD并行度与分区： 基本介绍：默认情况下，Spark可以将一个作业切分成多个任务(Task)后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。该数量可以在构建RDD时指定。这里的并行执行的任务数量并不是指的切分任务的数量，不要混淆了 案例： 内存分区案例1： 12345678910111213141516171819202122val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从内存中创建RDD// makeRDD// 参数1：seq: Seq[T] 数据源// 参数2：numSlices: Int = defaultParallelism(默认并行度——分区的数量)// 简单总结：RDD中分区的数量就是并行度,设定并行度就是在设定分区数量// scheduler.conf.getInt(\"spark.default.parallelism\", totalCores)// 并行度默认会从Spark配置信息中获取spark.default.parallelism的值// 如果获取不到指定参数,会采用默认值totalCores——机器的总核数// 机器总核数= 当前环境中可用核数// local -&gt; 单核(单线程) -&gt; 1// local[4] -&gt; 4核(4个线程) -&gt; 4// local[*] -&gt; 当前最大核数 -&gt; 8val rdd = sparkContext.makeRDD(List(1, 2, 3, 4))// println(rdd.collect().mkString(\",\"))// 将RDD的处理后的数据保存到分区文件中rdd.saveAsTextFile(\"output\")sparkContext.stop() 内存分区案例2： 12345678910111213141516171819202122val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 12,34// 内存中的集合按照平均分的方式进行分区处理// val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)// rdd.saveAsTextFile(\"output1\")// 1234// 1,2,34// 12345// 1,23,45// saveAsTextFile方法如果文件已存在,会发生错误// 内存中数据的分区基本上就是平均分,如果不能整除,会采用一个基本的算法实现分配val rdd1 = sparkContext.makeRDD(List(1, 2, 3, 4,5), 3)rdd1.saveAsTextFile(\"output2\")// 1,2,3,4// val rdd2 = sparkContext.makeRDD(List(1, 2, 3, 4), 4)// rdd2.saveAsTextFile(\"output3\")sparkContext.stop() 文件分区案例1： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// textFile// 参数1 path：读取文件的路径// 参数2 minPartitions：最小分区数量// minPartitions默认值为math.min(defaultParallelism, 2)// 其中defaultParallelism是totalCores// val fileRDD1 = sparkContext.textFile(\"input/w.txt\")// fileRDD1.saveAsTextFile(\"output\")// val fileRDD2 = sparkContext.textFile(\"input/w.txt\", 1)// fileRDD2.saveAsTextFile(\"output2\")// 1、Spark读取文件采用的是Hadoop的读取规则// 文件切片规则：以字节方式来切片// 数据读取规则：以行为单位来读取// 2、问题：// 文件到底切成几片(分区的数量)// 文件字节数,预计切片数量(2)// 所谓的最小分区数,取决于总的字节数是否能整除分区数并且剩余的字节小于一定比率(10%,hadoop方式)// 实际产生的分区数量可能大于最小分区数val fileRDD1 = sparkContext.textFile(\"input/w.txt\", 2)fileRDD1.saveAsTextFile(\"output3\")// 分区的数据如何存储?// 分区数据是以行为单位读取的,不是以字节// 数据是以行的方式读取,但是会考虑偏移量(数据的offset)的设置// 1@@ =&gt; 012// 2@@ =&gt; 345// 3@@ =&gt; 678// 4 =&gt; 9// 10 byte / 4 = 2 .... 2 =&gt; 5// 以行为单位...// 以下左右都是闭区间(取得到)// 0 =&gt; (0, 2) =&gt; 1// 1 =&gt; (2, 4) =&gt; 2// 2 =&gt; (4, 6) =&gt; 3// 3 =&gt; (6, 8) =&gt;// 4 =&gt; (8,10) =&gt; 4// val fileRDD3 = sparkContext.textFile(\"input/w.txt\", 4)// fileRDD3.saveAsTextFile(\"output3\")//// val fileRDD4 = sparkContext.textFile(\"input/w.txt\", 3)// fileRDD4.saveAsTextFile(\"output4\")sparkContext.stop 文件分区案例2： 12345678910111213141516171819val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 6 / 2 = 3// (0 , 0 + 3)// (3 , 3 + 3)// 1@@ =&gt; 012// 234 =&gt; 345// hadoop分区是以文件为单位进行划分的// 读取数据不能跨越文件// 10 / 3 = 3 ... 1 =&gt; 4// (0,3) (3,6)val fileRDD1 = sparkContext.textFile(\"input\", 3)fileRDD1.saveAsTextFile(\"output\")sparkContext.stop 分区原理 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下： 1234567def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = &#123; (0 until numSlices).iterator.map &#123; i =&gt; val start = ((i * length) / numSlices).toInt val end = (((i + 1) * length) / numSlices).toInt (start, end) &#125;&#125; 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下： 123456789101112131415161718192021222324252627282930public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException &#123; long totalSize = 0; // compute total size for (FileStatus file: files) &#123; // check we have valid files if (file.isDirectory()) &#123; throw new IOException(\"Not a file: \"+ file.getPath()); &#125; totalSize += file.getLen(); &#125; long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits); long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input. FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize); // ... for (FileStatus file: files) &#123; // ... if (isSplitable(fs, path)) &#123; long blockSize = file.getBlockSize(); long splitSize = computeSplitSize(goalSize, minSize, blockSize); // ... &#125; &#125;&#125;protected long computeSplitSize(long goalSize, long minSize, long blockSize) &#123; return Math.max(minSize, Math.min(goalSize, blockSize));&#125; RDD转换算子：RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型 Value类型 map 函数签名： 1def map[U: ClassTag](f: T =&gt; U): RDD[U] 函数说明：将处理的数据逐条进行映射转换(可以是类型的转换,也可以是值的转换) 案例1： 123456789101112131415161718192021val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// Spark - RDD - 算子(方法)val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)// 旧RDD -&gt; 转换算子 -&gt; 新RDD// 转换算子能将旧的RDD通过方法转换为新的RDD,但是不会触发作业的执行// 分区问题// RDD中有分区列表// 默认分区数量不变,数据会转换后输出val rdd1 = rdd.map(_ * 2)// 读取数据// collect方法不会转换RDD,会触发作业的执行// 所以将collect这样的方法称之为行动(action)算子// println(rdd1.collect.mkString(\",\"))// rdd1.saveAsTextFile(\"output\")println(rdd1.collect.mkString(\",\"))sparkContext.stop 案例2： 123456789101112131415161718192021val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)val rdd1 = rdd.map(x =&gt; &#123; println(s\"Map 1st : $x\") x&#125;)val rdd2 = rdd1.map(x =&gt; &#123; println(s\"Map 2nd : $x\") x&#125;)// (1, 2) 1(1) 1(2) 2(1) 2(2)// (3, 4) 3(1) 3(2) 4(1) 4(2)// 分区内数据按照顺序依次执行,每一条数据的所有逻辑全部执行完毕后才会执行下一条数据// 分区间数据执行没有顺序,而且无需等待println(rdd2.collect.mkString(\",\"))sparkContext.stop 小功能：从服务器日志数据apache.log中获取用户请求URL资源路径 12345678910111213val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从服务器日志数据apache.log中获取用户请求URL资源路径val fileRDD = sparkContext.textFile(\"input/apache.log\")val urlRDD = fileRDD.map( line =&gt; &#123; val datas = line.split(\" \") datas(6) &#125;)urlRDD.collect.foreach(println)sparkContext.stop mapPartitions 函数签名： 123def mapPartitions[U: ClassTag]( f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] 函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据) 案例： 1234567891011121314151617181920val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// mapPartitions// 以分区为单位进行计算,和map算子很相似// 区别就在于map算子是一个一个执行,mapPartitions是一个分区一个分区执行// 类似于批处理// map方法是全量数据操作,不能丢失数据// mapPartitions一次性获取分区的所有数据,那么可以执行迭代器集合的所有操作(filter、max、sum)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 2)// val rdd = dataRDD.mapPartitions(iter =&gt; &#123;// iter.map(_ * 2)// &#125;)// println(rdd.collect.mkString(\",\"))val rdd = dataRDD.mapPartitions(iter =&gt; &#123; iter.filter(_ % 2 == 0)&#125;)println(rdd.collect.mkString(\",\"))sparkContext.stop 小功能：获取每个数据分区的最大值 12345678val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 4, 3, 2, 5, 6), 3)// 获取每个数据分区的最大值val rdd = dataRDD.mapPartitions(iter =&gt; List(iter.max).iterator)println(rdd.collect.mkString(\",\"))sparkContext.stop mapPartitionsWithIndex 函数签名： 123def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] 函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据)，在处理时同时可以获取当前分区索引 案例： 123456789101112val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 获取每个分区最大值以及分区号val dataRDD = sparkContext.makeRDD(List(1, 3, 9, 6, 5, 4), 3)val rdd = dataRDD.mapPartitionsWithIndex( (index, iter) =&gt; &#123; List((index, iter.max)).iterator &#125;)println(rdd.collect.mkString(\",\"))sparkContext.stop 小功能：获取第二个数据分区的数据 123456789101112131415val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 获取第二个数据分区的数据val dataRDD = sparkContext.makeRDD(List(1, 3, 9, 6, 5, 4), 3)// 获取的分区索引是从0开始的val rdd = dataRDD.mapPartitionsWithIndex( (index, iter) =&gt; &#123; if (index == 1) iter else Nil.iterator &#125;)println(rdd.collect.mkString(\",\"))sparkContext.stop flatMap 函数签名： 1def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] 函数说明：将处理的数据进行扁平化后再进行映射处理，也称之为扁平映射 案例： 123456789101112val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD( List( List(1, 2), List(3, 4) ))val rdd = dataRDD.flatMap(list =&gt; list)println(rdd.collect.mkString(\",\"))sparkContext.stop 小功能：将List(List(1, 2), 3, List(4, 5))进行扁平化操作 123456789101112131415val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD( List( List(1, 2), 3, List(4, 5) ))val rdd = dataRDD.flatMap &#123; case list: List[_] =&gt; list case d =&gt; List(d)&#125;println(rdd.collect.mkString(\",\"))sparkContext.stop glom 函数签名： 1def glom(): RDD[Array[T]] 函数说明：将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变 案例： 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// glom =&gt; 将每个分组的数据在转换为数组val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 2)val valueRDD = dataRDD.glomvalueRDD.foreach(array =&gt; println(array.mkString(\",\")))sparkContext.stop 小功能：计算所有分区最大值求和(分区内取最大值,分区间最大值求和) 123456789101112131415161718val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// glom// 计算所有分区最大值求和(分区内取最大值,分区间最大值求和)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6), 2)// 将每个分区的数据转换为数组val glomRDD = dataRDD.glom// 将数组中的最大值取出// Array -&gt; maxval maxRDD = glomRDD.map(array =&gt; array.max)// 将取出的最大值求和val sum = maxRDD.collect.sumprintln(sum)sparkContext.stop groupBy 函数签名： 1def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] 函数说明：将数据根据指定的规则进行分组，分区默认不变，但数据会被打乱重新组合(将这样的操作称之为shuffle)。极限情况下，数据可能被分在同一个分区中。一个组的数据在一个分区中，但并不是说一个分区中只有一个组 案例： 12345678910111213141516171819202122232425val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6), 3)// 分组// groupBy方法可以根据指定的规则进行分组,指定的规则的返回值就是分组的key// groupBy方法的返回值为元组// 元组的第一个元素：表示分组的key// 元组的第二个元素：表示相同key的数据形成的可迭代的集合// groupBy方法执行完毕后,会将数据进行分组操作,但是分区不会改变// 不同组的数据会打乱分散到不同的分区中// 如果将上游的分区数据打乱重新组合到下游的分区中,那么这个操作称之为shuffle// 如果数据被打乱重新组合,那么数据就可能出现不均匀的情况,可以改变下游RDD的数据分区// groupBy方法可能会导致数据不均匀,如果想改变分区,可以传递参数val rdd = dataRDD.groupBy(_ % 2, 2)rdd.saveAsTextFile(\"output\")// println(s\"Group Num = $&#123;rdd.glom.collect.length&#125;\")//// rdd.collect.foreach &#123;// case (key, list) =&gt; println(s\"Key = $key , list = &#123; $&#123;list.mkString(\",\")&#125; &#125;\")// &#125;sparkContext.stop 小功能：将List(“Hello”, “hive”, “hbase”, “Hadoop”)根据单词首写字母进行分组 1234567891011val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 根据单词首写字母进行分组val dataRDD = sparkContext.makeRDD(List(\"Hello\", \"hive\", \"hbase\", \"Hadoop\"), 2)// StringOps =&gt; String(0),隐式转换,取首个字符串Charval valueRDD = dataRDD.groupBy(word =&gt; word(0))valueRDD.foreach(println)sparkContext.stop 小功能：从服务器日志数据apache.log中获取每个时间段访问量 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val fileRDD = sparkContext.textFile(\"input/apache.log\")val timeRDD = fileRDD.map(log =&gt; log.split(\" \")(3))val hourRDD = timeRDD.groupBy(time =&gt; time.substring(11, 13))hourRDD.map(it =&gt; (it._1, it._2.size)).foreach(println)sparkContext.stop filter 函数签名： 1def filter(f: T =&gt; Boolean): RDD[T] 函数说明：将数据根据指定的规则进行过滤筛选，符合规则的数据保留，不符合规则的数据丢弃。当数据进行筛选过滤后分区不变，但分区内的数据可能不均衡。生产环境下，可能会出现数据倾斜 案例： 12345678910val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6), 3)// filter过滤// 根据指定的规则对数据进行筛选过滤,满足条件的数据保留,不满足的数据丢弃val rdd = dataRDD.filter(_ % 2 == 0)rdd.collect.foreach(println)sparkContext.stop sample 函数签名： 1234def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] 函数说明：根据指定的规则从数据集中抽取数据 案例： 1234567891011121314151617val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6))// sample用于从数据集中抽取数据// 参数1：withReplacement(Boolean)表示数据抽取后是否放回,可以重复抽取// 参数2：fraction(Double)表示数据抽取的几率(每个数据被抽取的几率,不是数据总量的比率)——不放回的场合可重复抽取// 参数3：seed(Long,默认Utils.random.nextLong)表示随机数种子,可以确定数据的抽取// 随机数不随机,所谓的随机数依靠随机算法实现val valueDRR = dataRDD.sample(withReplacement = false, 0.5, 1)println(valueDRR.collect.mkString(\",\"))val valueDRR2 = dataRDD.sample(withReplacement = true, 0.5)println(valueDRR2.collect.mkString(\",\"))sparkContext.stop distinct 函数签名： 12def distinct()(implicit ord: Ordering[T] = null): RDD[T]def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] 函数说明：将数据集中重复数据去重 案例： 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 1, 2, 4))// distinct 去重// distinct 可以改变分区的数量dataRDD.distinct.foreach(println)sparkContext.stop coalesce 函数签名： 123def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] 函数说明：根据数据量缩减分区，用于大数据集过滤后提高小数据集的执行效率。当spark程序中存在过多小任务时，可以通过coalesce()方法收缩合并分区、减少分区的个数、减小任务调度成本 案例： 123456789101112131415161718192021222324val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 1, 1, 2, 2, 2), 2)// val filterRDD = dataRDD.filter(num =&gt; num % 2 == 0)// filterRDD.saveAsTextFile(\"output\")// 多 -&gt; 少// 当数据过滤后,发现数据不够均匀,那么可以缩减分区// filterRDD.coalesce(1).saveAsTextFile(\"output\")// 如果发现数据分区不合理,也可以缩减分区// coalesce主要目的是缩减分区,扩大分区时没有效果// 为什么不能扩大分区? 因为在分区缩减时,数据不会打乱重新组合,没有shuffle的过程// dataRDD.coalesce(2).saveAsTextFile(\"output\")// 如果非得要将数据扩大分区,那么必须打乱数据后重新组合,必须使用shuffle// coalesce()// 参数1：numPartitions: Int——表示缩减分区后的分区数量// 参数2：shuffle: Boolean = false——表示分区改变时是否会打乱重新组合数据dataRDD.coalesce(6, shuffle = true).saveAsTextFile(\"output\")sparkContext.stop repartition 函数签名： 1def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] 函数说明：该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程 案例： 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 1, 1, 2, 2, 2), 3)// 缩减分区 -&gt; coalesce()// 扩大分区 -&gt; repartition()——底层就是coalesce(..., true)dataRDD.repartition(5).saveAsTextFile(\"output\")sparkContext.stop sortBy 函数签名： 12345def sortBy[K]( f: (T) =&gt; K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] 函数说明：该操作用于排序数据。在排序之前可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为正序排列。排序后新产生的RDD的分区数与原RDD的分区数一致 案例： 123456789101112val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 4, -2, 2))// sortBy// 默认排序规则为升序// sortBy可以通过传递第二个参数改变排序的方式(false逆序)// sortBy可以设定第三个参数,用于改变分区dataRDD.sortBy(num =&gt; num, ascending = false).foreach(println)sparkContext.stop 核心属性 执行原理","categories":[],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Scala","slug":"Language/Scala/Scala","date":"2020-07-24T07:02:46.000Z","updated":"2020-09-20T14:26:31.352Z","comments":true,"path":"2020/07/24/Language/Scala/Scala/","link":"","permalink":"https://sobxiong.github.io/2020/07/24/Language/Scala/Scala/","excerpt":"内容 Scala概述 变量 运算符 程序流程控制 函数式编程基础 面向对象编程-基础 面向对象编程-中级","text":"内容 Scala概述 变量 运算符 程序流程控制 函数式编程基础 面向对象编程-基础 面向对象编程-中级 Scala概述 学习Scala的原因 Spark是新一代内存级大数据计算框架，是大数据的重要内容 Spark就是使用Scala编写的。因此为了更好的学习Spark, 需要掌握Scala这门语言 Scala是Scalable Language的简写，是一门多范式(范式/编程方式[面向对象/函数式编程])的编程语言 联邦理工学院洛桑(EPFL)的Martin Odersky于2001年开始设计Scala(2003年推出) Spark的兴起，带动Scala语言的发展 Scala由来创始人马丁·奥德斯基(Martin Odersky)是编译器及编程的狂热爱好者，长时间的编程之后，希望发明一种语言，能够让写程序这样的基础工作变得高效，简单。所以当接触到JAVA语言后，对JAVA这门便携式，运行在网络，且存在垃圾回收的语言产生了极大的兴趣，所以决定将函数式编程语言的特点融合到JAVA中，由此发明了两种语言(Pizza &amp; Scala)Pizza和Scala极大地推动了Java编程语言的发展(jdk5.0的泛型，for循环增强, 自动类型转换等，都是从Pizza引入的新特性;jdk8.0的类型推断，Lambda表达式就是从scala引入的特性)现在主流JVM的javac编译(jdk5.0、8.0)就是马丁·奥德斯基编写出来的 Scala和Java以及JVM的关系分析图 Scala语言的特点Scala是一门以java虚拟机(JVM)为运行环境并将面向对象和函数式编程的最佳特性结合在一起的静态类型编程语言 Scala是一门多范式(multi-paradigm)的编程语言，Scala支持面向对象和函数式编程 Scala源代码(.scala)会被编译成Java字节码(.class)，然后运行于JVM之上，并可以调用现有的Java类库，实现两种语言的无缝对接 Scala简洁高效 Scala设计时参考了Java的设计思想，源于Java Mac上搭建Scala开发环境(Window/Linux类似) Scala需要Java运行时库，首先先安装JDK环境 在http://www.scala-lang.org/下载mac版本tar.gz包 解压tar.gz包(不配置环境变量) 在命令行下cd进入解压包的bin目录下 输入scala进入Scala Cli，会打印版本信息 搭建IDEA的Scala开发环境 在Plugin面板中安装(如果下载太慢,去官网下载对应版本的插件到本地,在安装) 新建空的maven项目 当前默认不支持scala的框架，需要引入scala框架，右键\b项目点击add framework support 选中scala，在use library中设定解压的目录 右键main目录创建一个diretory，名为scala，右键scala目录，mark directory，选择source root Scala执行流程 .scala源文件通过scalac编译成.class字节码，再通过scala运行 .scala源文件直接通过scala运行(运行慢) Scala程序特点 以.scala为扩展名 执行入口为main()函数 严格区分大小写 方法由一条条语句构成，每个语句后不需要添加分号 如果在一行有多条语句，除了最后一行语句不要分号，其他语句都需要分号 Scala输出的三种方式 字符串通过’+’方式(类似Java) printf方式进行格式化(%,类似C) 字符串通过$引用(类似Kotlin) Scala在IDEA下进行源码关联 在官网下载source源码包 解压到本地 在IDEA中打开一个源码文件，在右上角上点击Attach Sources 选中解压后的本地目录 Scala注释 单行/多行注释(同Java) 文档注释：scaladoc -d 源码.scala 变量 Scala变量声明 基础语法：var | val 变量名 [:变量类型] = 变量值 声明变量时，类型可以省略(编译器自动推导,即类型推导) 类型确定后，就不能修改，说明Scala是强数据类型语言 在声明/定义一个变量时，可以使用var或者val来修饰，var修饰的变量可改变，val修饰的变量不可改(同Kotlin) val修饰的变量在编译后，等同于加上final(同Kotlin) var修饰的对象引用可以改变，val修饰的则不可改变，但对象的状态(值,属性)却是可以改变的(比如自定义对象、数组、集合等等) 变量声明时，需要初始值 数据类型 Scala与Java有着相同的数据类型，在Scala中数据类型都是对象(包装了基础数据类型)，也就是说scala没有java中的原生类型(同Kotlin) Scala数据类型分为两大类AnyVal(值类型)和AnyRef(引用类型)——注意：不管是AnyVal还是AnyRef都是对象 在Scala中有一个根类型Any，他是所有类的父类 scala中一切皆为对象，分为两类AnyVal和AnyRef，它们都是Any子类 Null类型是scala的特殊类型，只有一个值null，是bottom class，也是所有AnyRef类型的子类 Nothing类型也是bottom class，是所有类的子类，开发中通常可以将Nothing类型的值返回给任意变量或函数(抛异常使用很多) 数据类型列表 数据类型 描述 Byte[1] 8位有符号补码整数。数值区间为-128～127 Short[2] 16位有符号补码整数。数值区间为-32768～32767 Int[3] 32位有符号补码整数。数值区间为-2^31～2^31-1 Long[4] 64位有符号补码整数。数值区间为-2^63～2^63-1 Float[4] 32位，IEEE754标准的单精度浮点数 Double[8] 64位，IEEE754标准的双精度浮点数 Char[2] 16位无符号Unicode字符, 区间值为U+0000～U+FFFF String 字符序列 Boolean[1] true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成() Null null Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 整数类型使用细节 Scala各整数类型有固定的表数范围和字段长度，不受具体OS的影响，以保证Scala程序的可移植性 Scala的整型常量/字面量默认为Int型，声明Long型常量/字面量需后加’l’或’L’ 表示特大整数：BigInt类 浮点类型使用细节 与整数类型类似，Scala浮点类型也有固定的表数范围和字段长度，不受具体OS的影响 Scala的浮点型常量默认为Double型，声明Float型常量，须后加’f’或’F’ 两种表示方式 十进制数形式：如5.12、512.0f、.512(必须有小数点) 科学计数法形式：如5.12e2 通常情况下应使用Double类型，比Float类型更精准(小数点后大致7位) 表示更为精确的小数：BigDecimal 字符类型使用细节 字符常量是用单引号’’括起来的单个字符 Scala也允许使用转义字符’&#39;来将其后的字符转变为特殊字符型常量(同Java) Char相当于一个整数，可以进行运算 字符类型存取本质 存储：字符 -&gt; 码值 -&gt; 二进制 -&gt; 存储 读取：二进制 -&gt; 码值 -&gt; 字符 -&gt; 读取 Unit、Null和Nothing类型使用细节 Null类只有一个实例对象null，类似于Java中的null引用。null可以赋值给任意引用类型(AnyRef)，但是不能赋值给值类型 Unit类型用来标识过程，也就是没有明确返回值的函数，类似于Java里的void。Unit只有一个实例() Nothing可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于Nothing是其他任意类型的子类，它还能跟要求返回值的方法兼容 值类型转换 隐式转换：当Scala程序在进行赋值或者运算时，精度小的类型自动转换为精度大的数据类型细节说明： 有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算 当我们把精度(容量)大的数据类型赋值给精度(容量)小的数据类型时会报错 byte、short和char之间不会相互自动转换，三者计算时首先都转换为int类型 自动提升原则：表达式结果的类型自动提升为操作数中(容量、精度)最大的类型 强制类型转换：自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转函数，但可能造成精度降低或溢出 1234// javaint num = (int)2.5// scalavar num : Int = 2.7.toInt 细节说明： 强转符号只针对于最近的操作数有效，往往会使用小括号提升优先级 Char类型可以保存Int的常量值，但不能保存Int的变量值，需要强转 值类型和String类型的转换 基本类型转String类型：将基本类型的值 + “”(同Java) String类型转基本数据类型：调用String.toXxx方法 小数字符串转Int会抛出异常，不会进行截取 标识符命名规则 基本和Java一致 首字符为字母，后续字符任意字母和数字，美元符号，可后接下划线_ 数字不可以开头 首字符为操作符(比如+ - * /)，后续字符也需跟至少一个操作符(反编译后scala将其转译) 操作符(比如+-*/)不能在标识符中间和最后 用反引号``包括的任意字符串，即使是关键字也可以 预定义标识符可以用，如Int，但不推荐 Scala的39个关键字 package, import, class, object, trait, extends, with, type, forSome private, protected, abstract, sealed, final, implicit, lazy, override try, catch, finally, throw if, else, match, case, do, while, for, return, yield def, val, var this, super new true, false, null 运算符 运算符分类 算术运算符 %的运算原则：a % b = a - a / b * b(同Java) 在scala中没有++和–，使用+=1和-=1代替 /的整数除和小数除是有区别的；整数除只保留整数部分而舍弃小数部分 关系运算符(==、!=、&gt;、&lt;、&lt;=、&gt;=) 关系运算的结果都是Boolean类型(true/false) 如果两个浮点数进行比较，应当保证数据类型一致 逻辑运算符(&amp;&amp;、||、!) 赋值运算符(=、+=、-=、*=、/=、%=、&lt;&lt;=、&gt;&gt;=、&amp;=、^=、|=) 运算顺序从右往左 赋值运算符的左边只能是变量，右边可以是变量、表达式、常量值/字面量 位运算符(&amp;、|、^、~、&lt;&lt;、&gt;&gt;、&gt;&gt;&gt;) Scala不支持三目运算符(x ? x : x)，使用if-else代替(类似kotlin) 运算符优先级(同Java) ()[] 单目运算 算术运算 移位运算 比较运算 位运算 关系运算 赋值运算 , 键盘输入语句 输入String：StdIn.readLine() 输入Int：StdIn.readInt … 程序流程控制 三大流程控制 顺序控制 分支控制 Scala中任意表达式都是有返回值的，也就意味着if else表达式其实是有返回结果的，具体返回结果的值取决于满足条件的代码体的最后一行内容 Scala中没有switch，使用模式匹配(match-case)来处理 循环控制 for循环 基本语法： for(i &lt;- start to end)：其中i表示循环的变量，i将会从start～end循环，前后闭合 for(item &lt;- list)：集合遍历 for(i &lt;- start until end)：与to不同的是前闭后开 for(i &lt;- start to end if i % 2 != 0)：循环守卫，即循环保护式(也称条件判断式,守卫)。保护式为true则进入循环体内部，为false则跳过，类似于continue for(i &lt;- start to end ; j = f(i))：引入变量，’;’不可少；其次i和j均为val不可变类型变量 for(i &lt;- start1 to end1 ; j &lt;- start2 to end2)：嵌套循环，’;’不可少；上面代码不常用，基本用单层for的嵌套 val res = for(i &lt;- start to end) yield i：循环返回值，将遍历过程中的每个结果i返回到一个新的Vector集合中；yield后可以是一个代码块，在最后一行返回 for(i &lt;- Range(start,end,step))：控制for循环的步长 补充 {}和()对于for表达式都可以 有一个不成文的约定：当for推导式仅包含单一表达式时使用圆括号，当其包含多个表达式时使用大括号 当使用{}来换行写表达式时，分号就不用写了 while/do-while循环(同Java) 与if语句不同，while语句本身没有返回值，即结果是Unit类型 while没有返回值，所以用while语句来计算并返回结果时，不可避免地使用声明在while外部的变量，那么就等同于循环的内部对外部的变量造成了影响，所以不推荐使用，而是推荐使用for循环 while循环的中断 说明：Scala内置控制结构特地去掉了break和continue，是为了更好的适应函数化编程，推荐使用函数式的风格解决break和contine的功能，而不是一个关键字 if-else和循环守卫也可以实现continue效果 举例说明(break的使用)： 12345678910111213141516171819202122232425262728// 导入break相关函数import util.control.Breaks._/*breakable()函数 1、是一个高阶函数：可以接受函数的函数 2、源码实现： def breakable(op: =&gt; Unit) &#123; try &#123; op &#125; catch &#123; case ex: BreakControl =&gt; if (ex ne breakException) throw ex &#125; &#125; op：=&gt; Unit表示接受的参数是一个没有参数和返回值的函数，可简单理解为一段代码块 3、breakable对break()抛出的异常做了处理，代码就继续执行 4、传入代码块时，一般将()改为&#123;&#125;(类似Kotlin)*/breakable&#123; while(n &lt;= 20)&#123; n+=1 if(n == 18)&#123; // 在scala中使用函数break()中断循环 // def break(): Nothing = &#123; throw breakException &#125; break() &#125; &#125;&#125; 函数式编程基础 函数式编程介绍 概念说明 函数式编程 是一种”编程范式” 属于”结构化编程”的一种，主要思想是把运算过程尽量写成一系列嵌套的函数调用 函数式编程中，将函数也当做数据类型，因此可以接受函数当作输入(参数)和输出(返回值) 在scala中，方法和函数几乎可以等同(比如他们的定义、使用、运行机制都一样的)，只是函数的使用方式更加的灵活多样 函数式编程是从编程方式(范式)的角度来谈的，可以这样理解：函数式编程把函数当做一等公民，充分利用函数、支持的函数的多种使用方式。比如：在Scala当中，函数是一等公民，像变量一样，既可以作为函数的参数使用，也可以将函数赋值给一个变量。函数的创建不用依赖于类或者对象；而在Java当中，函数的创建则要依赖于类、抽象类或者接口 Scala中函数式编程和面向对象编程(是以对象为基础的编程方式)融合在一起 函数式编程、面向对象编程关系分析 函数的定义(为完成某一功能的程序指令(语句)的集合) 基本语法 1234def 函数名 ([参数名: 参数类型], ...) [[: 返回值类型] =] &#123; 语句... return 返回值&#125; 语法介绍 函数声明关键字为def(definition) [参数名: 参数类型], …：表示函数的输入(参数列表)，可以没有。如果有，多个参数使用逗号间隔 函数返回值 (: 返回值类型 =) 明确返回值的类型 (=) 表示返回值类型不确定，使用类型推导完成 (空)，表示没有返回值，return不生效 如果没有return，默认以执行到最后一行的结果作为返回值 函数注意事项 递归调用的规则 程序执行一个函数时，就创建一个新的受保护的独立空间(新函数栈) 函数的局部变量是独立的，不会相互影响 递归调用必须有递归出口，否则就是无限递归 函数的形参列表可以是多个，如果函数没有形参，调用时可以不带() 形参列表和返回值列表的数据类型可以是值类型和引用类型 Scala中的函数可以根据函数体最后一行代码自行推断函数返回值类型。那么在这种情况下，return关键字可以省略 因为Scala可以自行推断，所以在省略return关键字的场合，返回值类型也可以省略 如果函数明确使用return关键字，那么函数返回就不能使用自行推断了，这时要明确写成(: 返回类型 = )，当然如果你什么都不写，即使有return，返回值也为() 如果函数明确声明无返回值(声明Unit)，那么函数体中即使使用return关键字也不会有返回值 如果明确函数无返回值或不确定返回值类型，那么返回值类型可以省略(或声明为Any) Scala语法中任何的语法结构都可以嵌套其他语法结构(灵活)，即：函数中可以再声明/定义函数，类中可以再声明类，方法中可以再声明/定义方法 Scala函数的形参，在声明参数时，直接赋初始值(默认值)，这时调用函数时，如果没有指定实参，则会使用默认值。如果指定了实参，则实参会覆盖默认值 如果函数存在多个参数，每一个参数都可以设定默认值，那么这个时候，传递的参数到底是覆盖默认值，还是赋值给没有默认值的参数，就不确定了(默认按照声明顺序[从左到右])。在这种情况下，可以采用带名参数(类似Kotlin) Scala函数的形参默认是val的，因此不能在函数中进行修改 递归函数未执行之前是无法推断出来结果类型，在使用时必须有明确的返回值类型 Scala函数支持可变参数，可变参数必须放在最后，如args :Int* 过程 基本介绍：将函数的返回类型为Unit的函数称之为过程(procedure)，如果明确函数没有返回值，那么等号可以省略 注意区分：如果函数声明时没有返回值类型，但是有等号，可以进行类型推断(最后一行代码)；这时这个函数实际是有返回值的，该函数并不是过程 惰性函数 一种应用场景惰性计算(尽可能延迟表达式求值)是许多函数式编程语言的特性。惰性集合在需要时提供其元素，无需预先计算它们，这带来了一些好处：首先，可以将耗时的计算推迟到绝对需要的时候；其次，可以创造无限个集合，只要它们继续收到请求，就会继续提供元素。函数的惰性使用能够得到更高效的代码。Java并没有为惰性提供原生支持，Scala提供了 Java实现懒加载(单例模式——懒汉式) 12345678910111213public class LazyDemo &#123; private LazyDemo instance = null; private LazyDemo()&#123;&#125; public LazyDemo getInstance() &#123; // 如果没有初始化过,那么进行初始化 if (instance == null) &#123; instance = new LazyDemo(); &#125; return instance; &#125;&#125; 介绍当函数返回值被声明为lazy时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称之为惰性函数，在Java的某些框架代码中称之为懒加载(延迟加载) 案例 12345678910111213141516def main(args: Array[String]): Unit = &#123; lazy val res = sum(10, 20) println(\"-----------------\") println(\"res = \" + res)&#125;def sum(n1 : Int, n2 : Int): Int = &#123; println(\"sum() ~~~\") n1 + n2&#125;/*结果：-----------------sum() ~~~res = 30*/ 注意事项和细节 lazy不能修饰var类型的变量 不但在调用函数时，加了lazy会导致函数的执行被推迟；在声明一个变量时，如果声明了lazy，那么变量值得分配也会推迟。比如lazy val i = 10 异常 介绍 Scala提供try和catch块来处理异常：try块用于包含可能出错的代码；catch块用于处理try块中发生的异常。可以根据需要在程序中有任意数量的try…catch块 语法处理上和Java类似，但是又不尽相同(许多异常包装了Java中的Exception——类似Kotlin) Java异常回顾 示例： 1234567891011try &#123; int i = 0; int b = 10; // ArithmeticException,除0异常 int c = b / i;&#125; catch(Exception e) &#123; e.printStackTrace();&#125; finally &#123; // 最终要执行的代码 System.out.println(\"java finally\");&#125; Java异常处理的注意点： java语言按照try—catch-catch-…—finally的方式来处理异常 不管有没有异常捕获，都会执行finally，因此通常可以在finally代码块中释放资源 可以有多个catch，分别捕获对应的异常，这时需要把范围小的异常类写在前面，把范围大的异常类写在后面，否则编译错误。会提示”Exception ‘java.lang.xxxxxx’ has already been caught” Scala异常处理 示例： 123456789try &#123; val r = 10 / 0&#125; catch &#123; case ex: ArithmeticException=&gt; println(\"ArithmeticException!\") case ex: Exception =&gt; println(\"Normal Exception!\")&#125; finally &#123; // 最终要执行的代码 println(\"scala finally\")&#125; Scala异常处理的注意点： 在scala异常处理中只有一个catch；在catch中有多个case，每个case可以匹配一种异常；”=&gt;”是一个关键符号，表示后面是对该异常的处理代码块 我们将可疑代码封装在try块中。在try块之后使用了一个catch处理程序来捕获异常。如果发生任何异常，catch处理程序将处理它，程序将不会异常终止 Scala的异常的工作机制和Java一样，但是Scala没有”checked(编译期)”异常，即Scala没有编译异常这个概念，异常都是在运行的时候捕获处理 可使用throw关键字抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing(因为Nothing是所有类型的子类型，所以throw表达式可以用在任何需要类型的地方) 1234567def main(args: Array[String]): Unit = &#123; val res = test() println(res)&#125;def test(): Nothing = &#123; throw new Exception(\"My Exception!\")&#125; 在Scala里，借用了模式匹配的思想来做异常的匹配。因此可以在catch的代码里，使用一系列case子句来匹配异常。”=&gt;”可以接着多条语句(换行)，类似java的switch-case语句 异常捕捉的机制与其他语言中一样，如果有异常发生，catch子句是按次序捕捉的。因此在catch子句中，越具体的异常越要靠前，越普遍的异常越靠后。如果把越普遍的异常写在前，把具体的异常写在后，scala不会报错，但这样是非常不好的编程风格 finally子句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作，这点和Java一样 Scala提供了throws关键字来声明异常。可以使用方法定义声明异常。它向调用者函数提供了此方法可能引发此异常的信息。它有助于调用函数处理并将该代码包含在try-catch块中，以避免程序异常终止。在scala中，可以使用throws注释来声明异常 123456789def main(args: Array[String]): Unit = &#123; f11()&#125;// 等同于NumberFormatException.class@throws(classOf[NumberFormatException])def f11() = &#123; \"abc\".toInt&#125; 面向对象编程-基础 Scala是面向对象的 Java是面向对象的编程语言。但由于历史原因，Java中还存在着非面向对象的内容：基本类型、null、静态方法等 Scala来源于Java，所以天生就是面向对象的语言，而且Scala是纯粹的面向对象的语言(即在Scala中，一切皆为对象) Scala定义类 基本语法(基本与Java一致)： 123[修饰符] class 类名 &#123; 类体&#125; 注意事项 在scala语法中类并不声明为public，所有这些类都具有公有可见性(即默认就是public) 一个Scala源文件可以包含多个类(同Java)，而且默认都是public 属性/成员变量注意事项 属性的定义语法同变量：[访问修饰符] var/val 属性名称 [：类型] = 属性值 属性的定义类型可以为任意类型，包含值类型或引用类型 Scala中声明一个属性必须显式初始化，Scala可根据初始化数据的类型自动推断，此时属性类型可以省略(这与Java不同) 如果赋值为null，则一定要加类型。因为不加类型，那么该属性的类型就是Null类型 如果在定义属性时暂时不赋值，也可以使用符号”_”，让系统分配默认值 类型 _对应的值 Byte/Short/Int/Long 0 Float/Double 0.0 Boolean false String和其他引用类型 null 同一类型不同对象的属性相互独立，互不影响 创建对象 基本语法：val | var 对象名 [: 类型] = new 类型() 说明 如果我们不希望改变对象的引用(即内存地址)，应该声明为val，否则声明为var。scala设计者推荐使用val：因为一般来说，在程序中，我们只是改变对象的属性的值，而不是改变对象的引用 scala在声明对象变量时，可以根据创建对象的类型自动推断，所以类型声明可以省略(java不可省略,类似Kotlin)。但当类型和后面new的对象类型有继承关系即多态时，就必须写 访问属性 基本语法：对象名.属性名 原理： 示例： 123456789101112def main(args: Array[String]): Unit = &#123; val test = new Person test.age = 5 test.name = \"xiong\" println(s\"age = $&#123;test.age&#125; , name = $&#123;test.name&#125; , tag = $&#123;test.tag&#125;\")&#125;class Person &#123; var name: String = _ var age: Int = _ val tag = \"SOBXiong\"&#125; 反编译的.class文件 12345678910111213141516171819202122232425262728293031323334// test$.classpublic final class test$ &#123; public static test$ MODULE$; public void main(String[] args) &#123; test.Person test = new test.Person(); // 对象名.属性名修改属性是通过底层编译器翻译包装了java方法实现的 test.age_$eq(5); test.name_$eq(\"xiong\"); // 对象名.属性名获取属性也是通过底层编译器翻译包装了java方法实现的 Predef$.MODULE$.println((new StringBuilder(25)).append(\"age = \").append(test.age()).append(\" , name = \").append(test.name()).append(\" , tag = \").append(test.tag()).toString()); &#125; private test$() &#123; MODULE$ = this;&#125;&#125;public class Person &#123; // var变量翻译后自动生成getter/setter方法 private String name; private int age; public String name() &#123; return this.name; &#125; public void name_$eq(String x$1) &#123; this.name = x$1; &#125; public int age() &#123; return this.age; &#125; public void age_$eq(int x$1) &#123; this.age = x$1; &#125; // val变量翻译为final变量(不可变),且只提供getter方法,不提供setter修改的方法 private final String tag = \"SOBXiong\"; public String tag() &#123; return this.tag; &#125;&#125; 方法 基本说明：Scala中的方法其实就是函数 基本语法： 123def 方法名(参数列表) [：返回值类型] = &#123; 方法体&#125; 方法调用机制原理 当scala程序开始执行时，先在栈区开辟一个main栈。main栈最后被销毁(scala程序终止) 当scala程序执行到一个方法时，总会开一个新的栈 每个栈是独立的空间，变量(基本数据类型)是独立的，相互不影响(引用类型除外) 当方法执行完毕后，该方法开辟的栈就会被JVM机回收 构造器 基本介绍：构造器(constructor)又叫构造方法，是类的一种特殊的方法，主要作用是完成对新对象的初始化 Java构造器回顾 基本语法 123[修饰符] 类名(参数列表) &#123; 构造方法体&#125; 特点 在Java中一个类可以定义多个不同的构造方法(构造方法重载) 如果没有定义构造方法，系统会自动生成一个默认无参构造方法(也叫默认构造器)，比如Person(){} 一旦定义了自己的构造方法，默认的构造方法就被覆盖了，就不能再使用默认的无参构造方法，除非显式地定义一下 Scala构造器 介绍：和Java一样，Scala创建新对象也需要调用构造方法，并且可以有任意多个构造方法(即scala中构造器也支持重载)。Scala类的构造器包括：主构造器和辅助构造器 基本语法 12345class 类名(形参列表) &#123;// 主构造器 // 类体 def this(形参列表) &#123;...&#125;// 辅助构造器1 def this(形参列表) &#123;...&#125;// 辅助构造器2、3...&#125; 构造器参数 Scala类的主构造器的形参若未用任何修饰符修饰，那么这个参数是局部变量 如果参数使用val关键字声明，那么Scala会将参数作为类的私有的只读属性使用 如果参数使用var关键字声明，那么那么Scala会将参数作为类的成员属性使用，并会提供属性对应的xxx()[类似getter]以及xxx_$eq()[类似setter]方法(这时的成员属性是私有的，但是可读写) Bean属性 介绍JavaBeans规范定义了Java的属性是像getXxx()和setXxx()的方法。许多Java框架都依赖这个命名习惯。为了与Java的互操作性，产生了@BeanProperty注解。在Scala字段前加@BeanProperty时会自动生成规范的setXxx()以及getXxx()方法。这时可以使用对象.setXxx()和对象.getXxx()来修改和获取属性值 注意给某个属性加入@BeanPropetry注解后，会生成getXXX和setXXX的方法，并且对原来底层自动生成类似xxx(),xxx_$eq()方法，没有冲突，二者可以共存 注意事项和细节 Scala构造器作用是完成对新对象的初始化，构造器没有返回值 主构造器的声明直接放置于类名之后 主构造器会执行类定义中的所有语句，这可以体会到Scala把函数式编程和面向对象编程融合在一起(构造器也是方法/函数) 如果主构造器无参数，小括号可省略，构建对象时调用的构造方法的小括号也可以省略 辅助构造器名称为this(和Java、Kotlin不一样)，多个辅助构造器通过不同参数列表进行区分(底层就是构造器重载) 如果想让主构造器变成私有的，可以在()之前加上private，这样只能通过辅助构造器来构造对象 辅助构造器的声明不能和主构造器的声明一致，否则会发生错误(构造器名重复) 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def main(args: Array[String]): Unit = &#123; val p1 = new Person(\"sobxiong\") p1.showInfo()&#125;class Person() &#123; println(\"类定义语句~~~\") @BeanProperty var name: String = _ @BeanProperty var age: Int = _ def this(name: String) &#123; /* 辅助构造器无论是直接或间接,最终都一定要调用主构造器,执行主构造器的逻辑 而且需要放在辅助构造器的第一行(这点和Java一样,Java中一个构造器要调用同类的其它构造器也需要放在第一行) */ // 直接调用主构造器 this() this.name = name &#125; def this(name: String, age: Int) &#123; // 直接调用主构造器 this() this.name = name this.age = age &#125; def this(age: Int) &#123; // 间接调用主构造器 this(\"匿名\") this.age = age &#125; def showInfo(): Unit = &#123; println(\"person信息如下:\") println(\"name = \" + this.name) println(\"age = \" + this.age) &#125;&#125;/*结果如下：类定义语句~~~person信息如下:name = sobxiongage = 0*/ 反编译的.class文件 12345678910111213141516171819202122232425262728293031323334353637383940public class Person &#123; private String name; private int age; public String name() &#123; return this.name; &#125; public void name_$eq(String x$1) &#123; this.name = x$1; &#125; public String getName() &#123; return name(); &#125; public void setName(String x$1) &#123; name_$eq(x$1); &#125; public int age() &#123; return this.age; &#125; public void age_$eq(int x$1) &#123; this.age = x$1; &#125; public int getAge() &#123; return age(); &#125; public void setAge(int x$1) &#123; age_$eq(x$1); &#125; public Person() &#123; Predef$.MODULE$.println(\"类定义语句~~~\"); &#125; public Person(int age) &#123; this(\"\"); age_$eq(age); &#125; public Person(String name) &#123; this(); name_$eq(name); &#125; public Person(String name, int age) &#123; this(); name_$eq(name); age_$eq(age); &#125; public void showInfo() &#123; Predef$.MODULE$.println(\"person信息如下:\"); Predef$.MODULE$.println((new StringBuilder(7)).append(\"name = \").append(name()).toString()); Predef$.MODULE$.println((new StringBuilder(6)).append(\"age = \").append(age()).toString()); &#125;&#125; 面向对象编程-中级 包 回顾Java包 Java包的三大作用 区分相同名字的类 当类很多时，可以很好的管理类 控制访问范围 Java包的声明：package xxx; Java包的本质：创建不同的文件夹保存类文件 Java包的要求： 包名和源码所在的系统文件目录结构要一致 编译后的字节码文件路径也和包名保持一致 Scala包 基本语法：package xxx Scala的作用： 区分相同名字的类 当类很多时，可以很好的管理类 控制访问范围 可以对类的功能进行扩展 Scala包名和源码所在的系统文件目录可以不一致，但编译后的.class字节码文件路径和包名会保持一致(该工作由编译器完成) 包的命名规则：只能包含数字、字母、下划线、小圆点(.)，但不能用数字开头，也不要使用关键字 包的命名规范：com.公司名.项目名.业务模块名 Scala自动引用的常用包：java.lang.*、scala、Predef Scala包注意事项和使用细节： scala多种等价的包形式 123456789101112131415161718192021222324252627282930// 传统的包形式package com.atguigu.scalaclass Person&#123; val name = \"Nick\" def play(message: String): Unit = &#123; println(this.name + \" \" + message) &#125;&#125;// 等价的第二种包形式package com.atguigupackage scalaclass Person&#123; val name = \"Nick\" def play(message: String): Unit = &#123; println(this.name + \" \" + message) &#125;&#125;// 嵌套包形式package com.atguigu&#123; package scala&#123; class Person&#123; val name = \"Nick\" def play(message: String): Unit = &#123; println(this.name + \" \" + message) &#125; &#125; &#125;&#125; 嵌套包的好处：可以灵活地在同一个文件中将类(class, object)、特质(trait)创建在不同的包中 作用域原则：可以直接向上访问。即：Scala可在子包中直接访问父包中的内容，大括号体现作用域。(提示: Java中子包使用父包的类,需要import)。在子包和父包类重名时，默认采用就近原则，如果希望指定使用某个类，需要指定包名 父包要访问子包的内容时，需要import对应的类 可以在同一个.scala文件中，声明多个并列的package(建议嵌套的pakage不要超过3层) 包名可以相对也可以绝对。在一般情况下，我们使用相对路径来引入包，只有当包名冲突时，使用绝对路径来处理 包对象 基本介绍：包可以包含类(class, object)和特质(trait)，但不能包含函数/方法或变量的定义。这是Java虚拟机的局限。为了弥补这一点不足，scala提供了包对象的概念来解决这个问题 示例： 示例代码： 123456789101112131415161718192021222324252627282930313233package com.xiong &#123; // 每个包都可以有一个包对象 // 需要在父包中定义它,且名称与子包一样。 package object scala &#123; var name = \"jack\" def sayOk(): Unit = &#123; println(\"package object sayOk!\") &#125; &#125; package scala &#123; class Test &#123; def test(): Unit = &#123; // 包对象scala中声明的name变量 println(name) // 调用包对象scala中声明的sayOk方法 sayOk() &#125; &#125; object TestObj &#123; def main(args: Array[String]): Unit = &#123; val t = new Test() t.test() // 因为TestObj和scala这个包对象在同一包,因此也可以使用name属性 println(\"name =\" + name) &#125; &#125; &#125;&#125; 机制分析： 12345678910111213141516171819202122232425262728293031323334353637383940// 当创建包对象后,在该包下生成final修饰的package和package$类// package$.classpublic final class package$ &#123; public static package$ MODULE$; private String name; public String name() &#123; return this.name; &#125; public void name_$eq(String x$1) &#123; this.name = x$1; &#125; public void sayOk() &#123; scala.Predef$.MODULE$.println(\"package object sayOk!\"); &#125; private package$() &#123; MODULE$ = this; this.name = \"jack\"; &#125;&#125;// Test.classpublic class Test &#123; public void test() &#123; Predef$.MODULE$.println(package$.MODULE$.name()); package$.MODULE$.sayOk(); &#125;&#125;// TestObj$.classpublic final class TestObj$ &#123; public static TestObj$ MODULE$; public void main(String[] args) &#123; Test t = new Test(); t.test(); scala.Predef$.MODULE$.println((new StringBuilder(6)).append(\"name =\").append(package$.MODULE$.name()).toString()); &#125; private TestObj$() &#123; MODULE$ = this; &#125;&#125; 注意事项 每个包都可以有一个包对象，需要在父包中定义它 包对象名称需要和包名一致，一般用来对包的功能进行补充 包的可见性 回顾Java 访问修饰符介绍(控制方法和变量的访问权限、范围) 公开级别：用public修饰，对外公开 受保护级别：用protected修饰，对子类和同一个包中的类公开 默认级别：没有修饰符号，向同一个包的类公开 私有级别：用private修饰，只有类本身可以访问，不对外公开 访问级别 访问控制修饰符 同类 同包 子类 不同包 公开 public √ √ √ √ 受保护 protected √ √ √ × 默认 / √ √ × × 私有 private √ × × × 修饰符注意事项 修饰符可以用来修饰类中的属性，成员方法以及类 只有默认和public才能修饰类，并且遵循上述访问权限的特点 Scala的包的可见性(四种修饰符与Java一样) 当属性访问权限为默认时，从底层看属性是private的，但是因为提供了xxx_$eq()[类似setter]/xxx()[类似getter]方法，因此从使用效果看任何地方都可以访问 当方法访问权限为默认时，默认为public访问权限 private为私有权限，只在类的内部和伴生对象中可用 protected为受保护权限，scala中受保护权限比Java中更严格，只能子类访问，同包无法访问(编译器) 在scala中没有public关键字，即不能用public显式地修饰属性和方法 包访问权限(表示属性有了限制，同时包也有了限制)，这点和Java不一样，体现出Scala包使用的灵活性 1234567891011package com.xiong.scalaclass Person &#123; /* 增加包访问权限后 1、private同时起作用,不仅同类可以使用 2、同时com.xiong.scala中包下其他类也可以使用 3、修饰符也可以设置为public、protected等 4、包可见性可以延展到上曾,如改为xiong */ private[scala] val pname = \"hello\"&#125; 包的引入 基本介绍：Scala引入包也是使用import，基本的原理和机制和Java一样，但Scala中的import功能更加强大，也更灵活 使用细节和注意事项： 在Scala中，import语句可以出现在任何地方，并不仅限于文件顶部，import语句的作用一直延伸到包含该语句的块末尾。这种语法的好处是：在需要时在引入包，缩小import包的作用范围，提高效率 Java中如果想要导入包中所有的类，可以通过通配符*，Scala中采用下_ 如果不想要某个包中全部的类，而是其中的几个类，可以采用选取器(大括号) 如果引入的多个包中含有相同的类，那么可以将不需要的类进行重命名进行区分，这个就是重命名 如果某个冲突的类根本就不会用到，那么这个类可以直接隐藏掉 12345// 将HashMap重命名为JavaHashMapimport java.util.&#123; HashMap=&gt;JavaHashMap, List&#125;// 引入java.util包的所有类,但是忽略HashMapimport java.util.&#123; HashMap=&gt;_, _&#125; 继承 Java继承回顾 语法：class 子类名 extends 父类名 { 类体 } 子类继承父类的属性和方法 单继承 Scala继承 语法同Java 单继承同Java","categories":[],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"},{"name":"Scala","slug":"Scala","permalink":"https://sobxiong.github.io/tags/Scala/"}]},{"title":"HBase","slug":"BigData/HBase","date":"2020-07-17T03:08:53.000Z","updated":"2020-09-20T14:26:05.713Z","comments":true,"path":"2020/07/17/BigData/HBase/","link":"","permalink":"https://sobxiong.github.io/2020/07/17/BigData/HBase/","excerpt":"内容 HBase简介 HBase快速入门 HBase进阶 HBase-API HBase优化","text":"内容 HBase简介 HBase快速入门 HBase进阶 HBase-API HBase优化 HBase简介 HBase定义：HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库 HBase数据模型：逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构(K-V)来看，HBase更像是一个multi-dimensional map(多维度Map) 逻辑结构 物理存储结构 数据模型 Name Space命名空间，类似于关系型数据库的DatabBase概念，每个命名空间下有多个表。HBase有两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间 Region类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列簇即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景 RowHBase表中的每行数据都由一个RowKey和多个Column(列)组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要 ColumnHBase中的每个列都由Column Family(列簇)和Column Qualifier(列限定符)进行限定，例如info：name，info：age。建表时，只需指明列簇，而列限定符无需预先定义 Time Stamp用于标识数据的不同版本(version)，每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间 Cell由{rowkey,column Family:column Qualifier,time Stamp}唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮 HBase基本架构架构角色： Region ServerRegion Server为Region的管理者，其实现类为HRegionServer，主要作用如下:对于数据的操作：get,put,delete对于Region的操作：splitRegion、compactRegion MasterMaster是所有Region Server的管理者，其实现类为HMaster，主要作用如下：对于表的操作：create,delete,alter对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移 ZookeeperHBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作 HDFSHDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持 HBase快速入门 HBase安装部署 Zookeeper正常部署 Hadoop(主要是HDFS)正常部署 HBase正常部署 解压HBase：tar -zxvf hbase-2.2.5-bin.tar.gz -C /opt/module 修改HBase的配置文件 修改hbase-env.sh的内容： 123export JAVA_HOME=/opt/module/jdk1.8.0_251# 自行管理zookeeperexport HBASE_MANAGES_ZK=false 修改hbase-site.xml的内容： 1234567891011121314151617181920&lt;!-- 设置为分布式部署 --&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HBase的默认存储文件的路径 --&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000/HBase&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置zookeeper的集群地址 --&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置zookeeper的data目录 --&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.6.1/zkData&lt;/value&gt;&lt;/property&gt; 修改regionservers文件(配置集群节点)： 123hadoop1hadoop2hadoop3 软链接hadoop配置文件到HBase： 12ln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-2.2.5/conf/core-site.xmlln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-2.2.5/conf/hdfs-site.xml 将HBase远程发送到集群其他节点：xsync hbase-2.2.5/ 启动HBase服务 单独启动方式(提示:如果集群之间的节点时间不同步，会导致 regionserver 无法启动，抛出ClockOutOfSyncException 异常) 12bin/hbase-daemon.sh start masterbin/hbase-daemon.sh start regionserver 群起集群启动方式：bin/start-hbase.sh(停止:bin/stop-hbase.sh) 查看HBase页面：http://hadoop1:16010 HBase Shell操作 基本操作 进入HBase客户端命令行：bin/hbase shell 查看帮助命令：help 查看当前数据库的所有表：list 表的操作 创建表 1create 'student','info' 插入(更新)数据 1put 'student','1001','info:sex','male' 扫描查看表数据 12345scan 'student'# 左闭右开scan 'student',&#123;STARTROW =&gt; '1001', STOPROW =&gt; '1003'&#125;# 查看多版本的表数(无需在表上设置存放版本)scan 'student',&#123;RAW =&gt; true, VERSIONS =&gt; 10&#125; 查看表结构 1describe 'student' 查看指定行或指定行的指定列的数据 1234get 'student','1001'get 'student','1001','info:name'# 查看name列的三个版本的数据(需要在表上设置存放版本)get 'student','1001',&#123;COLUMN =&gt; 'info:name',VERSIONS =&gt; 3&#125; 统计表数据行数 1count 'student' 删除数据 1234# 删除某rowkey的全部数据deleteall 'student','1001'# 删除某rowkey的某一列数据delete 'student','1002','info:sex' 清空表数据 12# 提示: 清空表的操作顺序为先disable,然后再truncatetruncate 'student' 删除表 1234# 首先要将表设置为disable状态disable 'student'# 删除表drop 'student' 变更表信息 12# 设置info列簇的数据存放3个版本alter 'student',&#123;NAME =&gt; 'info',VERSIONS =&gt; 3&#125; HBase进阶 架构原理 StoreFile保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile(HFile)，数据在每个StoreFile中都是有序的 MemStore写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的HFile WAL由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件(简称WAL)中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建 写流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问 与目标Region Server进行通讯 将数据顺序写入(追加)到WAL 将数据写入对应的MemStore，数据会在MemStore进行排序 向客户端发送ack 等达到MemStore的刷写时机后，将数据刷写到HFile MemStore FlushMemStore刷写时机： 当某个memstore的大小达到了hbase.hregion.memstore.flush.size(默认值128M)，其所在 region的所有memstore都会刷写。当memstore的大小达到了hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier(默认值4)时，会阻止继续往该memstore写数据 当region server中memstore的总大小达到java_heapsize * hbase.regionserver.global.memstore.size * hbase.regionserver.global.memstore.size.lower.limit(默认值0.95)，region会按照其所有memstore的大小顺序(由大到小)依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下(当前还可以写数据)。当region server中memstore的总大小达到java_heapsize * hbase.regionserver.global.memstore.size时，会阻止继续往所有的memstore写数据 到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置：hbase.regionserver.optionalcacheflushinterval(默认1小时) 当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log 以下(该属性名已经废弃——无法手动设置，默认值为32;但是可以设置每个log文件的blockSize,相当于增加单个log文件的存储量,默认为hdfs的块大小128M) 读流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问 与目标Region Server进行通讯 分别在Block Cache(读缓存——内存)，MemStore(内存)和Store File(HFile磁盘文件)中查询目标数据(同时查内存和磁盘,因为不知道哪个timestamp先)，并将查到的所有数据进行合并(Merge)。此处所有数据是指同一条数据的不同版本(timestamp)或者不同的类型(Put/Delete) 将从文件中查询到的数据块(Block,HFile数据存储单元,默认大小为64KB)缓存到Block Cache 将合并后的最终结果返回给客户端 StoreFile Compaction由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本(timestamp)和不同类型(Put/Delete)有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile CompactionCompaction分为两种，分别是Minor Compaction和Major Compaction。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据 Region Split默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个 Region转移给其他的Region Server。Region Split时机： 当1个region中的某个Store下所有StoreFile的总大小超过Min(R ^ 2 * hbase.hregion.memstore.flush.size , hbase.hregion.max.filesize)，该Region就会进行拆分，其中 R为当前Region Server中属于该Table的个数 HBase-API 环境准备新建maven项目后在pom.xml中添加依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt; HBaseAPI 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205public class TestApi &#123; private static Connection connection; private static Admin admin; static &#123; try &#123; // 1、获取配置文件信息 Configuration configuration = HBaseConfiguration.create(); configuration.set(\"hbase.zookeeper.quorum\", \"hadoop1,hadoop2,hadoop3\"); // 2、创建连接对象 connection = ConnectionFactory.createConnection(configuration); // 3、创建admin对象 admin = connection.getAdmin(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 关闭资源 public static void close() &#123; if (admin != null) &#123; try &#123; admin.close(); admin = null; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null) &#123; try &#123; connection.close(); connection = null; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 1、测试表是否存在 public static boolean isTableExist(String tableName) throws IOException &#123; return admin.tableExists(TableName.valueOf(tableName)); &#125; // 2、创建表 public static void createTable(String tableName, String... columnFamilies) throws IOException &#123; // 1、判断是否存在列簇信息 if (columnFamilies.length == 0) &#123; System.out.println(\"请设置列簇信息!\"); return; &#125; // 2、判断表是否存在 if (isTableExist(tableName)) &#123; System.out.println(tableName + \"表已存在!\"); return; &#125; List&lt;ColumnFamilyDescriptor&gt; columnFamilyList = new ArrayList&lt;ColumnFamilyDescriptor&gt;(); // 3、循环添加列簇信息 for (String columnFamily : columnFamilies) &#123; // 添加列簇信息 columnFamilyList.add(ColumnFamilyDescriptorBuilder.newBuilder(columnFamily.getBytes()).build()); &#125; // 4、创建表描述器 TableDescriptor tableDescriptor = TableDescriptorBuilder .newBuilder(TableName.valueOf(tableName)) .setColumnFamilies(columnFamilyList) .build(); // 5、创建表 admin.createTable(tableDescriptor); &#125; // 3、删除表 public static void dropTable(String tableName) throws IOException &#123; // 1、判断表是否存在 if (!isTableExist(tableName)) &#123; System.out.println(tableName + \"表不存在!\"); &#125; // 2、下线表 admin.disableTable(TableName.valueOf(tableName)); // 3、删除表 admin.deleteTable(TableName.valueOf(tableName)); &#125; // 4、创建命名空间 public static void createNameSpace(String nameSpaceName) &#123; // 1、创建命名空间描述器 NamespaceDescriptor namespaceDescriptor = NamespaceDescriptor.create(nameSpaceName) .build(); // 2、创建命名空间 try &#123; admin.createNamespace(namespaceDescriptor); &#125; catch (NamespaceExistException e) &#123; System.out.println(nameSpaceName + \"命名空间已存在!\"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 5、向表中插入数据 public static void putData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、创建put对象 Put put = new Put(Bytes.toBytes(rowKey)); // 3、给put对象赋值 put.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); // 4、插入数据 table.put(put); // 5、关闭表 table.close(); &#125; // 6、获取数据(get) public static void getData(String tableName, String rowKey, String columnFamily, String column) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、创建get对象 Get get = new Get(Bytes.toBytes(rowKey)); // 指定获取的列簇和列 if (column != null &amp;&amp; !column.isEmpty() &amp;&amp; columnFamily != null &amp;&amp; !columnFamily.isEmpty()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column)); &#125; else if (columnFamily != null &amp;&amp; !columnFamily.isEmpty()) &#123; get.addFamily(Bytes.toBytes(columnFamily)); &#125; // 设置获取数据的版本数 get.readVersions(2); // 3、获取数据 Result result = table.get(get); // 4、解析result for (Cell cell : result.rawCells()) &#123; // 5、打印数据 System.out.println(\"columnFamily = \" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\"column = \" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\"value = \" + Bytes.toString(CellUtil.cloneValue(cell))); &#125; // 6、关闭表连接 table.close(); &#125; // 7、获取数据(scan) public static void scanTable(String tableName) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、构建scan对象 Scan scan = new Scan(); // 构建过滤器 // RowFilter rowFilter = new RowFilter(CompareOperator.EQUAL, new SubstringComparator(uid + '_')); // scan.setFilter(rowFilter); // 3、扫描表 ResultScanner resultScanner = table.getScanner(scan); // 4、解析resultScanner for (Result result : resultScanner) &#123; // 5、解析result for (Cell cell : result.rawCells()) &#123; // 6、打印数据 System.out.println(\"rowKey = \" + Bytes.toString(CellUtil.cloneRow(cell))); System.out.println(\"columnFamily = \" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\"column = \" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\"value = \" + Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125; // 5、关闭table对象 table.close(); &#125; // 8、删除数据,delete是一种特殊的put操作,打标记 public static void deleteData(String tableName, String rowKey, String columnFamily, String column) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、构建删除对象,deleteFamily标记,rowKey下所有columnFamily的所有version Delete delete = new Delete(Bytes.toBytes(rowKey)); // 设置删除的列簇 deleteFamily标记对应HBase Cli的deleteall命令 // 删除列对应部分delete命令,删除指定columnFamily的所有version // delete.addFamily(Bytes.toBytes(columnFamily)); // 设置删除的列 deleteColumn标记 // delete.addColumns(Bytes.toBytes(columnFamily), Bytes.toBytes(column)); /* 设置删除的列 delete标记 普通情况：上一个老值冒出来了,新值删去 另一种情况：接连着push两条name,flush之后再使用该方式删除,没数据了 无法确定flush时机,删除最好使用addColumns(防止出错) */ // delete.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column)); // 设置删除的列 deleteColumn标记,删除在小于等于timestamp的所有版本 // delete.addColumns(Bytes.toBytes(columnFamily), Bytes.toBytes(column), 1595307747400L); // 设置删除的列 delete标记,删除等于timestamp的那一个版本 delete.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column), 1595307747419L); // 3、执行删除操作 table.delete(delete); // 4、关闭连接 table.close(); &#125; public static void main(String[] args) throws IOException &#123; // System.out.println(\"isTableExist = \" + isTableExist(\"stu4\")); // createTable(\"stu6\", \"info1\", \"info2\"); // System.out.println(\"isTableExist = \" + isTableExist(\"stu5\")); // dropTable(\"stu6\"); // createNameSpace(\"test\"); // createTable(\"test:xixi\", \"info\"); // putData(\"stu4\", \"1006\", \"info\", \"name\", \"haha\"); // scanTable(\"fruit\"); deleteData(\"stu\", \"1008\", \"info1\", \"name\"); close(); &#125;&#125; MapReduce通过HBase的相关Java API，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析 官方HBase-MapReduce案例 查看HBase的MapReduce任务的执行需要的依赖包：bin/hbase mapredcp以及bin/hbase classpath 环境变量的导入(/etc/profile中配置;在生产环境中最好使用临时操作,以下命令在命令行下输入,只在当前次登录生效) 12345# HBASE_HOMEexport HBASE_HOME=/opt/module/hbase-2.2.5export HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase classpath`export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:`$&#123;HBASE_HOME&#125;/bin/hbase mapredcp`# 保存后执行source /etc/profile 执行官方的MapReduce任务(输出都在控制台上) 案例一：统计stu表中有多少行数据 1hadoop jar lib/hbase-mapreduce-2.2.5.jar rowcounter stu 案例二：使用MapReduce将本地数据导入到HBase 在本地创建一个tsv格式的文件：fruit.tsv(csv格式以’,’隔开,而tsv格式以’\\t’隔开) 1231001 Apple Red1002 Pear Yellow1003 Pineapple Yellow 创建HBase表(不存在会报错)： 1create 'fruit','info' 将fruit.tsv文件上传到HDFS上(根目录上)：hdfs dfs -put fruit.tsv / 执行MapReduce任务 123hadoop jar lib/hbase-mapreduce-2.2.5.jar \\importtsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\hdfs://hadoop1:9000/fruit.tsv 使用scan命令查看导入后的结果 1scan 'fruit' 自定义HBase-MapReduce 导入相应的依赖包 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-mapreduce&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-auth&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-common&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 示例一：将HDFS上数据表fruit.tsv导入到HBase的fruit1表中(打包扔到集群上运行) FruitMapper类，用于读取HDFS上数据 123456public class FruitMapper extends Mapper&lt;LongWritable, Text, LongWritable, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; FruitReducer类，用于将数据写入到HBase中 12345678910111213141516public class FruitReducer extends TableReducer&lt;LongWritable, Text, NullWritable&gt; &#123;@Overrideprotected void reduce(LongWritable key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、遍历values: 1001 Apple red for (Text value : values) &#123; // 2、获取每一行数据 String[] fields = value.toString().split(\"\\t\"); // 3、构建Put对象 Put put = new Put(Bytes.toBytes(fields[0])); // 4、给Put对象复制 put.addColumn(Bytes.toBytes(\"info\"), Bytes.toBytes(\"name\"), Bytes.toBytes(fields[1])); put.addColumn(Bytes.toBytes(\"info\"), Bytes.toBytes(\"color\"), Bytes.toBytes(fields[2])); // 5、写出 context.write(NullWritable.get(), put); &#125;&#125; FruitDriver类 12345678910111213141516171819202122232425262728293031323334353637383940public class FruitDriver implements Tool &#123; // 定义一个Configuration private Configuration configuration = null; public int run(String[] args) throws Exception &#123; // 1、获取job对象 Job job = Job.getInstance(configuration); // 2、设置驱动类路径 job.setJarByClass(FruitDriver.class); // 3、设置Mapper和Mapper输出的KV类型 job.setMapperClass(FruitMapper.class); job.setMapOutputKeyClass(LongWritable.class); job.setMapOutputValueClass(Text.class); // 4、设置Reducer类 TableMapReduceUtil.initTableReducerJob( args[1], FruitReducer.class, job ); // 5、设置输入参数 FileInputFormat.setInputPaths(job, new Path(args[0])); // 6、提交任务 boolean result = job.waitForCompletion(true); return result ? 0 : 1; &#125; public void setConf(Configuration configuration) &#123; this.configuration = configuration; &#125; public Configuration getConf() &#123; return configuration; &#125; public static void main(String[] args) &#123; try &#123; Configuration configuration = new Configuration(); int run = ToolRunner.run(configuration, new FruitDriver(), args); System.exit(run); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 使用maven进行package打包操作，将jar包上传到集群上 在命令行中操作 1hadoop jar HBaseTest-1.0-SNAPSHOT.jar com.xiong.mr.FruitDriver /fruit.tsv fruit1 在HBase Cli上查看fruit1表的数据 1scan 'fruit1' 示例二：读取fruit表并过滤数据，将结果输出到fruit2表(远端连接运行) Fruit2Mapper类，用于读取和过滤HBase fruit表中的数据 1234567891011121314151617public class Fruit2Mapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; // 构建Put对象 Put put = new Put(key.get()); // 1、获取数据 for (Cell cell : value.rawCells()) &#123; // 2、判断当前的cell是否为name列 if (\"name\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) &#123; // 3、给Put对象赋值 put.add(cell); &#125; &#125; // 4、写出 context.write(key, put); &#125;&#125; Fruit2Reducer类，用于将处理后的数据输出到fruit2表中 123456789public class Fruit2Reducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; // 遍历写出 for (Put value : values) &#123; context.write(NullWritable.get(), value); &#125; &#125;&#125; Fruit2Driver类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Fruit2Driver implements Tool &#123; // 定义配置信息 private Configuration configuration = null; public int run(String[] args) throws Exception &#123; // 1、获取Job对象 Job job = Job.getInstance(configuration); // 2、设置主类路径 job.setJarByClass(Fruit2Driver.class); // 3、设置Mapper及输出KV类型 TableMapReduceUtil.initTableMapperJob( \"fruit\", new Scan(), Fruit2Mapper.class, ImmutableBytesWritable.class, Put.class, job ); // 4、设置Reducer及输出表 TableMapReduceUtil.initTableReducerJob( \"fruit2\", Fruit2Reducer.class, job ); // 5、提交任务 boolean result = job.waitForCompletion(true); return result ? 0 : 1; &#125; public void setConf(Configuration configuration) &#123; this.configuration = configuration; &#125; public Configuration getConf() &#123; return configuration; &#125; public static void main(String[] args) &#123; try &#123; Configuration configuration = HBaseConfiguration.create(); int run = ToolRunner.run(configuration, new Fruit2Driver(), args); System.exit(run); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 将集群上HBase的hbase-site.xml内容复制到当前工程下的resource/hbase-site.xml上，文件名不得修改 12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase默认存储文件的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;!-- zk的集群地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt; &lt;/property&gt; &lt;!-- zk的data目录 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.6.1/zkData&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;./tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 运行查看结果(在HBase Cli中scan fruit2表) 与Hive的集成 HBase和Hive的对比 Hive 数据仓库：Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询 用于数据分析、清洗：Hive适用于离线的数据分析和清洗，延迟较高 基于HDFS、MapReduce：Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行 HBase 数据库：是一种面向列簇存储的非关系型数据库 用于存储结构化和非结构化的数据：适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作 基于HDFS：数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理 延迟较低，接入在线业务使用：面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度 HBase和Hive集成使用(可能会有版本兼容问题,生产环境会采用CDH方式或者运维人员帮忙处理) 环境准备后续可能会在操作Hive的同时会对HBase产生影响，所以Hive需要持有操作HBase的Jar，那么需要拷贝Hive所依赖的Jar(或者使用软连接的形式) 123456789101112131415161718# 可以临时设置,也可在/etc/profile中永久设置export HBASE_HOME=/opt/module/hbase-2.2.5export HIVE_HOME=/opt/module/hive-3.1.2# 设置软链接,'\\'为shell命令的分隔符,多行时最好采用ln -s $HBASE_HOME/lib/hbase-common-2.2.5.jar \\$HIVE_HOME/lib/hbase-common-2.2.5.jarln -s $HBASE_HOME/lib/hbase-server-2.2.5.jar \\$HIVE_HOME/lib/hbase-server-2.2.5.jarln -s $HBASE_HOME/lib/hbase-client-2.2.5.jar \\$HIVE_HOME/lib/hbase-client-2.2.5.jarln -s $HBASE_HOME/lib/hbase-protocol-2.2.5.jar \\$HIVE_HOME/lib/hbase-protocol-2.2.5.jarln -s $HBASE_HOME/lib/hbase-it-2.2.5.jar \\$HIVE_HOME/lib/hbase-it-2.2.5.jarln -s $HBASE_HOME/lib/hbase-hadoop2-compat-2.2.5.jar \\$HIVE_HOME/lib/hbase-hadoop2-compat-2.2.5.jarln -s $HBASE_HOME/lib/hbase-hadoop-compat-2.2.5.jar \\$HIVE_HOME/lib/hbase-hadoop-compat-2.2.5.jar 同时需要在hive-site.xml中修改zookeeper的属性(连接hbase需要与zk交互) 123456789101112&lt;!-- 设置zk节点 --&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;&lt;!-- 设置zk client通信端口 --&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt; 实操 案例一：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表 在Hive中创建表同时关联HBase(完成后可在Hive和HBase的Cli中查看) 123456789101112CREATE TABLE hive_hbase_emp_table(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\")TBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\"); 在Hive中创建中间临时表，用于装载文件中的数据(因为hbase的文件格式不是txt,所有不能直接由txt导入,需要中间表) 12345678910CREATE TABLE emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)row format delimited fields terminated by '\\t'; 向Hive中间表中装载数据 1load data local inpath '/opt/module/data/hive/emp.txt' into table emp; 通过insert命令将中间表的数据导入Hive与HBase关联的那张表中：insert into table hive_hbase_emp_table select * from emp; 查看Hive和HBase各自的表中是否已同步地插入了数据 1234# Hiveselect * from hive_hbase_emp_table;# HBasescan 'hbase_emp_table' 案例二：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据 在Hive中创建外部表 123456789101112CREATE EXTERNAL TABLE relevance_hbase_emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\")TBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\"); 查看关联后Hive中外部表：select * from relevance_hbase_emp; 之后便可使用Hive进行一些数据分析 HBase优化 高可用在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果 HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置 关闭启动的HBase集群：bin/stop-hbase.sh 在conf目录下创建backup-masters文件(文件名不得更改)：touch conf/backup-masters 在backup-masters中加入备用HMaster节点(当前HMaster配置是hadoop1,文件不得多出空格和换行)： 12hadoop2hadoop3 分发conf目录到其他hbase集群节点：xsync backup-masters 在hadoop1上启动HBase集群：bin/start-hbase.sh 打开页面查看：http://hadoop1:16010 使用jps查看hadoop1的HMaster进程，并使用kill -9 进程号杀死(杀两次,第二次杀失败,假活) 打开页面查看：http://hadoop2:16010或http://hadoop3:16010 预分区每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致规划好，以提高HBase性能 手动设定预分区 1234# 具体分区信息可以在web页面的table中查看# 四个键分五个区# 逐个字符比较create 'staff1','info','partition1',SPLITS =&gt; ['1000','2000','3000','4000'] 生成16禁止序列预分区(基本不用) 1create 'staff2','info','partition2',&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'&#125; 按照文件中设置的规则预分区创建splits.txt文件内容如下： 1234aabbddcc 采用文件设置预分区形式 12# 默认会对分区文件做排序,不然左开右闭会出问题create 'staff3','partition3',SPLITS_FILE =&gt; '/opt/module/data/hbase/splits.txt' 使用JavaAPI创建预分区 1234// 自定义算法，产生一系列hash散列值存储在二维数组中byte[][] splitKeys = ...// 还有四个参数的方法...admin.createTable(tableDescriptor,splitKeys); RowKey设计一条数据的唯一标识就是RowKey，那么这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内，设计RowKey的主要目的，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈RowKey常用的设计方案 生成随机数、hash、散列值：比如SHA1 字符反转 字符串拼接 示例讲解电信要求统计用户的时间段内的通话流水，根据什么分区，假如要分近300个区？电话号码11位，为了将对应到300个分区，而且要求一个号码需要对应到一个分区。那么如果有些号码电话打的很多，那么一个分区可能还是有问题，所以根据号码和时间进行分区。如果分到1年，时间颗粒度太大；最好采用1月。最终采用取余方式：hash(15988814888(电话号码) + 2020(年) + 07(月)) % 300——此处的’+’只做一个示意作用，具体需要实践和经验。那么假如我需要获取15988814888用户在2020年6月份的通话流水，怎么根据startRowKey和endRowKey获取数据呢？startRowKey(xxx为分区号,是根据公式计算得出的;最后是rowKey比较规则:有比没有大)：xxx_15988814888_2020_06endRowKey(‘|’的ascii码很大,或者也可以为2020_03;这里不会出现第一位大于遮蔽第二位的问题)：xxx_15988814888_2020_06| 内存优化HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死 基础优化 允许的HDFS的文件中追加内容(hdfs-site.xml、hbase-site.xml) 12属性：dfs.support.append解释：开启HDFS追加同步，可以优秀地配合HBase的数据同步和持久化。默认值为true 优化 DataNode 允许的最大文件打开数(hdfs-site.xml) 12属性：dfs.datanode.max.transfer.threads解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096 优化延迟高的数据操作的等待时间(hdfs-site.xml) 12属性：dfs.image.transfer.timeout解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值(默认60000毫秒)，以确保socket不会被timeout掉 优化数据的写入效率(mapred-site.xml) 123属性：mapreduce.map.output.compress mapreduce.map.output.compress.codec解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式 设置RPC监听数量(hbase-site.xml) 12属性：hbase.regionserver.handler.count解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值 优化HStore文件大小(hbase-site.xml) 12属性：hbase.hregion.max.filesize解释：默认值10737418240(10GB)，如果需要运行HBase的MR任务，可以减小此值，因为一个 region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile 优化HBase客户端缓存(hbase-site.xml) 12属性：hbase.client.write.buffer解释：用于指定Hbase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的 指定scan.next扫描HBase所获取的行数(hbase-site.xml) 12属性：hbase.client.scanner.caching解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大 flush、compact、split机制当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二涉及属性：hbase.hregion.memstore.flush.size = 134217728(即128M就是Memstore的默认阈值)这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM 12hbase.regionserver.global.memstore.upperLimit &#x3D; 0.4hbase.regionserver.global.memstore.lowerLimit &#x3D; 0.38 当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Hive","slug":"BigData/Hive","date":"2020-07-06T12:57:06.000Z","updated":"2020-09-20T14:30:20.037Z","comments":true,"path":"2020/07/06/BigData/Hive/","link":"","permalink":"https://sobxiong.github.io/2020/07/06/BigData/Hive/","excerpt":"内容 Hive基本概念 Hive安装 Hive数据类型 DDL数据定义 DML数据操作 查询 例题实战(蚂蚁金服) 函数 压缩和存储 企业级调优 谷粒影音Hive实战 常见错误及解决方案","text":"内容 Hive基本概念 Hive安装 Hive数据类型 DDL数据定义 DML数据操作 查询 例题实战(蚂蚁金服) 函数 压缩和存储 企业级调优 谷粒影音Hive实战 常见错误及解决方案 Hive基本概念 什么是Hive 由Facebook开源用于解决海量结构化日志的数据统计 基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能 Hive处理的数据存储在HDFS Hive分析数据底层的实现是MapReduce 执行程序运行在Yarn上 本质是将HQL(Hive Query Language)转化成MapReduce程序 Hive的优缺点 优点： 操作接口采用类SQL语法，提供快速开发的能力(简单、容易上手) 避免了去写MapReduce，减少开发人员的学习成本 Hive的执行延迟比较高，因此Hive常用于数据分析和对实时性要求不高的场合 Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数 缺点： Hive的HQL表达能力有限： 迭代式算法无法表达 数据挖掘方面不擅长 Hive的效率比较低： Hive自动生成的MapReduce作业，通常情况下不够智能化 Hive调优比较困难，粒度较粗 Hive架构原理 用户接口：ClientCLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive) 元数据：Metastore元数据包括：表名、表所属的数据库(默认是default)、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在目录等； 默认存储在自带的derby数据库中(存在bug)，推荐使用MySQL存储Metastore Hadoop：使用HDFS进行存储，使用MapReduce进行计算 驱动器：Driver 解析器(SQL Parser)：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误 编译器(Physical Plan)：将AST编译生成逻辑执行计划 优化器(Query Optimizer)：对逻辑执行计划进行优化 执行器(Execution)：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/SparkHive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将 执行返回的结果输出到用户交互接口 Hive和数据库比较由于Hive采用了类似SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。下面将从多个方面来阐述Hive和数据库的差异。数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性 查询语言由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言 HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发 数据存储位置Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中 数据更新由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的 索引Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于MapReduce的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive不适合在线数据查询 执行Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的。而数据库通常有自己的执行引擎 执行延迟Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小。当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势 可扩展性由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的(世界上最大的Hadoop集群在Yahoo，2009年的规模在4000台节点左右)。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右 数据规模由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小 Hive安装 安装地址 Hive官网地址：http://hive.apache.org 文档查看地址：https://cwiki.apache.org/confluence/display/Hive/GettingStarted 下载地址：http://archive.apache.org/dist/hive github地址：https://github.com/apache/hive Hive安装部署 Hive安装及配置 上传：把apache-hive-3.1.2-bin.tar.gz上传到/opt/software目录下 解压：tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/ 修改目录名：mv apache-hive-3.1.2-bin hive-3.1.2 修改配置文件(conf目录下)： 备份一份配置文件：cp hive-env.sh.template hive-env.sh.template.copy 修改配置文件后缀：mv hive-env.sh.template hive-env.sh 配置hive-env.sh文件(底部加入)： 12export HADOOP_HOME=/opt/module/hadoop-3.1.3export HIVE_CONF_DIR=/opt/module/hive-3.1.2/conf Hadoop集群启动(hadoop1启动hdfs——sbin/start-dfs.sh,hadoop2启动yarn——sbin/start-yarn.sh) Hive基本操作 初始化默认的derby数据库：bin/schematool -dbType derby -initSchema(初始化后在hive根目录会产生derby.log和metastore目录) 启动hive：bin/hive 启动时会发生Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V错误 这是因为hive内依赖的guava和hadoop内的版本不一致 分别查看hive(lib目录下)和hadoop(share/hadoop/common/lib目录下)的guava依赖版本：guava-19.0.jar和guava-27.0-jre.jar 删除hive的低版本guava-19.0.jar，将hadoop的高版本guava-27.0-jre.jar复制到hive的lib目录下 重启启动hive 查看数据库：show databases; 打开默认数据库：use default; 显示default数据库中的表：show tables; 创建一张表(数据类型为java中类型)：create table student(id int,name string); 查看表的结构：desc student; 向表中插入数据：insert into student values(1,”SOBXiong”); 查询表中数据：select * from student; 退出hive：quit; 本地文件导入Hive需求：将本地/opt/module/data/hive/student.txt的数据导入到hive的student表中 数据准备(tab键隔开) 1231 xixi2 haha3 hehe Hive操作 导入student.txt的数据到之前创建的student表中：load data local inpath ‘/opt/module/data/hive/student.txt’ into table student; 查询结果(发现都是NULL NULL,因为格式不对)：select * from student; 删除已创建的student表：drop table student; 创建新的student表(声明文件分隔符’\\t’)：create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’; 重新导入数据并重新查询结果 查看http://hadoop1:9870中的HDFS文件，发现/user/hive/warehouse/student下就有数据 第二种插入数据的方式：直接将文件上传至HDFS服务器 上传本地文件(相当于cp复制)：hadoop fs -put stu1.txt /user/hive/warehouse/student 上传HDFS文件(相当于mv移动)：hadoop fs -put /stu2.txt /user/hive/warehouse/student derby存储元数据的问题(推荐使用mysql)： 只能开启一个hive客户端 在不同的目录开启hive客户端会在当前目录下创建derby.log和metastore文件，相当于数据不共享 MySql安装 安装包准备： 查看yum中历史的mysql或者mariadb的依赖：rpm -qa | grep mysql/mariadb 如有历史依赖，删除：yum remove mysql-libs/mariadb-libs 下载mysql的rpm包：前往https://dev.mysql.com/downloads/mysql/下载5.7.30的Red Hat Enterprise Linux7版本(CentOS7)的RPM Bundle包 安装MySql 解压tar包：tar -xvf mysql-5.7.30-1.el7.x86_64.rpm-bundle.tar 使用rpm命令安装MySql组件 12345# 依赖关系为common→libs→client→serverrpm -ivh commonrpm -ivh libsrpm -ivh clientrpm -ivh server 启动MySql：systemctl start mysqld.service 查看MySql状态：systemctl status mysqld.service 查看初始化的随机密码：grep ‘temporary password’ /var/log/mysqld.log 登录MySql：mysql -u root -p 修改密码校验策略(不然设置新密码会提示密码错误)：set global validate_password_policy=0; 修改密码：alter user root@localhost identified by ‘your password’; 授权root用户远程访问权限 12grant all privileges on *.* to 'root' @'%' identified by 'your password';flush privileges; 设置MySql完毕，退出：quit; Hive元数据配置到MySql 拷贝mysql-connector JDBC驱动文件 前往https://dev.mysql.com/downloads/connector/j/下载驱动文件5.1.49版本 解压文件mysql-connector-java-5.1.49.tar.gz，拷贝mysql-connector-java-5.1.49-bin.jar到hive的lib目录下 配置metastore到MySql 在conf目录下创建hive-site.xml配置文件：touch hive-site.xml 修改配置文件： 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- xml下的&amp;需要转义为&amp;amp; --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop1:3306/metastore?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;your password&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 初始化Hive的MySql元数据数据库：bin/schematool -dbType mysql -initSchema 启动hive，MySql中新增了metastore数据库(表DBS和TBS比较重要) HiveJDBC访问 停止hadoop： 1234# hadoop1stop-dfs.sh# hadoop2stop-yarn.sh 修改hadoop配置： 12345678910111213141516171819202122&lt;!-- hdfs-site.xml 启用webhdfs --&gt;&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- core-site.xml 设置hadoop的代理用户 hadoop.proxyuser.xxx.hosts xxx是操作的用户 org.apache.hadoop.security.authorize.AuthorizationException: User: sobxiong is not allowed to impersonate root(state=08S01,code=0) User:xxx即为下面该填入的用户--&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.sobxiong.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.sobxiong.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 修改hive配置： 1234567891011&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;description&gt;Bind host on which to run the HiveServer2 Thrift service.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;11000&lt;/value&gt; &lt;description&gt;Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.&lt;/description&gt;&lt;/property&gt; 启动hiveserver2服务：bin/hiveserver2 启动beeline：bin/beeline 连接hiveserver2： 1234beeline&gt; !connect jdbc:hive2://hadoop1:11000Enter username for jdbc:hive2://hadoop102:10000: sobxiongEnter password for jdbc:hive2://hadoop102:10000: your password(数据库的密码)# 接下来的操作就跟Hive Cli使用类似SQL Hive常用交互命令 -e &lt;quoted-query-string&gt;：不进入hive的交互窗口执行sql语句，例如：bin/hive -e “select * from student;” -f &lt;filename&gt;：执行脚本中sql语句 结果打印在terminal上：bin/hive -f /opt/module/data/hive/hive.hql 结果打印在指定文件中：bin/hive -f /opt/module/data/hive/hive.hql &gt; ./hive_result.txt Hive其他命令操作 在Hive Cli命令窗口中查看hdfs文件系统：dfs -ls / 在Hive Cli命令窗口中查看本地文件系统：! ls / 查看在hive中输入的所有历史命令：cat ~/.hivehistory Hive常见属性配置 Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse 在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹 修改default数据仓库原始位置(hive-default.xml.template -&gt; hive-site.xml) 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 查询后信息显示配置 在hive-site.xml加入如下配置： 12345678910&lt;!-- 表列名显示 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 当前使用数据库显示 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 重启Hive Cli Hive运行日志信息配置 Hive的日志信息默认存放在/tmp/{current_user}目录下 修改Hive的日志信息存放在hive安装目录的logs文件夹下修改conf/hive-log4j.properties配置文件(hive-log4j.properties.template -&gt; hive-log4j.properties)：hive.log.dir=/opt/module/hive-3.1.2/logs 参数配置方式 查看当前所有的配置信息(Hive Cli命令窗口下)：set; 参数配置的三种方式 配置文件方式默认配置文件：hive-default.xml用户自定义配置文件：hive-site.xml注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效 命令行参数方式启动Hive时，可以在命令行添加-hiveconf param=value来设定参数例如：bin/hive -hiveconf mapred.reduce.tasks=10;(注意：仅对本次hive启动有效)查看参数设置：set mapred.reduce.tasks; 参数声明方式在HQL中使用SET关键字设定参数(注意：仅对本次hive启动有效)例如：set mapred.reduce.tasks=100;上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了 Hive数据类型 基本数据类型(Hive数据类型大小写不敏感) Hive数据类型 Java数据类型 长度 TINYINT byte 1byte有符号整数 SMALLINT short 2byte有符号整数 INT int 4byte有符号整数 BIGINT long 8byte有符号整数 BOOLEAN boolean 布尔类型，true或false FLOAT float 单精度浮点数 DOUBLE double 双精度浮点数 STRING string 字符系列，可以指定字符集，可以使用单引号或者双引号 TIMESTAMP - 时间类型 BINARY - 字节数组 Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过他不能声明其最多能存储多少个字符，理论上它可以存储2GB的字符数 集合数据类型 数据类型 描述 语法示例 STRUCT 和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用 struct() MAP MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取键last对应的值数据 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用 Array() Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套 集合数据类型案例实操 假设JSON为原始数据，具体如下： 123456789101112&#123; \"name\": \"songsong\", \"friends\": [\"bingbing\" , \"lili\"], \"children\": &#123; \"xiao song\": 18 , \"xiaoxiao song\": 19 &#125;, \"address\":&#123; \"street\": \"hui long guan\" , \"city\": \"beijing\" &#125;&#125; 基于上述数据结构，建立本地测试文件test.txt，具体格式如下： 12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 注意：MAP、STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用’_’ Hive上创建测试表test 1234567891011121314create table test(name string,friends array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)/* 设置列分隔符为',' */row format delimited fields terminated by ','/* 设置map、struct和array的分隔符(数据分割符号)为'_' */collection items terminated by '_'/* 设置map中的key/value的分隔符为',' */map keys terminated by ':'/* 设置行分隔符为'\\n'(也是默认值) */lines terminated by '\\n'; 导入文本数据到测试表中 1load data local inpath '/opt/module/data/hive/test.txt' into table test; 访问三种集合列里的数据 1select friends[1],children['xiao song'],address.city from test; 类型转换Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化。例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作 隐式类型转换规则 任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT 所有整数类型、FLOAT和STRING(符合数字)类型都可以隐式地转换成DOUBLE TINYINT、SMALLINT、INT都可以转换为FLOAT BOOLEAN类型不可以转换为任何其它的类型 使用CAST操作显示进行数据类型转换例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值NULL DDL数据定义 创建数据库 创建一个数据库，默认在HDFS上的存储路径式/user/hive/warehouse/*.db：create database if not exists db_hive;(if not exists避免要创建的数据库已存在) 创建一个数据库，指定在HDFS上存放的路径 1create database db_hive2 location '/db_hive2.db' 查询数据库 显示数据库 显示数据库：show databases; 过滤查询显示的数据库 1show databases like 'db_hive'; 查看数据库 显示数据库信息：desc database db_hive; 显示数据库详细信息(extended)：desc database extended db_hive; 切换当前数据库：use db_hive; 修改数据库用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置修改数据库属性值： 1alter database db_hive set dbproperties('createtime'='20200708') 查看修改结果：desc database extended db_hive; 删除数据库 删除空数据库：drop database if exists db_hive;(if exists避免要删除的数据库不存在) 如果数据库中表不为空，可以采用cascade命令集联强制删除：drop database db_hive cascade; 创建表 建表语法 123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)][CLUSTERED BY (col_name, col_name, ...)[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS][ROW FORMAT row_format][STORED AS file_format][LOCATION hdfs_path] 字段解释说明 CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径(LOCATION)，Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据 COMMENT：为表和列添加注释 PARTITIONED BY创建分区表 CLUSTERED BY创建分桶表 SORTED BY不常用 ROW FORMATDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIE (property_name=property_value, property_name=property_value, …)]用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化 STORED AS指定存储文件类型常用的存储文件类型：SEQUENCEFILE(二进制序列文件)、TEXTFILE(文本)、RCFILE(列式存储格式文件)如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE LOCATION：指定表在HDFS上的存储位置 LIKE允许用户复制现有的表结构，但是不复制数据 管理表 介绍默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会(或多或少地)控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据 实际操作 创建普通表 123456create table if not exists student(id int, name string)row format delimited fields terminated by '\\t'stored as textfilelocation '/user/hive/warehouse/student2'; 根据查询结果创建表(查询的结果会添加到新创建的表中)：create table if not exists student2 as select id, name from student; 根据已存在的表结构创建表：create table if not exists student3 like student; 查询表的类型：desc formatted student; 外部表 介绍：因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉 实际操作(创建表,其余操作与管理表类似) 123456create external table if not exists default.dept(deptno int,dname string,loc int)row format delimited fields terminated by '\\t'; 管理表与外部表 相互转换 12345-- 修改内部表为外部表：alter table student2 set tblproperties('EXTERNAL'='TRUE');-- 修改外部表为内部表：alter table student2 set tblproperties('EXTERNAL'='FALSE');-- 注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写 使用场景每天将收集到的网站日志定期流入HDFS文本文件。在外部表(原始日志表)的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表 分区表分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多 分区表基本操作 创建分区表语法 12345create table dept_partition(deptno int, dname string, loc string)partitioned by (month string)row format delimited fields terminated by '\\t'; 加载数据到分区表中 123hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201709');hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201708');hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201707’); 查询分区表中数据 单分区查询 1select * from dept_partition where month='201709'; 多分区联合查询 12345678-- 查询多个分区select * from dept_partition where month='201709'unionselect * from dept_partition where month='201708'unionselect * from dept_partition where month='201707';-- 查询全部select * from dept_partition; 增加分区 创建单个分区 1alter table dept_partition add partition(month='201706'); 同时创建多个分区 1alter table dept_partition add partition(month='201705') partition(month='201704'); 删除分区： 删除单个分区 1alter table dept_partition drop partition (month='201704'); 同时删除多个分区 1alter table dept_partition drop partition (month='201705'), partition (month='201706'); 查看分区表有多少分区：show partitions dept_partition; 查看分区表结构：desc formatted dept_partition; 分区表扩展用法 创建二级分区表 12345create table dept_partition2(deptno int, dname string, loc string)partitioned by (month string, day string)row format delimited fields terminated by '\\t'; 加载二级分区数据 1load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709', day='13'); 查询二级分区数据 1select * from dept_partition2 where month='201709' and day='13'; 将数据上传到分区目录后，让分区表和数据产生关联的方式 上传数据后修复(适用于数据较多的情况) 上传数据： 123# Hive Cli命令环境下dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;dfs -put /opt/module/data/hive/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; 查询数据(查询不到) 1select * from dept_partition2 where month='201709' and day='12'; 执行修复命令：msck repair table dept_partition2; 上传数据后添加分区 上传数据(同上) 执行添加分区 1alter table dept_partition2 add partition(month='201709',day='11'); 创建文件夹后load数据到分区 创建目录：dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; 上传数据 1load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709',day='10'); 修改表 重命名表 语法：ALTER TABLE table_name RENAME TO new_table_name 实例：alter table dept_partition2 rename to dept_partition3; 添加、修改和删除表分区(同上) 增加、修改、替换列信息 语法： 更新列：ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 添加和替换列：ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …) 注意：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段 实操 查询表结构(用于查看修改结果)：desc dept_partition; 添加列：alter table dept_partition add columns(deptdesc string); 更新列：alter table dept_partition change column deptdesc desc int;(貌似需要符合隐式转换规则) 替换列：alter table dept_partition replace columns(deptno string, dname string, loc string); 删除表：drop table dept_partition; DML数据操作 数据导入 向表中装载数据(Load) 语法 1load data [local] inpath 'path_name' [overwrite] into table table_name [partition (partcol1=val1,...)]; 参数解释 load data：表示加载数据 local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表 inpath：表示加载数据的路径 overwrite：表示覆盖表中已有数据，否则表示追加 into table：表示加载到哪张表 table_name：表示具体的表 partition：表示上传到指定分区 实际操作 加载本地文件到Hive： 1load data local inpath '/opt/module/data/hive/student.txt' into table default.student; 加载(覆盖)HDFS文件数据到Hive中 123# Hive Cli命令环境下dfs -put /opt/module/data/hive/student.txt /user/sobxiong/hive;load data inpath '/user/sobxiong/hive/student.txt' (overwrite)into table default.student; 通过查询语句向表中插入数据(Insert) 基本插入数据 1insert into table student partition(month='201709') values(1,'wangwu'); 根据单表查询结果插入数据 12insert overwrite table student partition(month='201708')select id, name from student where month='201709'; 根据多表查询结果插入数据 12345from studentinsert overwrite table student partition(month='201707')select id, name where month='201709'insert overwrite table student partition(month='201706')select id, name where month='201709'; 查询语句中创建表并加载数据(As Select,查询的结果会添加到新创建的表中)：create table if not exists student3 as select id, name from student; 创建表时通过Location指定加载数据路径 指定在HDFS上的位置创建表 12345create table if not exists student5(id int, name string)row format delimited fields terminated by '\\t'location '/user/hive/warehouse/student5'; 上传数据到HDFS上 查询数据 Import数据到指定Hive表中(注意：先用export导出后,才能导入) 1import table student2 partition(month='201709') from '/user/hive/warehouse/export/student'; 数据导出 Insert导出 将查询的结果导出到本地 1insert overwrite local directory '/opt/module/data/hive/export/student' select * from student; 将查询的结果格式化导出到本地 1insert overwrite local directory '/opt/module/data/hive/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' select * from student; 将查询结果导出到HDFS上(取消local) Hadoop命令导出到本地(Hive Cli命令环境下)：dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/data/hive/export/student3.txt; Hive Shell命令导出：基本语法(hive -f/-e 执行语句或脚本 &gt; file_name) 1bin/hive -e 'select * from default.student;' &gt; /opt/module/data/hive/export/student4.txt; Export导出到HDFS上(导出数据包括表数据和元数据) 1export table default.student to '/user/hive/warehouse/export/student'; Sqoop导出(敬请期待) 清除表中数据(注意：只能删除管理表,不能删除外部表中数据)：truncate table student; 查询 查询语句语法官方wiki文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select 12345678910[WITH CommonTableExpression (, CommonTableExpression)*]SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [ORDER BY col_list] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] ] [LIMIT number] 基本查询 全表和特定列查询注意： SQL大小写不敏感 SQL可以写在一行/多行 关键字不能被缩写也不能分行 各子句一般要分行写 使用缩进提高语句的可读性 列别名 重命名一个列 便于计算 紧跟列名(或在列名和别名之间加入关键字AS) 算术运算符 +、-、*、/、%、&amp;、|、^、- 常用函数 求总行数：count() 求最大值/最小值：max()/min() 求总和：sum() 求平均值：avg() limit语句：典型的查询会返回多行数据。LIMIT子句用于限制返回的行数 Where语句：使用Where子句可以过滤掉不满足条件的行，需要紧跟From子句 比较运算符(同样可以用于Join… on和Having语句)以下只介绍除=、&gt;和&lt;等简单的运算符 操作符 支持的数据类型 描述 A&lt;=&gt;B 基本数据类型 如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL A&lt;&gt;B, A!=B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE A [NOT] BETWEEN B AND C 基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果 A IS [NOT] NULL 所有数据类型 如果A(不)等于NULL，则返回TRUE(FALSE)，反之返回FALSE(TRUE) [NOT] IN(数值1, 数值2) 所有数据类型 (不)使用IN运算显示列表中的值 A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果 A RLIKE B, A REGEXP B STRING 类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配 Like和RLike 使用LIKE运算选择类似的值 选择条件可以包含字符或数字： % 代表零个或多个字符(任意个字符) _ 代表一个字符 RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件 案例 查找以2开头薪水的员工信息 1select * from emp where sal LIKE '2%'; 查找第二个数值为2的薪水的员工信息 1select * from emp where sal LIKE '_2%'; 查找薪水中含有2的员工信息 1select * from emp where String(sal) RLIKE '[2]'; 逻辑运算符(And/Or/Not) 分组 Group By语句GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作 Having语句与where不同点： where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据 where后面不能写分组函数，而having后面可以使用分组函数 having只用于group by分组统计语句 Join语句 等值Join 12select e.empno, e.ename, d.deptno, d.dname from emp e join dept don e.deptno = d.deptno; 表的别名好处：(1)简化查询；(2)有限地提高执行效率 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来 12select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno= d.deptno; 左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回 12select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno= d.deptno; 右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回 12select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno= d.deptno; 满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代 12select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno= d.deptno; 多表连接：连接n个表，一般至少需要n-1个连接条件 123456SELECT e.ename, d.deptno, l. loc_nameFROM emp eJOIN dept dON d.deptno = e.deptnoJOIN location lON d.loc = l.loc; 多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的 笛卡尔积一般会在下面情况下出现： 省略连接条件 连接条件无效 所有表中的所有行互相连接一般会设置禁止出现笛卡尔积，如果有特殊情况，需要在单独在命令执行前设置一次性环境 连接谓词在新版中支持or 123-- 无实际意义(ename = dname),只做可行性试验select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno= d.deptno or e.ename=d.dname; 排序 全局排序(Order By:一个Reducer) 排序方式：ASC(ascend升序,默认)、DESC(descend降序) Order By子句在Select语句的结尾 多个列排序(同MySQL) 内部排序(Sort By:每个Reduce内部进行排序,对全局结果集来说不是排序) 注意：如果使用sort by不使用distribute by(即没有指定分区字段)，那么就采用一种产生随机数的函数分配分区(避免数据倾斜) 设置reduce数：set mapreduce.job.reduce=3; 按照部门编号降序排序： 123-- 三个结果文件insert overwrite local directory '/opt/module/data/hive/sortby-result'select * from emp sort by deptno desc; 分区排序(Distribute By:类似MR中partition,进行分区,结合sort by使用) DISTRIBUTE BY语句要写在SORT BY语句之前。distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果 设置reduce数：set mapreduce.job.reduces=3; 先按照部门编号分区，再按照员工编号降序排序 1insert overwrite local directory '/opt/module/data/hive/distribute-result' select * from emp distribute by deptno sort by empno desc; Clubster By 当distribute by和sorts by字段相同时，可以使用cluster by方式。 cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC 具体实操 123-- 以下两种写法等价select * from emp cluster by deptno;select * from emp distribute by deptno sort by deptno; 注意：按照部门编号分区，不一定就是固定死的数值(要看具体数据表中的字段不同的数目以及reduce设置的数目)，可以是20号和30号部门分到一个分区里面去 分桶及抽样查询 分桶表数据存储 介绍：分区针对的是数据的存储路径；分桶针对的是数据文件。分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。分桶是将数据集分解成更容易管理的若干部分的另一个技术 通过导入数据文件方式创建分桶表 创建表 1234create table stu_buck(id int, name string)clustered by(id)into 4 bucketsrow format delimited fields terminated by '\\t'; 查看表结构：desc formatted stu_back;(Num Buckets: 4) 导入数据到分桶表 1load data local inpath '/opt/module/data/hive/student.txt' into table stu_buck; 在浏览器上查看创建的分桶表是否分成4个桶(4个文件)：在新版本中分成四个桶 通过子查询导入数据方式创建分桶表 先创建普通的stu表 1create table stu(id int, name string) row format delimited fields terminated by '\\t'; 向普通stu表中导入数据 1load data local inpath '/opt/module/data/hive/student.txt' into table stu; 清空stu_buck表中数据：truncate table stu_buck; 子查询方式导入数据到分桶表：insert into table stu_buck select id, name from stu; 浏览器查看：新版本中有4个分桶(文件) 需要设置Hive的属性(老版本) 1234567# 设置启用分桶set hive.enforce.bucketing=true;# 设置reduce数目为-1,会自动使用分桶数作为reduce的数目set mapreduce.job.reduces=-1;# 清空分桶表,重新导入数据,再去查看浏览器中的分桶truncate table stu_back;insert into table stu_buck select id, name from stu; 分桶抽样查询 介绍：对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求 实操：select * from stu_buck tablesample(bucket 1 out of 4 on id); 语法：tablesample是抽样语句，语法：tablesample(bucket x out of y) 参数解释 y：y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2)2个bucket的数据，当y=8时，抽取(4/8)1/2个bucket的数据 x：x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取(4/2)2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据 注意：x的值必须小于等于y的值 其他常用查询函数 空字段赋值 函数说明：NVL：给值为NULL的数据赋值，它的格式是NVL(string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL，则返回NULL(replace_with可以是常量也可以是同表的另一个列) 查询(常量)：select nvl(comm,-1) from emp; 查询(另一列)：select nvl(comm,mgr) from emp; 时间类 date_format(格式化时间,第一个变量时间串只能是以’-‘分割) 123select date_format('2020-07-11','yyyy:MM:dd');-- 时间不以'-'分割,可以通过regex正则表达式替换/自定义函数select regexp_replace('2020/07/11','/','-'); date_add/sub(时间跟天数相加/相减) 1select date_add('2020-07-11',-5/5); datediff(时间相差的间隔,前者-后者) 1select datediff('2020-07-11','2020-07-08'); CASE WHEN 数据准备 123456悟空 A 男大海 A 男宋宋 B 男凤姐 A 女婷姐 B 女婷婷 B 女 需求：求出不同部门的男女各多少人 创建emp_set.txt，复制数据 创建Hive表并导入数据 12345678-- 建表create table emp_sex(name string,dept_id string,sex string)row format delimited fields terminated by \"\\t\";-- 导入本地数据load data local inpath '/opt/module/data/hive/emp_sex.txt' into table emp_sex; 查询数据 12345678select dept_id, sum(case sex when '男' then 1 else 0 end) male_count, sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id; 行转列 相关函数说明CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段 数据准备 12345孙悟空 白羊座 A大海 射手座 A宋宋 白羊座 B猪八戒 白羊座 A凤姐 射手座 A 需求：把星座和血型一样的人归类到一起。结果如下 person_info.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table person_info(name string,constellation string,blood_type string)row format delimited fields terminated by \"\\t\";-- 导入数据load data local inpath '/opt/module/data/hive/person_info.txt' into table person_info; 查询数据 123456789101112131415161718192021222324252627-- 总查询语句select t1.constellation_blood_type, concat_ws('|', collect_set(t1.name)) namefrom (select name, concat(constellation, \",\", blood_type) constellation_blood_type from person_info ) t1group by t1.constellation_blood_type;-- 第一步,查询出'射手座,A' '大海'select concat(constellation, \",\", blood_type) constellation_blood_type, namefrom person_info;-- 第二步,连接相同星座和血型的nameselect constellation_blood_type, concat_ws('|', collect_set(t1.name)) namefrom t1group by t1.constellation_blood_type;-- 最后一步,替换from后的t1为第一步中的临时表 列转行 函数说明EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行LateRal View： 用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias 解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合 数据准备 123《疑犯追踪》 悬疑,动作,科幻,剧情《Lie to me》 悬疑,警匪,动作,心理,剧情《战狼2》 战争,动作,灾难 需求：将电影分类中的数组数据展开，结果如下所示 123456789101112《疑犯追踪》 悬疑《疑犯追踪》 动作《疑犯追踪》 科幻《疑犯追踪》 剧情《Lie to me》 悬疑《Lie to me》 警匪《Lie to me》 动作《Lie to me》 心理《Lie to me》 剧情《战狼2》 战争《战狼2》 动作《战狼2》 灾难 创建本地movie.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table movie_info(movie string,category array&lt;string&gt;)row format delimited fields terminated by \"\\t\"collection items terminated by \",\";-- 导入数据load data local inpath \"/opt/module/data/hive/movie.txt\" into table movie_info; 按需查询数据 12345select movie, category_namefrom movie_info lateral view explode(category) table_tmp as category_name; 窗口函数 函数说明OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化 参数说明 Over()内 CURRENT ROW：当前行 n PRECEDING：往前n行数据 n FOLLOWING：往后n行数据 UNBOUNDED：起点，UNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点 Over()外 LAG(col,n)：往前第n行数据 LEAD(col,n)：往后第n行数据 NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型 数据准备 123456789101112131415&#x2F;&#x2F; name,orderdate,costjack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94 需求 查询在2017年4月份购买过的顾客及总人数 查询顾客的购买明细及月购买总额 上述的场景，要将cost按照日期进行累加 查询顾客上次的购买时间 查询前20%时间的订单信息 创建本地business.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table business(name string,orderdate string,cost int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';-- 导入数据load data local inpath \"/opt/module/data/hive/business.txt\" into table business; 按需查询数据 查询在2017年4月份购买过的顾客及总人数 1234select name,count(*) over()from businesswhere substring(orderdate,1,7) = '2017-04'group by name; 查询顾客的购买明细及月购买总额 12select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) frombusiness; 上述的场景,要将cost按照日期进行累加 12345678910-- partition by ... order by和distribute by ... sort by效果相同,可替换select name,orderdate,cost, sum(cost) over() as sample1,--所有行相加 sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行from business; 查看顾客上次的购买时间 1select name,orderdate,cost,lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate) as last_time from business; 查询前20%时间的订单信息 12345select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) ntile_id from business) bwhere ntile_id = 1; Rank排名函数 函数说明： Rank()：排序相同时会重复，总数不会变 DENSE_RANK()：排序相同时会重复，总数会减少 ROW_NUMBER()：会根据顺序计算 数据准备 12345678910111213&#x2F;&#x2F; name subject score孙悟空 语文 87孙悟空 数学 95孙悟空 英语 68大海 语文 94大海 数学 56大海 英语 84宋宋 语文 64宋宋 数学 86宋宋 英语 84婷婷 语文 65婷婷 数学 85婷婷 英语 78 需求：计算各学科成绩排名 创建本地score.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table score(name string,subject string,score int)row format delimited fields terminated by \"\\t\";-- 导入数据load data local inpath '/opt/module/data/hive/score.txt' into table score; 按需查询 12345678select name, subject, score, rank() over(partition by subject order by score desc) rank, dense_rank() over(partition by subject order by score desc) dense_rank, row_number() over(partition by subject order by score desc) row_numberfrom score; 例题实战(蚂蚁金服) 背景说明用户每天的蚂蚁森林低碳生活领取的记录流水表(user_low_carbon表) 字段名 注释 user_id 用户编号 data_dt 日期 low_carbon 减少碳排放(g:克) 用于记录申领环保植物所需要减少的碳排放量的蚂蚁森林植物换购表(plant_carbon) 字段名 注释 plant_id 植物编号 plant_name 植物名 low_carbon 换购植物所需要的碳 题目 蚂蚁森林蚂蚁森林植物申领统计问题：假设2017年1月1日开始记录低碳数据(user_low_carbon)，假设2017年10月1日之前满足申领条件的用户都申领了一颗p004-胡杨，剩余的能量全部用来领取p002-沙柳统计在10月1日累计申领p002-沙柳排名前10的用户信息、以及他比后一名多领了几颗沙柳得到的统计结果如下表样式： user_id plant_count less_cout(比后一名多领的棵树) u_101 1000 100 u_088 900 400 u_103 500 … 蚂蚁森林低碳用户排名分析问题：查询user_low_carbon表中每日流水记录，条件为：用户在2017年，连续三天(或以上)的天数里，每天减少碳排放(low_carbon)都超过100g的用户低碳流水。需要查询返回满足以上条件的user_low_carbon表中的记录流水。例如用户u_002符合条件的记录如下，因为2017/1/2~2017/1/5连续四天的碳排放量之和都大于等于100g： seq(序号,不涉及当前列) user_id data_dt low_carbon xxxxx10 u_002 2017/1/2 150 xxxxx11 u_002 2017/1/2 70 xxxxx12 u_002 2017/1/3 30 xxxxx13 u_002 2017/1/3 80 xxxxx14 u_002 2017/1/4 150 xxxxx14 u_002 2017/1/5 101 备注：统计方法不限于sql、procedure、python,java等 解决 前期准备 创建表 12create table user_low_carbon(user_id String,data_dt String,low_carbon int) row format delimited fields terminated by '\\t';create table plant_carbon(plant_id string,plant_name String,low_carbon int) row format delimited fields terminated by '\\t'; 加载数据 12load data local inpath \"/opt/module/data/hive/user_low_carbon.txt\" into table user_low_carbon;load data local inpath \"/opt/module/data/hive/plant_carbon.txt\" into table plant_carbon; 设置本地模式(加快运行速度)：set hive.exec.mode.local.auto=true; 求解问题一 统计在10月1日前每个用户减少碳排放量的总和(取前11名：为了求与后一名的差值,并在第一阶段过滤数据集,加快运行速度) 1234567-- t1表select user_id,sum(low_carbon) sum_carbonfrom user_low_carbonwhere datediff(regexp_replace(data_dt,\"/\",\"-\"),\"2017-10-1\")&lt;0group by user_idorder by sum_carbon desclimit 11; 取出申领胡杨的碳量 12-- t2表select low_carbon from plant_carbon where plant_id=\"p004\"; 取出申领沙柳的碳量 12-- t3表select low_carbon from plant_carbon where plant_id=\"p002\"; 求出能申领沙柳的棵树 1234567891011121314151617181920212223242526-- t4表(floor下取整)select user_id, floor((t1.sum_carbon-t2.low_carbon) / t3.low_carbon) treeCountfrom t1,t2,t3;-- 替换t1,t2,t3select user_id, floor((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCountfrom ( select user_id,sum(low_carbon) sum_carbon from user_low_carbon where datediff(regexp_replace(data_dt,\"/\",\"-\"),\"2017-10-1\")&lt;0 group by user_id order by sum_carbon desc limit 11 )t1, ( select low_carbon from plant_carbon where plant_id=\"p004\" )t2, ( select low_carbon from plant_carbon where plant_id=\"p002\" )t3 求出前一名比后一名多几棵 12345678910111213141516171819202122232425262728293031323334select user_id, treeCount, treeCount - (lead(treeCount,1) over(order by treeCount desc))from t4limit 10;-- 替换t4表select user_id, treeCount, treeCount-(lead(treeCount,1) over(order by treeCount desc)) less_countfrom ( select user_id, floor((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCount from ( select user_id,sum(low_carbon) sum_carbon from user_low_carbon where datediff(regexp_replace(data_dt,\"/\",\"-\"),\"2017-10-1\")&lt;0 group by user_id order by sum_carbon desc limit 11 )t1, ( select low_carbon from plant_carbon where plant_id=\"p004\" )t2, ( select low_carbon from plant_carbon where plant_id=\"p002\" )t3 )t4limit 10; 求解问题二 方式一(Hive Sql简单版) 过滤出2017年且单日低碳量超过100g 123456789101112-- t1表select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dtfrom user_low_carbonwhere substring(data_dt,1,4) = '2017'group by user_id,data_dthaving sum(low_carbon) &gt;= 100; 将前两行数据以及后两行数据的日期放至当前行 123456789101112131415161718192021222324252627282930313233-- t2表select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2from t1;-- 替t1select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1; 计算当前日期跟前后两行时间的差值 12345678910111213141516171819202122232425262728293031323334353637383940414243-- t3表select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_difffrom t2;-- 替换t2select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_difffrom ( select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2 from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2; 过滤出连续3天超过100g的用户 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657-- t4表select user_id, data_dtfrom t3where (lag2_diff = 2 and lag1_diff = 1) or (lag1_diff = 1 and lead1_diff = -1) or (lead1_diff = -1 and lead2_diff = -2);-- 替换t3select user_id, data_dtfrom ( select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_diff from ( select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2 from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2 )t3where (lag2_diff = 2 and lag1_diff = 1) or (lag1_diff = 1 and lead1_diff = -1) or (lead1_diff = -1 and lead2_diff = -2); 关联原表，获取流水信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970select ulc.user_id, ulc.data_dt, ulc.low_carbonfrom t4join user_low_carbon ulcon t4.user_id = ulc.user_id and t4.data_dt = date_format(regexp_replace(ulc.data_dt,'/','-'),'yyyy-MM-dd');-- 替换t4select ulc.user_id, ulc.data_dt, ulc.low_carbonfrom ( select user_id, data_dt from ( select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_diff from ( select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2 from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2 )t3 where (lag2_diff = 2 and lag1_diff = 1) or (lag1_diff = 1 and lead1_diff = -1) or (lead1_diff = -1 and lead2_diff = -2) )t4join user_low_carbon ulcon t4.user_id = ulc.user_id and t4.data_dt = date_format(regexp_replace(ulc.data_dt,'/','-'),'yyyy-MM-dd'); 方式二(Hive Sql困难版,使用等差数列) 过滤出2017年且单日低碳量超过100g(同方式一第一步) 123456789101112-- t1表select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dtfrom user_low_carbonwhere substring(data_dt,1,4) = '2017'group by user_id,data_dthaving sum(low_carbon) &gt;= 100; 按照日期进行排序,并给每一条数据一个标记 123456789101112131415161718192021222324252627-- t2表select user_id, data_dt, rank() over(partition by user_id order by data_dt) rkfrom t1;-- 替换t1表select user_id, data_dt, rank() over(partition by user_id order by data_dt) rkfrom ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1; 将日期减去当前的rank值 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* 如果是连续的话,data_dt - rk结果一致 user_id , data_dt , rk ,data_sub_rk u_001 , 2017-01-02 , 1 , 2017-01-01 u_001 , 2017-01-06 , 2 , 2017-01-04 u_002 , 2017-01-02 , 1 , 2017-01-01 u_002 , 2017-01-03 , 2 , 2017-01-01 u_002 , 2017-01-04 , 3 , 2017-01-01 u_002 , 2017-01-05 , 4 , 2017-01-01*/-- t3表select user_id, data_dt, date_sub(data_dt,rk) data_sub_rkfrom t2;-- 替换t2select user_id, data_dt, date_sub(data_dt,rk) data_sub_rkfrom ( select user_id, data_dt, rank() over(partition by user_id order by data_dt) rk from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2; 过滤出连续3天超过100g的用户 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-- 当前适用于连续n天,只需要改3为n,具有通式性-- 当前只能过滤出用户,流水不行select user_idfrom t3group by user_id,data_sub_rkhaving count(*) &gt;= 3;-- 替换t3select user_idfrom ( select user_id, data_dt, date_sub(data_dt,rk) data_sub_rk from ( select user_id, data_dt, rank() over(partition by user_id order by data_dt) rk from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2 )t3group by user_id,data_sub_rkhaving count(*) &gt;= 3; 方式三(MapReduce) 123456789101112131415161718192021222324252627282930313233mapper(key:user_id + date,value:一行)grouping:user_idreduce()values:&#123; date = 1970-01-01 list = new ArrayList(); values.for( // 首次 if(date == 1970-01-01)&#123; list.add(value); date = value.date; &#125;else&#123; // 不是首次 if(value.date - date == 1)&#123; list.add(value); date = value.date; &#125;else&#123; if(list.size() &gt;= 3)&#123; context.write(list); &#125; list.clear(); list.add(value); date = value.date; &#125; &#125; ) // 防止漏了最后一行 if(list.size() &gt;= 3)&#123; context.write(list); &#125;&#125; 函数 系统内置函数 查看系统自带的函数：show functions; 显示自带的函数的用法：desc function split; 详细显示自带的函数的用法：desc function extened split; 自定义函数 Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展 当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF：user-defined function) 根据用户自定义函数类别分为以下三种： UDF(User-Defined-Function)：一进一出 UDAF(User-Defined Aggregation Function)：聚集函数，多进一出；类似于：count/max/min UDTF(User-Defined Table-Generating Functions)：一进多出，如lateral view explore() 官方文档地址：https://cwiki.apache.org/confluence/display/Hive/HivePlugins 编程步骤： 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF(org.apache.hadoop.hive.ql.UDF已被废弃) 重写三个方法 在Hive的命令行窗口创建函数 添加jar资源：add jar ‘jar_path’; 创建function：create [temporary] function [dbname.]function_name AS class_name;(temporary只在当前次使用Hive Cli有效,退出重进无效;dbname.标识限定使用函数的数据库,不写默认为default数据库) 在Hive的命令行窗口删除函数：Drop [temporary] function [if exists] [dbname.]function_name; 自定义UDF/UDTF函数 在IDE中创建一个Maven工程 导入依赖 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建自定义UDF函数 创建类继承GenericUDF 12345678910111213141516171819202122232425262728293031323334// UDF被废弃public class MyUDF extends GenericUDF &#123; // 输入类型int private transient IntObjectInspector arg0; // 返回值类型int private IntWritable res; // 这个方法只调用一次,并且在evaluate()方法之前调用 // 该方法接受的参数是一个ObjectInspectors数组 // 该方法检查接受正确的参数类型和参数个数 public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123; // 输入类型 this.arg0 = (IntObjectInspector) arguments[0]; // 返回值类型 this.res = new IntWritable(); // 确定返回值类型 return PrimitiveObjectInspectorFactory.writableIntObjectInspector; &#125; // 这个方法类似UDF的evaluate()方法。它处理真实的参数，并返回最终结果 public Object evaluate(DeferredObject[] arguments) throws HiveException &#123; Object arg0 = arguments[0].get(); int inputNum = this.arg0.get(arg0); res.set(inputNum + 5); return res; &#125; // 这个方法用于当实现的GenericUDF出错的时候，打印出提示信息。而提示信息就是你实现该方法最后返回的字符串 public String getDisplayString(String[] children) &#123; assert (children.length == 1); return \"param: \" + children[0]; &#125;&#125; 打成Jar包上传到服务器 将jar包添加到Hive的classpath：add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar; 创建临时函数与开发好的java class关联 12# className需要使用全类名create temporary function addFive as 'com.xiong.hive.MyUDF'; 在Hql中使用自定义的函数：select addFive(id) from cc; 创建自定义UDTF函数 创建类继承GenericUDTF 1234567891011121314151617181920212223242526272829303132333435public class MyUDTF extends GenericUDTF &#123; private List&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); // 定义输出数据的列名和数据类型 @Override public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException &#123; // 定义输出数据的列名 List&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;(); fieldNames.add(\"word\"); // 定义输出数据的类型 List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;(); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); &#125; public void process(Object[] args) throws HiveException &#123; // 1、获取数据 String inputStr = args[0].toString(); // 2、获取分隔符 final String splitKey = args[1].toString(); // 3、切粉数据 final String[] words = inputStr.split(splitKey); // 4、遍历写出 for (String word : words) &#123; // 5、将数据放至集合 dataList.clear(); dataList.add(word); // 6、写出数据 forward(dataList); &#125; &#125; public void close() throws HiveException &#123; &#125;&#125; 与自定义UDF类似 123add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar;create temporary function udtf_split as 'com.xiong.hive.MyUDTF';select udtf_split('hello,sobxiong,nice boy!',','); 压缩和存储 Hadoop源码编译支持Snappy压缩 资源准备 虚拟机准备：连接外网、采用root角色编译，减少文件夹权限问题 软件包准备 软件包安装 JDK安装 Maven安装 编译源码 Hadoop压缩配置 MR支持的压缩编码 压缩格式 是否hadoop自带 算法 文件扩展名 是否可切分 换成压缩格式后,原来程序是否需要修改 DEFLATE 是,直接使用 DEFLATE .deflate 否 和文本处理一样,不需要修改 Gzip 是,直接使用 DEFLATE .gz 否 和文本处理一样,不需要修改 bzip2 是,直接使用 bzip2 .bz2 是 和文本处理一样,不需要修改 LZO 否,需要安装 LZO .lzo 是 需要建索引,还需要指定输入格式 Snappy 否,需要安装 Snappy .snappy 否 和文本处理一样,不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec Gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s Snappy 8.3GB 较大 最快 最快 压缩参数配置要在Hadoop中启用压缩，可以配置如下参数(mapred-site.xml文件中) 参数 默认值 阶段 建议 io.compression.codecs(在core-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress(在mapred-site.xml中配置) false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置) false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置) RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 开启Map输出阶段压缩开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量具体操作： 开启Hive中间传输数据压缩功能：set hive.exec.compress.intermediate=true; 开启mapreduce中map输出压缩功能：set mapreduce.map.output.compress=true; 设置mapreduce中map输出数据的压缩方式：set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; 执行查询语句 开启Reduce输出阶段压缩当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能具体操作： 开启Hive最终输出数据压缩功能：set hive.exec.compress.output=true; 开启mapreduce最终输出数据压缩：set mapreduce.output.fileoutputformat.compress=true; 设置mapreduce最终数据输出压缩方式：set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; 设置mapreduce最终数据输出压缩为块压缩：set mapreduce.output.fileoutputformat.compress.type=BLOCK; 测试输出结果是否是压缩文件：insert overwrite local directory ‘/opt/module/data/hive/distribute-result’ select * from emp distribute by deptno sort by empno desc; 文件存储格式Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET 列式存储和行式存储 行存储的特点：查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快 列存储的特点：因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法 主要存储格式对应的存储方式 TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的 ORC和PARQUET是基于列式存储的 TextFile格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作 Orc格式Orc(Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer具体解释： Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储 Stripe Footer：存的是各个Stream的类型，长度等信息每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读 Parquet格式Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度一个Parquet文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页 主流文件存储格式对比实验从存储文件的压缩比和查询速度两个角度对比 存储文件的压缩比测试 TextFile 创建表(存储数据格式为TEXTFILE,默认) 1234567891011create table log_text (track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as textfile; 向表中加载数据 1load data local inpath '/opt/module/data/hive/log.data' into table log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_text; ORC 创建表(存储格式为ORC) 1234567891011create table log_orc(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orc; 向表中加载数据(不能直接导入数据)：insert into table log_orc select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_orc; Parquet 创建表(存储格式为parquet) 1234567891011create table log_parquet(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as parquet; 向表中加载数据(不能直接导入数据)：insert into table log_parquet select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_parquet; 存储文件的压缩比总结：ORC &gt; Parquet &gt; textFile 存储文件的查询速度总结(都运行select * from log_sortType)：查询速度相近 存储和压缩结合 修改Hadoop集群具有Snappy压缩方式 查看hadoop本地库支持情况：hadoop checknative 将编译好的支持Snappy压缩的hadoop源码包解压，将lib/native里面的内容复制到原本的hadoop下的lib/native下替换 分发集群：xsync native/ 再次查看hadoop本地库支持情况：hadoop checknative 重新启动hadoop集群和Hive 测试存储和压缩 创建一个非压缩的ORC存储方式 建表语句 1234567891011create table log_orc_none(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orc tblproperties (\"orc.compress\"=\"NONE\"); 插入数据：insert into table log_orc_none select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_none; 创建一个SNAPPY压缩的ORC存储方式 建表语句 1234567891011create table log_orc_snappy(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orc tblproperties (\"orc.compress\"=\"SNAPPY\"); 插入数据：insert into table log_orc_snappy select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_snappy; 上一节中默认的ORC存储方式，采用默认的ZLIB压缩 介绍ORC存储相关信息：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORCORC存储方式的压缩 Key Default Notes orc.compress ZLIB high level compression(one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries(must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter(must &gt; 0.0 and &lt; 1.0) 存储方式和压缩总结 在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo存储方式和压缩方式不是同一个东西，文件后缀名只是人为加上的(压缩后会带有压缩格式的后缀) 企业级调优 Fetch抓取Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees；在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce 12345678910111213&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;more&lt;/value&gt; &lt;description&gt; Expects one of [none, minimal, more]. Some select queries can be converted to single FETCH task minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins. 0. none : disable hive.fetch.task.conversion 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns) &lt;/description&gt;&lt;/property&gt; 本地模式大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化 123456# 开启本地mrset hive.exec.mode.local.auto=true;# 设置local mr的最大输入数据量,当输入数据量小于这个值时采用local mr的方式,默认为134217728,即128Mset hive.exec.mode.local.auto.inputbytes.max=50000000;# 设置local mr的最大输入文件个数,当输入文件个数小于这个值时采用local mr的方式,默认为4set hive.exec.mode.local.auto.input.files.max=10; 表的优化 小表、大表Join将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表(1000条以下的记录条数)先进内存。在map端完成reduce 实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别 大表Join大表 空Key过滤有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下： 空Key转换有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如 MapJoin如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理 开启MapJoin参数设置 设置自动选择MapJoin：set hive.auto.convert.join = true;(默认true) 大表小表的阈值设置：set hive.mapjoin.smalltable.filesize=25000000;(默认25MB) MapJoin工作机制 Group By默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果 开启Map端聚合参数设置 设置是否在Map端进行聚合：set hive.map.aggr = true;(默认为true) 设置在Map端进行聚合操作的条目数据：set hive.groupby.mapaggr.checkinterval = 100000; 设置有数据倾斜的时候是否进行负载均衡：set hive.groupby.skewindata = true;(默认为false) 当选项设定为true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中(这个过程可以保证相同的Group By Key被分布到同一个Reduce中)，最后完成最终的聚合操作 Count(Distinct)去重统计数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换(虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的)实际操作： 创建一张大表 123create table bigtable(id bigint, time bigint, uid string, keywordstring, url_rank int, click_num int, click_url string) row formatdelimited fields terminated by '\\t'; 加载数据 12load data local inpath '/opt/module/data/hive/bigtable' into tablebigtable; 设置reduce个数为5：set mapreduce.job.reduces = 5; 执行去重id查询 1select count(distinct id) from bigtable; 采用Group by去重id 1select count(id) from (select id from bigtable group by id) a; 笛卡尔积尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积 行列过滤列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤行处理实际操作： 测试先关联两张表，再用where条件过滤 123select o.id from bigtable bjoin ori o on o.id = b.idwhere o.id &lt;= 10; 通过子查询后，在关联表 12select b.id from bigtable bjoin (select id from ori where id &lt;= 10 ) o on b.id = o.id; 动态分区调整关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置 开启动态分区参数设置 开启动态分区功能(默认开启,为true)：hive.exec.dynamic.partition 设置为非严格模式(动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区)：hive.exec.dynamic.partition.mode 在所有执行MR的节点上最大一共可以创建动态分区的个数(默认为1000)：hive.exec.max.dynamic.partitions 在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错：hive.exec.max.dynamic.partitions.pernode 整个MR Job中，最大可以创建多少个HDFS文件(默认为100000)：hive.exec.max.created.files 当有空分区生成时，是否抛出异常。一般不需要设置(默认为false)：hive.error.on.empty.partition 案例实操 分桶：详见之前介绍 分区：详见之前介绍 数据倾斜 合理设置Map数 通常情况下，作业会通过input的目录产生一个或者多个map任务主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小 是不是map数越多越好？答案是否定的。如果一个任务有很多小文件(远远小于块大小128m)，则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的 是不是保证每个map处理接近128m的文件块，就高枕无忧了？答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数 小文件进行合并在map执行前合并小文件，减少map数；CombineHiveInputFormat具有对小文件进行合并的功能(系统默认的格式)。HiveInputFormat没有对小文件合并功能：hive.input.format 复杂文件增加Map数当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize))) = blocksize = 128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数实际操作 执行查询：select count(*) from emp; 设置最大切片值为100个字节：set mapreduce.input.fileinputformat.split.maxsize=100; 再次查询 合理设置Reduce数 调整reduce个数的方法一 每个Reduce处理的数据量默认是256MB：hive.exec.reducers.bytes.per.reducer 每个任务最大的reduce数，默认为1009：hive.exec.reducers.max=1009 计算reducer数的公式：N=min(参数2，总输入数据量/参数1) 调整reduce个数的方法二 在hadoop的mapred-default.xml文件中修改设置每个job的Reduce个数 在Hive Cli中设置：set mapreduce.job.reduces = 15; reduce个数并不是越多越好 过多的启动和初始化reduce也会消耗时间和资源 另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题 在设置reduce个数的时候也需要考虑这两个原则： 处理大数据量利用合适的reduce数 使单个reduce任务处理数据量大小要合适 并行执行Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。如果有更多的阶段可以并行执行，那么job可能就越快完成通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来参数设置： 1234# 打开任务并行执行set hive.exec.parallel=true;# 同一个sql允许最大并行度,默认为8set hive.exec.parallel.thread.number=16; 严格模式Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询 对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表 对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间 限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况 12345678910111213&lt;property&gt; &lt;name&gt;hive.mapred.mode&lt;/name&gt; &lt;value&gt;strict&lt;/value&gt; &lt;description&gt; The mode in which the Hive operations are being performed. In strict mode, some risky queries are not allowed to run. They include: Cartesian Product. No partition being picked up for a query. Comparing bigints and strings. Comparing bigints and doubles. Orderby without limit. &lt;/description&gt;&lt;/property&gt; JVM重用JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出 1234567&lt;property&gt; &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;How many tasks to run per jvm. If set to -1, there is no limit. &lt;/description&gt;&lt;/property&gt; 这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放 推测执行在分布式集群环境下，因为程序Bug(包括Hadoop本身的bug)，负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务(比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕)，则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行(Speculative Execution)机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果 设置开启推测执行参数，Hadoop的mapred-site.xml文件中进行配置： 12345678910111213&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; hive本身也提供了配置项来控制reduce-side的推测执行： 12345&lt;property&gt; &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;/description&gt;&lt;/property&gt; 关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大 压缩：详见之前介绍 执行计划(Explain) 基本语法：EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query 案例实操 查看执行计划：explain select deptno, avg(sal) avg_sal from emp group by deptno; 查看详细执行计划：explain extended select deptno, avg(sal) avg_sal from emp group by deptno; 谷粒影音Hive实战 需求描述统计硅谷影音视频网站的常规指标，各种TopN指标： 统计视频观看数Top10 统计视频类别热度Top10 统计出视频观看数Top20所属类别以及类别包含Top20视频的个数 统计视频观看数Top50所关联视频的所属类别Rank 统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 项目 数据结构 视频表 字段 备注 详细描述 videoId 视频唯一id 11位字符串 uploader 视频上传者 上传视频的用户名String age 视频年龄 视频在平台上的整数天 category 视频类别 上传视频指定的视频分类 length 视频长度 整形数字标识的视频长度 views 观看次数 视频被浏览的次数 rate 视频评分 满分5分 ratings 流量 视频的流量,整型数字 conments 评论数 一个视频的整数评论数 relatedIds 相关视频id 相关视频的id,最多20个 用户表 字段 备注 字符类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int ETL(Extraction-Transformation-Loading,数据抽取、转换和加载)原始数据通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用’\\t’进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用’&amp;’分割，同时去掉两边空格，多个相关视频id也使用’&amp;’进行分割 ETLUtil清洗数据 1234567891011121314151617181920212223242526/*** @param oriStr 原始数据* @return 过滤后的数据*/public static String etlStr(String oriStr) &#123; StringBuffer sb = new StringBuffer(); // 1、切割字符串 String[] fields = oriStr.split(\"\\t\"); // 2、过滤字段长度 if (fields.length &lt; 9) return null; // 3、去掉类别字段中的空格 fields[3] = fields[3].replaceAll(\" \", \"\"); // 4、修改相关视频ID字段的分隔符,把'\\t'替换为'&amp;' for (int i = 0; i &lt; fields.length; i++) &#123; // 非相关id if (i &lt; 9) &#123; if (i == fields.length - 1) sb.append(fields[i]); else sb.append(fields[i]).append('\\t'); &#125; else &#123; // 相关id if (i == fields.length - 1) sb.append(fields[i]); else sb.append(fields[i]).append('&amp;'); &#125; &#125; // 5、返回结果 return sb.toString();&#125; ETL之Mapper 123456789101112131415161718// 输出写NullWritable,不需要排序,节省资源public class ETLMapper extends Mapper&lt;LongWritable, Text, NullWritable, Text&gt; &#123; // 定义全局value private Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取数据 String oriStr = value.toString(); // 2、过滤数据 String eltStr = ETLUtil.etlStr(oriStr); // 3、写出 if (eltStr == null) &#123; return; &#125; v.set(eltStr); context.write(NullWritable.get(), v); &#125;&#125; ETL之Driver 123456789101112131415161718192021222324252627282930313233343536373839404142// 官方推荐采用继承Tool方式// 在ToolRunner中帮做了GenericOptionsParserpublic class ETLDriver implements Tool &#123; private Configuration conf; @Override public int run(String[] args) throws Exception &#123; // 1、获取job对象 Job job = Job.getInstance(conf); // 2、设置jar包路径 job.setJarByClass(ETLDriver.class); // 3、设置Mapper类和输出KV类型 job.setMapperClass(ETLMapper.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); // 4、设置最终输出的KV类型 job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class); // 5、设置输入输出的路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、提交任务 boolean result = job.waitForCompletion(true); return result ? 0 : 1; &#125; @Override public void setConf(Configuration conf) &#123; this.conf = conf; &#125; @Override public Configuration getConf() &#123; return conf; &#125; public static void main(String[] args) &#123; // 构建配置信息 Configuration conf = new Configuration(); try &#123; int result = ToolRunner.run(conf, new ETLDriver(), args); System.out.println(\"result = \" + result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 执行ETL jar包(经过maven的package,然后扔到集群上)：bin/hadoop jar /opt/module/data/hive/guli-vedio-1.0-SNAPSHOT.jar com.xiong.mr.ETLDriver /gulivideo/video/2008/0222 /guliOutput 准备工作 创建表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051-- gulivideo_oricreate table gulivideo_ori( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;)row format delimitedfields terminated by \"\\t\"collection items terminated by \"&amp;\"stored as textfile;-- gulivideo_user_oricreate table gulivideo_user_ori( uploader string, videos int, friends int)row format delimitedfields terminated by \"\\t\"stored as textfile;-- gulivideo_orccreate table gulivideo_orc( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;)clustered by (uploader) into 8 bucketsrow format delimited fields terminated by \"\\t\"collection items terminated by \"&amp;\"stored as orc;-- gulivideo_user_orccreate table gulivideo_user_orc( uploader string, videos int, friends int)row format delimitedfields terminated by \"\\t\"stored as orc; 导入ETL后的数据 12345678# gulivideo_oriload data inpath \"/guliOutput\" into table gulivideo_ori;# gulivideo_user_oriload data inpath \"/gulivideo/user/2008/0903\" into table gulivideo_user_ori;# gulivideo_orcinsert into table gulivideo_orc select * from gulivideo_ori;# gulivideo_user_orcinsert into table gulivideo_user_orc select * from gulivideo_user_ori; 业务分析 统计视频观看数Top10 12345678select videoId, viewsfrom gulivideo_orcorder by views desclimit 10; 统计视频类别热度Top10(某类视频的个数作为视频类别热度) 1234567891011121314151617181920212223242526272829303132333435363738-- 1、使用UDTF函数将类别炸裂select videoId, category_namefrom gulivideo_orclateral view explode(category) tmp_category as category_name;t1-- 2、按照category_name进行分组,统计每种类别视频的总数,同时按照该总数进行倒序排名,取前10select category_name, count(*) category_countfrom t1group by category_nameorder by category_count desclimit 10;-- 最终SQLselect category_name, count(*) category_countfrom ( select videoId, category_name from gulivideo_orc lateral view explode(category) tmp_category as category_name )t1group by category_nameorder by category_count desclimit 10; 统计出视频观看数Top20所属类别以及类别包含Top20视频的个数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758-- 1、统计视频观看数Top20select videoId, views, categoryfrom gulivideo_orcorder by views desclimit 20;t1-- 2、对t1表中的category进行炸裂select videoId, category_namefrom t1lateral view explode(category) tmp_category as category_name;t2-- 3、对t2表进行分组(category_name)求和(总数)select category_name, count(*) category_countfrom t2group by category_nameorder by category_count desc;-- 最终SQLselect category_name, count(*) category_countfrom ( select videoId, category_name from ( select videoId, views, category from gulivideo_orc order by views desc limit 20 )t1 lateral view explode(category) tmp_category as category_name )t2group by category_nameorder by category_count desc; 统计视频观看数Top50所关联视频的所属类别Rank 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687-- 1、统计视频观看数Top50select relatedId, viewsfrom gulivideo_orcorder by views desclimit 50;t1-- 2、对t1表中的relatedId进行炸裂并去重select related_idfrom t1lateral view explode(relatedId) tmp_related as related_idgroup by related_id;t2-- 3、取出观看数前50视频关联ID视频的类别select categoryfrom t2join gulivideo_orc orcon t2.related_id = orc.videoId;t3-- 4、对t3表中的category进行炸裂select explode(category) category_namefrom t3;t4-- 5、分组(类别)求和(总数)select category_name, count(*) category_countfrom t4group by category_nameorder by category_count desc;-- 最终SQLselect category_name, count(*) category_countfrom ( select explode(category) category_name from ( select category from ( select related_id from ( select relatedId, views from gulivideo_orc order by views desc limit 50 )t1 lateral view explode(relatedId) tmp_related as related_id group by related_id )t2 join gulivideo_orc orc on t2.related_id = orc.videoId )t3 )t4group by category_nameorder by category_count desc; 统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10 1234567891011121314151617181920212223242526-- 1、给每一种类别根据视频观看数添加rank值(倒序)select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rkfrom gulivideo_category;-- 2、过滤前十select categoryId, videoId, viewsfrom ( select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rk from gulivideo_category )t1where rk &lt;= 10; 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-- 1、统计上传视频最多的用户Top10select uploader, videosfrom gulivideo_user_orcorder by videos desclimit 10;t1-- 2、取出这10个人上传的所有视频,按照观看次数进行排名,取前20select video.videoId, video.viewsfrom t1join gulivideo_orc videoon t1.uploader = video.uploaderorder by views desclimit 20;-- 最终SQLselect video.videoId, video.viewsfrom ( select uploader, videos from gulivideo_user_orc order by videos desc limit 10 )t1join gulivideo_orc videoon t1.uploader = video.uploaderorder by views desclimit 20; 常见错误及解决方案 启动MR任务报错：virtual memory used. Killing container(虚拟内存不足)修改hadoop的配置，修改检查虚拟内存的属性为false 12345&lt;!-- yarn-site.xml --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Zookeeper","slug":"BigData/Zookeeper","date":"2020-06-26T09:26:19.000Z","updated":"2020-09-20T14:27:02.531Z","comments":true,"path":"2020/06/26/BigData/Zookeeper/","link":"","permalink":"https://sobxiong.github.io/2020/06/26/BigData/Zookeeper/","excerpt":"内容 Zookeeper入门 Zookeeper安装 Zookeeper实战 Zookeeper内部原理 面试真题","text":"内容 Zookeeper入门 Zookeeper安装 Zookeeper实战 Zookeeper内部原理 面试真题 Zookeeper入门 概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目 特点 数据结构 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等 统一命名服务 统一配置管理 统一集群管理 服务器节点动态上下线 软负载均衡 下载地址：官网地址——https://zookeeper.apache.org Zookeeper安装 本地模式安装部署 安装前准备 安装jdk 拷贝Zookeeper安装包到Linux系统下 解压到指定目录：tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/ 配置修改 修改配置文件(conf目录下)：mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件，修改dataDir路径：dataDir=/opt/module/zookeeper-3.6.1/zkData 新建zkData目录(不同于Hadoop目录不能存在)：mkdir zkData 操作Zookeeper 启动Zookeeper Server(服务端)：bin/zkServer.sh start 查看进程是否启动：jps(正常会有一个QuorumPeerMain) 查看状态：bin/zkServer.sh status 启动Zookeeper Client(客户端)：bin/zkCli.sh 查看文件列表：ls /(一开始只有[zookeeper]) 退出Zookeeper Client：quit 停止Zookeeper Server：bin/zkServer.sh stop 配置参数解读Zookeeper中的配置文件zoo.cfg中参数含义解读如下： tickTime = 2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) initLimit = 10：LF初始通信时限集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数(tickTime的数量)，用它来限定集群中的Zookeeper服务器连接到Leader的时限 syncLimit = 5：LF同步通信时限集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer dataDir：数据文件目录 + 数据持久化路径主要用于保存Zookeeper中的数据 clientPort = 2181：客户端连接端口监听客户端连接的端口 Zookeeper实战 分布式安装部署 集群规划：在hadoop1、hadoop2、hadoop3三个节点上部署Zookeeper形成集群 安装：分发zookeeper到hadoop2、hadoop3：xsync zookeeper-3.6.1/ 配置服务器编号： 在zookeeper-3.6.1目录下创建zkData目录：mkdir zkData 在zkData目录下创建myid文件：touch myid 编辑myid文件(设置当前server编号)：1 同步zkData到hadoop2、hadoop3上：xsync zkData/ 在hadoop2、hadoop3上修改myid中的内容为2、3 配置zoo.cfg文件 新增集群节点配置 1234# clusterserver.1=hadoop1:2888:3888server.2=hadoop2:2888:3888server.3=hadoop3:2888:3888 同步zoo.cfg文件：xsync zoo.cfg 配置参数解读：server.A=B:C:D A是一个数，表示这是第几号服务器。集群模式下配置文件myid中的数字就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server B是这个服务器的地址 C是这个服务器Follower与集群中的Leader服务器交换信息的端口 D是用来执行选举时服务器相互通信的端口：万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader 集群操作 分别启动Zookeeper(启动前需要关闭Linux防火墙,使得各节点能够相互通信)：bin/zkServer.sh start 查看状态：bin/zkServer.sh status 客户端命令行操作(启动命令行：bin/zkCli.sh) 命令基本语法 功能描述 help(打错也是一个效果,当前无help命令) 显示所有操作命令 ls [-s] [-w] [-R] path 使用ls命令来查看当前path下znode中所包含的内容(-s：查看更新次数等详细数据,替代ls2;-w：设置watcher监听器,只有效一次;-R：递归查看节点) create [-s] [-e] path [data] 创建节点(-s：含有序列;-e：临时(重启或者超时消失);data：写入path的内容,如果没有data创建不出节点) get [-s] [-w] path 获得节点的值(-s：获取更加详细的节点数据;-w：设置watcher监听器,只有效一次) set path data 设置节点的具体值 stat path 查看节点状态 delete path 删除节点 deleteall path 递归删除节点 API应用 IDEA环境搭建 创建空maven项目 添加pom文件： 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.13.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在resources目录下新建一个日志配置文件log4j.properties 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建ZooKeeper客户端 123456789101112131415// 访问的ipprivate String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\";// 会话超时时间private int sessionTimeout = 2000;// zookeeper客户端private ZooKeeper zkClient;@Beforepublic void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; &#125; &#125;);&#125; 创建子节点 1234567// 1、创建节点@Testpublic void createNode() throws IOException, KeeperException, InterruptedException &#123; String path = zkClient.create(\"/sobxiong\", \"sobxiong,xixixihahaha\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(\"path = \" + path);&#125; 获取子节点并监听节点变化 1234567891011121314151617181920212223242526272829// 2、获取子节点,并监控节点的变化@Testpublic void getDataAndWatch() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(\"child = \" + child); &#125; System.out.println(\"------------\"); Thread.sleep(Long.MAX_VALUE);&#125;// 设置watcher中的process方法,使其继续调用自身继续监听@Beforepublic void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(\"child = \" + child); &#125; System.out.println(\"------------\"); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;);&#125; 判断Znode是否存在 123456// 3、判断节点是否存在@Testpublic void judgeNodeExist() throws KeeperException, InterruptedException &#123; Stat stat = zkClient.exists(\"/sobxiong\", false); System.out.println(\"stat = \" + stat);&#125; 监听服务器节点动态上下线案例 需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线 案例分析： 具体实现 先在集群上创建/servers节点：create /servers “servers” 服务端向Zookeeper注册： 12345678910111213141516171819202122232425262728293031323334public class DistributeServer &#123; // 访问的ip private String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\"; // 会话超时时间 private int sessionTimeout = 2000; // zookeeper客户端 private ZooKeeper zkClient; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeServer server = new DistributeServer(); // 1、连接zookeeper集群 server.getConnect(); // 2、注册节点 server.register(args[0]); // 3、业务逻辑 server.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void register(String hostName) throws KeeperException, InterruptedException &#123; String path = zkClient.create(\"/servers/server\", hostName.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostName + \" is online...\"); &#125; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123;&#125; &#125;); &#125;&#125; 客户端注册监听： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class DistributeClient &#123; // 访问的ip private String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\"; // 会话超时时间 private int sessionTimeout = 2000; // zookeeper客户端 private ZooKeeper zkClient; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeClient client = new DistributeClient(); // 1、获取zookeeper集群连接 client.getConnect(); // 2、注册监听 client.getChildren(); // 3、业务逻辑处理 client.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void getChildren() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/servers\", true); // 存储服务器节点主机名称集合 List&lt;String&gt; hostNames = new ArrayList(); for (String child : children) &#123; byte[] data = zkClient.getData(\"/servers/\" + child, false, null); hostNames.add(new String(data)); &#125; // 将所有在线主机名称打印 System.out.println(\"hostNames = \" + hostNames); &#125; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; getChildren(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;&#125; Zookeeper内部原理 节点类型 Stat结构体 cZxid：创建节点的事务zxid——每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生 ctime：znode被创建的毫秒数(从1970年开始) mzxid：znode最后更新的事务zxid mtime：znode最后修改的毫秒数(从1970年开始) pZxid：znode最后更新的子节点zxid cversion：znode子节点变化号，znode子节点修改次数 dataversion：znode数据变化号 aclVersion：znode访问控制列表的变化号 ephemeralOwner：如果是临时节点，这个是znode拥有者的session id；如果不是临时节点则是0 dataLength：znode的数据长度 numChildren：znode子节点数量 监听器原理 选举机制 半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器 Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的 选举过程的举例(假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的——没有历史数据，在存放数据量这一点上，都是一样的)： 服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上(3票)，选举无法完成，服务器1状态保持为LOOKING 服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的(服务器1)大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1、2状态保持LOOKING 服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING 服务器4启动，发起一次选举。此时服务器1、2、3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING 服务器5启动，同4一样当小弟 写数据流程 面试真题 请简述ZooKeeper的选举机制？参见4.4 ZooKeeper的监听原理是什么？参见4.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？(1)部署方式：单机模式、集群模式；(2)角色：Leader和Follower；(3)集群最少需要机器数：3 ZooKeeper的常用命令有哪些？ls create get delete set","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Spring注解驱动开发","slug":"Spring/Spring注解驱动开发","date":"2020-06-19T06:57:26.000Z","updated":"2020-06-26T08:58:59.887Z","comments":true,"path":"2020/06/19/Spring/Spring注解驱动开发/","link":"","permalink":"https://sobxiong.github.io/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/","excerpt":"内容 容器 扩展原理 Web","text":"内容 容器 扩展原理 Web 容器 @Configuration：类注解(配置类=配置文件,注解告诉Spring这是一个配置类) @Bean：方法注解，在类方法中给出返回Bean的方法，并在方法上添加@Bean注解(给容器中注册一个Bean,类型为返回值的类型,id默认为方法名,可复写注解的value属性复写id)。 @Scope：方法注解，设置作用域。常用值为： prototype：多实例，ioc容器诶懂并不会去调用方法创建对象放在容器中。每次获取的时候才会调用方法创建对象 singleton(默认单实例)：ioc容器启动会调用方法创建对象放到ioc容器中，以后每次获取就是直接从容器中(可看作使用map.get())拿 @Lazy：懒加载，只有在singleton单实例下才生效，且需要在返回bean的方法上加上@Lazy注解。单实例bean默认在容器启动的时候创建对象；懒加载在容器启动时不创建对象，第一次使用(获取)Bean创建对象并初始化 123456@Scope(\"singleton\")@Lazy@Bean(\"person\")public Person person() &#123; return new Person(\"SOBXiong\", 22);&#125; @ComponentScans：指定扫描规则组(value为ComponentScan集合) @ComponentScan：类注解，指定组件扫描规则 value：指定包名，这样Spring会扫描包下的所有组件(SpringBoot情况可能不同,不需要) excludeFilters：指定排除的过滤器，filter可根据注解排除(排除规则)，classed指定注解的类 includeFilters：指定只需要包含的过滤器 useDefaultFilters：是否适用缺省的过滤器，默认true；如果要使includeFilters生效，则必须设置为false FilterType： FilterType.ANNOTATION：按照注解方式 FilterType.ASSIGNABLE_TYPE：按照指定的类型(具体的类,包括子类和实现类) FilterType.REGEX：适用正则表达式 FilterType.CUSTOM：使用自定义规则，需要自定义实现TypeFilter接口的类 123456789101112131415161718192021222324252627282930313233@Configuration@ComponentScan(value = \"packageName\",excludeFilters = &#123; @Filter(type=FilterType.ANNOTATION,classes=&#123;Controller.class,Service.class&#125;)&#125;)public class MainConfig &#123; @Bean public Person person() &#123; return new Person(\"SOBXiong\", 22); &#125;&#125;public class TestTypeFilter implements TypeFilter &#123; /** * * @param metadataReader 读取到的当前正在扫描的类的信息 * @param metadataReaderFactory 可以获取到其他任何类信息的工厂 * @return boolean * @throws IOException */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; // 获取当前类注解的信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); // 获取当前正在扫描的类的类信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); // 获取当前类的资源信息(类路径等) Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println(\"className = \" + className); return className.contains(\"test\"); // return false; &#125;&#125; @Conditional：按照一定的条件进行判断，满足条件给容器中注册bean(Spring底层大量用到);可以设置在类上，也可以设置在方法上。设置在返回bean的方法上：只根据条件解决是否注册bean。设置在类上：类中注册统一设置，满足条件时，这个类中配置的所有bean注册才能生效 1234567891011121314151617181920212223242526272829303132@Conditional(TestCondition.class)@Bean(\"person2\")public Person person2() &#123; return new Person(\"SOBXiong\", 22);&#125;public class TestCondition implements Condition &#123; /** * @param conditionContext 判断条件能使用的上下文(环境) * @param annotatedTypeMetadata 注释信息 * @return boolean */ @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) &#123; // 1、能获取到ioc使用的beanFactory ConfigurableListableBeanFactory beanFactory = conditionContext.getBeanFactory(); // 2、获取类加载器 ClassLoader classLoader = conditionContext.getClassLoader(); // 3、获取当前环境信息 Environment environment = conditionContext.getEnvironment(); // 4、获取到bean定义的注册类 BeanDefinitionRegistry registry = conditionContext.getRegistry(); // 可以判断容器中的bean注册情况,也可以给容器中注册bean boolean isDefinition = registry.containsBeanDefinition(\"person\"); // 获取运行系统的名称 String osName = environment.getProperty(\"os.name\"); if (osName.contains(\"Windows\")) &#123; return true; &#125; return false; &#125;&#125; @Import：导入组件，id默认是组件的全类名 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 给容器中注册组件： * 1、包扫描+组件标注注解(@Controller、@Service、@Repository、@Component) * 2、@Bean[导入第三方包里面的组件] * 3、@Import[快速给容器中导入一个组件] * 1、容器会自动注册这个组件,id默认是全类名 * 2、ImportSelector：返回需要导入的组件的全类名数组(SpringBoot源码中许多地方用到); * 3、ImportBeanDefinitionRegistrar：手动注册bean到容器中 * 4、使用Spring提供的FactoryBean(工厂Bean),其他与Spring整合的框架使用的特别多 * 1、默认获取的是工厂bean调用getObject创建的对象 * 2、要获取工厂bean本身,需要给id前面加一个&amp; */@Configuration@Import(&#123;TestImportSelector.class, TestImportBeanDefinitionRegistrar.class&#125;)public class MainConfig &#123;&#125;// 自定义逻辑返回需要导入的组件public class TestImportSelector implements ImportSelector &#123; /** * @param annotationMetadata 当前标注@Import注解的类的所有注解信息 * @return String[] 导入到容器中的组件全类名数组 */ @Override public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; return new String[]&#123;\"com.xiong.test.Animal\", \"com.xiong.test.Person\"&#125;; &#125;&#125;public class TestImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; /** * 可以把所有需要添加到容器中的bean通过调用BeanDefinitionRegistry.registerBeanDefinition手工注册 * @param importingClassMetadata 当前类的注解信息 * @param registry BeanDefinition注册类 */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; boolean isWorldExist = registry.containsBeanDefinition(\"World\"); if (!isWorldExist) &#123; // 注册一个bean,指定bean的名称和bean的定义信息(bean的类型,bean的Scope...) registry.registerBeanDefinition(\"world\", new RootBeanDefinition(World.class)); &#125; &#125;&#125; FactoryBean(工厂Bean)： 1234567891011121314151617181920212223242526// 创建一个Spring定义的FactoryBeanpublic class TestFactoryBean implements FactoryBean&lt;Animal&gt; &#123; // 返回一个Animal对象,这个对象会添加到容器中 // 调用此方法得到对象 @Override public Animal getObject() throws Exception &#123; return new Animal(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Animal.class; &#125; // 是否是单实例 @Override public boolean isSingleton() &#123; return true; &#125;&#125;@Configurationpublic class MainConfig &#123; @Bean public TestFactoryBean testFactoryBean()&#123; return new TestFactoryBean(); &#125;&#125; @Bean指定初始化和销毁方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class Car &#123; public Car()&#123; System.out.println(\"Car 构造方法\"); &#125; public void init()&#123; System.out.println(\"Car init\"); &#125; public void destroy()&#123; System.out.println(\"Car destroy\"); &#125;&#125;/** * bean的生命周期： * bean创建 --&gt; 初始化 --&gt; 销毁 * 容器管理bean的生命周期; * 我们可以自定义初始化和销毁方法;容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法 * * 构造(对象创建)： * 单实例：在容器启动的时候创建对象 * 多实例：在每次获取的时候创建对象 * 初始化：对象创建完成,并赋值结束,调用初始化方法 * 销毁： * 单实例：容器关闭的时候 * 多实例：容器不会管理这个bean,容器不会调用销毁方法 * * 1、指定初始化和销毁方法(通过@Bean注解指定init-method和destroy-method) * 2、通过让Bean实现InitializingBean(定义初始化方法逻辑),DisposableBean(定义销毁逻辑) * 3、可以使用JSR250： * @PostConstruct：在bean创建完成并且属性赋值完毕再执行初始化方法 * @PreDestroy：在容器销毁bean之前通知进行清理工作 * 4、BeanPostProcessor：bean的后置处理器(在bean初始化前后进行一些工作) * postProcessBeforeInitialization：在初始化之前工作 * postProcessAfterInitialization：在初始化之后工作 */@Configurationpublic class MainConfigLifecycle &#123; // @Scope(\"prototype\") @Bean(initMethod = \"init\", destroyMethod = \"destroy\") public Car car() &#123; return new Car(); &#125;&#125;@ComponentScan(\"com.xiong.test\")@Componentpublic class Cat implements InitializingBean, DisposableBean &#123; public Cat() &#123; System.out.println(\"Cat构造函数...\"); &#125; @Override public void destroy() throws Exception &#123; System.out.println(\"Cat destroy...\"); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println(\"Cat init...\"); &#125;&#125;/** * 后置处理器：在bean初始化前后进行处理工作 */@Componentpublic class TestBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessBeforeInitialization: \" + beanName + \" , \" + bean); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessAfterInitialization: \" + beanName + \" , \" + bean); return bean; &#125;&#125; BeanPostProcessor原理 1234567891011// 遍历得到容器中所有的BeanPostProcessor;挨个执行beforeInitialization,// 一旦返回null,跳出for循环,不追执行后面的BeanPostProcessor.postProcessBeforeInitialization()// 给bean进行属性赋值populateBean(beanName, mbd, instanceWrapper);initializeBean(beanName, exposedObject, mbd);&#123; // 以下就是initializeBean的粗略内容 applyBeanPostProcessorsBeforeInitialization(bean, beanName); invokeInitMethods(beanName, wrappedBean, mbd); applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);&#125; Spring底层对BeanPostProcessor的使用 ApplicationContextAwareProcessor：可让bean获取容器对象context BeanValidationPostProcessor：Web表单校验的处理器 InitDestroyAnnotationBeanPostProcessor：@PostConstruct和@Bean的init-method等方法的具体实现 AutowiredAnnotationBeanPostProcessor：@Autowired自动注入功能的具体实现 属性赋值 使用@Value赋值： 基本数值 SpEL：#{} ${}：取出配置文件(properties或yaml)中的值(在运行环境变量里面的值) 使用@PropertySource加载外部配置文件 1234567891011121314151617181920212223// 使用@PropertySource读取外部配置文件中的k/v保存到运行的环境变量中// 加载完外部的配置文件以后使用$&#123;&#125;取出配置文件的值// 当前只能加载properties文件,yaml不能,是采用的加载器问题@PropertySource(value = &#123;\"classpath:application.properties\"&#125;, encoding = \"utf-8\")@Configurationpublic class MainConfigPropertyValues &#123; @Bean public Person person() &#123; return new Person(); &#125;&#125;public class Person &#123; @Value(\"SOBXiong\") private String name; @Value(\"#&#123;22+5&#125;\") private int age; @Value(\"$&#123;person.nickName&#125;\") private String nickName;&#125;// application.properties// person.nickName=熊哈哈 自动装配 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * 自动装配： * Spring利用依赖注入(DI),完成对IOC容器中各个组件的依赖关系赋值 * @Autowired：自动注入(Spring定义的) * TestService&#123; * @Autowired TestDao testDao; * &#125; * 1、默认优先按照类型去容器中找对应的组件：context.getBean(TestDao.class); * 2、如果找到多个相同类型的组件,将属性名作为组件的id去容器中查找 * 3、@Qualifier(\"testDao\")：使用@Qualifier指定需要装配的组件id,而不是使用属性名 * 4、自动装配默认一定要将属性赋值好,没有就会报错(可以使用@Autowired注解中的required=false避免报错) * 5、@Primary：让Spring进行自动装配的时候默认使用首选的bean(此时@Qualifier不能使用);也可以使用@Qualifier指定需要装配的具体bean * 6、Spring还支持使用@Resource(JSR250)和@Inject(JSR330)[java规范的注解] * @Resource：可以和@Autowired一样实现自动装配功能,但默认是按照组件名称进行装配的(也可以通过name属性进行指定id);不能支持@Qualifier和required=false * @Inject：需要导入javax.inject的包,和@Autowired的功能一样,但没有required属性 * 7、@Autowired可以在构造器、参数、方法和属性上标注,都是从容器中获取组件的值 * 1、[标注在方法位置]：@Bean标注方法的方法参数;参数从容器中获取;默认不写@Autowired效果是一样的;都能自动装配 * 2、[标注在构造器位置]：如果组件只有一个有参构造器,这个有参构造器的@Autowired可以省略,参数位置的组件还是可以自动从容器中获取; * 但如有既有有参又有无参,会优先调用无参构造器,这使得boss的car属性和容器中的car不是同一个 * 3、[标注在参数位置] * 8、自定义组件想要使用Spring容器底层的的一些组件(ApplicationContext、BeanFactory等) * 自定义组件实现xxxAware接口：在创建对象的时候,会调用接口规定的方法注入相关组件; * xxxAware使用xxxProcessor：applicationContextAware =&gt; applicationContextAwareProcessor(BeanPostProcessor的实现类) */@Configuration@ComponentScan(\"com.xiong.test2\")public class MainConfigAutowired &#123; @Primary @Bean(\"testDao2\") public TestDao testDao() &#123; return new TestDao(\"2\"); &#125; // @Bean标注的方法创建对象的时候,方法参数的值从容器中获取 @Bean public Boss boss(Car car)&#123; Boss boss = new Boss(); boss.setCar(car); return boss; &#125;&#125;@Componentpublic class Car &#123;&#125;// 默认加在ioc容器中的组件，容器启动会调用无参构造器创建对象，在进行初始化赋值等操作// @Componentpublic class Boss &#123; private Car car; public Boss() &#123; &#125; // 构造器要用的组件，都是从容器中获取 // @Autowired public Boss(@Autowired Car car) &#123; this.car = car; System.out.println(\"Boss constructor with one parameter!\"); &#125; public Car getCar() &#123; return car; &#125; // 标注在方法上，Spring容器创建当前对象，就会调用方法完成赋值 // 方法使用的参数，自定义类型的值从ioc容器中获取 // @Autowired public void setCar(Car car) &#123; this.car = car; &#125; @Override public String toString() &#123; return \"Boss&#123;\" + \"car=\" + car + '&#125;'; &#125;&#125;@Servicepublic class TestService &#123; @Qualifier(\"testDao2\") @Autowired private TestDao testDao; @Override public String toString() &#123; return \"TestService&#123;\" + \"testDao=\" + testDao + '&#125;'; &#125;&#125;@Repositorypublic class TestDao &#123; private String label = \"1\";&#125; @Profile：Spring为我们提供的可以根据当前环境,动态地激活和切换一系列组件的共功能;指定组件在哪个环境的情况下才能被注册到容器中,不指定,任何环境下都能注册这个组件(开发、测试/生产环境,数据源(A/B/C)) 加了环境标志性的bean,只有这个环境被激活的时候才能注册到容器中(默认环境是default) 123@Profile(\"test\")@Bean(\"testDataSource\")public DataSource dataSourceTest() throws Exception &#123; ... &#125; 写在配置类上,只有是指定的环境的时候,整个配置类里面的内容才能生效 没有标注环境标识的bean在任何环境下都是加载的 环境的激活： 使用命令行动态参数：虚拟机参数位置加载 -Dspring.profiles.active=test 代码的方式激活某种环境 1234567891011121314151617@Testvoid contextLoads() &#123; // 1、创建ioc容器 AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); // 2、设置需要激活的环境 context.getEnvironment().setActiveProfiles(\"test\"); // 3、注册主配置类 context.register(MainConfigProfile.class); // 4、启动刷新容器 context.refresh(); ... // 关闭容器 context.close();&#125; AOP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220/** * AOP：指在程序运行期间动态地将某段代码切入到指定方法指定位置进行运行地编程方式 * 1、导入AOP模块：Spring AOP(SpringBoot导入MVC模块会连着导入AOP模块) * 2、定义一个业务逻辑类(MathCalculator)：在业务逻辑运行地时候将日志进行打印(方法之前、方法运行结束、方法出现异常...) * 3、定义一个日志切面类(LogAspect)：切面类里面地方法需要动态感知MathCalculator.div运行到哪里然后执行 * 通知方法： * 前置通知@Before：logStart(在目标方法div运行之前运行) * 后置通知@After：logEnd(在目标方法div运行结束之后运行——无论方法是正常结束还是异常结束) * 返回通知@AfterReturning：logReturn(在目标方法div正常返回之后运行) * 异常通知@AfterThrowing：logException(在目标方法div出现异常以后运行) * 环绕通知@Around：动态代理,手动推进目标方法div运行(jointPoint.proceed()) * 4、给切面类的目标方法标注何时何地运行(通知注解) * 5、将切面类和业务逻辑类(目标方法所在类)都加入到容器中 * 6、必须告诉Spring哪个类是切面类(给切面类上加一个注解@Aspect) * 7、给配置类中加@EnableAspectJAutoProxy(开启基于注解的AOP模式);在Spring中有很多的@EnableXXX注解,替代以前的xml配置 * * 三步： * 1、将业务逻辑组件和切面类都加入到容器中;告诉Spring哪个是切面类(@Aspect) * 2、在切面类上的每一个通知方法上标注通知注解,告诉Spring何时何地运行(切入点表达式) * 3、开启基于注解的AOP模式：@EnableAspectJAutoProxy * * AOP原理：[看给容器中注册了什么组件,这个组件什么时候工作以及这个组件的功能是什么?] * @EnableAspectJAutoProxy： * 1、@EnableAspectJAutoProxy是什么？ * @Import(AspectJAutoProxyRegistrar.class)：给容器中导入AspectJAutoProxyRegistrar * 利用AspectJAutoProxyRegistrar自定义给容器中注册bean：BeanDefinition * internalAutoProxyCreator = AnnotationAwareAspectJAutoProxyCreator * 给容器中注册一个AnnotationAwareAspectJAutoProxyCreator,id为internalAutoProxyCreator * * 2、AnnotationAwareAspectJAutoProxyCreator： * AbstractAutoProxyCreator实现了SmartInstantiationAwareBeanPostProcessor,BeanFactoryAware接口 * AbstractAdvisorAutoProxyCreator * AspectJAwareAdvisorAutoProxyCreator * AnnotationAwareAspectJAutoProxyCreator * 关注后置处理器(bean初始化完成前后做事情)、自动装配beanFactory * * 阅读线索： * AbstractAutoProxyCreator.setBeanFactory() * AbstractAutoProxyCreator.postProcessBeforeInstantiation() * AbstractAutoProxyCreator.postProcessAfterInitialization() * * AbstractAdvisorAutoProxyCreator.setBeanFactory()复写父类方法 * 方法内还执行了initBeanFactory() * * AnnotationAwareAspectJAutoProxyCreator.initBeanFactory()复写父类方法 * * 1、传入配置类,创建ioc容器 * 2、调用配置类,调用refresh()刷新容器 * 3、registerBeanPostProcessors(beanFactory)：注册bean的后置处理器来方便拦截bean的创建 * 1、先获取ioc容器已定义了的需要创建的所有BeanPostProcessor(第一步传入配置类带入的)： * String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); * 2、给容器中加入别的BeanPostProcessor * 3、优先注册实现了PriorityOrdered接口的BeanPostProcessor * 4、再给容器中注册实现了Ordered接口的BeanPostProcessor * 5、最后注册没实现优先级接口的BeanPostProcessor * 6、注册BeanPostProcessor,实际上就是创建BeanPostProcessor对象并保存在容器中 * 创建internalAutoProxyCreator的BeanPostProcessor[AnnotationAwareAspectJAutoProxyCreator对象] * 1、创建Bean的实例 * 2、populateBean()：给bean的各种属性赋值 * 3、initializeBean()：初始化bean * 1、invokeAwareMethods()：处理Aware接口的方法回调 * 2、applyBeanPostProcessorsBeforeInitialization()：应用后置处理器的postProcessBeforeInitialization()方法 * 3、invokeInitMethods()：执行自定义的初始化方法 * 4、applyBeanPostProcessorsAfterInitialization()：应用后置处理器的postProcessAfterInitialization()方法 * 4、BeanPostProcessor(AnnotationAwareAspectJAutoProxyCreator)创建成功 -&gt; aspectJAdvisorsBuilder * 7、把BeanPostProcessor注册到BeanFactory中：beanFactory.addBeanPostProcessor(postProcessor); * -----------------------------------------创建和注册AnnotationAwareAspectJAutoProxyCreator的过程---------------------------- * 4、finishBeanFactoryInitialization(beanFactory)：完成BeanFactory初始化工作,创建剩下来的单实例bean * 1、遍历获取容器中所有的Bean,依次创建对象getBean(beanName) * getBean -&gt; doGetBean() -&gt; getSingleton() * 2、创建bean[AnnotationAwareAspectJAutoProxyCreator在所有bean创建之前会有一个拦截, * InstantiationAwareBeanPostProcessor,会调用postProcessBeforeInstantiation()方法] * 1、先从缓存中获取当前bean,如果能获取到,说明bean是被创建过的,直接使用;否则再创建 * 只要被创建好的bean都会被缓存起来 * 2、createBean()：创建bean——AnnotationAwareAspectJAutoProxyCreator会在任何bean创建之前先尝试返回bean的实例 * [BeanPostProcessor是在Bean对象创建完成初始化前后调用的] * [InstantiationAwareBeanPostProcessor是在创建Bean实例之前先尝试用后置处理器返回对象的] * 1、resolveBeforeInstantiation()：希望后置处理器在此能返回一个代理对象,如果能返回 * 代理对象就使用; * 1、后置处理器先尝试返回对象： * bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); * 拿到所有后置处理器,如果是InstantiationAwareBeanPostProcessor * 就执行postProcessBeforeInstantiation()方法 * if (bean != null) &#123; * bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); * &#125; * 否则就进行第二步 * 2、doCreateBean()：真正地去创建bean实例,和3.6流程一样 * 3、 * * AnnotationAwareAspectJAutoProxyCreator[InstantiationAwareBeanPostProcessor]的作用 * 1、每一个bean创建之前,调用postProcessBeforeInstantiation() * 关心MathCalculator和LogAspect的创建 * 1、判断当前bean是否在advisedBeans中(保存了需要增强的bean——需要切面) * 2、判断当前bean是否是基础类型的(Advice、Pointcut、Advisor、AopInfrastructureBean) * 或者是否是切面(@Aspect) * 3、判断是否需要跳过 * 1、获取候选的增强器(切面里面的通知方法),每一个封装的通知方法的增强器是InstantiationModelAwarePointcutAdvisor * 判断每一个增强器是否是AspectJPointcutAdvisor类型(返回true) * 2、返回false * 2、创建对象 * postProcessAfterInitialization * // 在需要的时候包装 * return wrapIfNecessary() * * 1、获取当前bean的所有增强器(通知方法) * 1、找到候选的所有增强器(找哪些通知方法是需要切入当前bean方法的) * 2、获取到能在bean使用的增强器 * 3、给增强器排序 * 2、保存当前bean到advisedBeans * 3、如果当前bean需要增强,创建当前bean的代理对象 * 1、获取所有增强器(通知方法) * 2、保存到proxyFactory * 3、创建代理对象：Spring自动决定 * JdkDynamicAopProxy()：jdk动态代理 * ObjenesisCglibAopProxy()：cglib的动态代理 * 4、给容器中返回当前组件使用cglib增强了的代理对象 * 5、以后容器中获取到的就是这个组件的代理对象，执行目标方法的时候，代理对象就会执行通知方法的流程 * * 3、目标方法执行: * 容器中保存了组件的代理对象(cglib增强后的对象)，这个对象里面保存了详细信息(比如增强器、目标对象...) * 1、CglibAopProxy.intercept()拦截目标方法的执行 * 2、根据ProxyFactory对象获取将要执行的目标方法的拦截器链 * List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); * 1、List&lt;Object&gt; interceptorList保存所有拦截器(长度为5) * 一个默认的ExposeInvocationInterceptor和四个增强器 * 2、遍历所有的增强器,将其转为Interceptor * 3、将增强器转为List&lt;MethodInterceptor&gt; * 如果是MethodInterceptor,直接加入集合 * 如果不是,使用AdvisorAdapter转为MethodInterceptor * 转换完成返回MethodInterceptor数组 * 3、如果没有拦截器链,直接执行目标方法 * 拦截器链(每一个通知方法又被包装为方法拦截器,利用MethodInterceptor机制) * 4、如果有拦截器链，把需要执行的目标对象、目标方法、拦截器链等信息传入创建一个CglibMethodInvocation对象， * 并调用它的proceed()方法 * 5、拦截器的触发过程 * 1、如果没有拦截器(或者拦截器的索引为拦截器数组大小-1——到了最后一个拦截器),直接执行目标方法 * 2、链式获取每一个拦截器,拦截器执行invoke()方法,每一个拦截器等待下一个拦截器执行完成返回以后再来执行; * 拦截器链的机制,保证通知方法与目标方法的执行顺序 * * 总结： * 1、@EnableAspectJAutoProxy开始AOP功能 * 2、@EnableAspectJAutoProxy会给容器中注册一个组件AnnotationAwareAspectJAutoProxyCreator * 3、AnnotationAwareAspectJAutoProxyCreator是一个后置处理器 * 4、容器的创建流程： * 1、refresh()容器刷新后registerBeanPostProcessors()注册后置处理器：创建AnnotationAwareAspectJAutoProxyCreator对象 * 2、finishBeanFactoryInitialization()初始化剩下的单实例bean * 1、创建业务逻辑组件和切面组件 * 2、AnnotationAwareAspectJAutoProxyCreator拦截组件的创建过程 * 3、组件创建完之后,判断组件是否需要增强 * 是：把切面的通知方法包装成增强器(Advisor),给业务逻辑组件创建一个代理对象(cglib) * 5、执行目标方法： * 1、代理对象执行目标方法 * 2、CglibAopProxy.intercept()进行拦截： * 1、得到目标方法的拦截器链(增强器包装成拦截器MethodInterceptor) * 2、利用拦截器的链式机制,依次进入每一个拦截器进行执行 * 3、效果： * 正常执行：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 返回通知 * 出现异常：前置通知 -&gt; 目标方法 -&gt; 后置通知 -&gt; 异常通知 */@EnableAspectJAutoProxy@Configurationpublic class MainConfigAOP &#123; // 业务逻辑类加入容器中 @Bean public MathCalculator calculator()&#123; return new MathCalculator(); &#125; // 切面类加入到容器中 @Bean public LogAspect logAspect()&#123; return new LogAspect(); &#125;&#125;public class MathCalculator &#123; public int div(int i, int j) &#123; return i / j; &#125;&#125;/** * 切面类,方法中joinPoint必须写在参数表的第一位(否则报错) */@Aspectpublic class LogAspect &#123; // 抽取公共的切入点表达式(参考Spring官方文档) // 1、本类引用(方法名()) // 2、其他的切面引用(全类名方法名()) @Pointcut(\"execution(public int com.xiong.test3.MathCalculator.div(int, int))\") public void pointCut() &#123;&#125; // @Before在目标方法之前切入：切入点表达式(指定在哪个方法切入) @Before(\"pointCut()\") public void logStart(JoinPoint joinPoint) &#123; Object[] args = joinPoint.getArgs(); System.out.println(joinPoint.getSignature().getName() + \"运行开始... 参数列表是: &#123;\" + Arrays.asList(args) + \"&#125;\"); &#125; @After(\"pointCut()\") public void logEnd(JoinPoint joinPoint) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行结束...\"); &#125; @AfterReturning(value = \"pointCut()\", returning = \"result\") public void logReturn(JoinPoint joinPoint, Object result) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行正常返回... 结果: &#123;\" + result + \"&#125;\"); &#125; @AfterThrowing(value = \"pointCut()\", throwing = \"exception\") public void logException(JoinPoint joinPoint, Exception exception) &#123; System.out.println(joinPoint.getSignature().getName() + \"运行出现异常... 异常信息: &#123;\" + exception.getMessage() + \"&#125;\"); &#125;&#125;@Testvoid contextLoads() &#123; // 1、创建ioc容器 AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(MainConfigAOP.class); // 必须使用Spring容器中的组件 MathCalculator calculator = context.getBean(MathCalculator.class); calculator.div(1, 0); context.close();&#125;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"}]},{"title":"Hadoop","slug":"BigData/Hadoop","date":"2020-06-03T13:48:10.000Z","updated":"2020-09-20T14:30:07.349Z","comments":true,"path":"2020/06/03/BigData/Hadoop/","link":"","permalink":"https://sobxiong.github.io/2020/06/03/BigData/Hadoop/","excerpt":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode HDFS2.X新特性 MapReduce概述 Hadoop序列化 MapReduce框架原理 Hadoop数据压缩 Yarn资源调度器 Hadoop企业优化 MapReduce扩展案例 常见错误及解决方案","text":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode HDFS2.X新特性 MapReduce概述 Hadoop序列化 MapReduce框架原理 Hadoop数据压缩 Yarn资源调度器 Hadoop企业优化 MapReduce扩展案例 常见错误及解决方案 概论 概念：大数据指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。需要解决的问题：海量数据的存储和海量数据的分析计算问题。 大数据特点(4V)： Volume(大量) Velocity(高速) Variety(多样)：结构化/非结构化数据，结构化数据以数据库/文本为主，非结构化数据包括网络日志、音频、视频、图片和地理位置信息等。 Value(低价值密度)：价值密度的高度与数据总量的大小成反比，如何快速对有价值数据“提纯”称为目前大数据背景下待解决的难题。 大数据应用场景：物流仓储、零售、旅游、商品广告推荐、保险、金融、房产、人工智能 大数据部门业务流程：产品人员提需求(统计总用户数、日活跃用户数、回流用户数等) =&gt; 数据部门搭建数据平台、分析数据指标 =&gt; 数据可视化(报表展示、邮件发送、大屏幕展示等) 大数据部门组织结构： Hadoop介绍 Hadoop是什么： 是一个由Apache基金会开发的分布式系统基础架构。 主要解决海量数据的存储和海量数据的分析计算问题。 广义上来说，Hadoop通常指更广泛的概念——Hadoop生态圈。 Hadoop发展历史 Lucene框架是Doug Cutting开创的开源软件，用Java编写，实现与Goole类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询和索引引擎。 2001年年底Lucene称为Apache基金会的一个子项目。 对于海量数据的场景，Lucene面对与Google同样的困难，存储数据困难，检索速度慢。 学习和模仿Google解决这些问题的办法：微型版Nutch。 Google是Hadoop的思想之源(其在大数据方面的三篇论文)GFS -&gt; HDFS Map-Reduce -&gt; MR BigTable -&gt; HBase 2003年~04年，Google公开部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。 2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。 2006年3月份，Map-Reduce和Nutch Distributed File System(NDFS)分别被纳入到Hadoop项目中，Hadoop就正式诞生，标志着大数据十代来临。 Hadoop名字来源于Doug Cutting儿子的玩具大象 Hadoop三大发行版本 Apache：最原始(基础)的版本，对于入门学习最好 Cloudera：在大型互联网企业中用的较多，产品主要为CDH，Cloudera Manager，Cloudera Support： CDH是Cloudera的Hadoop发行版，完全开源，比Apache版本在兼容性、安全性、稳定性上有所增强。 Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。 Cloudera Support即是对Hadoop的技术支持。 Hortonworks：文档较好 Hortonworks的主打产品是Hortonworks Data Platform(HDP)，也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari——一款开源的安装和管理系统。 HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 Hadoop的优势(4高) 高可靠性：Hadoop底层维护多个数据副本，即使某个计算元素或存储出现故障，也不会导致数据的丢失。 高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点。 高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。 高容错性：能够自动将失败的任务重新分配。 Hadoop组成 1.x：Common(辅助工具)、HDFS(数据存储)、MapReduce(计算+资源调度) 2.x：Common(辅助工具)、HDFS(数据存储)、Yarn(资源调度)、MapReduce(计算) HDFS架构概述： HDFS全名——Hadoop Distributed File System 组成： NameNode(nn)：存储文件的元数据，如文件名、文件目录结构、文件属性(生成时间、副本数、文件权限)以及每个文件的块列表和块所在的DataNode等——类似书的目录(索引) DataNode(dn)：在本地文件系统存储文件块数据以及块数据的校验和——具体的书章节内容 Secondary NameNode(2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照——辅助NameNode工作 Yarn架构概述 MapReduce架构概述 将计算分为两个阶段：Map和Reduce Map阶段并行处理输入数据 Reduce阶段对Map结果进行汇总 大数据技术生态体系 环境搭建 配置Java环境变量 123456789&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##JAVA_HOMEexport JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_251export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Java版本java -version 配置Hadoop环境变量 12345678910&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##HADOOP_HOMEexport HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;binexport PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Hadoop版本hadoop version Hadoop目录说明 bin目录：存放对Hadoop相关服务(HDFS,YARN)进行操作的脚本 etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 lib目录：存放Hadoop的本地库(对数据进行压缩解压缩功能) sbin目录：存放启动或停止Hadoop相关服务的脚本 share目录：存放Hadoop的依赖jar包、文档、和官方案例 Hadoop运行模式 本地模式 官方WordCount案例(统计单词数目)： 1234567891011&#x2F;&#x2F; 创建wcinput文件夹mkdir wcinput&#x2F;&#x2F; 创建wc.input文件cd wcinputtouch wc.input&#x2F;&#x2F; 编辑wc.input随意输入字符vim wc.input&#x2F;&#x2F; 回到Hadoop目录执行程序hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput&#x2F; wcouput&#x2F;&#x2F; 查看结果cat wcoutput&#x2F;part-r-00000 伪分布式模式 配置集群 设置hadoop-env.sh：在文件中设置JAVA_HOME为本地JDK地址 设置core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; 设置hdfs-site.xml： 12345&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 启动集群 格式化NameNode：bin/hdfs namenode -format 启动NameNode和DataNode：sbin/start-dfs.sh(关闭stop) 查看集群 查看是否启动成功：jps(JDK中的命令,不是Linux命令,类似ps) web端查看HDFS文件系统：http://192.168.232.100:9870(需要CentOS主机上设置关闭防火墙,在3.x版本端口号默认为9870) 查看产生的log日志：cd /hadoop/logs 注意：不能一直格式化NameNode，格式化NameNode会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。(最好关闭NameNode和DataNode) 操作集群(所有命令类似于在Linux Terminal的命令行操作,需要加上固定前缀bin/hdfs dfs -) 在HDFS文件系统上创建一个input文件夹：bin/hdfs dfs -mkdir -p /user/sobxiong/input 将测试文件内容上传到文件系统上：bin/hdfs dfs -put wcinput/wc.input /user/sobxiong/input/ 查看上传的文件是否正确： 12bin/hdfs dfs -ls /user/sobxiong/input/bin/hdfs dfs -cat /user/sobxiong/input/wc.input 运行MapReduce程序：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/sobxiong/input/ /user/sobxiong/output 查看输出结果：bin/hdfs dfs -cat /user/sobxiong/output/* 也可以在浏览器的文件系统中查看 将测试文件内容下载到本地：bin/hdfs dfs -get /user/sobxiong/output/part-r-00000 ./wcoutput/ 删除输出结果：bin/hdfs dfs -rm -r /user/sobxiong/output 启动Yarn并运行MapReduce程序 配置集群 配置yarn-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置yarn-site.xml 1234567891011121314&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置Yarn应用的classPath --&gt;&lt;!-- 不配置出现：错误: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster --&gt;&lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;命令行下输入hadoop classpath的一长串环境&lt;/value&gt;&lt;/property&gt;&lt;!-- 不需要设置yarn.resourcemanager.hostname，可以直接通过ip+端口号的方式访问 --&gt; 配置mapred-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置mapred-site.xml： 123456&lt;!-- 指定MR运行在YARN上 --&gt;&lt;!-- 默认是local，本地文件 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 启动集群 启动前必须保证NameNode和DataNode已启动 启动ResourceManager和NodeManager：sbin/start-yarn.sh(关闭stop) 集群操作 yarn浏览器页面查看：8088端口 删除文件系统上的output文件：bin/hdfs dfs -rm -r /user/sobxiong/output 执行MapReduce程序：同上hadoop操作 查看结果：同上cat操作，也可以在浏览器端查看 配置历史服务器 配置mapred-site.xml： 1234567891011&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;172.16.85.130:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;172.16.85.130:19888&lt;/value&gt;&lt;/property&gt; 启动历史服务器：bin/mapred –daemon start historyserver(stop关闭) 查看历史服务器是否启动：jps 查看JobHistory：http://172.16.85.130:19888/jobhistory 配置日志的聚集： 概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上 好处：可以方便的查看到程序运行详情，方便开发调试 注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryManager 配置yarn-site.xml： 1234567891011&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 配置文件说明 默认配置文件： core-defalut.xml - hadoop-common-3.1.3.jar/core-default.xml hdfs-default.xml - hadoop-hdfs-3.1.3.jar/hdfs-default.xml yarn-default.xml - hadoop-yarn-common-3.1.3.jar/yarn-default.xml mapred-default.xml - hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml 自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置(优先级高) 完全分布式运行模式 虚拟机准备(3台，完全复制) 编写集群分发脚本xsync scp(secure copy)安全拷贝 定义：scp可以实现服务器与服务器之间的数据拷贝 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 scp -r(递归) $pdir/$fname $user@$host:$pdir/$fname rsync远程同步工具 作用：主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点 rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 rsync -r(递归)v(显示复制过程)l(拷贝符号连接) $pdir/$fname $user@$host:$pdir/$fname xsync集群分发脚本 需求：循环复制文件到所有节点的相同目录下 需求分析： rsync命令原始：rsync -rvl /opt/module root@hadoop2:/opt/ 期望脚本：xsync 需同步的文件名 说明：在home/sobxiong/bin这个目录下存放的脚本，sobxiong用户在系统任何地方都可以直接执行 脚本实现 在/home/sobxiong目录下创建bin目录，并在bin目录下创建xsync文件 在xsync中键入如下代码： 12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=2; host&lt;4; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 修改脚本xsync具有执行权限：chmod 777 xsync 调用脚本形式：xsync 文件名 集群配置 集群部署规划： 类型 hadoop1 hadoop2 hadoop3 HDFS NameNode、DataNode DataNode SecondaryNameNode、DataNode YARN NodeManager ResourceManager、NodeManager NodeManager 配置集群 核心配置文件core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; HDFS配置文件hdfs-site.xml 1234567891011&lt;!-- 副本数目 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop3:50090&lt;/value&gt;&lt;/property&gt; YARN配置文件yarn-site.xml 1234567891011&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop2&lt;/value&gt;&lt;/property&gt; MapReduce配置文件mapred-site.xml 12345&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 在集群上分发配置好的hadoop配置文件：xsync /opt/module/hadoop-3.1.3/etc 查看文件分发情况 集群单点启动 集群第一次启动，需要格式化NameNode：hadoop namenode -format(把log和data文件删除) 在hadoop1上启动NameNode：hadoop-daemon.sh start namenode 在hadoop1、2、3上分别启动DataNode：hadoop-daemon.sh start datanode SSH免密登陆配置 配置ssh 基本语法：ssh ip 无密钥配置 免密登录原理： 生成公钥和私钥：ssh-keygen -t rsa(生成文件id_rsa-私钥,id_rsa.pub-公钥) 将公钥拷贝到要免密登录的目标机器上：ssh-copy-id hadoop2(只是当前用户,root还需要另外配置) .ssh文件下(~/.ssh)的文件功能 known_hosts：记录ssh访问过的计算机的公钥 id_rsa：生成的私钥 id_rsa.pub：生成的公钥 authorized_keys：存放授权过的无密登录服务器公钥 群起集群 配置workers：vim etc/hadoop/workers；加入hadoop1、hadoop2、hadoop3(不能有空行和空格)；同步所有节点配置文件 - xsync etc/hadoop/workers 启动集群 集群第一次启动，需要格式化NameNode(格式化前关闭启动的所有namenode和datanode进程,然后再删除data和log数据)：bin/hdfs namenode -format 启动HDFS：sbin/start-dfs.sh(在hadoop1上启动,这样hadoo1、2、3均会启动对应的进程) 启动YARN：sbin/start-yarn.sh(在hadoop2上启动,在ResourceManager所在机器hadoop2上启动Yarn) 查看NameNode：hadoop1:9870 集群基本测试 上传文件到集群：bin/hdfs dfs -put xx xx 查看上传文件存储位置 查看HDFS文件存储路径：/opt/module/hadoop-3.1.3/data/tmp/dfs/data/current/BP-1002151198-172.16.85.130-1591848799222/current/finalized/subdir0/subdir0 查看HDFS在磁盘存储文件的内容：cat blk_xxx(文本文件) 拼接大文件：cat blk_xxx &gt;&gt; temp，最后temp就是初始的文件 集群启动/停止方式总结 各个服务组件逐一启动/停止 分别启动/停止HDFS组件：hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode 启动/停止YARN：yarn-daemon.sh start/stop resourcemanager/nodemanager 各个模块分开启动/停止(配置ssh是前提)常用 整体启动/停止HDFS：start-dfs.sh/stop-dfs.sh 整体启动/停止YARN：start-yarn.sh/stop-yarn.sh 集群时间同步 crontab定时任务： 基本语法：crontab[选项] 选项说明 -e：编辑crontab定时任务 -l：查询crontab任务 -r：删除当前用户所有的crontab任务 参数说明：***** [任务] *的含义： 第一个：一小时当中的第几分钟(0~59) 第二个：一天当中的第几个小时(0~23) 第三个：一个月当中的第几天(1~31) 第四个：一年当中的第几月(1~12) 第五个：一周当中的星期几(0~7,0和7均代表星期日) 特殊符号： ：代表任何时间。比如第一个“”代表一小时中每分钟都执行一次 ,：代表不连续的时间。如“0 8,12,16 * * *”命令，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 -：代表连续的时间范围。比如“0 5 * * 1-6”命令，代表在周一到周六的凌晨5点0分执行命令 /n：代表每隔多久执行一次。比如“/10 * * * *”命令，代表每隔10分钟就执行一遍命令 ntp方式进行同步 具体思路：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 具体实操 时间服务器配置： 检查ntp是否安装：rpm -qa&#124;grep ntp，有ntp、fontpackages-filesystem以及ntpdate 修改ntp配置文件 123456789101112# 授权1172.16.85.0-172.16.85.255网段上的所有机器可以从这台机器上查询和同步时间restrict 172.16.85.0 mask 172.16.85.130.0 nomodify notrap# 修改集群在局域网中,不使用其他互联网上的时间#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst# 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步server 127.127.1.0fudge 127.127.1.0 stratum 10 修改/etc/sysconfig/ntpd文件：SYNC_HWCLOCK=yes(让硬件时间与系统时间一起同步) 重新启动ntpd服务： 12service ntpd statusservice ntpd start 设置ntpd服务开机自启动：chkconfig ntpd on 其他机器配置(root用户)： 配置10分钟与时间服务器同步一次： 12crontab -e*/10 * * * * /usr/sbin/ntpdate hadoop1 修改任意机器时间：date -s “2020-11-11 11:11:11” 十分钟后查看机器是否与时间服务器同步：date Hadoop编译源码 前期准备 jar包准备(hadoop源码、JDK8、Maven、Ant、Protobuf)：Protobuf在Google的github中的Release页面(3.1.3Hadoop对应2.5.0版本) jar包安装 安装JDK 12345678tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module/# JAVA_HOME(/etc/profile)export JAVA_HOME=/opt/module/jdk1.8.0_251export PATH=$PATH:$JAVA_HOME/binsource /etc/profilejava -version 安装Maven 12345678910111213141516tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/# MAVEN_HOME(/etc/profile)export MAVEN_HOME=/opt/module/apache-maven-3.6.3export PATH=$PATH:$MAVEN_HOME/binsource /etc/profilemvn -version# 修改maven仓库镜像&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 安装Ant 12345678tar -zxvf apache-ant-1.10.8-bin.tar.gz -C /opt/module/# ANT_HOME(/etc/profile)export ANT_HOME=/opt/module/apache-ant-1.10.8export PATH=$PATH:$ANT_HOME/binsource /etc/profileant -version 安装glibc-headers和g++：yum install glibc-headers、yum install gcc-c++ 安装make和cmake：yum install make 安装cmake(要装3.x版本,低版本编译不通过) 1234567891011tar -zxvf cmake-3.17.3.tar.gz -C /opt/modulecd /opt/module/cmake-3.17.3./configuremakemake install# CMAKE_HOME(/etc/profile)export CMAKE_HOME=/opt/module/cmake-3.17.3export PATH=$PATH:$CMAKE_HOME/binsource /etc/profilecmake --version 安装protobuf： 12345678910111213tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/cd /opt/module/protobuf-2.5.0/./configuremakemake checkmake installldconfig# LD_LIBRARY_PATH(/etc/profile)export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0export PATH=$PATH:$LD_LIBRARY_PATHprotoc --version 安装openssl库：yum install openssl-devel 安装ncurses-devel库：yum install ncurses-devel 编译源码 解压源码到/opt目录 进入hadoop源码主目录 通过maven执行编译命令：mvn package -Pdist,native -DskipTests -Dtar HDFS概述 HDFS产出背景及定义 产生背景：随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种 定义：HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色 使用背景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用 HDFS优缺点 优点： 高容错性 数据自动保存多个副本。它通过增加副本的形式，提高容错性 某一个副本丢失以后，它可以自动恢复(通过将副本复制到另一个可用的节点) 适合处理大数据 数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据 文件规模：能够处理百万规模以上的文件数量，数量相当之大 可构建在廉价机器上，通过多副本机制，提高可靠性 缺点： 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的 无法高效的对大量小文件进行存储： 存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的 小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标 不支持并发写入、文件随机修改： 一个文件只能有一个写，不允许多个线程同时写 仅支持数据appen(追加)，不支持文件的随机修改 HDFS组成架构 NameNode(nn)：Master，一个主管、管理者 管理HDFS的名称空间 配置副本策略 管理数据块(Block)映射信息 处理客户端读写请求 DataNode：Slave。NameNode下达命令，DataNode执行实际的操作 存储实际的数据块 执行数据块的读/写操作 Client：客户端 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传 与NameNode交互，获取文件的位置信息 与DataNode交互，读取或者写入数据 Client提供一些命令来管理HDFS，比如NameNode格式化 Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作 Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务 辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode 在紧急情况下，可辅助恢复NameNode HDFS文件块大小HDFS中的文件在物理上是分块存储(Block)，块的大小可以通过配置参数(dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M为什么文件块的大小不能设置太小，也不能设置太大？ HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢 总结：HDFS块的大小设置主要取决于磁盘传输速率 HDFS的Shell操作 基本语法bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令其中dfs是fs的实现类 命令大全：bin/hadoop fs 使用命令： -help：输出命令的帮助(hadoop fs -help rm) -ls：显示目录信息(hadoop fs -ls /) -mkdir：在HDFS上创建目录[-p递归](hdoop fs -mkdir -p /sobxiong/test) -moveFromLocal：从本地剪切粘贴到HDFS[前路径为本地,后路径为HDFS](hadoop fs -moveFromLocal ./test.txt /sobxiong/test/) -appendToFile：追加一个文件到已经存在的文件末尾[前路径为本地,后路径为HDFS](hadoop fs -appendToFile ./test.txt /sobxiong/test/test.txt) -cat：显示文件内容(hadoop fs -cat /sobxiong/test/test.txt) -chgrp、-chmod、-chown：修改文件所属的权限，同Linux文件系统中的用法 -copyFromLocal：从本地文件系统拷贝文件到HDFS中，同-moveFromLocal -copyToLocal：从HDFS拷贝文件到本地[前路径为HDFS,后路径为本地](hadoop fs -copyToLocal /sobxiong/test/test.txt ./) -cp：把文件从HDFS的一个路径拷贝到HDFS的另一个路径 -mv：把文件从HDFS的一个路径移动到HDFS的另一个路径 -get：等同于copyToLocal(用法同copyToLocal)，从HDFS下载文件到本地 -getmerge：合并下载多个文件(hadoop fs -getmerge /sobxiong/test/* ./all.txt) -put：等同于copyFromLocal(用法同copyFromLocal) -tail：显示一个文件的末尾(hadoop fs -tail /sobxiong/test/test.txt) -rm：删除文件或文件夹[-r递归删除目录] -rmdir：删除空目录 -du：统计文件夹的大小信息[-h显示单位,-s总和](hadoop fs -du -h -s /) -setrep：设置HDFS中文件的副本数目[这里设置的副本数只是记录在NameNode的元数据中,是否真的会有这么多副本还得看DataNode的数量.因为目前只有3台设备,最多也就3个副本,只有节点数的增加到10台时,副本数才能达到10;只要加入一台设备,就会把副本复制到设备上,直到加到10台](hadoop fs -setrep 10 /sobxiong/test/test.txt) HDFS客户端操作 客户端环境准备 将Hadoop安装到mac上，并设置环境变量 123456# HADOOP_HOME(~/.bash_profile)export HADOOP_HOME=\"/Users/sobxiong/module/hadoop-3.1.3\"export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource ~/.bash_profilehadoop version 创建Maven工程测试：idea创建quickstart项目 导入依赖： 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt; 创建测试类 12345678910111213141516public class HDFSClient &#123; public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException &#123; Configuration configuration = new Configuration(); // configuration.set(\"fs.defaultFS\", \"hdfs://hadoop1:9000\"); // 1、获取hdfs客户端对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、在hdfs上创建路径 fileSystem.mkdirs(new Path(\"/sobxiong2/test\")); // 3、关闭资源 fileSystem.close(); System.out.println(\"finish\"); &#125;&#125; HDFS的API操作 文件上传 12345678910111213141516171819202122232425262728293031/** * 参数优先级： * 1、客户端代码中设置的值 * 2、ClassPath(resources)下的用户自定义配置文件(hdfs-site.xml) * 3、服务器的默认配置 * @throws Exception */// 1、文件上传@Testpublic void testCopyFromLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); configuration.set(\"dfs.replication\", \"2\"); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行上传API fileSystem.copyFromLocalFile(new Path(\"/Users/sobxiong/Documents/文件块大小大致计算.png\"), new Path(\"/sobxiong/test2.png\")); // 3、关闭资源 fileSystem.close();&#125;// 将hdfs-site.xml拷贝至项目的根目录resources资源文件夹下&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 文件下载 123456789101112131415// 2、文件下载@Testpublic void testCopyToLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行下载操作 // fileSystem.copyToLocalFile(new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test.png\")); // 本地模式,true,不会产生crc文件 fileSystem.copyToLocalFile(false, new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test1.png\"), true); // 3、关闭资源 fileSystem.close();&#125; 文件删除 12345678910111213// 3、文件删除@Testpublic void testDelete() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、文件删除(第二个参数,是否递归删除,文件夹时有效) fileSystem.delete(new Path(\"/sobxiong/test2.png\"), false); // 3、关闭资源 fileSystem.close();&#125; 文件更名 12345678910111213// 4、文件更名@Testpublic void testRename() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行更名操作 fileSystem.rename(new Path(\"/sobxiong/test1.png\"), new Path(\"/sobxiong/1tset.png\")); // 3、关闭资源 fileSystem.close();&#125; 文件详情查看 1234567891011121314151617181920212223242526272829303132// 5、文件详情查看@Testpublic void testListFiles() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、查看文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(new Path(\"/\"), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); // 查看文件名称、权限、长度 System.out.println(\"name: \" + fileStatus.getPath().getName()); System.out.println(\"permission: \" + fileStatus.getPermission()); System.out.println(\"length: \" + fileStatus.getLen()); // 查看块信息 BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(\"host = \" + host); &#125; System.out.println(\"----------------\"); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; 判断是文件还是文件夹 1234567891011121314151617181920// 6、判断是文件还是文件夹@Testpublic void testListStatus() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、判断操作 FileStatus[] fileStatuses = fileSystem.listStatus(new Path(\"/\")); for (FileStatus fileStatus : fileStatuses) &#123; if (fileStatus.isFile()) &#123; System.out.println(\"file = \" + fileStatus.getPath().getName()); &#125; else &#123; System.out.println(\"dir = \" + fileStatus.getPath().getName()); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; HDFS的I/O流操作 HDFS文件上传 1234567891011121314151617// 把本地文件上传到HDFS根目录@Testpublic void upload() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FileInputStream fileInputStream = new FileInputStream(new File(\"/Users/sobxiong/Downloads/课件.rar\")); // 3、获取输出流 FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path(\"/test.rar\")); // 4、流的对拷 IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fsDataOutputStream); IOUtils.closeStream(fileInputStream); fileSystem.close();&#125; HDFS文件下载 1234567891011121314151617// 从HDFS下载文件到本地磁盘@Testpublic void download() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/test.rar\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/test1.rar\")); // 4、流的对拷 IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125; 定位文件获取 12345678910111213141516171819202122232425262728293031323334353637383940414243// 下载第一块@Testpublic void readFileSeek1() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part1\")); // 4、流的对拷(只拷贝第一个块128MB) byte[] buf = new byte[1024]; for (int i = 0; i &lt; 1024 * 128; i++) &#123; fsDataInputStream.read(buf); fileOutputStream.write(buf); &#125; // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载第二块@Testpublic void readFileSeek2() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、设置指定读取的起点 fsDataInputStream.seek(1024 * 1024 * 128); // 4、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part2\")); // 5、流的对拷(拷贝剩下的两个Block块) IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 6、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载完第一块和剩余的部分后,可通过cat hadoop-3.1.3.tar.gz.part2 &gt;&gt; hadoop-3.1.3.tar.gz.part1将剩余部分追加到第一块上,修改文件名(删去.part1),就得到完整的文件 HDFS的数据流 HDFS写数据流程 剖析文件写入： 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在 NameNode返回是否可以上传 客户端请求第一个Block上传到哪几个DataNode服务器上 NameNode返回3个DataNode节点，分别为dn1、dn2、dn3 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成 dn1、dn2、dn3逐级应答客户端 客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器(此后重复执行3-7步) 网络拓扑-节点距离计算在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。节点距离：两个节点到达最近的共同祖先的距离总和 机架感知(2.7.2版本副本节点选择,性能和安全的综合考量) 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个 第二个副本和第一个副本位于相同机架，随机节点 第三个副本位于不同机架，随机节点 HDFS读数据流程 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址 挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据 DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验) 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件 NameNode和SecondaryNameNode NN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并 第一阶段：NameNode启动 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存 客户端对元数据进行增删改的请求 NameNode记录操作日志，更新滚动日志(先记日志,类似数据库) NameNode在内存中对数据进行增删改 第二阶段：Secondary NameNode工作 Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果 Secondary NameNode请求执行CheckPoint NameNode滚动正在写的Edits日志 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode Secondary NameNode加载编辑日志和镜像文件到内存，并合并 生成新的镜像文件fsimage.chkpoint 拷贝fsimage.chkpoint到NameNode NameNode将fsimage.chkpoint重新命名成fsimage 补充：Fsimage：NameNode内存中元数据序列化后形成的文件。Edits：记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)。NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息)，如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并(所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage)。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。SecondaryNameNode首先会询问NameNode是否需要CheckPoint(触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了)。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中 Fsimage和Edits解析 概念 NameNode被格式化之后，将在/data/tmp/dfs/name/current目录中产生如下文件fsimage_0000000000000000000fsimage_0000000000000000000.md5seen_txidVERSION Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息 Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中 seen_txid文件保存的是一个数字，就是最后一个edits_的数字 每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并 查看Fsimage文件：hdfs oiv -p 文件类型(XML) -i 镜像文件 -o 转换后文件输出路径例：hdfs oiv -p XML -i fsimage_0000000000000000025 o fsimage.xmlFsimage中没有记录块所对应的DataNode，为什么？在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报 查看Edits文件：hdfs oev -p 文件类型(XML) -i 编辑日志 -o 转换后文件输出路径例：hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o edits.xmlNameNode如何确定下次开机启动的时候合并那些Edits？通过seen_txid查看 CheckPoint时间设置 通常情况下，SecondaryNameNode每隔一小时执行一次 一分钟检查一次操作次数 当操作次数达到1百万时，SecondaryNameNode执行一次 123456789101112131415&lt;!-- hdfs-default.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt;1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; NameNode故障处理 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录 kiil -9 NameNode进程编号(用jps查看NameNode的进程编号) 删除NameNode存储的数据(data/tmp/dfs/name)：rm -rf /data/tmp/dfs/name/* 拷贝SecondaryNameNode(hadoop2)中数据到原NameNode(hadoop1)存储数据目录：scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary/* ./name/ 重新启动NameNode(hadoop1)：sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中 修改hdfs-site.xml(加入下述内容)： 123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; kill -9 NameNode进程 删除NameNode存储的数据(同方法一) 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 123scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary /data/tmp/dfs/cd /data/tmp/dfs/namesecondaryrm -rf in_use.lock 导入检查点数据(等待一会ctrl+c结束掉) 启动NameNode：sbin/hadoop-daemon.sh start namenode 集群安全模式 概述 NameNode启动NameNode启动时，首先将镜像文件(Fsimage)载入内存，并执行编辑日志(Edits)中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的 DataNode启动 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统 安全模式退出判断如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值：dfs.replication.min=1)。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式 基本语法集群处于安全模式，不能执行重要操作(写操作)。集群启动完成后，自动退出安全模式 查看安全模式状态：bin/hdfs dfsadmin -safemode get 进入安全模式状态：bin/hdfs dfsadmin -safemode enter 离开安全模式状态：bin/hdfs dfsadmin -safemode leave 等待安全模式状态：bin/hdfs dfsadmin -safemode wait 案例模拟等待安全模式 查看当前模式：bin/hdfs dfsadmin -safemode get 先进入安全模式：bin/hdfs dfsadmin -safemode enter 创建并执行下面的脚本 12345678910touch safemode.shvim safemode.sh# safemode.sh#!/bin/bashhdfs dfsadmin -safemode waithdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /chmod 777 safemode.sh./safemode.sh 再打开一个窗口，执行：hdfs dfsadmin -safemode leave 安全模式退出，HDFS集群上已经有上传的数据了 NameNode多目录配置 NameNode的本地目录可以配置成多个，但每个目录存放内容相同(相当于备份)，增加了可靠性 具体配置如下 在hdfs-site.xml文件中增加如下内容 1234&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 停止集群，删除data和logs中所有数据 12345hadoop2：sbin/stop-yarn.shhadoop2：rm -rf data/ logs/hadoop1：sbin/stop-dfs.shhadoop2：rm -rf data/ logs/shadoop3：rm -rf data/ logs/s 格式化集群并启动 123hadoop1：bin/hdfs namenode -formathadoop1：sbin/start-dfs.shhadoop2：sbin/start-yarn.sh 查看结果：dfs目录下出现两个目录name1和name2 DataNode DataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳 DataNode启动后向NameNode注册，通过后，周期性(1小时)的向NameNode上报所有的块信息 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用 集群运行中可以安全加入和退出一些机器 数据完整性DataNode节点保证数据完整性的方法： 当DataNode读取Block的时候，它会计算CheckSum(类似crc校验位) 如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏 Client读取其他DataNode上的Block DataNode在其文件创建后周期验证CheckSum 掉线时限参数设置\bhdfs-default.xml： 12345678910&lt;!-- heartbeat.recheck.interval单位为毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;!-- dfs.heartbeat.interval单位为秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 服役新数据节点(hadoop4未服役) 需求：随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点 环境准备 利用hadoop3主机再克隆一台hadoop4主机 修改hadoop4主机IP地址和主机名称 在hadoop1主机上将/etc/hosts下添加hadoop4的ip地址映射条目，并分发到hadoop2-4 hadoop4主机删除原来HDFS文件系统留存的文件(data和log目录)——不然会发生3和4轮换出现的问题,因为3和4有着一样的data和log reboot重启加载配置 服役新节点具体步骤 hadoop1-3按之前步骤已启动 在hadoop4主机上单独启动： 12hdfs --daemon start datanodeyarn --daemon start nodemanager 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 刷新http://hadoop1:9870web页面，等待 在hadoop4上上传文件 如果数据不均衡，可以使用命令实现集群的在平衡：sbin/start-balancer.sh 结束后在workers文件中加入hadoop4，之后直接start-dfs.sh和start-yarn.sh即可启动 退役旧数据节点(hadoop4未退役) 添加白名单(hadoop4未退役)添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出 在NameNode的hadoop-3.1.3/etc/hadoop目录下创建dfs.hosts文件 1234567touch dfs.hostsvim dfs.hosts# dfs.hosts(不添加hadoop4,不允许有空行和空格)hadoop1hadoop2hadoop3 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 配置文件分发：xsync hdfs-site.xml;xsync dfs.hosts 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 在web页面刷新等待 黑名单退役(hadoop4未退役)在黑名单上面的主机都会被强制退出。注意：不允许白名单和黑名单中同时出现同一个主机名称 在hadoop-3.1.3/etc/hadoop下创建dfs.hosts.exclude文件，并加入要退役节点 12345touch dfs.hosts.excludevim dfs.hosts.exclude# dfs.hosts.excludehadoop4 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 刷新web页面等待 DataNode多目录配置 DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 需要在hdfs-site.xml上修改配置 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; 关闭当前运行的hadoop节点 删除各节点的data和log目录 格式化dataNode 重启部署hadoop节点 HDFS2.X新特性 集群间数据拷贝 scp实现两个远程主机之间的文件复制 123456# 从当前主机向目的主机 推 pushscp -r hello.txt root@hadoop2:/user/sobxiong/hello.txt# 从目的主机向当前主机 拉 pullscp -r root@hadoop2:/user/sobxiong/hello.txt hello.txt# 通过本主机中转实现两个远程主机的文件复制scp -r root@hadoop2:/user/sobxiong/hello.txt root@hadoop3:/user/sobxiong 采用distcp命令实现两个Hadoop集群之间的递归数据复制 1hadoop distcp hdfs://haoop1:9000/user/sobxiong/hello.txt hdfs://hadoop2:9000/user/sobxiong/hello.txt 小文件存档 HDFS存储小文件弊端每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB 解决存储小文件办法之一HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存 实际操作 启动YARN进程：start-yarn.sh(hadoop2) 归档文件(归档后的路径不得实现存在)把/sobxiong目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/sobxiongOutput路径下：hadoop archive -archiveName input.har -p /sobxiong /sobxiongOutput 查看归档： 123456789101112# 普通查看文件命令hadoop fs -ls R /sobxiongOutput/input.har# -rw-r--r-- 3 sobxiong supergroup 0 2020-06-24 16:06 /sobxiongOutput/input.har/_SUCCESS# -rw-r--r-- 3 sobxiong supergroup 305 2020-06-24 16:06 /sobxiongOutput/input.har/_index# -rw-r--r-- 3 sobxiong supergroup 23 2020-06-24 16:06 /sobxiongOutput/input.har/_masterindex# -rw-r--r-- 3 sobxiong supergroup 317029 2020-06-24 16:06 /sobxiongOutput/input.har/part-0# 采用har格式查看文件(可以像以往一样操作内部文件,也需要har格式)hadoop fs -ls R har:///sobxiongOutput/input.har# -rw-r--r-- 3 sobxiong supergroup 84942 2020-06-21 11:38 har:///sobxiongOutput/input.har/1tset.png# -rw-r--r-- 3 sobxiong supergroup 84942 2020-06-21 11:33 har:///sobxiongOutput/input.har/test.png# -rw-r--r-- 3 sobxiong supergroup 147145 2020-06-23 13:52 har:///sobxiongOutput/input.har/test6.txt 解归档文件：hadoop fs -cp har:///sobxiongOutput/input.har/* / 回收站开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用 开启回收站功能参数说明： 默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间(分钟为单位) 默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。该值设置和fs.trash.interval的参数值相同(要求fs.trash.checkpoint.interval &lt;= fs.trash.interval) 回收站工作机制 启动回收站：修改core-site.xml，配置垃圾回收时间为1分钟 1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 查看回收站：回收站在集群中的路径：/user/sobxiong/.Trash/ 修改访问回收站用户名称：进入垃圾回收站用户名称，默认是dr.who，修改为sobxiong(同样是core-site.xml) 1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt; 恢复回收站数据：hadoop fs -mv /user/sobxiong/.Trash/Current/user/sobxiong/input / 清空回收站：hadoop fs -expunge 快照管理 命令介绍 实际操作 开启/禁用指定目录的快照功能：hdfs dfsadmin -allowSnapshot(-disallowSnapshot) /user/sobxiong/input 对目录创建快照 1234567hdfs dfs -createSnapshot /user/sobxiong/input# 快照和源文件使用相同数据hdfs dfs -ls R /user/sobxiong/input/.snapshot/# 指定名称创建快照hdfs dfs -createSnapshot /user/sobxiong/input test 重命名快照：hdfs dfs -renameSnapshot /user/sobxiong/input test test01 列出当前用户所有可快照目录：hdfs lsSnapshottableDir 比较两个快照目录的不同之处(可以是源文件和快照,使用’.’,此时”.snapshot/name”用于指定具体快照)：hdfs snapshotDiff /user/sobxiong/input . .snapshot/test01 恢复快照：hdfs dfs -cp /user/sobxiong/input/.snapshot/s20200624-134303.027 / MapReduce概述 MapReduce定义 MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上 MapReduce优缺点 优点： MapReduce易于编程它简单地实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行 良好的扩展性(hadoop)当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力 高容错性MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的 适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力 缺点： 不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果 擅长流式计算流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的 不擅长DAG(有向图)计算多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下 核心思想 MapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调 MapTask：负责Map阶段的整个数据处理流程 ReuceTask：负责Reduce阶段的整个数据处理流程 常用数据序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable MapReduce编程规范用户编写的程序分成三个部分：Mapper、Reducer和Driver Mapper 用户自定义的Mapper要继承自己的父类 Mapper的输入数据是KV对的形式(KV的类型可自定义) Mapper中的业务逻辑写在map()方法中 Mapper的输出数据是KV对的形式(KV的类型可自定义) map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次 Reducer 用户自定义的Reducer要继承自己的父类 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 Driver：相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象 WordCount案例实操 需求：在给定的文本文件中统计输出每一个单词出现的总次数 需求分析 环境准备 创建maven空项目 改pom 123456789101112131415161718192021222324252627282930&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.13.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt; 在resources资源文件夹下新建log4j.properties文件 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 编写MapReduce程序 编写mapper类 12345678910111213141516171819202122232425262728/*** map阶段* KEYIN：输入数据的key类型(默认写LongWritable:偏移量)* VALUEIN：输入数据的value类型* KEYOUT：输出数据的key类型* VALUEOUT：输出数据的value类型* &lt;sobxiong,1&gt;输出数据* 输出作为reduce阶段的输入*/public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private Text k = new Text(); private IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // sobxiong sobxiong // 1、获取一行 String lineStr = value.toString(); // 2、切割单词 String[] words = lineStr.split(\" \"); // 3、循环写出 for (String word : words) &#123; // &lt;sobxiong,1&gt; k.set(word); context.write(k, v); &#125; &#125;&#125; 编写Reducer类 1234567891011121314151617181920/*** reduce阶段* KEYIN,VALUEIN：map阶段输出的kv* KEYOUT,VALUEOUT：reduce输出的kv*/public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; // 1、累加求和 for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 &lt;sobxiong,2&gt; v.set(sum); context.write(key, v); &#125;&#125; 编写Driver驱动类 123456789101112131415161718192021222324252627public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设置jar存储位置 job.setJarByClass(WordCountDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job // job.submit(); // true：打印一些信息 boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1); &#125;&#125; 本地测试：启动旁边的下箭头Edit Configuration，新增Application，选择Main Class并在Program arguments中加入两个参数中间用空格隔开，前者是input文件所在文件夹，后者是输出文件夹，要求不能存在(不然会出错)。例如：/Users/sobxiong/Downloads/input /Users/sobxiong/Downloads/ouputTest 在集群上测试 用maven打jar包，添加打包插件依赖 12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.xiong.hadoop.WordCountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 将程序打成jar包，maven install即可 将获取的两个jar包(一个不带依赖,另一带)，将不带依赖的jar上传到hadoop1主机上 启动Hadoop集群 执行WordCount程序：hadoop jar WordCount.jar com.xiong.hadoop.WordCountDriver /sobxiong /outputTest(第四个参数为启动的主类名,第五个为输入文件所在文件夹,第六个为输出文件夹——不能事先存在) Hadoop序列化 序列化概述 什么是序列化：序列化就是把内存中的对象，转换成字节序列(或其他数据传输协议)以便于存储到磁盘(持久化)和网络传输；反序列化就是将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象 为什么要序列化：一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机 为什么不用Java的序列化：Java的序列化是一个重量级序列化框架(Serializable)，一个对象被序列化后，会附带很多额外的信息(各种校验信息，Header，继承体系等)，不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制(Writable) Hadoop序列化特点： 紧凑：高效使用存储空间 快速：读写数据的额外开销小 可扩展：随着通信协议的升级而可升级 互操作：支持多语言的交互 自定义bean对象实现序列化接口(Writable)在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。具体实现bean对象序列化步骤如下7步： 实现Writable接口 反序列化时，需要反射调用空参构造函数，所以必须有空参构造 重写序列化方法 重写反序列化方法(注意反序列化的顺序和序列化的顺序完全一致) 要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用 如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框架中的Shuffle过程要求对key必须能排序 序列化案例实操 需求：统计每一个手机号耗费的总上行流量、下行流量、总流量 案例分析 编写MapReduce程序 编写流量统计的Bean对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*测试数据1;13736230513;192.196.100.1;www.atguigu.com;2481;24681;2002;13846544121;192.196.100.2;264;0;2003;13956435636;192.196.100.3;132;1512;2004;13966251146;192.168.100.1;240;0;4045;18271575951;192.168.100.2;www.atguigu.com;1527;2106;2006;84188413;192.168.100.3;www.atguigu.com;4116;1432;2007;13590439668;192.168.100.4;1116;954;2008;15910133277;192.168.100.5;wwww.haol23.com;3156;2936;2009;13729199489;192.168.100.6;240;0;20010;13630577991;192.168.100.7;www.shouhu.com;6960;690;20011;15043685818;192.168.100.8;www.baidu.com;3659;3538;20012;15959002129;192.168.100.9;www.atguigu.com;1938;180;50013;13560439638;192.168.100.10;918;4938;20014;13470253144;192.168.100.11;180;180;20015;13682846555;192.168.100.12;wwww.qq.com;1938;2910;20016;13992314666;192.168.100.13;www.gaga.com;3008;3720;20017;13509468723;192.168.100.14;www.qinghua.com;7335;110349;40418;18390173782;192.168.100.15;www.sogou.com;9531;2412;20019;13975057813;192.168.100.16;www.baidu.com;11058;48243;20020;13768778790;192.168.100.17;120;120;20021;13568436656;192.168.100.18;www.alibaba.com;2481;24681;20022;13568436656;192.168.100.19;1116;954;200*/public class FlowBean implements Writable &#123; // 上行流量 private long upFlow; // 下行流量 private long downFlow; // 总流量 private long sumFlow; // 空参构造,为了后续反射 public FlowBean() &#123; &#125; // 序列化方法 @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeLong(upFlow); dataOutput.writeLong(downFlow); dataOutput.writeLong(sumFlow); &#125; // 反序列化方法 @Override public void readFields(DataInput dataInput) throws IOException &#123; // 必须要求和序列化方法顺序一致 upFlow = dataInput.readLong(); downFlow = dataInput.readLong(); sumFlow = dataInput.readLong(); &#125; @Override public String toString() &#123; return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125;&#125; 编写mapper类 123456789101112131415161718public class FlowMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; private Text k = new Text(); private FlowBean flowBean = new FlowBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割\\t String[] fields = lineStr.split(\"\\t\"); // 3、封装对象 k.set(fields[1]); long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); flowBean.set(upFlow, downFlow); // 4、写出 context.write(k, flowBean); &#125;&#125; 编写Reducer类 12345678910111213141516public class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; private FlowBean v = new FlowBean(); @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 long sumUpFlow = 0; long sumDownFlow = 0; for (FlowBean value : values) &#123; sumUpFlow += value.getUpFlow(); sumDownFlow += value.getDownFlow(); &#125; // 2、写出 v.set(sumUpFlow, sumDownFlow); context.write(key, v); &#125;&#125; 编写Driver驱动类 12345678910111213141516171819202122public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设计jar路径 job.setJarByClass(FlowDriver.class); // 3、关联mapper和reducer job.setMapperClass(FlowMapper.class); job.setReducerClass(FlowReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce框架原理 InputFormat数据输入 切片与MapTask并行度决定机制 问题引出：MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ MapTask并行度决定机制 数据块：Block是HDFS物理上把数据分成一块一块数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储 Job提交流程源码和切片源码详解 Job提交流程源码详解 1234567891011121314151617181920212223242526waitForCompletion();submit(); // 1、建立连接 connect(); // 1)创建提交Job的代理 new Cluster(getConfiguration()); // 判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2、提交job submitter.submitJobInternal(Job.this, cluster); // 1)创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2)获取jobid,并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3)拷贝jar包到集群 copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // 4)计算切片,生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job); // 5)向Stag路径写XML配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 6)提交Job,返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); FileInputFormat切片源码解析(input.getSplits(job)) 程序先找到你数据存储的目录 开始遍历处理(规划切片)目录下的每一个文件 遍历第一个文件ss.txt 获取文件大小fs.sizeOf(ss.txt) 计算切片大小：computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M(YARN集群默认128M——2.x,62M-1.x;本地运行默认32M) 默认情况下，切片大小=blocksize 开始切，形成第1个切片：ss.txt—0:128M、第2个切片ss.txt—128:256M、第3个切片ss.txt—256M:300M(每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片) 将切片信息写到一个切片规划文件中 整个切片的核心过程在getSplit()方法中完成 InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等 提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数 FileInputFormat切片机制 切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于Block大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 案例分析 输入数据有两个文件：file1.txt - 320M;file2.txt - 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： 文件 切片区间 file1.txt.split1 0~128 file1.txt.split2 129~256 file1.txt.split3 257~320 file2.txt.split1 0~10 切片大小参数配置 源码中计算切片大小的公式：Math.max(minSize, Math.min(maxSize, blockSize));mapreduce.input.fileinputformat.split.minsize=1 默认值为1mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值Long.MAXValue 默认情况下，切片大小=blocksize 切片大小设置maxsize(切片最大值)：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值minsize(切片最小值)：参数调的比blockSize大，则可以让切片变得比blockSize还大 获取切片信息API 1234// 获取切片的文件名称String name = inputSplit.getPath().getName();// 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) context.getInputSplit(); CombineTextInputFormat切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下 应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理 虚拟存储切片最大值设置：CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);(注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值) 切片机制切片机制：生成切片过程包括虚拟存储过程和切片过程二部分 虚拟存储过程将输入目录下所有文件的大小依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块(防止出现太小切片)例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成(2.01M和2.01M)两个文件 切片过程 判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片 如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片 测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M、(2.55M、2.55M)、3.4M、(3.4M、3.4M)。最终会形成3个切片，大小分别为：(1.7M + 2.55M)，(2.55M + 3.4M)，(3.4M + 3.4M) CombineTextInputFormat案例实操 需求：将输入的大量小文件合并成一个切片统一处理 输入数据：准备4个小文件 期望：期望一个切片处理4个文件 实现过程 不做任何处理，运行之前的WordCount案例程序，观察切片个数为4——(number of splits:4) 在WordcountDriver中增加如下代码(设置切片最大值为4m)，运行程序，观察运行的切片个数为3 1234// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304); 在WordcountDriver中增加如下代码(设置切片最大值为20m)，运行程序，观察运行的切片个数为1(代码同上,值改为20971520) FileInputFormat实现类思考：在运行MapReduce程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢?FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等 TextInputFormatTextInputFormat是默认的FileInputFormat实现类。按行读取每条记录，键是存储该行在整个文件中的起始字节偏移量——LongWritable类型；值是这行的内容，不包括任何行终止符(换行符和回车符)——Text类型 1234567891011&#x2F;&#x2F; 示例：Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对：(0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为&lt;key,value&gt;对。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “\\t”)来设定分隔符——默认分隔符是tab(\\t) 1234567891011&#x2F;&#x2F; 示例(其中——&gt;表示一个水平方向的制表符)：line1 ——&gt;Rich learning formline2 ——&gt;Intelligent learning engineline3 ——&gt;Learning more convenientline4 ——&gt;From the real demand for more close to the enterprise&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对(键是每行排在制表符之前的Text序列)：(line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) NLineInputFormat如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按Block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数 / N = 切片数，如果不整除，切片数 = 商 + 1 123456789101112&#x2F;&#x2F; 示例：Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise&#x2F;&#x2F; 如果N是2,则每个输入分片包含两行。开启2个MapTask(键和值与TextInputFormat生成的一样)：(0,Rich learning form)(19,Intelligent learning engine)&#x2F;&#x2F; 另一个mapper则收到后两行：(47,Learning more convenient)(72,From the real demand for more close to the enterprise) KeyValueTextInputFormat使用案例 需求：统计输入文件中每一行的第一个单词相同的行数 案例分析 代码实现 Mapper类 123456789public class KVTextMapper extends Mapper&lt;Text, Text, Text, IntWritable&gt; &#123; private IntWritable intWritable = new IntWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、封装对象 // 2、写出 context.write(key, intWritable); &#125;&#125; Reducer类 1234567891011121314public class KVTextReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable intWritable = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 intWritable.set(sum); context.write(key, intWritable); &#125;&#125; Driver类 12345678910111213141516171819202122232425public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration configuration = new Configuration(); configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \"); Job job = Job.getInstance(configuration); // 2、设置jar存储位置 job.setJarByClass(KVTextDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setInputFormatClass(KeyValueTextInputFormat.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; NLineInputFormat使用案例 需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中 案例分析 代码实现 Mapper和Reducer类参照WordCount Driver类 123456789101112131415161718192021222324252627public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 设置切片InputSplit中划分三条记录 NLineInputFormat.setNumLinesPerSplit(job, 3); // 使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class); // 2、设置jar存储位置 job.setJarByClass(NLineDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(NLineMapper.class); job.setReducerClass(NLineReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; 观察控制台打印的number of splits 自定义InputFormat在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题自定义InputFormat步骤如下： 自定义一个类继承FileInputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件 自定义InputFormat案例实操 需求：将多个小文件合并成一个SequenceFile文件(SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式)，SequenceFile里面存储着多个文件，存储的形式为key——文件路径 + 名称，value——文件内容 案例分析 代码实现 自定义InputFormat 12345678public class WholeFileInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt; &#123; @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; 自定义RecordReader类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt; &#123; private FileSplit split; private Configuration configuration; private Text k = new Text(); private BytesWritable v = new BytesWritable(); boolean isProgress = true; @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; // 初始化 this.split = (FileSplit) split; configuration = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; // 核心业务逻辑 // 每个文件创建一次reader if (isProgress) &#123; // 1、获取fs对象 Path path = split.getPath(); FileSystem fileSystem = path.getFileSystem(configuration); // 2、获取输入流 FSDataInputStream fis = fileSystem.open(path); // 3、拷贝 byte[] buffer = new byte[(int) split.getLength()]; IOUtils.readFully(fis, buffer, 0, buffer.length); // 4、封装v v.set(buffer, 0, buffer.length); // 5、封装key k.set(path.toString()); // 6、关闭资源 IOUtils.closeStream(fis); isProgress = false; return true; &#125; return false; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return k; &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return v; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123;&#125;&#125; Mapper类 123456public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; Reducer类 123456789public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 循环写出 for (BytesWritable value : values) &#123; context.write(key, value); &#125; &#125;&#125; Driver类 123456789101112131415161718192021222324public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setInputFormatClass(WholeFileInputFormat.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); // 2、设计jar路径 job.setJarByClass(SequenceFileDriver.class); // 3、关联mapper和reducer job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce工作流程 流程示意图 流程详解上面的流程是整个MapReduce的全部工作流程，Shuffle过程是从第7步开始到第16步结束，具体Shuffle过程详解，如下： MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序 ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据 ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并(归并排序) 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程——从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法 注意Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M Shuffle Shuffle机制：Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle Partition分区 问题引出：要求将统计结果按照条件输出到不同文件中(分区)。比如：将统计结果按照手机归属地不同省份输出到不同文件中(分区) 默认Partitionr分区：默认分区是根据key的hashCode对ReduceTasks个数取模得到的(如果分区数大于1)。用户没法控制哪个key存储到哪个分区 1234567public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; &#123; public void configure(JobConf job) &#123;&#125; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 自定义Partitioner步骤 自定义类继承Partitioner，重写getPartition()方法 在Job驱动中，设置自定义Partitioner：job.setPartitionerClass(CustomPartitioner.class); 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask：job.setNumReduceTasks(5); 分区总结 如果ReduceTask的数量 &gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx 如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，会抛出异常 如果ReduceTask的数量 = 1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000 分区号必须从零开始，逐一累加 案例分析：假设自定义分区数为5，则 job.setNumReduceTasks(1)：会正常运行，只不过会产生一个输出文件 job.setNumReduceTasks(2)：会报错 job.setNumReduceTasks(6)：大于5，程序会正常运行，会产生空文件part-r-00005 Partition分区案例实操 需求：将统计结果按照手机归属地不同省份输出到不同文件中(分区) 案例分析 实操 在FlowBean案例基础上，增加一个分区类 12345678910111213141516171819public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text text, FlowBean flowBean, int numPartitions) &#123; // key是手机号,value是流量信息 // 获取手机号前三位 String prePhoneNum = text.toString().substring(0, 3); int partition = 4; if (\"136\".equals(prePhoneNum)) &#123; partition = 0; &#125; else if (\"137\".equals(prePhoneNum)) &#123; partition = 1; &#125; else if (\"138\".equals(prePhoneNum)) &#123; partition = 2; &#125; else if (\"139\".equals(prePhoneNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在Driver驱动主函数中添加自定义数据分区设置和ReduceTask设置 1234// 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 指定ReduceTask的数目job.setNumReduceTasks(6); WritableComparable排序 排序概述：排序是MapReduce框架中最重要的操作之一。MapTask和ReduceTask均会对数据按照key进行排序，该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 排序的分类 部分排序：MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序 全排序：最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构 辅助排序(GroupingComparator分组)：在Reduce端对key进行分组。应用——在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序 二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序 自定义排序WritableComparable 原理分析：bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序 案例实操(全排序) 需求：对FlowBean案例产生的结果再次对总流量进行排序 案例分析 代码实现 FlowBean对象在之前案例基础上实现WritableComparable接口，实现compareTo()方法 12345@Overridepublic int compareTo(FlowBean bean) &#123; // 按总流量倒序 return this.sumFlow &gt; bean.sumFlow ? -1 : 1;&#125; 编写Mapper类 12345678910111213141516171819public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; private Text v = new Text(); private FlowBean flowBean = new FlowBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、封装对象 v.set(fields[1]); long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); flowBean.set(upFlow, downFlow); // 4、写出 context.write(flowBean, v); &#125;&#125; 编写Reducer类 12345678public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; context.write(value, key); &#125; &#125;&#125; 编写Driver类 1234567891011121314151617181920212223public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设计jar路径 job.setJarByClass(FlowCountSortDriver.class); // 3、关联mapper和reducer job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Combiner合并 Combiner介绍 自定义Combiner实现步骤 自定义一个Combiner继承Reducer，重写reduce方法 在Driver驱动类中设置：job.setCombinerClass(xxCombiner.class); 自定义Combiner实操 需求：统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能 案例分析 代码实现 方案一 12345678910111213141516171819// 新建WordCountCombiner类public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 v.set(sum); context.write(key, v); &#125;&#125;// 在WordCountDriver驱动类中指定Combinerjob.setCombinerClass(WordcountCombiner.class); 方案二：将WordCountReducer作为Combiner 结果查看：控制台打印中的Map-Reduce Framework中的Combine记录 GroupingComparator分组(辅助排序) 简要介绍：对Reduce阶段的数据根据某一个或几个字段进行分组 分组排序步骤 自定义类继承WritableComparator 重写compare()方法 创建一个构造将比较对象的类传给父类 GroupingComparator分组实操 需求：求出每一个订单中最贵的商品 案例分析 利用“订单id和成交金额price”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序。Reduce将从Map中获取排序好的数据 在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品 代码实现 编写订单信息OrderBean类 12345678910111213141516171819202122232425262728// 忽略了空参/全参构造器、getter/setter和toString方法public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int orderId; private double price; @Override public int compareTo(OrderBean bean) &#123; int result; if (orderId &gt; bean.orderId) &#123; result = 1; &#125; else if (orderId &lt; bean.orderId) &#123; result = -1; &#125; else &#123; result = price &gt; bean.price ? -1 : 1; &#125; return result; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(orderId); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; orderId = in.readInt(); price = in.readDouble(); &#125;&#125; 编写OrderGroupingComparator类 123456789101112131415public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; // 要求只要id相同,就认为是相同的key OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; if (aBean.getOrderId() == bBean.getOrderId()) &#123; return 0; &#125; return aBean.getOrderId() &gt; bBean.getOrderId() ? 1 : -1; &#125;&#125; 编写OrderMapper类 12345678910111213141516public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; private OrderBean orderBean = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、封装对象 orderBean.setOrderId(Integer.parseInt(fields[0])); orderBean.setPrice(Double.parseDouble(fields[2])); // 4、写出 context.write(orderBean, NullWritable.get()); &#125;&#125; 编写OrderReducer类 1234567891011public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 输出第一个 context.write(key, NullWritable.get()); // 循环几次输出前几 //for (NullWritable value : values) &#123; // context.write(key, NullWritable.get()); //&#125; &#125;&#125; 编写OrderDriver类 123456789101112131415161718192021222324public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、设置jar存储位置 job.setJarByClass(OrderDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); job.setGroupingComparatorClass(OrderGroupingComparator.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; MapTask工作机制 Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区(调用Partitioner)，并写入一个环形内存缓冲区中 Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作 溢写步骤1：利用快速排序算法对缓存区内的数据进行排序。排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序 溢写步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out(N表示当前溢写次数)中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作 溢写步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中 Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor(默认10)个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销 ReduceTask ReduceTask工作机制 Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中 Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多 Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可 Reduce阶段：reduce()函数将计算结果写到HDFS上 设置ReduceTask并行度(个数)ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置(默认值为1)：job.setNumReduceTasks(4); 一个测试ReduceTask数目的实验 实验环境：1个Master节点，16个Slave节点；CPU：8GHZ，内存: 2G 实验结论(数据量为1G) ReduceTask数目 总时间 1 892 5 146 10 110 15 92 16 88 20 100 25 128 30 101 45 145 60 104 注意事项 ReduceTask = 0，表示没有Reduce阶段，输出文件个数和Map个数一致 ReduceTask默认值就是1，所以输出文件个数为一个 如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜(一些节点很忙,其余空闲) ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask 具体多少个ReduceTask，需要根据集群性能而定 如果分区数不是1，但是ReduceTask为1，是否执行分区过程？答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行 OutputFormat数据输出 OutputFormat接口实现类OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。以下是几种常见的OutputFormat实现类 文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串 SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩 自定义OutputFormat：根据用户需求，自定义实现输出 使用场景为了实现控制最终文件的输出路径和输出格式，可以自定义OutputFormat例如：要在一个MapReduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义OutputFormat来实现 自定义OutputFormat步骤 自定义一个类继承FileOutputFormat 改写RecordWriter，具体改写输出数据的方法write() 自定义OutputFormat案例实操 需求：过滤输入的log日志，指定包含某特定字段的记录输出到一个文件，其他则输出到另一个文件 案例分析 代码编写 Mapper类 1234567public class FilterMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // http://www.baidu.com context.write(value, NullWritable.get()); &#125;&#125; Reducer类 12345678910111213public class FilterReducer extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; &#123; private Text k = new Text(); @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String line = key.toString(); line += \"\\r\\n\"; k.set(line); for (NullWritable value : values) &#123; context.write(k, NullWritable.get()); &#125; &#125;&#125; 自定义OutputFormat、RecordWriter类 12345678910111213141516171819202122232425262728293031323334353637383940public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; return new FRecordWriter(job); &#125;&#125;public class FRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; private FSDataOutputStream fosSobxiong; private FSDataOutputStream fosOther; public FRecordWriter(TaskAttemptContext job) &#123; try &#123; // 1、获取文件系统 FileSystem fs = FileSystem.get(job.getConfiguration()); // 2、创建输出到sobxiong.log的输出流 fosSobxiong = fs.create(new Path(\"/Users/sobxiong/Downloads/sobxiong.log\")); // 3、创建输出到other.log的输出流 fosOther = fs.create(new Path(\"/Users/sobxiong/Downloads/other.log\")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断key中是否有sobxiong,如果有写出到sobxiong,否则输出到other if (key.toString().contains(\"sobxiong\")) &#123; fosSobxiong.write(key.toString().getBytes()); &#125; else &#123; fosOther.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; IOUtils.closeStream(fosOther); IOUtils.closeStream(fosSobxiong); &#125;&#125; Driver类 1234567891011121314151617181920212223242526public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FilterDriver.class); job.setMapperClass(FilterMapper.class); job.setReducerClass(FilterReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要将自定义的输出格式组件设置到job中 job.setOutputFormatClass(FilterOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 虽然我们自定义了outputFormat,但是因为我们的outputFormat继承自fileOutputFormat // 而fileOutputFormat要输出一个_SUCCESS文件,所以,在这还得指定一个输出目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Join多种应用 Reduce Join 工作原理Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在Map阶段已经打标志)分开，最后进行合并 案例实操 需求：将商品信息表中数据根据商品pid合并到订单数据表中 案例分析：通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联 代码编写 合并后的Bean类 123456789101112131415161718192021222324252627282930313233343536// 略去空参/全参构造器、getter/setter方法public class TableBean implements Writable &#123; // 订单id private String id; // 产品id private String pid; // 数量 private int amount; // 产品名称 private String pName; // 标记: 产品/订单 private String flag; @Override public String toString() &#123; return id + '\\t' + amount + '\\t' + pName; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(id); out.writeUTF(pid); out.writeInt(amount); out.writeUTF(pName); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; id = in.readUTF(); pid = in.readUTF(); amount = in.readInt(); pName = in.readUTF(); flag = in.readUTF(); &#125;&#125; Mapper类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt; &#123; private String fileName; private final TableBean tableBean = new TableBean(); private final Text key = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取文件的名称 FileSplit inputSplit = (FileSplit) context.getInputSplit(); fileName = inputSplit.getPath().getName(); &#125; // id pid amount // 1001 01 1 // pid pname // 01 小米 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); String[] fields = lineStr.split(\";\"); if (fileName.startsWith(\"order\")) &#123; // 订单表 // 封装kv tableBean.setId(fields[0]); tableBean.setPid(fields[1]); tableBean.setAmount(Integer.parseInt(fields[2])); // 属性不能为空,不然会序列化会出错 tableBean.setpName(\"\"); tableBean.setFlag(\"order\"); this.key.set(fields[1]); &#125; else &#123; // 产品表 // 封装kv tableBean.setId(\"\"); tableBean.setPid(fields[0]); tableBean.setAmount(0); tableBean.setpName(fields[1]); tableBean.setFlag(\"pd\"); this.key.set(fields[0]); &#125; // 写出 context.write(this.key, tableBean); &#125;&#125; Reducer类 1234567891011121314151617181920212223242526272829303132public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 存储所有订单集合 List&lt;TableBean&gt; beans = new ArrayList&lt;&gt;(); // 存储产品信息 TableBean pdBean = new TableBean(); for (TableBean value : values) &#123; if (\"order\".equals(value.getFlag())) &#123; TableBean tmpBean = new TableBean(); try &#123; // value是引用,tmpBean是实实在在的对象 BeanUtils.copyProperties(tmpBean, value); beans.add(tmpBean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; else &#123; try &#123; BeanUtils.copyProperties(pdBean, value); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 拼接表,设置商品名称 for (TableBean bean : beans) &#123; bean.setpName(pdBean.getpName()); context.write(bean, NullWritable.get()); &#125; &#125;&#125; Driver类 12345678910111213141516171819202122public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取配置信息,创建job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定本程序的jar包所在的本地路径 job.setJarByClass(TableDriver.class); // 3、指定本业务job要使用的Mapper/Reducer业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 4、指定Mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 5、指定最终输出的数据的kv类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 6、指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 测试结果： pid pname amount 1001 小米 1 1001 小米 1 1002 华为 2 1002 华为 2 1003 格力 3 1003 格力 3 总结 缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜 解决方案：Map端实现数据合并 Map Join 使用场景：适用于一张表十分小、一张表很大的场景 优点思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜 具体方法：采用DistributedCache 在Mapper的setup阶段，将文件读取到缓存集合中 在驱动函数中加载缓存(缓存普通文件到Task运行节点)：job.addCacheFile(new URI(“file:///Users/sobxiong/Downloads/testInput3/pd.txt”)) 案例实操 需求同Reduce Join 案例分析 代码编写 Driver类 1234567891011121314151617181920212223public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException &#123; // 1、获取job信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、设置加载jar包路径 job.setJarByClass(DistributedCacheDriver.class); // 3、关联map job.setMapperClass(DistributedCacheMapper.class); // 没有reduce阶段,map阶段输出即为最终输出 // 4、设置最终输出数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、加载缓存数据 job.addCacheFile(new URI(\"file:///Users/sobxiong/Downloads/testInput3/pd.txt\")); // 7、Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0 job.setNumReduceTasks(0); // 8、提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Mapper类 1234567891011121314151617181920212223242526272829303132333435363738394041public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; private Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(5); private Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 缓存小表 String cachePath = context.getCacheFiles()[0].getPath(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(new FileInputStream(cachePath), StandardCharsets.UTF_8)); String lineStr; while (StringUtils.isNotEmpty(lineStr = bufferedReader.readLine())) &#123; // 1、切割 // pid pname // 01 小米 String[] fields = lineStr.split(\";\"); // 2、封装到集合去 pdMap.put(fields[0], fields[1]); &#125; // 2、关闭资源 IOUtils.closeStream(bufferedReader); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // id pid amount // 1001 01 1 // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、获取pid String pid = fields[1]; // 4、取出pname String pName = pdMap.get(pid); // 5、拼接 lineStr = lineStr.replace(';', '\\t') + '\\t' + pName; k.set(lineStr); // 6、写出 context.write(k, NullWritable.get()); &#125;&#125; 计数器应用Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量 计数器API 采用枚举的方式统计计数 123enum MyCounter&#123;MALFORORMED,NORMAL&#125;//对枚举定义的自定义计数器加1context.getCounter(MyCounter.MALFORORMED).increment(1); 采用计数器组、计数器名称的方式统计 12// 组名和计数器名称随便起,但最好有意义context.getCounter(\"counterGroup\", \"counter\").increment(1); 计数结果在程序运行后的控制台上查看 数据清洗(ETL)在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序 案例实操(简单解析版——运用计数器)——复杂版(字段多,过滤的需求多,思路与下面无差) 需求：去除日志中字段长度小于等于11的日志 代码编写 Mapper类 123456789101112131415161718192021222324public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String line = value.toString(); // 2、解析数据 boolean isDirty = parseLog(line, context); if (!isDirty) &#123; // 3、解析通过,写出 context.write(value, NullWritable.get()); &#125; &#125; private boolean parseLog(String line, Context context) &#123; String[] fields = line.split(\" \"); if (fields.length &gt; 11) &#123; context.getCounter(\"map\", \"clean\").increment(1); return false; &#125; else &#123; context.getCounter(\"map\", \"dirty\").increment(1); return true; &#125; &#125;&#125; Driver类 1234567891011121314151617181920public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、加载jar包 job.setJarByClass(LogDriver.class); // 3、关联map job.setMapperClass(LogMapper.class); // 4、设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置reduceTask个数为0 job.setNumReduceTasks(0); // 5、设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce开发总结编写MapReduce程序时，需要考虑如下方面 输入数据接口：InputFormat 默认使用的实现类是：TextInputFormat TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回 KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key/value。默认分隔符是tab(\\t) NlineInputFormat按照指定的行数N来划分切片 CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率 自定义InputFormat 逻辑处理接口：Mapper用户根据业务需求实现其中三个方法：map()、setup()、cleanup() Partitioner分区 默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号(分区数大于1时) 1key.hashCode() &amp; Integer.MAXVALUE % numReduces 如果业务上有特别的需求，可以自定义分区 Comparable排序 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法 部分排序：对最终输出的每一个文件进行内部排序 全排序：对所有数据进行排序，通常只有一个Reduce 二次排序：排序的条件有两个 Combiner合并Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果 Reduce端分组：GroupingComparator在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序 逻辑处理接口：Reduce用户根据业务需求实现其中三个方法：reduce()、setup()、cleanup() 输出数据接口：OutputFormat 默认实现类是TextOutputFormat，功能逻辑是：将每一个kv对向目标文本文件输出一行 将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩 自定义OutputFormat Hadoop数据压缩 概述压缩技术能够有效减少底层存储系统(HDFS)读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价 压缩策略和原则压缩是提高Hadoop运行效率的一种优化策略通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度 注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能压缩基本原则 运算密集型的job，少用压缩 IO密集型的job，多用压缩 MR支持的压缩编码 压缩格式 是否hadoop自带 算法 文件扩展名 是否可切分 换成压缩格式后,原来程序是否需要修改 DEFLATE 是,直接使用 DEFLATE .deflate 否 和文本处理一样,不需要修改 Gzip 是,直接使用 DEFLATE .gz 否 和文本处理一样,不需要修改 bzip2 是,直接使用 bzip2 .bz2 是 和文本处理一样,不需要修改 LZO 否,需要安装 LZO .lzo 是 需要建索引,还需要指定输入格式 Snappy 否,需要安装 Snappy .snappy 否 和文本处理一样,不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec Gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s Snappy 8.3GB 较大 最快 最快 压缩方式选择 Gzip压缩 优点压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便 缺点：不支持Split(切片) 应用场景 当每个文件压缩之后在130M以内的(1个块大小内)，都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件 Bzip2压缩 优点：支持Split(切片)；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便 缺点：压缩/解压速度慢 应用场景适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split(切片)，而且兼容之前的应用程序的情况 Lzo压缩 优点：压缩/解压速度也比较快，合理的压缩率；支持Split(切片)，是Hadoop中最流行的压缩格式之一；可以在Linux系统下安装lzop命令，使用方便 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建索引，还需要指定InputFormat为Lzo格式) 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显 Snappy压缩 优点：高速压缩速度和合理的压缩率 缺点：不支持Split(切片)；压缩率比Gzip要低；Hadoop本身不支持，需要安装 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入 压缩位置选择压缩可以在MapReduce作用的任意阶段启用 压缩参数配置要在Hadoop中启用压缩，可配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs(在core-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress(在mapred-site.xml中配置) false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置) false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置) RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 压缩实操 数据流的压缩和解压缩CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据： 要想对正在被写入一个输出流的数据进行压缩，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流 相反，要想对从输入流读取而来的数据进行解压缩，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据测试如下的压缩方式： 压缩格式 编解码类 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec 12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args) throws Exception &#123; compress(\"/Users/sobxiong/Downloads/test.txt\", \"org.apache.hadoop.io.compress.BZip2Codec\"); decompress(\"/Users/sobxiong/Downloads/test.txt.bz2\");&#125;// 解压缩private static void decompress(String filePath) throws IOException &#123; // 1、压缩方式检查 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filePath)); if (codec == null) &#123; System.out.println(\"Can't process!\"); return; &#125; // 2、获取输入流 FileInputStream fis = new FileInputStream(new File(filePath)); CompressionInputStream cis = codec.createInputStream(fis); // 3、获取输出流 FileOutputStream fos = new FileOutputStream(new File(filePath + \".decode\")); // 4、流的对拷 IOUtils.copyBytes(cis, fos, 1024 * 1024 * 10, false); // 5、关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(cis); IOUtils.closeStream(fis);&#125;// 压缩private static void compress(String filePath, String compressTypeClassName) throws IOException, ClassNotFoundException &#123; // 1、获取输入流 FileInputStream fis = new FileInputStream(new File(filePath)); // 2、获取输出流 Class&lt;?&gt; classCodec = Class.forName(compressTypeClassName); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(classCodec, new Configuration()); FileOutputStream fos = new FileOutputStream(new File(filePath + codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // 3、流的对拷 IOUtils.copyBytes(fis, cos, 1024 * 1024 * 10, false); // 4、关闭资源 IOUtils.closeStream(cos); IOUtils.closeStream(fos); IOUtils.closeStream(fis);&#125; Map输出端采用压缩(以万能的WordCount案例为例)即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可具体实现(只修改Driver部分代码,Mapper和Reducer不变)： 123456789// 1、获取Job对象Configuration conf = new Configuration();// 开启map端输出压缩conf.setBoolean(\"mapreduce.map.output.compress\", true);// 设置map端输出压缩方式conf.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class, CompressionCodec.class);Job job = Job.getInstance(conf);// 之后代码保持原样 Reduce输出端采用压缩(以万能的WordCount案例为例)具体实现(只修改Driver部分代码,Mapper和Reducer不变)： 1234567891011// 之前代码保持不变// 设置reduce端输出压缩开启FileOutputFormat.setCompressOutput(job, true);// 设置压缩的方式FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);// 6、设置程序运行的输入和输出路径FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1]));// 之后代码保持不变 Yarn资源调度器Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序 Yarn基本架构YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成 Yarn工作机制 Yarn工作机制图解 工作机制详解 MR程序提交到客户端所在的节点 YarnRunner向ResourceManager申请一个Application RM将该应用程序的资源路径返回给YarnRunner 该程序将运行所需资源提交到HDFS上 程序资源提交完毕后，申请运行MrAppMaster RM将用户的请求初始化成一个Task 其中一个NodeManager领取到Task任务 该NodeManager创建容器Container，并产生MrAppmaster Container从HDFS上拷贝资源到本地 MrAppmaster向RM申请运行MapTask的资源 RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器 MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序 MrAppMaster等待所有MapTask运行完毕后，向RM申请容器运行ReduceTask ReduceTask向MapTask获取相应分区的数据 程序运行完毕后，MR会向RM申请注销自己 作业提交全过程 作业提交过程之Yarn图解 作业提交过程之MapReduce图解 作业提交过程详解 作业提交 Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业 Client向RM申请一个作业id RM给Client返回该job资源的提交路径和作业id Client提交jar包、切片信息和配置文件到指定的资源提交路径 Client提交完资源后，向RM申请运行MrAppMaster 作业初始化 当RM收到Client的请求后，将该job添加到容量调度器中 某一个空闲的NM领取到该Job 该NM创建Container，并产生MrAppmaster 下载Client提交的资源到本地 任务分配 MrAppMaster向RM申请运行多个MapTask任务资源 RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器 任务运行 MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序 MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask ReduceTask向MapTask获取相应分区的数据 程序运行完毕后，MR会向RM申请注销自己 进度和状态更新YARN中的任务将其进度和状态(包括counter)返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新，展示给用户 作业完成除了向应用管理器请求作业进度外，客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后，应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。 先进先出调度器(FIFO) 容量调度器(Capacity Scheduler) 公平调度器(Fair Scheduler) Hadoop3.1.3默认的资源调度器是Capacity Scheduler。具体设置详见yarn-default.xml文件 12345&lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 任务的推测执行 作业完成时间取决于最慢的任务完成时间一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？ 推测执行机制发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果 执行推测任务的前提条件 每个Task只能有一个备份任务 当前Job已完成的Task必须不小于0.05(5%) 开启推测执行参数设置。mapred-site.xml文件中默认是打开的 1234567891011&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; 不能启用推测执行机制情况 任务间存在严重的负载倾斜 特殊任务，比如任务向数据库中写数据 算法原理 Hadoop企业优化 MapReduce跑的慢的原因MapReduce程序效率的瓶颈在于两点： 计算机性能：CPU、内存、磁盘健康、网络 I/O操作优化 I/O操作优化 Map和Reduce数设置不合理 Map运行时间太长，导致Reduce等待过久 小文件过多 大量的不可分块的超大文件 Spill次数过多 Merge次数过多等 MapReduce优化方法MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数 数据输入 合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢 采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景 Map阶段 减少溢写(Spill)次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO 减少合并(Merge)次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间 在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O Reduce阶段 合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误 设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间 规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗 合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整 I/O传输 采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器 使用SequenceFile二进制文件 数据倾斜问题 数据倾斜现象 数据频率倾斜：某一个区域的数据量要远远大于其他区域 数据大小倾斜：部分记录的大小远远大于平均值 减少数据倾斜的方法 抽样和范围分区：可以通过对原始数据进行抽样得到的结果集来预设分区边界值 自定义分区：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例 Combine：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据 采用Map Join，尽量避免Reduce Join 常用的调优参数 资源相关参数 以下参数是在用户自己的MR应用程序中配置就可以生效(mapred-default.xml) 配置参数 参数说明 mapreduce.map.memory.mb 一个MapTask可使用的资源上限(单位:MB)，默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死 mapreduce.reduce.memory.mb 一个ReduceTask可使用的资源上限(单位:MB)，默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死 mapreduce.map.cpu.vcores 每个MapTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.cpu.vcores 每个ReduceTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.shuffle.parallelcopies 每个Reduce去Map中取数据的并行数。默认值是5 mapreduce.reduce.shuffle.merge.percent Buffer中的数据达到多少比例开始写入磁盘。默认值0.66 mapreduce.reduce.shuffle.input.buffer.percent Buffer大小占Reduce可用内存的比例。默认值0.7 mapreduce.reduce.input.buffer.percent 指定多少比例的内存用来存放Buffer中的数据，默认值是0.0 应该在YARN启动之前就配置在服务器的配置文件中才能生效(yarn-default.xml) 配置参数 参数说明 yarn.scheduler.minimum-allocation-mb 给应用程序Container分配的最小内存，默认值：1024 yarn.scheduler.maximum-allocation-mb 给应用程序Container分配的最大内存，默认值：8192 yarn.scheduler.minimum-allocation-vcores 每个Container申请的最小CPU核数，默认值：1 yarn.scheduler.maximum-allocation-vcores 每个Container申请的最大CPU核数，默认值：32 yarn.nodemanager.resource.memory-mb 给Containers分配的最大物理内存，默认值：8192 Shuffle性能优化的关键参数，应在YARN启动之前就配置好(mapred-default.xml) 配置参数 参数说明 mapreduce.task.io.sort.mb Shuffle的环形缓冲区大小，默认100m mapreduce.map.sort.spill.percent 环形缓冲区溢出的阈值，默认80% 容错相关参数(MapReduce性能优化) 配置参数 参数说明 mapreduce.map.maxattempts 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4 mapreduce.reduce.maxattempts 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4 mapreduce.task.timeout Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间(单位毫秒)，默认是600000。如果你的程序对每条输入数据的处理时间过长(比如会访问数据库，通过网络拉取数据等)，建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.” HDFS小文件优化方法 HDFS小文件弊端HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件。一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢 HDFS小文件解决方案 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS 在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并 在MapReduce处理时，可采用CombineTextInputFormat提高效率 具体方案 Hadoop Archive是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了NameNode的内存使用 Sequence FileSequence File由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件 CombineFileInputFormatCombineFileInputFormat是一种新的InputFormat，用于将多个文件合并成一个单独的Split，另外，它会考虑数据的存储位置 开启JVM重用对于大量小文件Job，可以开启JVM重用会减少45%运行时间。JVM重用原理：一个Map运行在一个JVM上，开启重用的话，该Map在JVM上运行完毕后，JVM继续运行其他Map。具体设置：mapreduce.job.jvm.numtasks值在10-20之间 MapReduce扩展案例 倒排索引案例(多job串联) 需求：有大量的文本(文档、网页)，需要建立搜索索引 案例分析 第一次处理 OneIndexMapper 123456789101112131415161718192021222324public class OneIndexMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private String fileName; private Text k = new Text(); private IntWritable v = new IntWritable(1); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; FileSplit fileSplit = (FileSplit) context.getInputSplit(); fileName = fileSplit.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\" \"); // 3、写出 for (String field : fields) &#123; k.set(field + \"--\" + fileName); context.write(k, v); &#125; &#125;&#125; OneIndexReducer 123456789101112131415public class OneIndexReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; // 1、累加求和 for (IntWritable value : values) &#123; sum += value.get(); &#125; v.set(sum); // 2、写出 context.write(key, v); &#125;&#125; OneIndexDriver 1234567891011121314public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(OneIndexDriver.class); job.setMapperClass(OneIndexMapper.class); job.setReducerClass(OneIndexReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true);&#125; 第二次处理 TwoIndexMapper 123456789101112131415161718public class TwoIndexMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; private Text k = new Text(); private Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // haha--a.txt 2 // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\"--\"); // 3、封装 k.set(fields[0]); v.set(fields[1]); // 4、写出 context.write(k, v); &#125;&#125; TwoIndexReducer 1234567891011121314public class TwoIndexReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123; private Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text value : values) &#123; sb.append(value.toString().replace(\"\\t\", \"--&gt;\")) .append('\\t'); &#125; v.set(sb.toString()); context.write(key, v); &#125;&#125; TwoIndexDriver 123456789101112131415public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration config = new Configuration(); Job job = Job.getInstance(config); job.setJarByClass(TwoIndexDriver.class); job.setMapperClass(TwoIndexMapper.class); job.setReducerClass(TwoIndexReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result?0:1);&#125; 先运行OneIndexDriver，将得到的输入作为TwoIndexDriver的输入，在运行TwoIndexDriver得到最终结果 TopN案例 需求：输出流量使用量在前10的用户信息 案例分析 代码实现 TopNMapper 12345678910111213141516171819202122232425262728293031323334353637383940public class TopNMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123; // 定义一个TreeMap作为存储数据的容器(天然按key排序) private TreeMap&lt;FlowBean, Text&gt; flowMap = new TreeMap&lt;FlowBean, Text&gt;(); private FlowBean kBean; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; kBean = new FlowBean(); Text v = new Text(); // 1、获取一行 String line = value.toString(); // 2、切割 String[] fields = line.split(\"\\t\"); // 3、封装数据 String phoneNum = fields[0]; long upFlow = Long.parseLong(fields[1]); long downFlow = Long.parseLong(fields[2]); long sumFlow = Long.parseLong(fields[3]); kBean.setDownFlow(downFlow); kBean.setUpFlow(upFlow); kBean.setSumFlow(sumFlow); v.set(phoneNum); // 4、向TreeMap中添加数据 flowMap.put(kBean, v); // 5、限制TreeMap的数据量,超过10条就删除掉流量最小的一条数据 if (flowMap.size() &gt; 10) &#123; flowMap.remove(flowMap.lastKey()); &#125; &#125; @Override protected void cleanup(Context context) throws IOException, InterruptedException &#123; // 6、遍历treeMap集合,输出数据 Iterator&lt;FlowBean&gt; bean = flowMap.keySet().iterator(); while (bean.hasNext()) &#123; FlowBean k = bean.next(); context.write(k, flowMap.get(k)); &#125; &#125;&#125; TopNReducer 12345678910111213141516171819202122232425262728public class TopNReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; // 定义一个TreeMap作为存储数据的容器（天然按key排序） TreeMap&lt;FlowBean, Text&gt; flowMap = new TreeMap&lt;FlowBean, Text&gt;(); @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context)throws IOException, InterruptedException &#123; for (Text value : values) &#123; FlowBean bean = new FlowBean(); bean.set(key.getDownFlow(), key.getUpFlow()); // 1、向treeMap集合中添加数据 flowMap.put(bean, new Text(value)); // 2、限制TreeMap数据量,超过10条就删除掉流量最小的一条数据 if (flowMap.size() &gt; 10) &#123; flowMap.remove(flowMap.lastKey()); &#125; &#125; &#125; @Override protected void cleanup(Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context) throws IOException, InterruptedException &#123; // 3、遍历集合,输出数据 Iterator&lt;FlowBean&gt; it = flowMap.keySet().iterator(); while (it.hasNext()) &#123; FlowBean v = it.next(); context.write(new Text(flowMap.get(v)), v); &#125; &#125;&#125; TopNDriver 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; // 1、获取配置信息,或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定本程序的jar包所在的本地路径 job.setJarByClass(TopNDriver.class); // 3、指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(TopNMapper.class); job.setReducerClass(TopNReducer.class); // 4、指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 5、指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 找博客共同好友案例 需求以下是博客的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友(数据中的好友关系是单向的)。求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？ 示例数据 1234567891011121314A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J 案例分析 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&#x2F;&#x2F; 第一次先求出A、B、C...等是谁的好友A I,K,C,B,G,F,H,O,DB A,F,J,EC A,E,B,H,F,G,KD G,C,K,A,L,F,E,HE G,M,L,H,A,F,B,DF L,M,D,C,G,AG MH OI O,CJ OK BL D,EM E,FO A,H,I,J,F&#x2F;&#x2F; 第二次找出共同好友A-B E CA-C D FA-D E FA-E D B CA-F O B C D EA-G F E C DA-H E C D OA-I OA-J O BA-K D CA-L F E DA-M E FB-C AB-D A EB-E CB-F E A CB-G C E AB-H A E CB-I AB-K C AB-L EB-M EB-O AC-D A FC-E DC-F D AC-G D F AC-H D AC-I AC-K A DC-L D FC-M FC-O I AD-E LD-F A ED-G E A FD-H A ED-I AD-K AD-L E FD-M F ED-O AE-F D M C BE-G C DE-H C DE-J BE-K C DE-L DF-G D C A EF-H A D O E CF-I O AF-J B OF-K D C AF-L E DF-M EF-O AG-H D C E AG-I AG-K D A CG-L D F EG-M E FG-O AH-I O AH-J OH-K A C DH-L D EH-M EH-O AI-J OI-K AI-O AK-L DK-O AL-M E F 代码实现 第一次Mapper 123456789101112131415161718public class OneShareFriendsMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 1、获取一行 A:B,C,D,F,E,O String line = value.toString(); // 2、切割 String[] fields = line.split(\":\"); // 3、获取person和好友 String person = fields[0]; String[] friends = fields[1].split(\",\"); // 4、写出去 for(String friend: friends)&#123; // 输出 &lt;好友，人&gt; context.write(new Text(friend), new Text(person)); &#125; &#125;&#125; 第一次Reducer 1234567891011121314public class OneShareFriendsReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); // 1、拼接 for(Text person: values)&#123; sb.append(person).append(\",\"); &#125; // 取出多余的',' sb.deleteCharAt(sb.length() - 1); // 2、写出 context.write(key, new Text(sb.toString())); &#125;&#125; 第一次Driver 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; // 1、获取job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定jar包运行的路径 job.setJarByClass(OneShareFriendsDriver.class); // 3、指定map/reduce使用的类 job.setMapperClass(OneShareFriendsMapper.class); job.setReducerClass(OneShareFriendsReducer.class); // 4、指定map输出的数据类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); // 5、指定最终输出的数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); // 6、指定job的输入原始所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 第二次Mapper 12345678910111213141516171819public class TwoShareFriendsMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // A I,K,C,B,G,F,H,O,D // 友 人,人,人 String line = value.toString(); String[] friend_persons = line.split(\"\\t\"); String friend = friend_persons[0]; String[] persons = friend_persons[1].split(\",\"); Arrays.sort(persons); for (int i = 0; i &lt; persons.length - 1; i++) &#123; for (int j = i + 1; j &lt; persons.length; j++) &#123; // 发出 &lt;人-人,好友&gt;,这样，相同的“人-人”对的所有好友就会到同1个reduce中去 context.write(new Text(persons[i] + \"-\" + persons[j]), new Text(friend)); &#125; &#125; &#125;&#125; 第二次Reducer 12345678910public class TwoShareFriendsReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text friend : values) &#123; sb.append(friend).append(\" \"); &#125; context.write(key, new Text(sb.toString())); &#125;&#125; 第二次Driver 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; // 1、获取job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定jar包运行的路径 job.setJarByClass(TwoShareFriendsDriver.class); // 3、指定map/reduce使用的类 job.setMapperClass(TwoShareFriendsMapper.class); job.setReducerClass(TwoShareFriendsReducer.class); // 4、指定map输出的数据类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); // 5、指定最终输出的数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); // 6、指定job的输入原始所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 常见错误及解决方案 导包错误，尤其是Text和CombineTextInputFormat Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable。报的错误是类型转换异常 java.lang.Exception: java.io.IOException: Illegal partition for 13926435656(4)；说明Partition和ReduceTask个数没对上，调整ReduceTask个数 如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行 报类型转换异常：通常都是在驱动函数中设置Map输出和最终输出时编写错误；Map输出的key如果没有排序，也会报类型转换异常 集群中运行wc.jar时出现了无法获得输入文件。原因：WordCount案例的输入文件不能放用HDFS集群的根目录 自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]}],"categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://sobxiong.github.io/tags/LeetCode/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://sobxiong.github.io/tags/Algorithm/"},{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"},{"name":"C/C++","slug":"C-C","permalink":"https://sobxiong.github.io/tags/C-C/"},{"name":"Middleware","slug":"Middleware","permalink":"https://sobxiong.github.io/tags/Middleware/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"},{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"},{"name":"Scala","slug":"Scala","permalink":"https://sobxiong.github.io/tags/Scala/"},{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"}]}