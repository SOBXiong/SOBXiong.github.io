{"meta":{"title":"SOBXiong的博客","subtitle":"","description":"一只编程菜鸡，对Android、Java后端、大数据和Vue雨露均沾","author":"SOBXiong","url":"https://sobxiong.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-06-23T14:47:17.665Z","updated":"2020-06-23T14:47:17.656Z","comments":true,"path":"404.html","permalink":"https://sobxiong.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2020-06-23T14:45:30.095Z","updated":"2020-06-23T14:45:30.085Z","comments":true,"path":"about/index.html","permalink":"https://sobxiong.github.io/about/index.html","excerpt":"","text":"一只编程菜鸡，对Android、Java后端、大数据和Vue雨露均沾"},{"title":"所有分类","date":"2020-06-23T14:46:03.448Z","updated":"2020-06-23T14:46:03.438Z","comments":true,"path":"categories/index.html","permalink":"https://sobxiong.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-06-23T14:46:49.268Z","updated":"2020-06-23T14:46:49.258Z","comments":true,"path":"tags/index.html","permalink":"https://sobxiong.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"刷题记录","slug":"BasicSkill/Algorithm/ProblemRecord","date":"2020-12-11T13:04:18.000Z","updated":"2020-12-11T13:09:08.182Z","comments":true,"path":"2020/12/11/BasicSkill/Algorithm/ProblemRecord/","link":"","permalink":"https://sobxiong.github.io/2020/12/11/BasicSkill/Algorithm/ProblemRecord/","excerpt":"内容 218天际线问题 514自由之路","text":"内容 218天际线问题 514自由之路 218天际线问题 问题描述： 解法1：分治法(归并的思想)每次将问题划分成更小的子问题，当问题分解到单个建筑时进行解决。如果单个建筑的输入信息为[x1, x2, h]，那么返回[[x1, h], [x2, 0]]。分解完后，进入归并Merge阶段。归并的关键是从轮廓看去只能看到不同的高度，归并模拟过程如下： 假如有以下划分好的结果：skyline1 = {(1, 11), (3, 13), (9, 0), (12, 7), (16, 0)}skyline2 = {(14, 3), (19, 18), (22, 3), (23, 13), (29, 0)}假设i、j分别表示skyline1、skyline2的当前遍历下标，h1、h2分别代表skyline1、skyline2当前的高度首先比较(1, 11)和(14, 3)，由于1 &lt; 14，选取(1, 11)，h1置为当前大小11，hmax = max(h1, h2) = 11，ans结果集加入(1, 11)，i++之后比较(3, 13)和(14, 3)，h1置为当前大小13，hmax = max(h1, h2) = 13，同理ans结果集加入(3, 13)，i++…直到(16, 0)，此时比较(16, 0)和(14, 3)，14 &lt; 3，选取(14, 3)，h2 = 3。而此时h1 = 7，h2 = 3，hmax = max(h1, h2) = 7，因此结果集加入(14, 7)，由于之前的结果集存在(12, 7)，所以(14, 7)不应该加入结果集，j++比较(16, 0)和(19, 18)，此时16 &lt; 19，选取(16, 0)，h1 = 0。此时h1 = 0，h2 = 3，hmax = max(h1, h2) = 3。因此ans结果集加入(16, 3)，至此skyline1全部遍历完，只需将skyline2的剩余部分加入结果集 代码实现如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859vector&lt;vector&lt;int&gt;&gt; divideAndConquer(vector&lt;vector&lt;int&gt;&gt; &amp;buildings, int left, int right) &#123; vector&lt;vector&lt;int&gt;&gt; ans; // 如果left、right一致,相当于只处理单个建筑 if (left == right) &#123; ans.push_back(&#123;buildings[left][0], buildings[left][2]&#125;); ans.push_back(&#123;buildings[left][1], 0&#125;); &#125; else &#123; // 找到中间下标 int mid = (left + right) &gt;&gt; 1; // 递归分治 vector&lt;vector&lt;int&gt;&gt; leftAns = divideAndConquer(buildings, left, mid), rightAns = divideAndConquer(buildings, mid + 1, right); // 定义下标和各自的高度变量 // 每次都需要更新高度信息 int i = 0, j = 0, h1 = 0, h2 = 0; while (i &lt; leftAns.size() &amp;&amp; j &lt; rightAns.size()) &#123; // 记录结果 vector&lt;int&gt; cur(2, 0); // 如果起始相同,做了简化 if (leftAns[i][0] == rightAns[j][0]) &#123; h1 = leftAns[i][1], h2 = rightAns[j][1]; cur[0] = leftAns[i][0], cur[1] = max(h1, h2); i++, j++; &#125; else if (leftAns[i][0] &lt; rightAns[j][0]) &#123; // 起始不同,取起始小的作为当前结果开头,取当前的最大高度为当前结果高度 h1 = leftAns[i][1]; cur[0] = leftAns[i][0], cur[1] = max(h1, h2); i++; &#125; else &#123; // 起始不同,取起始小的作为当前结果开头,取当前的最大高度为当前结果高度 h2 = rightAns[j][1]; cur[0] = rightAns[j][0], cur[1] = max(h1, h2); j++; &#125; // 如果当前结果集为空或当前结果与上次高度不同,加入结果集 if (ans.empty() || ans.back()[1] != cur[1]) &#123; ans.push_back(cur); &#125; &#125; // 依次加入剩余的结果 if (i &lt; leftAns.size()) &#123; for (int k = i; k &lt; leftAns.size(); ++k) ans.push_back(leftAns[k]); &#125; else if (j &lt; rightAns.size()) &#123; for (int k = j; k &lt; rightAns.size(); ++k) ans.push_back(rightAns[k]); &#125; &#125; return ans;&#125;vector&lt;vector&lt;int&gt;&gt; getSkyline(vector&lt;vector&lt;int&gt;&gt; &amp;buildings) &#123; int size = buildings.size(); if (size == 1) &#123; vector&lt;vector&lt;int&gt;&gt; ans; ans.push_back(&#123;buildings[0][0], buildings[0][2]&#125;); ans.push_back(&#123;buildings[0][1], 0&#125;); return ans; &#125; return divideAndConquer(buildings, 0, size - 1);&#125; 514自由之路 题目描述： 前提： 每次转到指定位置后还需要按一次按钮，那么结果还需要加上key的长度 转动到指定位置有两种方向，单考虑转到指定位置的话(不考虑再按一次的触发)；假设curRingIndex表示当前转动前的位置下标，curKeyIndex表示转动到的指定位置的下标： l1 = abs(curRingIndex - curKeyIndex) l2 = ring.length() - l1 每次转动后curKeyIndex变为下一次curRingIndex，即从新指定位置开始转动 截止条件是curKeyIndex = key.length()，即当前所有key的字符都被转到12点方向过 解法1：动态规划方式记录每次从当前位置转动到顺序读取到的key字符的位置所花费的转动次数，最终返回到达最后key字符的转动次数的最小值 动态转移方程如下 1234567// 默认dp中元素为INT_MAX// 得到第一步的转动次数dp[0][i] = (ring[i] == key[0]) ? min(i, m - i);// 如果dp[i - 1][k]不为INT_MAX,即上一步(第i步)能转动到ring的第k个字符,且为到第k个字符的最小值// 当前为第i+1步,将要走到第j个字符// 最小距离为已知的最小值与当前走法的较小值dp[i][j] = min(dp[i][j], dp[i - 1][k] + min(abs(j - k), m - abs(j - k))); 最终代码如下 1234567891011121314151617181920212223242526272829int findRotateSteps(string ring, string key) &#123; int m = ring.size(), n = key.size(); vector&lt;int&gt; pos[26]; // 记录字符下标 for (int i = 0; i &lt; m; i++) &#123; pos[ring[i] - 'a'].push_back(i); &#125; vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(m, INT_MAX)); // 遍历首字符匹配的下标集合 for (const int &amp;index : pos[key[0] - 'a']) &#123; // 走第一步 dp[0][index] = min(index, m - index); &#125; for (int i = 1; i &lt; n; ++i) &#123; // 遍历下一步字符的下标集合 for (const int &amp;index : pos[key[i] - 'a']) &#123; for (int j = 0; j &lt; m; j++) &#123; // 如果第i步不可达j,跳过 if (dp[i - 1][j] == INT_MAX) continue; // 当前走第i+1步的最小步数 int steps = min(abs(index - j), m - abs(index - j)); // 记录走i+1步的最小步数 dp[i][index] = min(dp[i - 1][j] + steps, dp[i][index]); &#125; &#125; &#125; // 每次移动到指定位置还需要转动一次触发,因此要+n return *min_element(dp[n - 1].begin(), dp[n - 1].end()) + n;&#125; 解法2：动态规划方式，使用滚动数组的方式，节省空间 根据方式一，发现第i+1步只与第i步的位置有关，因此只需保留两个数组用于保存上一步的最小步数和当前步的最小步数即可 最终代码如下 12345678910111213141516171819202122232425262728293031int findRotateSteps(string ring, string key) &#123; int m = ring.size(), n = key.size(); vector&lt;int&gt; pos[26]; // 记录字符下标 for (int i = 0; i &lt; m; i++) &#123; pos[ring[i] - 'a'].push_back(i); &#125; vector&lt;int&gt; last(m), current(m, INT_MAX); // 遍历首字符匹配的下标集合 for (const int &amp;index : pos[key[0] - 'a']) &#123; // 走第一步 current[index] = min(index, m - index); &#125; for (int i = 1; i &lt; n; ++i) &#123; last = current; fill(current.begin(), current.end(), INT_MAX); // 遍历下一步字符的下标集合 for (const int &amp;index : pos[key[i] - 'a']) &#123; for (int j = 0; j &lt; m; j++) &#123; // 如果第i步不可达j,跳过 if (last[j] == INT_MAX) continue; // 当前走第i+1步的最小步数 int steps = min(abs(index - j), m - abs(index - j)); // 记录走i+1步的最小步数 current[index] = min(last[j] + steps, current[index]); &#125; &#125; &#125; // 每次移动到指定位置还需要转动一次触发,因此要+n return *min_element(current.begin(), current.end()) + n;&#125; 解法3：dfs + 记忆化方式 12345678910111213141516171819202122232425262728293031323334353637383940int findRotateSteps(string ring, string key) &#123; int m = ring.size(), n = key.size(); vector&lt;int&gt; pos[26]; // 记录字符下标 for (int i = 0; i &lt; m; i++) &#123; pos[ring[i] - 'a'].push_back(i); &#125; // 记录从ring的第i个字符转动到key的第j个字符的最小转动次数 vector&lt;vector&lt;int&gt;&gt; memo(m, vector&lt;int&gt;(n, -1)); /* curRingIndex：当前处于ring转盘的第几个字符 curKeyIndex：当前需匹配的key的第几个字符 memo：记录表 */ std::function&lt;int(int, int, vector&lt;vector&lt;int&gt;&gt; &amp;)&gt; dfs = [&amp;, m, pos, key](int curRingIndex, int curKeyIndex, vector&lt;vector&lt;int&gt;&gt; &amp;memo) -&gt; int &#123; // 递归出口 if (curKeyIndex == key.size()) &#123; return 0; &#125; // 如果已记录过,直接返回 if (memo[curRingIndex][curKeyIndex] != -1) &#123; return memo[curRingIndex][curKeyIndex]; &#125; // 记录此次转动的最小次数 int res = INT_MAX; // 遍历ring中当前key对应字符的位置 for (const int &amp;index : pos[key[curKeyIndex] - 'a']) &#123; int d1 = abs(curRingIndex - index); int d2 = m - d1; // 此次转动的最小次数 // 当前转动成功后,curRingIndex自动变为index,curKeyIndex移动到下一个 res = min(res, min(d1, d2) + dfs(index, curKeyIndex + 1, memo)); &#125; // 更新记录表 memo[curRingIndex][curKeyIndex] = res; return res; &#125;; // 每次移动到指定位置还需要转动一次触发,因此要+n return n + dfs(0, 0, memo);&#125;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BasicSkill","slug":"BasicSkill","permalink":"https://sobxiong.github.io/tags/BasicSkill/"},{"name":"LeetCode题解","slug":"LeetCode题解","permalink":"https://sobxiong.github.io/tags/LeetCode%E9%A2%98%E8%A7%A3/"}]},{"title":"MySQL基础","slug":"Middleware/MySQL/MySQL基础","date":"2020-12-08T12:14:49.000Z","updated":"2020-12-11T07:55:25.369Z","comments":true,"path":"2020/12/08/Middleware/MySQL/MySQL基础/","link":"","permalink":"https://sobxiong.github.io/2020/12/08/Middleware/MySQL/MySQL%E5%9F%BA%E7%A1%80/","excerpt":"内容 SQL语句 表约束和范式 多表查询和子查询 事务 其他","text":"内容 SQL语句 表约束和范式 多表查询和子查询 事务 其他 SQL语句 SQL概述 什么是SQL：SQL(Structured Query Language)是”结构化查询语言”，它是对关系型数据库的操作语言，可以应用到所有关系型数据库中。很多数据库还都有标准以外的一些语法，称之为”方言”。例如MySQL中的LIMIT语句就是MySQL独有的方言，其它数据库都不支持 SQL语法要求： SQL语句可单行或多行书写，以分号结尾 可用空格和缩进来增强语句的可读性 关键字不区别大小写，建议使用大写 SQL分类： DDL(Data Definition Language)：数据定义语言，用来定义数据库对象：库、表、列等 DML(Data Manipulation Language)：数据操作语言，用来定义数据库记录(数据) DCL(Data Control Language)：数据控制语言，用来定义访问权限和安全级别 DQL(Data Query Language)：数据查询语言，用来查询记录(数据) DDL介绍： 操作数据库： 创建数据库： 基础创建：CREATE DATABSE dbName 不存在则创建：CREATE DATABASE IF NOT EXISTS dbName 指定字符集：CREATE DATABSE dbName CHARACTER SET charsetName 查看数据库： 查看所有数据库：SHOW DATABASES 查看某个数据库的定义信息：SHOW CREATE DATABASE dbName 修改数据库(默认)字符集：ALTER DATABASE dbName (DEFAULT) CHARACTER SET charsetName 删除数据库：DROP DATABSE dbName 使用数据库： 查看正在使用的数据库：SELECT DATABASE() 使用/切换数据库：USE dbName 操作表结构： 创建表：CREATE TABLE tableName(field1 fieldType1, field2 fieldType2) 查看某个数据库的所有表：SHOW TABLES 查看表结构：DESC tableName 查看建表语句：SHOW CREATE TABLE tableName 快速创建表结构相同的表：CREATE TABLE newTableName LIKE oldTableName 删除表 直接删除：DROP TABLE tableName 存在则删除：DROP TABLE IF EXISTS tableName 修改表结构： 添加列：ALTER TABLE tableName ADD fieldName fieldType 修改列：ALTER TABLE tableName MODIFY fieldName fieldType 修改列名：ALTER TABLE tableName CHANGE oldFieldName newFieldName fieldType 删除列：ALTER TABLE tableName DROP fieldName 修改表名：RENAME TABLE oldTableName TO newTableName 修改表字符集：ALTER TABLE tableName CHARACTER SET charsetName DML介绍： 插入记录： 插入全部字段：INSERT INTO tableName VALUES(v1, v2, v3, ...) 插入部分字段：INSERT INTO tableName(field1, field2, ...) VALUES(v1, v2, ...) 插入的数据应与字段的数据类型相同，且在范围内 在VALUES中列出的数据位置必须与被加入的列的排列位置相对应 字符和日期型数据应包含在单引号中。MySQL中也可以使用双引号做为分隔符 不指定列或使用NULL，表示插入空值 蠕虫复制：将一张已存在表中数据复制到另一张表中 复制所有列：INSERT INTO tableName1 SELECT * FROM tableName2 复制部分列：INSERT INTO tableName1(field1, field2, ...) SELECT field1, field2, ... FROM tableName2 更新记录： 不带条件修改(修改所有行)：UPDATE tableName SET field1=v1, field2=v2, ... 带条件修改：UPDATE tableName SET field1=v1,field2=v2, ... WHERE ... 删除记录： 不带条件删除(删除所有行)：DELETE FROM tableName 带条件删除：DELETE FROM tableName WHERE ... 使用truncate删除：TRUNCATE TABLE tableName truncate相当于删除表的结构，再创建一张表；而delete仅删除数据 DCL介绍： 创建用户： 语法：CREATE USER &#39;username&#39; @&#39;hostname&#39; IDENTIFIED BY &#39;password&#39; 单引号’’不可少 说明： username：将创建的用户名 hostname：指定该用户在哪个主机上可以登陆，如果是本地用户可用localhost；如果想让该用户可以从任意远程主机登陆，可以使用通配符% password：该用户的登陆密码，可以为空，如果为空则该用户可以不需要密码登陆服务器 授予用户权限 语法：GRANT authority1, authority2, ... ON database.tableName TO &#39;username&#39;@&#39;hostname&#39; 单引号’’不可少 说明： GRANT … ON … TO：授权关键字 authority：授予用户的权限，如CREATE、ALTER、SELECT、INSERT和UPDATE等。如果要授予所有的权限可使用ALL database.tableName：该用户可以操作哪个数据库的哪些表。如果要授予该用户对所有数据库和表的相应操作权限则可用*表示，如*.* ‘username’@’hostname’：给哪个用户授权 撤销用户权限：REVOKE authority1, authority2, ... ON database.tableName FROM &#39;username&#39;@&#39;hostname&#39; 说明参见授予权限 查看权限：SHOW GRANTS FOR &#39;username&#39;@&#39;hostname&#39; usage是指连接(登陆)权限，建立一个用户就会自动授予其usage权限(默认授予) 删除用户：DROP USER &#39;username&#39;@&#39;hostname&#39; 修改管理员密码：mysqldadmin -u root -p password newPassword 需要在未登陆MySQL的情况下操作，新密码不需要加上引号；password是关键字，newPassword是新密码串 修改普通用户密码：SET PASSWORD FOR &#39;username&#39;@&#39;hostname&#39;=password(&#39;newPassword&#39;) 需要在登陆MySQL的情况下操作 DQL介绍： 总体语法： 1234567891011121314SELECT selection_list /*要查询的列名称*/FROM table_list /*要查询的表名称*/WHERE condition /*行条件*/GROUP BY grouping_columns /*对结果分组*/HAVING condition /*分组后的行条件*/ORDER BY sorting_columns /*对结果分组*/LIMIT offset_start, row_count /*结果限定*/ 简单查询： 查询所有列：SELECT * FROM tableName 查询指定列：SELECT field1, field2, ... FROM tableName 指定别名查询： 指定列别名：SELECT field1 AS newField1, field2 AS newField2, ... FROM tableName 指定列和表别名：SELECT field1 AS newField1, field2 AS newField2, ... FROM tableName AS newTableName 指定表别名主要用于多表查询，其中别名的AS可以省略；别名可用双引号””括起 清除指定列的重复值：SELECT DISTINCT field FROM tableName 查询结果参与运算(参与运算的必须是数值类型)： 条件查询： 语法：SELECT field FROM tableName WHERE condition 比较运算符： 运算符 说明 &gt;、&lt;、&lt;=、&gt;=、=、&lt;&gt; &lt;&gt;表示不等于,MySQL中也可以使用!=代替 IS NULL 查询某一列为NULL的值,不能使用=NULL 逻辑运算符： 运算符 说明 and/&amp;&amp; 与,and更通用 or/|| 或 not/! 非 IN关键字：IN中每个数据都会作为一次条件，只要满足条件就会被查询出SELECT field FROM tableName WHERE field in (value1, value2, ...) BETWEEN v1 AND v2：范围查询，包括v1和v2 LIKE关键字：模糊查询 通配符 说明 % 匹配任意多个字符(包含0个字符) _ 匹配一个字符 ESCAPE 转义指定字符 SELECT username FROM user WHERE username LIKE &#39;%pbo/_%&#39; ESCAPE &#39;/&#39; 排序查询：SELECT field FROM tabelName WHERE field = value ORDER BY field1 [ASC|DESC], field2[ASC|DESC], ... 同时对多个字段进行排序时，如果第一个字段相同则按第二个字段排序，依此类推 聚合函数： 介绍：聚合函数查询是纵向查询，它对一列的值进行计算，然后返回一个结果值。聚合函数会忽略空值NULL 主要的聚合函数： 函数 作用 max(列) 求列的最大值 min(列) 求列的最小值 avg(列) 求列的平均值 count(列) 统计列的数据条数 sum(列) 求列数据的总和 语法：SELECT fcuntion(field) FROM tableName 对于NULL的记录不会统计，如果要统计需要借助IFNULL函数IFNULL(fieldName, defaultValue)：如果列不为空返回这列的值。如果为NULL则返回默认值修改后的SQL语句：SELECT function(IFNULL(fieldName, defaultValue)) FROM tableName 分组查询: 介绍：GROUP BY语句会将分组字段结果中相同内容作为一组，并且返回每组的第一条数据。因此单独分组没什么用处。分组的目的是为了统计，一般分组查询会跟聚合函数一起使用 语法：SELECT field1, field2, ... FROM tableName GROUP BY field [HAVING condition] HAVING和WHERE子句的区别： 语句 作用 WHERE子句 1. 对查询结果进行分组前，将不符合WHERE条件的行去掉，即在分组之前过滤数据，即先过滤再分组 2. WHERE子句后不可以使用聚合函数 HAVING子句 1. HAVING子句的作用是筛选满足条件的组，即在分组之后过滤数据，即先分组再过滤 2. HAVING子句后可以使用聚合函数 LIMIT语句： 介绍：限制查询记录的条数，用于语句的末尾 语法：LIMIT offset, length offset：起始行数，从0开始计数。省略默认就是0length：返回的行数 表约束和范式 表约束 约束作用：可以一定程度保证数据的正确性、有效性和完整性 约束种类： 约束名 关键字 主键 PRIMARY KEY 唯一 UNIQUE 非空 NOT NULL 外键 FOREIGN KEY(主表中主键列,从表中外键列) 检查约束 CHECK(MySQL不支持) 主键： 特点： 非空，NOT NULL 唯一，UNIQUE 设置语句： 创建表时添加 在已有表中添加主键：ALTER TABLE tableName ADD PRIMARY KEY(fieldName) 删除主键：ALTER TABLE tableName DROP PRIMARY KEY 主键自增： 介绍：建表时使用AUTO_INCREMENT关键字可以指定主键自增，默认开始值为1，且主键必须是整型类型的一个字段 创建表时指定起始值： 12345CREATE TABLE tableName( field1 INT PRIMARY KEY AUTO_INCREMENT, field2 ..., ...)AUTO_INCREMENT=v; 修改主键起始值：ALTER TABLE tableName AUTO_INCREMENT=value DELETE和TRUNCATE对自增长的影响：前者删除记录后自增长无影响；后者删除后自增长从头开始 唯一约束： 特点：某列不能出现重复的值，NULL不存在重复问题 语法：field fieldType UNIQUE 非空约束： 特点：某列不能为NULL 语法：field fieldType NOT NULL 默认值：field fieldType DEFAULT value 外键约束： 语法： 新建表：[CONSTRAINT] [foreignKeyName] FOREIGN KEY(fieldName) REFERENCES tableName(fieldName) [ON UPDATE CASCADE|ON DELETE CASCADE] 已有表新增：ALTER TABLE table1 ADD [CONSTRAINT] [foreignKeyName] FOREIGN KEY(fieldName) REFERENCES table2(fieldName) [ON UPDATE CASCADE|ON DELETE CASCADE] 删除：ALTER TABLE tableName DROP FOREIGN KEY foreignKeyName 级联操作：在修改和删除主表的主键时，同时更新或删除副表的外键值(可同时设置级联更新和删除) 级联操作语法 描述 ON UPDATE CASCADE 级联更新,只能是创建表的时候创建级联关系。更新主表中的主键,从表中的外键列也自动同步更新 ON DELETE CASCADE 级联删除 范式 第一范式(1NF)：数据库表的每一列都是不可分割的原子数据项，不能是集合、数组等非原子数据项。即表中的某个列有多个值时，必须拆分为不同的列。简而言之，第一范式每一列不可再拆分，称为原子性 第二范式(2NF) 介绍：在满足第一范式的前提下，表中的每一个字段都完全依赖于主键所谓完全依赖是指不能存在仅依赖主键一部分的列。简而言之，第二范式就是在第一范式的基础上所有列完全依赖于主键列 特点： 一张表只描述一件事情 表中的每一列都完全依赖于主键 第三范式(3NF)：在满足第二范式的前提下，表中的每一列都直接依赖于主键，而不是通过其它的列来间接依赖于主键。简而言之，第三范式就是所有列不依赖于其它非主键列，也就是在满足2NF的基础上，任何非主列不得传递依赖于主键 多表查询和子查询 多表查询 内连接：用左边表的记录去匹配右边表的记录，如果符合条件的则显示 隐式内连接：看不到JOIN关键字，条件由WHERE指定 SELECT field FROM table1, table2 WHERE condition 显式内连接：使用INNER JOIN … ON格式，可以省略INNER SELECT field FROM table1 [INNER] JOIN table2 ON condition 外连接： 左外连接：用左表记录去匹配右表记录，如果符合条件的则显示原数据；否则显示NULL。可理解为在内连接基础上保证左表数据全部显示。使用LEFT OUTER JOIN … ON格式，可以省略OUTER SELECT field FROM table1 LEFT [OUTER] JOIN table2 ON condition 右外连接：用右表记录去匹配左表记录，如果符合条件的则显示原数据；否则显示NULL。可理解为在内连接基础上保证右表数据全部显示。使用RIGHT OUTER JOIN … ON格式，可以省略OUTER SELECT field FROM table1 RIGHT [OUTER] JOIN table2 ON condition 子查询 介绍： 一个查询的结果做为另一个查询的条件 有查询的嵌套，内部的查询称为子查询 子查询要使用括号 子查询的情况： 结果是一个数据(单行单列)：可以供父查询使用比较运算符 SELECT field FROM tableName WHERE field = (child selection) 结果是一个数组(多行单列)：可以供父查询使用IN运算符 SELECT field FROM tableName WHERE field IN (child selection) 结果是一张表(多行多列)：可作为查询的表或连接表，还需取表别名 SELECT field FROM (child seletion) aliasName WHERE conditionSELECT field FROM table1 [LEFT|RIGHT] JOIN (child selection) aliasName ON condition 事务 介绍：事务执行是一个整体，所有的SQL语句都必须执行成功。如果其中有1条SQL语句出现异常，则所有的SQL语句都要回滚，整个业务执行失败 MySQL执行事务的方式： 手动提交事务 执行语句： 开启事务：start transaction 提交事务：commit 回滚事务：rollback 使用过程： 执行成功：开始事务 -&gt; 执行多条SQL语句 -&gt; 成功完成,提交事务 执行失败：开启事务 -&gt; 执行多条SQL语句 -&gt; 出现异常,事务回滚 自动提交事务：MySQL默认每一条DML(增删改)语句都是一个单独的事务，每条语句都会自动开启一个事务，语句执行完毕自动提交事务 查看MySQL是否开启自动提交事务：SELECT @@autocommit @@表示全局变量，1表示开启，0表示关闭 取消自动提交事务：SET @@autocommit = 0 事务原理：事务开启之后，所有的操作都会临时保存到事务日志中。事务日志只有在收到commit命令才会同步到数据表中，其他任何情况都会清空事务日志(rollback,断开连接) 事务步骤： 客户端连接数据库服务器，创建连接时创建此用户临时日志文件 开启事务以后，所有的操作都会先写入到临时日志文件中 所有的查询操作从表中查询，但会经过日志文件加工后才返回 如果事务提交则将日志文件中的数据写到表中，否则清空日志文件 回滚点：在某些成功的操作完成之后，后续的操作有可能成功有可能失败，但是不管成功还是失败，前面操作都已经成功。可以在当前成功的位置设置一个回滚点，以供后续失败操作返回到该位置，而不是返回所有操作，这个点称之为回滚点 回滚点操作 语句 设置回滚点 savepoint name 回到回滚点 rollback to name 事务的隔离级别 事务四大特性：原子性(Atomicity)、一致性(Consistency)、隔离性(Isolation)、持久性(Durability) 事务的隔离级别： 问题 含义 脏读 一个事务读取到了另一个事务中尚未提交的数据 不可重复读 一个事务中两次读取的数据内容不一致 幻读 一个事务中两次读取的数据的数量不一致 数据库的隔离级别： 级别 名字 隔离级别 脏读 不可重读 幻读 说明 1 读未提交 read uncommitted 是 是 是 \\ 2 读已提交 read committed 否 是 是 Oracle和SQL Server默认的隔离级别 3 可重复读 repeatable read 否 否 是 MySQL默认的隔离级别 4 串行化 serializable 否 否 否 \\ 隔离级别越高，安全级越高，性能越差 MySQL事务隔离级别相关命令： 查询全局事务隔离级别：SELECT @@tx_isolation 设置隔离级别(需要退出后重新登录才生效)：SET GLOBAL TRANSACTION ISOLATION LEVEL 隔离级别字符串 其他 mysqld是MySQL的主程序(服务器端)；mysql是MySQL的命令行工具(客户端) 查看MySQL内部设置的编码：SHOW VARIABLES LIKE &#39;character%&#39; 备份语句：mysqldump -u username -p password database &gt; filePath mysqldump在bin文件目录下是一个可执行文件，用于执行文件备份上述语句用于将数据库备份到本地文件(.sql) 还原语句： 12USE database;SOURCE filePath;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Middleware","slug":"Middleware","permalink":"https://sobxiong.github.io/tags/Middleware/"},{"name":"MySQL","slug":"MySQL","permalink":"https://sobxiong.github.io/tags/MySQL/"}]},{"title":"SpringMVC基础","slug":"SpringSeries/SpringMVC/SpringMVC基础","date":"2020-12-08T11:23:44.000Z","updated":"2020-12-11T03:37:28.614Z","comments":true,"path":"2020/12/08/SpringSeries/SpringMVC/SpringMVC基础/","link":"","permalink":"https://sobxiong.github.io/2020/12/08/SpringSeries/SpringMVC/SpringMVC%E5%9F%BA%E7%A1%80/","excerpt":"内容 SpringMVC概述 常用注解介绍 RestFul介绍 其他细节介绍 SpringMVC核心技术","text":"内容 SpringMVC概述 常用注解介绍 RestFul介绍 其他细节介绍 SpringMVC核心技术 SpringMVC概述 什么是MVC： MVC是模型(Model)、视图(View)和控制器(Controller)的简写，是一种软件设计规范。使用将业务逻辑、数据、显示分离的方式来组织代码，降低了视图与业务逻辑间的双向耦合。MVC是一种架构模式，不同的MVC也存在差异。最典型的MVC就是JSP + servlet + javaBean MVC各自的解释： Model(模型)：数据模型，提供要展示的数据，包含数据和行为 View(视图)：负责进行模型的展示，一般指用户界面 Controller(控制器)：接收用户请求，委托给模型进行处理(状态改变)，处理完毕后把返回的模型数据返回给视图，由视图负责展示。控制器做了调度员的工作 MVC框架需要做的工作： 将url映射到java类或java类的方法 封装用户提交的数据 处理请求：调用相关的业务处理、封装响应数据 将响应的数据进行渲染(jsp/html等表示层数据) 简介：SpringMVC是Spring框架的一部分，是基于Java实现MVC的轻量级Web框架。围绕DispatcherServlet(调度Servlet)设计 优点： 轻量级，简单易学 简洁、灵活、高效，基于请求响应的MVC框架 与Spring兼容性好，无缝结合 约定优于配置 功能强大：RESTful、数据验证、格式化、本地化、主题等 SpringMVC的HelloWorld搭建： 新建maven web项目 引入servlet、SpringMVC依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.2.5.RELEASE&lt;/version&gt;&lt;/dependency&gt; 在web.xml中注册中央处理器 12345678910111213141516171819&lt;!-- 注册中央处理器 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;!-- 指定SpringMVC配置文件路径 --&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- 装载顺序 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- 处理器拦截映射 --&gt;&lt;!-- /会匹配所有请求,/*才会匹配.jsp --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; &lt;load-on-startup&gt; 标记是否在Web服务器(Tomcat)启动时会创建这个Servlet实例，即是否在Web服务器启动时调用执行该Servlet的init()方法，而不是在真正访问时才创建。值必须是一个整数： 当值大于等于0时，表示容器在启动时就加载并初始化这个Servlet；数值越小，该Servlet的优先级就越高，被创建的也就越早 当值小于0或者没有指定时，则表示该Servlet在真正被使用时才会去创建 当值相同时，容器会自己选择创建顺序 &lt;url-pattern/&gt; &lt;init-param&gt;\b自定义SpringMVC配置文件地址 默认要从项目根下的WEB-INF目录下找名称为Servlet名称-servlet.xml的配置文件(即springmvc-servlet.xml)，而一般情况下配置文件是放在类路径下(即resources目录下)。因此在注册中央调度器时还需要为中央调度器设置查找SpringMVC配置文件的路径DispatcherServlet继承自FrameworkServlet，该类中有一个属性contextConfigLocation用于设置SpringMVC配置文件的路径及文件名 声明组件扫描器(Spring配置文件中) 定义目标页面(show.jsp) 创建处理器 12345678910111213@Controller@Slf4jpublic class MyController &#123; @RequestMapping(value = \"/test.do\") public ModelAndView doSome() &#123; log.info(\"Handle /test.do request ~~~\"); ModelAndView modelAndView = new ModelAndView(); modelAndView.addObject(\"msg\", \"testMsg\"); modelAndView.addObject(\"code\", 200); modelAndView.setViewName(\"/show.jsp\"); return modelAndView; &#125;&#125; 若有多个请求路径均可匹配该处理器方法的执行，则@RequestMapping的value属性可赋值一个数组ModelAndView类中的addObject()方法用于向Model中添加数据。Model的底层为HashMap。Model中的数据存储在request作用域中，SringMVC默认采用转发的方式跳转到视图。本次请求结束，模型中的数据会被销毁 修改视图解析器的注册 12345678&lt;!-- 注册视图解析器：帮助处理视图的路径和扩展名,生成视图对象 --&gt;&lt;!-- 注册内部资源视图解析器InternalResourceViewResolver --&gt;&lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;!-- 前缀：表示视图所在的路径 --&gt; &lt;property name=\"prefix\" value=\"/jsp/\"/&gt; &lt;!-- 后缀：表示视图文件的扩展名 --&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt;&lt;/bean&gt; 为了避免对于请求资源路径与扩展名上的冗余，在视图解析器InternalResouceViewResolver中引入请求的前辍与后辍。此时ModelAndView中只需给出要跳转页面的文件名即可。对于具体的文件路径与扩展名，视图解析器会自动完成拼接 修改对应的处理器跳转地址和页面位置 中心控制器DispatcherServlet： 介绍：Spring的web框架围绕DispatcherServlet设计。DispatcherServlet的作用是将请求分发到不同的处理器。SpringMVC框架像许多其他MVC框架一样，以请求为驱动，围绕一个中心Servlet分派请求及提供其他功能。DispatcherServlet是一个实际的Servlet(它继承自HttpServlet基类) SpringMVC的原理：当发起请求时被前置的控制器拦截到请求，根据请求参数生成代理请求，找到请求对应的实际控制器，控制器处理请求，创建数据模型，访问数据库，将模型响应给中心控制器，控制器使用模型与视图渲染视图结果，将结果返回给中心控制器，再将结果返回给请求者 SpringMVC执行逻辑： DispatcherServlet表示前置控制器，是整个SpringMVC的控制中心。用户发出请求，DispatcherServlet接收请求并拦截请求 假设请求的url为：http://localhost:8080/SpringMVC/hellourl可以拆分成三部分： http://localhost:8080：服务器域名 SpringMVC部署在服务器上的web站点 hello表示控制器 如上url表示为：请求位于服务器localhost:8080上的SpringMVC站点的hello控制器 HandlerMapping为处理器映射。DispatcherServlet调用HandlerMapping，HandlerMapping根据请求url查找Handler HandlerExecution表示具体的Handler，其主要作用是根据url查找控制器。示例url被查找控制器为hello HandlerExecution将解析后的信息传递给DispatcherServlet，如解析控制器映射等 HandlerAdapter表示处理器适配器，其按照特定的规则去执行Handler Handler让具体的Controller执行 Controller将具体的执行信息返回给HandlerAdapter，如ModelAndView HandlerAdapter将视图逻辑名或模型传递给DispatcherServlet DispatcherServlet调用视图解析器(ViewResolver)来解析HandlerAdapter传递的逻辑视图名 视图解析器将解析的逻辑视图名传给DispatcherServlet DispatcherServlet根据视图解析器解析的视图结果，调用具体的视图 最终视图呈现给用户 常用注解介绍 @Controller：声明控制器，需要使用包扫描注册进Spring容器 @RequestMapping：定义请求规则 value属性：定义处理器对于请求的映射规则。可以注解在方法上，也可以注解在类上。常以’/‘开始，用于定义所匹配请求的URI 注解于类上：配合@Controller进行使用，表示类中所有响应请求方法以该地址作为父路径 注解于方法上：表示相对于父路径的请求地址 method属性：用于约束请求的类型，可以收窄请求范围。指定请求谓词的类型如GET、POST、HEAD、OPTIONS、PUT、PATCH、DELETE和TRACE等。支持指定多个method produces属性：设置输出结果类型，可用于设置响应体格式和字符集 @GetMapping、PostMapping、PutMapping、DeleteMapping、PatchMapping：组合注解，指定了请求方法的便于使用的@RequestMapping版本 @RequestParam：校正请求参数名 所谓校正请求参数名，是指若请求URL所携带的参数名称与处理方法中指定的参数名不相同时，则需在处理方法参数前添加注解@RequestParam指定请求URL所携带参数的名称。该注解是对处理器方法参数进行修饰的，value属性指定请求参数的名称 @PathVariable：使方法参数的值对应绑定到一个URI模版变量上 路径变量的好处： 使路径变得更加简洁 获得参数更加方便，框架会自动进行类型转换 通过路径变量的类型可以约束访问参数，如果类型不一样，则访问不到对应的请求方法 RestFul介绍 概念：Restful就是一个资源定位及资源操作的风格。不是标准也不是协议，只是一种风格。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制 优点：可以通过不同的请求方式来实现不同的效果。请求地址一样，但是功能可以不同 http://127.0.0.1/item/1 查询,GEThttp://127.0.0.1/item 新增,POSThttp://127.0.0.1/item 更新,PUThttp://127.0.0.1/item/1 删除,DELETE 其他细节介绍 处理器方式参数：处理器方法可以包含以下四类参数，这些参数会在系统调用时由系统自动赋值，可在方法内直接使用 HttpServletRequest HttpServletResponse HttpSession 请求中所携带的请求参数 请求参数中文乱码： 问题介绍：对于接收的请求参数，若含有中文，则会出现中文乱码问题。Spring对于请求参数中的中文乱码问题，给出了专门的字符集过滤器：org.springframework.web.filter包下的CharacterEncodingFilter类 解决方式：在web.xml中注册字符集过滤器即可解决Spring请求参数的中文乱码问题。不过最好将该过滤器注册在其它过滤器之前，因为过滤器的执行是按照注册顺序进行的 1234567891011121314151617181920212223&lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;!-- 指定字符集 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- 强制request使用字符集encoding --&gt; &lt;init-param&gt; &lt;param-name&gt;forceRequestEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- 强制response使用字符集encoding --&gt; &lt;init-param&gt; &lt;param-name&gt;forceResponseEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 处理器方式的返回值 处理器方法常用返回值有四种类型： ModelAndView String void 自定义类型对象 ModelAndView(适用于前后端不分离)若处理器方法处理完后需要跳转到其它资源，且又要在跳转的资源间传递数据，此时处理器方法返回ModelAndView比较好。若要返回ModelAndView，则处理器方法中需要定义ModelAndView对象若该处理器方法只是进行跳转而不传递数据，或只传递数据而并不向任何资源跳转(如对页面的Ajax异步响应)，此时若返回ModelAndView则将总有一部分多余————要么Model多余，要么View多余。此时返回ModelAndView将不合适 String(适用于前后端不分离)处理器方法返回的字符串可以指定逻辑视图名，通过视图解析器(InternalResourceViewResolver内部资源视图解析器)解析可以将其转换为物理视图地址也可以直接返回资源的物理视图名，此时就不需要在视图解析器中再配置前辍与后辍 返回void(了解)若处理器对请求处理后无需跳转到其它任何资源，此时处理器方法可定义为返回void。例如对Ajax异步请求的响应，此时需要使用HttpServletResponse将返回结果通过writer写出 返回对象(常用,适用于前后端分离)返回的对象不是作为逻辑视图出现的，而是作为直接在页面显示的数据出现的。常使用json格式返回，此时需要使用@ResponseBody注解于处理器方法上(将转换后的json数据放入到响应体中) 在spring配置文件中配置&lt;mvc:annotation-driven/&gt;：Object数据转为Json数据需要消息转换器HttpMessageConverter完成。转换器的开启需要由&lt;mvc:annotation-driven/&gt;完成 导入jackson相关的包：spring的转换器底层依赖了jackson将对象转为json数据 &lt;url-pattern&gt;解读： *.xxx：一般情况下SpringMVC的中央调度器DispatcherServlet的&lt;url-pattern&gt;常使用后辍匹配方式，如写为*.do、*.action或*.mvc /：此时中央调度器DispatcherServlet会拦截所有url，会将向静态资源(例如css、js、jpg、png等资源)的获取请求当作是一个普通的处理器请求。此时中央调度器会调用处理器映射器为其查找相应的处理器，这当然是找不到的。因此在这种情况下，所有的静态资源获取请求均会报404错误 SpringMVC静态资源处理： &lt;mvc:default-servlet-handler/&gt;：声明了&lt;mvc:default-servlet-handler/&gt;后SpringMVC框架会在容器中创建DefaultServletHttpRequestHandler处理器对象。它像一个检查员，会对进入DispatcherServlet的URL进行筛查。如果发现是静态资源的请求，就将该请求转由Web应用服务器默认的Servlet处理。一般的服务器都有默认的Servlet &lt;mvc:resources/&gt;：在Spring3.0版本后，Spring定义了专门用于处理静态资源访问请求的处理器ResourceHttpRequestHandler。并且添加了&lt;mvc:resources/&gt;标签，专门用于解决静态资源无法访问问题 123456&lt;!-- location：表示静态资源所在目录;目录不要使用/WEB-INF/及其子目录 mapping：表示对该资源的映射后的请求地址--&gt;&lt;mvc:resources mapping=\"/images/**\" location=\"/images/\"/&gt;&lt;mvc:resources mapping=\"/js/**\" location=\"/js/\"/&gt; 定义方式实现控制器： 介绍：实现接口Controller定义控制器是较老的办法。且一个控制器中只有一个方法，如果要多个方法则需要定义多个Controller；定义的方式比较麻烦 定义步骤： 自定义控制器实现Controller接口，实现handleRequest方法 123456789public class TestController implements Controller &#123; public ModelAndView handleRequest(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) throws Exception &#123; // 返回一个模型视图对象 ModelAndView mv = new ModelAndView(); mv.addObject(\"msg\",\"TestController\"); mv.setViewName(\"test\"); return mv; &#125;&#125; 在SpringMVC配置文件中注册bean 12&lt;!-- name对应请求路径,class对应处理请求的类 --&gt;&lt;bean name=\"/test\" class=\"com.xiong.controller.TestController\"/&gt; 结果跳转方式： ModelAndView：根据设置的view名称和视图解析器跳到指定的页面。页面：{视图解析器前缀} + viewName + {视图解析器后缀} ServletApi：通过设置ServletAPI，不需要视图解析器 通过HttpServletResponse进行输出：httpServletResponse.getWriter().println(&quot;Hello~~~&quot;); 通过HttpServletResponse实现重定向：httpServletResponse.sendRedirect(&quot;/index.jsp&quot;); 通过HttpServletResponse实现转发：httpServletRequest.getRequestDispatcher(&quot;/WEB-INF/jsp/test.jsp&quot;).forward(httpServletRequest, httpServletResponse); SpringMVC 不使用视图解析器： 1234567891011121314151617181920@Controllerpublic class TestController &#123; @RequestMapping(\"/test/t1\") public String test1()&#123; // 转发 return \"/index.jsp\"; &#125; @RequestMapping(\"/test/t2\") public String test2()&#123; // 转发二 return \"forward:/index.jsp\"; &#125; @RequestMapping(\"/test/t3\") public String test3()&#123; // 重定向 return \"redirect:/index.jsp\"; &#125;&#125; 使用视图解析器： 12345678910111213141516@Controllerpublic class TestController2 &#123; @RequestMapping(\"/test2/t1\") public String test1()&#123; //转发 return \"test\"; &#125; @RequestMapping(\"/test2/t2\") public String test2()&#123; // 重定向 return \"redirect:/index.jsp\"; // 重定向到另一个请求 // return \"redirect:hello.do\"; &#125;&#125; 数据接收： 提交域名称和处理方法参数不一致：使用@RequestParam 提交的是一个表单对象：要求提交的表单域和对象属性名一致 数据显示 通过ModelAndView： 12345678@RequestMapping(\"/hello\")public ModelAndView hello()&#123; // 返回一个模型视图对象 ModelAndView mv = new ModelAndView(); mv.addObject(\"msg\",\"hello\"); mv.setViewName(\"test\"); return mv;&#125; 通过ModelMap： 1234567@RequestMapping(\"/hello\")public String hello(ModelMap model)&#123; // 封装要显示到视图中的数据 // 相当于req.setAttribute(\"name\", \"sobxiong\"); model.addAttribute(\"name\", \"sobxiong\"); return \"hello\";&#125; 通过Model： 1234567@RequestMapping(\"/hello\")public String hello(Model model)&#123; // 封装要显示到视图中的数据 // 相当于req.setAttribute(\"name\", \"sobxiong\"); model.addAttribute(\"msg\", \"sobxiong\"); return \"test\";&#125; Model：只有寥寥几个方法，只适合用于储存数据，简化了对于Model对象的操作和理解ModelMap：继承了LinkedMap，除了实现了自身的一些方法，同样继承了LinkedMap的方法和特性ModelAndView：可以在储存数据的同时设置返回的逻辑视图，进行控制展示层的跳转 SpringMVC核心技术 请求转发和重定向： 介绍：当处理器对请求处理完毕后，向其它资源进行跳转时，有两种跳转方式：请求转发与重定向。根据所要跳转的资源类型，又可分为两类：跳转到页面与跳转到其它处理器 对于请求转发的页面，可以是WEB-INF中页面；而重定向的页面，不能为WEB-INF中页面。因为重定向相当于用户再次发出一次请求，而用户是不能直接访问WEB-INF中资源的 原理：SpringMVC框架把原来Servlet中的请求转发和重定向操作进行了封装。现在可以使用简单的方式实现转发和重定向： forward：表示转发，示例如下： request.getRequestDispatcher(&quot;xx.jsp&quot;).forward() redirect：表示重定向，示例如下： response.sendRedirect(&quot;xxx.jsp&quot;) 请求转发： 处理器方法返回ModelAndView时，可在setViewName()指定的视图前添加 “forward:”，此时视图不再与视图解析器一同工作，这样可在配置了解析器时指定不同位置的视图。视图页面必须写出相对于项目根的路径。forward操作不需要视图解析器 处理器方法返回String时，需在视图路径前面加入”forward:视图完整路径” 请求重定向(同请求转发,把forward替换为redirect) 异常处理： 介绍：SpringMVC框架常用@ExceptionHandler和@ControllerAdvice注解处理异常 @ExceptionHandler注解： 介绍：使用注解@ExceptionHandler可以将一个方法指定为异常处理方法。该注解只有一个可选属性value(Class&lt;?&gt;数组)，该属性用于指定该注解的方法所要处理的异常类(即需要匹配的异常)被注解的方法返回值可以是ModelAndView、String或void，方法名随意，方法参数可以是Exception及其子类对象、HttpServletRequest、HttpServletResponse等。系统会自动为这些参数赋值 使用步骤： 准备(自定义异常,自定义异常响应页面等) 在处理器方法上添加注解： 1234567@ExceptionHandler(value = Throwable.class)public ModelAndView doException(Throwable t) &#123; ModelAndView mv = new ModelAndView(); mv.addObject(\"exception\", t); mv.setViewName(\"error\"); return mv;&#125; 不过一般不这样使用，而是将异常处理方法专门定义在一个类中，作为全局的异常处理类 @ControllerAdvice注解： 介绍：ControllerAdvice字面意思就是”控制器增强”，用于给控制器对象增强功能。使用@ControllerAdvice注解修饰的类中可以使用@ExceptionHandler。当使用@RequestMapping注解修饰的方法抛出异常时，会执行@ControllerAdvice修饰的类中的对应异常处理方法@ControllerAdvice由@Component注解修饰，需要由包扫描组件支持 使用步骤： 添加启动包扫描和注册注解驱动的xml配置 定义全局异常处理类 123456789101112131415161718@ControllerAdvicepublic class GlobalExceptionHandler &#123; @ExceptionHandler(value = Exception.class) public ModelAndView doException(Exception e) &#123; ModelAndView mv = new ModelAndView(); mv.addObject(\"exception\",e); mv.setViewName(\"error\"); return mv; &#125; @ExceptionHandler(value = ArithmeticException.class) public ModelAndView doArithmeticException(ArithmeticException e) &#123; ModelAndView mv = new ModelAndView(); mv.addObject(\"arithmeticException\",e); mv.setViewName(\"error\"); return mv; &#125;&#125; 拦截器 介绍：SpringMVC中的Interceptor拦截器非常重要，它的主要作用是拦截指定的用户请求，并进行相应的预处理与后处理。其拦截的时间节点在”处理器映射器根据用户提交的请求映射出了所要执行的处理器类,并且也找到了要执行该处理器类的处理器适配器,在处理器适配器执行处理器之前”。在处理器映射器映射出所要执行的处理器类时，已经将拦截器与处理器组合为了一个处理器执行链，并返回给了中央调度器 拦截器HandlerInterceptor： 介绍：自定义拦截器，需要实现HandlerInterceptor接口 接口方法介绍： preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)：该方法在处理器方法执行之前执行。其返回值为 boolean，若为true，则紧接着会执行处理器方法，且会将afterCompletion()方法放入到一个专门的方法栈中等待执行 postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView)：该方法在处理器方法执行之后执行。处理器方法若最终未被执行，则该方法不会执行。由于该方法是在处理器方法执行完后执行，且该方法参数中包含ModelAndView，因此该方法可以修改处理器方法的处理结果数据，且可以修改跳转地址 afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex)： 当preHandle()方法返回true时，会将该方法放到专门的方法栈中，等到对请求进行响应的所有工作完成之后才执行该方法。即该方法是在中央调度器渲染(数据填充)了响应页面之后执行的，此时对ModelAndView的操作也对响应没有影响。该方法是最后执行的方法，一般用于清除资源 使用步骤： 定义拦截器对象： 123456789101112131415161718@Slf4jpublic class MyInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; log.info(\"preHandle() ~~~\"); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; log.info(\"postHandle() ~~~\"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; log.info(\"afterCompletion() ~~~\"); &#125;&#125; 注册拦截器 1234567891011&lt;!-- 注册拦截器 --&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!-- &lt;mvc:mapping&gt;：用于指定当前注册的拦截器可以拦截的请求路径,/**表示拦截所有请求,可以设置多个 --&gt; &lt;mvc:mapping path=\"/**\"/&gt; &lt;bean class=\"com.xiong.springmvc.controller.MyInterceptor\"/&gt; &lt;/mvc:interceptor&gt; &lt;!-- &lt;mvc:interceptor&gt;可以设置多个 --&gt;&lt;/mvc:interceptors&gt;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://sobxiong.github.io/tags/SpringMVC/"}]},{"title":"DataStructure","slug":"BasicSkill/DataStructure","date":"2020-12-03T14:38:55.000Z","updated":"2020-12-07T15:06:47.188Z","comments":true,"path":"2020/12/03/BasicSkill/DataStructure/","link":"","permalink":"https://sobxiong.github.io/2020/12/03/BasicSkill/DataStructure/","excerpt":"内容 Trie字典树 树状数组","text":"内容 Trie字典树 树状数组 Trie字典树 介绍：又称单词查找树、Trie树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计、排序和保存大量的字符串(但不仅限于字符串)，所以经常被搜索引擎系统用于文本词频统计。优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高 核心思想：空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的 三个基本性质： 根节点不包含字符，除根节点外每一个节点都只包含一个字符 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串 每个节点的所有子节点包含的字符都不相同 基本操作：查找、插入和删除(删除较为少见) 举例：单词组：banana、band、bank、apple、apply、applet图示： 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 封装字典树节点数据结构class TrieNode &#123; private: // 是否为单词结尾字符的标记 bool wordTag = false; // 计数标记,用于删除 int countTag = 0; // 下层节点,26个指针的数组(a-z),并显式赋值为nullptr TrieNode *next[26] = &#123;nullptr&#125;; public: void insert(const string &amp;word) &#123; // 从头节点开始 TrieNode *cur = this; for (const char &amp;c : word) &#123; // 如果查找的字符节点为空则创建 if (!cur-&gt;next[c - 'a']) cur-&gt;next[c - 'a'] = new TrieNode; // 前往下一层节点,表示当前字符已满足 cur = cur-&gt;next[c - 'a']; // 技术标记增加 cur-&gt;countTag++; &#125; // 记录当前是单词结尾字符 cur-&gt;wordTag = true; &#125; bool search(const string &amp;word) &#123; // 从头节点开始 TrieNode *cur = this; for (const char &amp;c : word) &#123; // 如果查找的字符节点为空则返回false,查询不到 if (!cur-&gt;next[c - 'a']) return false; cur = cur-&gt;next[c - 'a']; &#125; // 返回当前节点是否为单词结尾 return cur-&gt;wordTag; &#125; bool searchPrefix(const string &amp;word) &#123; TrieNode *cur = this; for (const char &amp;c : word) &#123; if (!cur-&gt;next[c - 'a']) return false; cur = cur-&gt;next[c - 'a']; &#125; // 返回true,前缀无需关注是否为单词节点 return true; &#125;&#125;;// 具体测试TrieNode root;root.insert(\"banana\");root.insert(\"band\");root.insert(\"bank\");root.insert(\"apple\");root.insert(\"apply\");root.insert(\"applet\");cout &lt;&lt; root.search(\"banana\") &lt;&lt; endl;cout &lt;&lt; root.search(\"banan\") &lt;&lt; endl;cout &lt;&lt; root.searchPrefix(\"apply\") &lt;&lt; endl;cout &lt;&lt; root.search(\"apply\") &lt;&lt; endl;cout &lt;&lt; root.searchPrefix(\"appl\") &lt;&lt; endl;cout &lt;&lt; root.search(\"appl\") &lt;&lt; endl; 树状数组 介绍：树状数组是一个查询和修改复杂度都为logn的数据结构。主要用于数组的单点修改以及区间求和 背景问题：有一个数组a，下标从1到n。现在需要进行w次修改、q次查询，修改指修改数组中某一个元素的值；查询指查询数组中任意一个区间的和 问题分析：首先分析下朴素做法的时间复杂度，修改是O(1)的时间复杂度，而查询的话是O(n)的复杂度，总体时间复杂度为O(qn)；可能会想到使用前缀和来优化这个查询，此时查询的复杂度为O(1)，而修改时需要修改修改点之后的所有前缀和，因此修改的时间复杂度是O(n)，总体时间复杂度还是O(qn)。而树状数组的做法综合了这两种朴素方式，降低了整体时间复杂度 树状数组 lowbit函数 作用：求某个数的二进制表示中最低的一位1 代码表示：int lowbit(int x) return x &amp; -x; 原理： 核心思想：设原数组为a，节点值为A1～An，新开辟的数组为c，节点值为C1～Cn。每个节点不知管辖当前节点的值，而是管辖一个区域的数据。设节点编号为x，那么这个节点管辖的区间为2^k(其中k为x的二进制末尾0的个数)个元素。且区间最后一个元素必为Ax C1 = A1C2 = A1 + A2C3 = A3C4 = A1 + A2 +A3 + A4C5 = A5C6 = A5 + A6C7 = A7C8 = A1 + A2 + A3 + A4 + A5 + A6 + A7 + A8… \b关键操作 获取前缀和：通过二进制拆分获取所需下标。比如13的前缀和，13的二进制为1101，根据末尾的1获取数，每次抹去最后一个1。得到三个值1101(13,管辖2^0=1个数)、1100(12,管辖2^2=4个数)和1000(8,管辖2^3=8个数)，因此Sum(0~13) = C8 + C12 + C13。因此单次查询复杂度为O(logn) 单点修改：必须修改每个包含修改值的C元素，相当于查询的逆过程。以6为例，6的二进制值为0110，在末尾1处加1，得到1000(8)；再在末尾1处加1，得到10000(16)，完成，共需修改6、8、16处节点的值。因此单次修改复杂度也为O(logn) 实现代码： 1234567891011121314151617181920// 求某个数的二进制表示中最低的一位1int lowbit(int x) &#123; return x &amp; -x;&#125;// 查询1~x的前缀和int query(int x, vector&lt;int&gt; c) &#123; int ans = 0; while (x) &#123; ans += c[x]; x -= lowbit(x); &#125; return ans;&#125;// 对Ax原值基础上加上一个值vvoid add(int x, int v, vector&lt;int&gt; c) &#123; while (x &lt; c.size()) &#123; c[x] += v; x += lowbit(x); &#125;&#125; 扩展：区间修改 比如将C区间[3, 6]的每个数都加上5，只需对C3加5，C7减5，因为C[3, 6]被C3影响，而C7~Cn受到C7和C3的共同影响，结果不变","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BasicSkill","slug":"BasicSkill","permalink":"https://sobxiong.github.io/tags/BasicSkill/"}]},{"title":"Spring基础","slug":"SpringSeries/Spring/Spring基础","date":"2020-12-03T10:29:41.000Z","updated":"2020-12-07T11:34:07.793Z","comments":true,"path":"2020/12/03/SpringSeries/Spring/Spring基础/","link":"","permalink":"https://sobxiong.github.io/2020/12/03/SpringSeries/Spring/Spring%E5%9F%BA%E7%A1%80/","excerpt":"内容 Spring概述 IOC控制反转 AOP面向切面编程 事务操作 Spring5新功能","text":"内容 Spring概述 IOC控制反转 AOP面向切面编程 事务操作 Spring5新功能 Spring概述 Spring是什么：Spring是于2003年兴起的一个轻量级的Java开发框架，是为了解决企业应用开发的复杂性而创建的。核心是控制反转(IoC)和面向切面编程(AOP) Spring的优点： 轻量：Spring框架使用的jar都比较小，核心功能的所需的jar总共在3M左右。框架运行占用的资源少，运行效率高。不依赖其他jar 解耦合：提供了Ioc控制反转，由容器管理对象和对象的依赖关系。原来在程序代码中的对象创建方式现在由容器完成。对象之间的依赖解耦合 AOP编程的支持 方便集成各种优秀框架 Spring体系结构：Spring由20多个模块组成，可以分为数据访问/集成(Data Access/Integration)、Web、面向切面编程(AOP, Aspects)、提供JVM的代理(Instrumentation)、消息发送(Messaging)、核心容器(Core Container)和测试(Test) IOC控制反转 Ioc介绍：控制反转(IoC, Inversion of Control)是一个概念、一种思想。指将传统上由程序代码直接操控的对象控制权交给容器，通过容器来实现对象的装配和管理。控制反转就是将对象控制权的转移，从程序代码本身反转到了外部容器。通过容器实现对象的创建、属性赋值、依赖的管理IoC的实现方式多种多样，当前比较流行的实现方式是依赖注入(DI)依赖：A类中含有B的实例，在A实例中调用B实例的方法完成功能，即类A对类B存在依赖依赖注入(DI, Dependency Injection)：指程序运行过程中若需要调用另一个对象协助时，无须在代码中创建被调用者，而是依赖于外部容器，由外部容器创建后传递给程序Spring使用依赖注入(DI)实现IoC。Spring的依赖注入对调用者与被调用者几乎没有任何要求，完全支持对象之间依赖关系的管理。Spring容器是一个超级工厂，负责创建、管理所有的Java对象，这些Java对象被称为Bean。Spring容器管理着容器中Bean之间的依赖关系 IOC实现基本介绍： IOC思想基于IOC容器实现，IOC容器底层就是对象工厂 Spring提供IOC容器实现的两种方式(两种接口)： BeanFactory：IOC容器的基本实现，是Spring内部的使用接口，不提供开发人员进行使用。加载配置文件时不会创建对象，在获取对象(使用)才去创建对象 ApplicationContext：BeanFactory接口的子接口，提供更多更强大的功能，一般由开发人员进行使用。加载配置文件时候就会把在配置文件对象进行创建 具体ApplicationContext容器创建方式： 基于xml：ApplicationContext context = new ClassPathXmlApplicationContext(&quot;chapter2/bean1.xml&quot;); 基于注解：ApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class); ApplicationContext容器对象的装配时机：在容器对象初始化时，将其中的所有对象一次性全部装配好。以后代码中使用到这些对象只需从内存中直接获取即可。执行效率较高，但内存占用较多 IOC管理Bean 什么是Bean管理： Spring创建对象 Spring注入属性 Bean管理的两种方式： 基于xml： 基础语法：&lt;bean id=&quot;user&quot; class=&quot;chapter1.User&quot;/&gt; bean标签用于实现对象的创建 常用属性有： id：唯一标识 class：类全路径(包类路径)，只能是类，不能是接口 默认通过反射调用无参构造器创建对象 指定多个配置文件： 多个配置文件中有一个总文件，总配置文件将各其它子文件通过&lt;import/&gt;引入。在Java代码中只需要使用总配置文件对容器进行初始化即可也可使用通配符，此时要求父配置文件名不能满足所能匹配的格式，否则将出现循环递归包含 属性注入方式： 通过setter注入(最常用)： 123456&lt;bean id=\"book\" class=\"chapter1.Book\"&gt; &lt;!-- bean对应类需要有对应的setter方法 --&gt; &lt;!-- 引用类型通过ref指定引用关系,ref值为某bean的id值 --&gt; &lt;property name=\"bName\" value=\"深入理解JVM\"/&gt; &lt;property name=\"bAuthor\" value=\"周志明\"/&gt;&lt;/bean&gt; 通过有参构造函数注入(一般不用)： 123456&lt;bean id=\"book\" class=\"chapter1.Book\"&gt; &lt;!-- bean对应类需要有对应的有参构造器 --&gt; &lt;!-- index指明对应构造器的第几个参数,从0开始,一般不用,顺序与构造器声明顺序一致 --&gt; &lt;constructor-arg name=\"bName\" value=\"深入理解JVM\"/&gt; &lt;constructor-arg name=\"bAuthor\" value=\"周志明\"/&gt;&lt;/bean&gt; 通过p名称空间注入(也是调用setter方法,方式不同,一般不用)： 12&lt;!-- 添加p名称空间 --&gt;&lt;bean id=\"book\" class=\"chapter1.Book\" p:bName=\"深入理解JVM\" p:bAuthor=\"周志明\"/&gt; 自动装配：根据指定装配规则(属性名称或者属性类型)，Spring自动将匹配的属性值进行注入 1234567891011121314&lt;!-- bean标签属性autowire配置自动装配,常用的两个配置： byName：根据属性名称注入,注入属性的bean的id值和属性名称一样 byType：根据属性类型注入(要么与属性类型相同,要么有继承或实现的关系,但同源的bean只能有一个,多于一个容器就不知道如何匹配)--&gt;&lt;bean id=\"employee\" class=\"chapter1.Employee\" autowire=\"byName/byType\"&gt; &lt;property name=\"name\" value=\"SOBXiong\"/&gt; &lt;property name=\"age\" value=\"23\"/&gt;&lt;/bean&gt;&lt;bean id=\"dept\" class=\"chapter1.Department\"&gt; &lt;property name=\"name\" value=\"独栋小别墅\"/&gt; &lt;property name=\"id\" value=\"90001\"/&gt;&lt;/bean&gt; 导入外部属性文件(properties文件)： 传统声明方式 123456&lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/test\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"password\"/&gt;&lt;/bean&gt; 导入properties文件方式 1234prop.driverClass=com.mysql.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/testprop.userName=rootprop.password=password 123456789&lt;!-- 引入外部属性文件(需要引入context名称空间) --&gt;&lt;context:property-placeholder location=\"classpath:jdbc.properties\"/&gt;&lt;!-- 数据库连接池配置(引入properties文件) --&gt;&lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"$&#123;prop.driverClass&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;prop.url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;prop.userName&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;prop.password&#125;\"/&gt;&lt;/bean&gt; 注入属性类型： 字面量： null值 123&lt;property name=\"address1\"&gt; &lt;null/&gt;&lt;/property&gt; 属性值包含特殊字符 123456&lt;property name=\"address2\"&gt; &lt;!-- 使用&lt;![CDATA[]]转义 --&gt; &lt;value&gt;&lt;![CDATA[&lt;&lt;南京&gt;&gt;]]&gt;&lt;/value&gt; &lt;!-- 使用&amp;lt;/&amp;gt;代表'&lt;'和'&gt;' --&gt; &lt;!--&lt;value&gt;&amp;lt;&amp;lt;南京&amp;gt;&amp;gt;&lt;/value&gt;--&gt;&lt;/property&gt; 外部bean： 引用外部bean 12345678&lt;bean id=\"person\" class=\"chapter1.Person\"&gt; &lt;property name=\"name\" value=\"SOBXiong\"/&gt; &lt;property name=\"book\" ref=\"book1\"/&gt;&lt;/bean&gt;&lt;bean id=\"book1\" class=\"chapter1.Book\"&gt; &lt;property name=\"BName\" value=\"并发编程实战\"/&gt; &lt;property name=\"BAuthor\" value=\"Doug Li\"/&gt;&lt;/bean&gt; 内部bean赋值 123456789&lt;bean id=\"person\" class=\"chapter1.Person\"&gt; &lt;property name=\"name\" value=\"SOBXiong\"/&gt; &lt;property name=\"book\"&gt; &lt;bean class=\"chapter1.Book\"&gt; &lt;property name=\"BName\" value=\"并发编程实战\"/&gt; &lt;property name=\"BAuthor\" value=\"Doug Li\"/&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 级联赋值 12345678910&lt;!-- 引用设置重复属性会覆盖原来外部bean设置的属性值 --&gt;&lt;bean id=\"person\" class=\"chapter1.Person\"&gt; &lt;property name=\"name\" value=\"SOBXiong\"/&gt; &lt;property name=\"book\" ref=\"book1\"/&gt; &lt;property name=\"book.BAuthor\" value=\"Doug Li\"/&gt; &lt;property name=\"book.BName\" value=\"并发编程实战1\"/&gt;&lt;/bean&gt;&lt;bean id=\"book1\" class=\"chapter1.Book\"&gt; &lt;property name=\"BName\" value=\"并发编程实战\"/&gt;&lt;/bean&gt; 数组/集合类： 使用内置标签： 1234567891011121314151617181920212223242526272829303132333435&lt;!-- ref同理 --&gt;&lt;bean id=\"student\" class=\"chapter1.Student\"&gt; &lt;!-- 注入数组 --&gt; &lt;property name=\"arrayValue\"&gt; &lt;array&gt; &lt;value&gt;array01&lt;/value&gt; &lt;value&gt;array02&lt;/value&gt; &lt;value&gt;array03&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!-- 注入List --&gt; &lt;property name=\"listValue\"&gt; &lt;list&gt; &lt;value&gt;list01&lt;/value&gt; &lt;value&gt;list02&lt;/value&gt; &lt;value&gt;list03&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- 注入Map --&gt; &lt;property name=\"mapValue\"&gt; &lt;map&gt; &lt;entry key=\"mapKey01\" value=\"mapValue01\"/&gt; &lt;entry key=\"mapKey02\" value=\"mapValue02\"/&gt; &lt;entry key=\"mapKey03\" value=\"mapValue03\"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- 注入Set --&gt; &lt;property name=\"setValue\"&gt; &lt;set&gt; &lt;value&gt;setValue01&lt;/value&gt; &lt;value&gt;setValue02&lt;/value&gt; &lt;value&gt;setValue03&lt;/value&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt; 使用util名称空间 12345678910&lt;!-- 使用util标签注入list集合 --&gt;&lt;util:list id=\"testList\"&gt; &lt;value&gt;utilList01&lt;/value&gt; &lt;value&gt;utilList02&lt;/value&gt; &lt;value&gt;utilList03&lt;/value&gt;&lt;/util:list&gt;&lt;bean id=\"student1\" class=\"chapter1.Student\"&gt; &lt;property name=\"listValue\" ref=\"testList\"/&gt;&lt;/bean&gt; 基于注解： Spring针对Bean创建对象的注解类型(功能是一样的,名称只是规范和区别作用)： @Component @Service @Controller @Repository 基于注解方式实现对象创建(引入spring-aop依赖)： 开启组件扫描 xml方式： 123456&lt;!-- 1、可以使用多个context:component-scan标签指定不同的包路径 2、指定base-package的多个值可以使用分隔符(逗号、分号或空格均可,但不建议空格) 3、base-package代表指定到父包名,容器启动会扫描包及其子包中的注解--&gt;&lt;context:component-scan base-package=\"chapter2\"/&gt; 配置类注解(替代xml配置文件)方式： 123@Configuration // 作为配置类,替代xml@ComponentScan(basePackages = &#123;\"chapter2\"&#125;)public class SpringConfig &#123;&#125; 组件扫描规则设置： use-default-filters：是否使用默认filter(扫描@Component、@Service、@Controller、@Repository,默认true)，为true时不能和exclude-filter配合使用 include-filter：设置扫描哪些内容 exclude-filter：设置不扫描哪些内容 创建Bean类： 1234567// value属性值表示bean的id,可省略,默认是类名的首字母小写@Component(value = \"userService\")public class UserService &#123; public void method1() &#123; System.out.println(\"UserService method1() ~~~\"); &#125;&#125; 属性注入(注解在field字段上,无需setter)： @Autowired：根据属性类型进行自动装配(required属性可指定是否忽略注入失败,默认true) @Qualifier：根据名称进行注入，需要和@Autowired一起使用 @Resource：默认根据名称注入，可通过value和name属性指定注入规则(可以加到setter()上) @Value：注入普通类型属性，常量/配置文件中的值(可以加到setter()上) Bean的类型： 普通bean：在配置文件中定义的bean类型就是返回类型 工厂bean(FactoryBean)：在配置文件定义的bean类型可以和返回类型不一样 工厂bean创建的步骤： 自定义类实现接口FactoryBean，作为工厂bean 实现接口里面的方法，在实现的方法中定义返回的bean类型 在xml中定义 具体案例： 自定义工厂bean 12345678910111213public class TestBeanFactory implements FactoryBean&lt;Course&gt; &#123; // 返回Course对象 public Course getObject() throws Exception &#123; Course course = new Course(); course.setId(1L); course.setName(\"分布式系统\"); return course; &#125; public Class&lt;?&gt; getObjectType() &#123; return Course.class; &#125; public boolean isSingleton() &#123; return false; &#125;&#125; Spring的xml配置文件&lt;bean id=&quot;testBeanFactory&quot; class=&quot;chapter1.TestBeanFactory&quot;/&gt; Bean的作用域 Singleton：单实例，Spring默认的Bean作用域；加载Spring配置文件时就会创建单实例对象 Prototype：多实例，在调用getBean()方法时创建多实例对象 Bean的生命周期 生命周期介绍：从对象创建到销毁的过程 具体生命周期过程： 通过构造器创建bean实例(无参数构造) 为bean的属性设置值或对其他bean的引用(调用setter方法) 调用bean的初始化方法(需要配置初始化方法) bean使用(对象获取到并使用) 当容器关闭时候，调用bean的销毁的方法(需要配置销毁方法) 代码演示： xml配置如下 123&lt;bean id=\"order\" class=\"chapter1.Order\" init-method=\"initMethod\" destroy-method=\"destroyMethod\"&gt; &lt;property name=\"name\" value=\"testOrder\"/&gt;&lt;/bean&gt; Java代码如下 123456789101112131415161718192021222324252627282930class Order &#123; private String name; public Order() &#123; System.out.println(\"Order() ~~~\"); &#125; public void setName(String name) &#123; this.name = name; System.out.println(\"setName\"); &#125; public void initMethod() &#123; System.out.println(\"initMethod() ~~~\"); &#125; public void destroyMethod() &#123; System.out.println(\"destroyMethod() ~~~\"); &#125;&#125;@Testpublic void test4() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"chapter1/bean3.xml\"); Order order = context.getBean(\"order\", Order.class); System.out.println(\"Get order bean ~~~\"); System.out.println(\"order = \" + order); // 显式关闭容器对象,让bean实例销毁,否则不会触发destroyMethod ((ClassPathXmlApplicationContext) context).close();&#125; 具体运行结果如下 123456Order() ~~~setNameinitMethod() ~~~Get order bean ~~~order = chapter1.Order@dd0c991destroyMethod() ~~~ Bean的后置处理器对Bean生命周期的影响： 通过构造器创建bean实例(无参数构造) 为bean的属性设置值或对其他bean的引用(调用setter方法) 将bean实例传递给bean的后置处理器，执行postProcessBeforeInitialization()方法 调用bean的初始化方法(需要配置初始化方法) 将bean实例传递给bean的后置处理器，执行postProcessAfterInitialization()方法 bean使用(对象获取到并使用) 当容器关闭时候，调用bean的销毁的方法(需要配置销毁方法) 代码演示(基于上个例子)：增加的xml配置如下&lt;bean id=&quot;testBeanPostProcessor&quot; class=&quot;chapter1.TestBeanPostProcessor&quot;/&gt;增加的java代码如下 1234567891011public class TestBeanPostProcessor implements BeanPostProcessor &#123; public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessBeforeInitialization() ~~~ \" + beanName + \" , \" + bean); return bean; &#125; public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessAfterInitialization() ~~~ \" + beanName + \" , \" + bean); return null; &#125;&#125; 具体运行结果如下 12345678Order() ~~~setNamepostProcessBeforeInitialization() ~~~ order , chapter1.Order@5609159binitMethod() ~~~postProcessAfterInitialization() ~~~ order , chapter1.Order@5609159bGet order bean ~~~order = chapter1.Order@5609159bdestroyMethod() ~~~ AOP面向切面编程 介绍：AOP(Aspect Oriented Programming)意为面向切面编程。利用AOP可以对业务逻辑的各个部分进行隔离，从而使业务逻辑各部分之间的耦合度降低，提高程序的可重用性，提高开发的效率。AOP可以将交叉业务逻辑封装成切面，利用将切面织入到主业务逻辑中。所谓交叉业务逻辑是指通用的、与主业务逻辑无关的代码，如安全检查、事务、日志、缓存等 底层原理：动态代理 存在接口：使用JDK动态代理(创建接口实现类代理对象,增强类的方法) 案例： 123456789101112131415161718192021/* 调用Proxy类的newProxyInstance()方法返回代理对象 public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 参数解释如下： loader：类加载器 interfaces：增强方法所在的类实现的接口,支持多接口 InvocationHandler：创建代理对象,增强功能的主要逻辑*/@Testpublic void test1() &#123; UserDaoImpl impl = new UserDaoImpl(); UserDao userDao = (UserDao) Proxy.newProxyInstance(this.getClass().getClassLoader(), new Class[]&#123;UserDao.class&#125;, (proxy, method, args) -&gt; &#123; log.info(\"Before method: &#123;&#125; , args: &#123;&#125;\", method.getName(), Arrays.toString(args)); Object res = method.invoke(impl, args); log.info(\"After method return: &#123;&#125;\", res); return res; &#125;); userDao.add(5, 10); log.info(\"UserDao: &#123;&#125;\", userDao);&#125; 打印结果： 123452020-12-05 20:22:03.759 [main] INFO chapter3.ProxyTest - Before method: add , args: [5, 10]2020-12-05 20:22:03.762 [main] INFO chapter3.ProxyTest - After method return: 152020-12-05 20:22:03.763 [main] INFO chapter3.ProxyTest - Before method: toString , args: null2020-12-05 20:22:03.763 [main] INFO chapter3.ProxyTest - After method return: chapter3.UserDaoImpl@4b44655e2020-12-05 20:22:03.763 [main] INFO chapter3.ProxyTest - UserDao: chapter3.UserDaoImpl@4b44655e 不存在接口：使用Cglib动态代理(创建子类的代理对象,增强类的方法) AOP中的术语： 连接点：指可以被切面织入的具体方法。通常业务接口中的方法均为连接点 切入点：指声明的一个或多个连接点的集合。被标记为final的方法不能作为连接点与切入点 通知(增强)：表示切面的执行时间，也叫增强。通知定义了增强代码切入到目标代码的时间点。通知类型不同，切入时间不同。切入点定义切入的位置，通知定义切入的时间 切面：切面泛指交叉业务逻辑。实际就是对主业务逻辑的一种增强 目标对象：指将要被增强的对象。即包含主业务逻辑的类的对象 Spring中使用AOP 基本介绍：Spring底层使用到AspectJ实现AOP功能。Spring框架一般也是基于AspectJ提供AOP功能 相关依赖：Spring-aspects(包含aspectj) 切入点表达式： 作用：对哪个类的那个方法进行增强 语法结构： 1execution(modifiers-pattern? ret-type-pattern declaring-type-pattern?name-pattern(param-pattern) throws-pattern?) modifiers-pattern：访问权限类型ret-type-pattern：返回值类型declaring-type-pattern：包名类名name-pattern(param-pattern)：方法名(参数类型和参数个数)throws-pattern：抛出异常类型‘?’表示可选的部分总体可表示为：execution(访问权限 方法返回值 方法声明(参数) 异常类型) 涉及到的符号解释： 符号 意义 * 0至多个任意字符 .. 用在方法参数中表示任意多个参数;用在包名后表示当前包及其子包路径 + 用在类名后表示当前类及其子类;用在接口后表示当前接口及其实类 具体例子：execution(* com.xiong.dao.DocService.*(..)) 具体使用： 在业务类上声明@Component注解让其加入Spring容器 创建增强类(编写增强逻辑) 声明@Component加入Spring容器 声明@Aspect需要生成代理对象 配置不同类型的通知(使用切入点表达式) 可选项，有多个增强类对同一个方法进行增强时可以声明@Order(int priority)设置增强类优先级，数值类型越小优先级越高 注解配置切入点案例如下： 123456789101112131415161718192021222324252627282930313233343536373839@Slf4j@Component@Aspectclass UserProxy &#123; // 前置通知,可有JoinPoint参数 @Before(value = \"execution(* chapter3.User.add(..))\") public void methodBefore() &#123; log.info(\"UserProxy methodBefore() ~~~\"); &#125; // 后置通知(返回通知),注解中有returning属性用于接受目标方法返回值 @AfterReturning(value = \"execution(* chapter3.User.add(..))\") public void methodAfterReturning() &#123; log.info(\"UserProxy methodAfterReturning() ~~~\"); &#125; // 最终通知 @After(value = \"execution(* chapter3.User.add(..))\") public void methodAfter() &#123; log.info(\"UserProxy methodAfter() ~~~\"); &#125; // 异常通知,注解中有throwing属性用于接受抛出的异常 @AfterThrowing(value = \"execution(* chapter3.User.add(..))\") public void methodAfterThrowing() &#123; log.info(\"UserProxy methodAfterThrowing() ~~~\"); &#125; // 环绕通知 @Around(value = \"execution(* chapter3.User.add(..))\") public Object methodAround(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; log.info(\"UserProxy methodAround() ~~~\"); log.info(\"Before jointPoint invoke ~~~\"); // 被增强的方法执行 Object ans = proceedingJoinPoint.proceed(); log.info(\"After jointPoint invoke ~~~\"); return ans; &#125;&#125; 抽取相同的切入点进行化简： 12345678// 抽取相同的切入点@Pointcut(value = \"execution(* chapter3.User.add(..))\")public void pointcutMethod() &#123;&#125;@Before(value = \"pointcutMethod()\")public void pointcutBefore() &#123; log.info(\"UserProxy pointcutBefore() ~~~\");&#125; xml配置切入点案例如下： 12345678910&lt;!-- 配置aop增强 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut id=\"pointcut\" expression=\"execution(* chapter3.Book.buy())\"/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:aspect ref=\"bookProxy\"&gt; &lt;!-- 增强作用在具体的方法上 --&gt; &lt;aop:before method=\"beforeMethod\" pointcut-ref=\"pointcut\"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 配置Spring环境(xml/注解) 基于xml： 1234&lt;!-- 开启组件自动扫描 --&gt;&lt;context:component-scan base-package=\"chapter3\"/&gt;&lt;!-- 开启Aspect生成代理对象 --&gt;&lt;aop:aspectj-autoproxy/&gt; 基于注解： 1234@Configuration@ComponentScan(basePackages = &#123;\"chapter3\"&#125;)@EnableAspectJAutoProxy(proxyTargetClass = true)public class AspectConfig &#123;&#125; 事务操作 基础介绍：事务是数据库中的概念(一组操作要么全部成功,要么全部失败)，在Dao层，但一般将事务提升到Service业务层 四个特性(ACID)：原子性、一致性、隔离性、持久性 Spring中的事务管理： 编程式事务管理 123456789try &#123; transaction.begin(); doSomething();&#125;catch (Exception e)&#123; doSomethingAfterException(e); transaction.rollback();&#125;finally &#123; transaction.commit();&#125; 声明式事务管理 推荐使用，底层使用AOP Spring事务管理类： 基类为PlatformTransactionManager接口，定义了事务的提交、回滚以及获取事务的状态信息操作 两个常用的实现类： DataSourceTransactionManager：使用JDBC或MyBatis进行数据库操作时使用 HibernateTransactionManager：使用Hibernate(JPA)进行持久化数据时使用 Spring的回滚方式：默认发生运行时异常和error时回滚，发生受查(编译)异常时提交。对于受查异常也可以手工设置其回滚方式 Throwable类是Java语言中所有错误或异常的超类。只有当对象是此类(或其子类之一)的实例时，才能通过Java虚拟机或者throw语句抛出Error是程序在运行过程中出现的无法处理的错误，比如OutOfMemoryError、ThreadDeath、NoSuchMethodError等。当这些错误发生时，程序是无法处理(捕获或抛出)的，JVM一般会终止线程程序在编译和运行时出现的另一类错误称之为异常，它是JVM通知程序员的一种方式。通过这种方式，让程序员知道已经或可能出现错误，要求程序员对其进行处理。异常分为运行时异常与受查异常：运行时异常，是RuntimeException类或其子类，即只有在运行时才出现的异常。如NullPointerException、ArrayIndexOutOfBoundsException、IllegalArgumentException等均属于运行时异常。这些异常由JVM抛出，在编译时不要求必须处理(捕获或抛出)。只要代码编写足够仔细，程序足够健壮，运行时异常是可以避免的受查异常，也叫编译时异常(即在代码编写时要求必须捕获或抛出的异常)。若不处理，则无法通过编译。如SQLException、ClassNotFoundException和IOException等都属于受查异常RuntimeException及其子类以外的异常均属于受查异常。自定义的Exception的子类(即用户自定义的异常)也属受查异常。在定义异常时，只要未明确声明定义的为RuntimeException的子类，那么定义的就是受查异常 事务管理操作步骤： 声明需要事务操作的方法/类： 基于注解方式(声明@Transactional注解)： @Transactional用在方法上：只能用于public方法上。对于其他非public方法，如果声明了注解@Transactional，虽然Spring不会报错，但不会将指定事务织入到该方法中。Spring会忽略掉所有非public方法上的@Transaction注解@Transactional用在在类上：则表示该类上所有的方法均将在执行时织入事务 123456@Transactional(propagation = Propagation.REQUIRED, rollbackFor = Throwable.class)public void transferAccount(String fromUsername, String toUsername, Double money) &#123; accountDao.reduceMoney(fromUsername, money); // int i = 10 / 0; accountDao.addMoney(toUsername, money);&#125; 基于xml配置方式： 配置通知： 1234567&lt;!-- 配置通知 --&gt;&lt;tx:advice id=\"txadvice\"&gt; &lt;!-- 配置事务参数 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"transferAccount\" propagation=\"REQUIRED\"/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 配置切入点和切面： 1234567&lt;!-- 配置事务切入点和切面 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut id=\"pt\" expression=\"execution(* chapter5.AccountService.*(..))\"/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=\"txadvice\" pointcut-ref=\"pt\"/&gt;&lt;/aop:config&gt; 配置事务管理器 基于注解方式(此方式只支持注解方式声明)： 123456789101112131415161718192021222324252627282930313233@Configuration // 配置类@ComponentScan(basePackages = \"chapter5\") // 配置包扫描@EnableTransactionManagement // 开启事务public class SpringAnnoConfig &#123; // 创建数据库连接池 @Bean public DruidDataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(\"com.mysql.jdbc.Driver\"); dataSource.setUrl(\"jdbc:mysql:///test\"); dataSource.setUsername(\"root\"); dataSource.setPassword(\"password\"); return dataSource; &#125; // 创建JdbcTemplate对象 @Bean public JdbcTemplate jdbcTemplate(DataSource dataSource) &#123; // 会在ioc容器中根据类型找到dataSource JdbcTemplate jdbcTemplate = new JdbcTemplate(); // 注入dataSource jdbcTemplate.setDataSource(dataSource); return jdbcTemplate; &#125; // 创建事务管理器 @Bean public DataSourceTransactionManager transactionManager(DataSource dataSource) &#123; DataSourceTransactionManager transactionManager = new DataSourceTransactionManager(); transactionManager.setDataSource(dataSource); return transactionManager; &#125;&#125; 基于xml方式(是否声明支持注解根据声明事务的方式) 配置基于注解的声明方式： 123456789&lt;!-- 事务管理器tx --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 注入dataSource --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;!-- 开启事务注解,需要引入tx命名空间 --&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt;&lt;!-- 开始包扫描 --&gt;&lt;context:component-scan base-package=\"chapter5\"/&gt; 配置基于配置的声明方式： 12345&lt;!-- 事务管理器tx --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 注入dataSource --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt; 事务注解相关属性介绍： propagation：事务传播行为(处于不同事务中的方法在相互调用时,执行期间事务的维护情况)，默认REQUIRED 传播属性 描述 PROPAGATION_REQUIRED Spring默认的事务传播行为;如果存在当前事务则用当前事务;否则就新建一个事务 PROPAGATION_REQUIRES_NEW 如果当前存在事务把当前事务挂起,开启一个新事务,新事务执行完毕后唤醒之前挂起的事务继续执行。如果不存在当前事务则新建一个事务 PROPAGATION_SUPPORTS 支持当前事务,如果当前没有事务就以非事务方式执行 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作,如果当前存在事务就把当前事务挂起 PROPAGATION_MANDATORY 支持当前事务,如果当前没有事务就抛出异常 PROPAGATION_NEVER 以非事务方式执行,如果当前存在事务则抛出异常 PROPAGATION_NESTED 如果当前存在事务则在当前事务的嵌套事务内执行。否则启动一个新的事务并在自己的事务内运行 ioslation：事务隔离级别，默认DEFAULT 存在三种读问题： 读问题 描述 脏读 一个未提交事务读取到另一个未提交事务的数据 不可重复读 一个未提交事务读取到另一提交事务修改数据 幻读 一个未提交事务读取到另一提交事务添加数据 提供的四种隔离级别： 隔离级别 脏读 不可重复读 幻读 DEFAULT 采用数据库默认的事务隔离级别 \\ \\ READ_UNCOMMITTED(读未提交) 有 有 有 READ_COMMITTED(读已提交) 无 有 有 REPEATABLE_READ(可重复读) 无 无 有 SERIALIZABLE(串行化) 无 无 无 MySQL默认隔离级别为REPEATABLE_READ,Oracle默认为READ_COMMITTED timeout：用于设置本操作与数据库连接的超时时限 单位为秒，默认-1(即没有时限) readOnly：用于设置该方法对数据库的操作是否是只读的，默认false rollbackFor：指定需要回滚的异常类 类型为Class[]，默认值为空数组。若只有一个异常类时，可以不使用数组 rollbackForClassName：指定需要回滚的异常类类名 类型为String[]，默认值为空数组。若只有一个异常类时，可以不使用数组 noRollbackFor：指定不需要回滚的异常类 noRollbackForClassName：指定不需要回滚的异常类类名 Spring5新功能","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"}]},{"title":"SpringCloud基础","slug":"SpringSeries/SpringCloud/SpringCloud基础","date":"2020-11-18T11:03:17.000Z","updated":"2020-11-21T11:25:08.189Z","comments":true,"path":"2020/11/18/SpringSeries/SpringCloud/SpringCloud基础/","link":"","permalink":"https://sobxiong.github.io/2020/11/18/SpringSeries/SpringCloud/SpringCloud%E5%9F%BA%E7%A1%80/","excerpt":"内容 SpringCloud概述 Eureka注册中心介绍 Eureka停更后的替换组件 Ribbon负载均衡介绍","text":"内容 SpringCloud概述 Eureka注册中心介绍 Eureka停更后的替换组件 Ribbon负载均衡介绍 SpringCloud概述 微服务是什么：微服务架构下的一整套解决方案 SpringCloud是什么：分布式微服务架构的一站式解决方案，是多种微服务架构落地技术的集合体，俗称微服务全家桶 SpringCloud版本SpringCloud采用英国伦敦地铁站的名称来命名，并由地铁站名称字母A-Z依此类推的形式发布迭代版本SpringCloud是由许多子项目组成的综合项目，各子项目有不同的发布节奏，为了管理SpringCloud与各子项目的版本依赖关系，发布了一个清单，其中包括了某个SpringCloud版对应的子项目版本。为了避免SpringCloud版本号与子项目版本号混淆，SpringCloud版采用了名称而非版本号命名，例如Angel、Brixton。当SpringCloud的发布内容积累到临界点或者一个重大BUG被解决后，会发布一个Service releases版本，俗称SRX版本，比如Greenwich.SR2就是SpringCloud发布的Greenwich版本的第二个SRX版本 SpringBoot和SpringCloud的版本约束SpringBoot和SpringCloud的版本选择也不是任意的，而是应该参考官网的约束配置地址：https://spring.io/projects/spring-cloud#overview版本对应：https://start.spring.io/actuator/info SpringCloud各种组件的停更/升级/替换 停更的具体形式： 被动修复Bugs 不再接受合并请求 不再发布新版本 组件具体明细条目 服务调用 Eureka Zookeeper Consul Nacos(推荐) 服务调用 Feign OpenFeign(推荐) Ribbon LoadBalancer 服务降级 Hystrix resilience4j sentienl(推荐) 服务网关 Zuul Zuul2 Gateway(推荐) 服务配置 Config Nacos(推荐) 服务总线 Bus Nacos(推荐) SpringCloud资料官网文档：https://spring.io/projects/spring-cloud#learnSpringCloud中文文档：https://www.bookstack.cn/read/spring-cloud-docs/docs-index.md Eureka注册中心介绍 Eureka基础知识： 什么是服务注册？Eureka Server作为服务注册功能的服务器，它是服务注册中心，而系统中其他微服务，使用Eureka客户端连接到Eureka Server并维持心跳连接，这样系统维护人员就可以通过 Eureka Server来监控各个微服务是否正常运行在服务注册与发现中有一个注册中心，服务器启动时，会把当前自己的服务器信息比如服务地址、通信地址等注册到注册中心上，另一方(消费者)以别名的方式在注册中心上获取实际的服务器通讯地址，然后再实现本地RPC调用远程RPC Eureka的两个组件 Eureka Server：提供服务注册服务。各个微服务节点通过配置启动后，会在 Eureka Server中进行注册，这样Eureka Server中的服务注册表中将会存储所有可用服务节点的信息 Eureka Client：通过注册中心进行访问。是一个Java客户端，用于简化与Eureka Server的交互，客户端也同时具备一个内置的、使用轮询负载算法的负载均衡器。在应用启动后，将会向Eureka Server发送心跳(默认周期30秒)。如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，Eureka Server将会从服务注册表中将这个服务节点移除(默认90秒) Eureka工作原理： 服务注册：将服务信息注册进注册中心 服务发现：从注册中心上获取服务信息 实质：存key服务命名，取value调用地址 单机Eureka Server搭建(小口诀：建module,改pom,写yml,主启动)： 导入依赖： 12345&lt;!-- eureka server --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 修改yml配置文件： 1234567891011121314server: port: 7001eureka: instance: # eureka服务端实例名称 hostname: localhost client: # false表示不向注册中心注册自己 register-with-eureka: false # false表示自己就是注册中心,职责是维护服务实例,并不需要去检索服务 fetch-registry: false service-url: # 设置与eureka server交互的地址;查询服务和注册服务都需要依赖这个地址 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 启动类上添加注解声明启动Eureka服务端 1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaServer7001Application &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServer7001Application.class, args); &#125;&#125; 测试(看到eureka服务页面)：http://localhost:7001/ Eureka Client搭建： 导入依赖： 12345&lt;!-- eureka client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 修改yml配置文件： 1234567891011server: port: 8001eureka: client: # 表示是否将自己注册进Eureka Server,默认为true register-with-eureka: true # 是否从EurekaServer抓取已有的注册信息,默认为true; # 单节点无所谓,集群必须设置为true才能配合ribbon使用负载均衡 fetch-registry: false service-url: defaultZone: http://localhost:7001/eureka 启动类上添加注解声明启动Eureka客户端 1234567@SpringBootApplication@EnableEurekaClientpublic class ProviderPayment8001Application &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServer7001Application.class, args); &#125;&#125; 测试：http://localhost:7001/ 可以看到Instances中包含服务提供方 集群Eureka配置 \b背景：微服务RPC远程调用最核心就是高可用。假设注册中心只有一个，如果出现了故障，那么将会导致整个微服务不可用，所以需要搭建Eureka注册中心集群，实现负载均衡和故障容错 集群原理：相互注册、相互守望 前提：由于hostname不能重复，在本地hosts修改映射，将eureka7001.com、eureka7002.com都映射到localhost上 具体步骤： Eureka Server修改yml配置： EurekaServer7001配置 1234567891011121314server: port: 7001eureka: instance: # eureka服务端实例名称 hostname: eureka7001.com client: # 表示不向注册中心注册自己 register-with-eureka: false # false表示自己就是注册中心,职责就是维护服务实例,无需检索服务 fetch-registry: false service-url: # 向另外的eureka服务注册,如果多个用,隔开 defaultZone: http://eureka7002.com:7002/eureka/ EurekaServer7002配置 1234567891011121314server: port: 7002eureka: instance: # eureka服务端实例名称 hostname: eureka7002.com client: # 表示不向注册中心注册自己 register-with-eureka: false # false表示自己就是注册中心,职责就是维护服务实例,无需检索服务 fetch-registry: false service-url: # 向另外的eureka服务注册, defaultZone: http://eureka7001.com:7001/eureka/ 测试：http://eureka7001.com:7001、http://eureka7002.com:7002 分别发现另一方成为各自的DS Replicas Eureka Client集群(服务)注册进Eureka集群 背景：假设有两个微服务payment8001和payment8002需要注册进上面的Eureka集群 具体步骤： Eureka Client集群(服务)修改yml配置： Payment8001配置 1234567891011121314151617181920212223server: port: 8001spring: application: # 服务名称,为了保证服务对外暴露的是同一个服务提供者,服务名要保持一致 name: provider-paymenteureka: client: # 表示向注册中心注册自己,默认为true register-with-eureka: true # 是否从EurekaServer抓取已有的注册信息,默认为true # 单节点无所谓,集群必须设置为true才能配合ribbon使用负载均衡 fetch-registry: true service-url: # 入驻地址,向eurekaServer注册,多个地址用','分割 defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/ instance: # 设置实例编号,用于区分 instance-id: provider-payment-8001 # 是否显示ip prefer-ip-address: true Payment8002配置 1234567891011121314151617181920212223server: port: 8002spring: application: # 服务名称,为了保证服务对外暴露的是同一个服务提供者,服务名要保持一致 name: provider-paymenteureka: client: # 表示向注册中心注册自己,默认为true register-with-eureka: true # 是否从EurekaServer抓取已有的注册信息,默认为true # 单节点无所谓,集群必须设置为true才能配合ribbon使用负载均衡 fetch-registry: true service-url: # 入驻地址,向eurekaServer注册,多个地址用','分割 defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/ instance: # 设置实例编号,用于区分 instance-id: provider-payment-8001 # 是否显示ip prefer-ip-address: true 测试：http://eureka7001.com:7001 发现PAYMENT-SERVICE中两个服务提供者，分别为8001和8002 调用服务提供者 背景：上述工作完成了服务提供者payment-provider集群注册进eureka集群中，此时需要有一个消费者微服务调用服务 具体步骤： 导入依赖： 12345&lt;!-- eureka client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 编写yml： 1234567891011121314151617server: port: 80spring: application: name: consumer-order-80eureka: client: # 将自己注册进EurekaServer,默认为true register-with-eureka: true # 从EurekaServer抓取已有的注册信息,默认为true # 单节点无所谓,集群必须设置为true才能配合ribbon使用负载均衡 fetch-registry: true service-url: # defaultZone: http://eureka7001.com:7001/eureka/ defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/ 主启动： 1234567@SpringBootApplication@EnableEurekaClientpublic class ConsumerOrder80Application &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerOrder80Application.class, args); &#125;&#125; 设置RestTemplate的负载均衡策略： 12345678@Configurationpublic class ApplicationContextConfig &#123; @Bean // 赋予RestTemplate负载均衡能力 // 这是Ribbon的功能,Eureka默认自带Ribbon @LoadBalanced public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 具体消费端Controller： 1234567891011121314151617181920212223@RestController@Slf4j@RequestMapping(\"/order\")public class OrderController &#123; // 服务提供方的具体服务名,由eureka负责解析 public static final String PAYMENT_URL = \"http://PROVIDER-PAYMENT/payment\"; @Autowired private RestTemplate restTemplate; @Autowired private LoadBalancer loadBalancer; @GetMapping(\"/payment/add\") public Result add(Payment payment) &#123; return restTemplate.postForObject(PAYMENT_URL + \"/add\", payment, Result.class); &#125; @GetMapping(\"/payment/get/&#123;id&#125;\") public Result getById(@PathVariable(\"id\") Long id) &#123; return restTemplate.getForObject(PAYMENT_URL + \"/get/\" + id, Result.class); &#125;&#125; 测试：http://localhost/order/payment/get/1 actuator微服务信息完善 需要导入的依赖： 12345678910&lt;!-- web启动器 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 监控 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 查看健康信息：http://hostname:port/actuator/health 设置微服务显示名称和ip显示 123456Erueka: instance: # 服务名称 instance-id: payment8001 # 访问路径显示IP地址 prefer-ip-address: true 服务发现Discovery 作用：可以通过服务发现获取注册进eureka的微服务的信息 相关类： @EnableDiscoveryClient DiscoveryClient 具体API： 获取列表：List&lt;String&gt; services = discoveryClient.getServices(); 获取实例：List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(&quot;PAYMENT-SERVICE&quot;); 获取ServiceId：serviceInstance.getServiceId(); 获取端口号：serviceInstance.getPort(); 获取URL：serviceInstance.getURL(); 具体步骤： 在payment8001的controller中添加代码： 1234567891011121314@Autowiredprivate DiscoveryClient discoveryClient;@GetMapping(\"/discovery\")public Result discovery() &#123; List&lt;String&gt; services = discoveryClient.getServices(); for (String service : services) &#123; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(service); for (ServiceInstance instance : instances) &#123; log.info(\"ServiceId: &#123;&#125; , host: &#123;&#125; , port: &#123;&#125; , uri: &#123;&#125;\", instance.getServiceId(), instance.getHost(), instance.getPort(), instance.getUri()); &#125; &#125; return new Result(200, \"查询成功~~~\", discoveryClient);&#125; 测试：http://localhost:8001/payment/discovery 控制台输出如下 122020-11-19 19:16:39.535 INFO 4394 --- [nio-8001-exec-9] c.x.cloud.controller.PaymentController : ServiceId: PROVIDER-PAYMENT , host: 10.21.176.180 , port: 8001 , uri: http://10.21.176.180:80012020-11-19 19:16:39.535 INFO 4394 --- [nio-8001-exec-9] c.x.cloud.controller.PaymentController : ServiceId: PROVIDER-PAYMENT , host: 10.21.176.180 , port: 8002 , uri: http://10.21.176.180:8002 返回json结果如下 123456789101112131415&#123; \"resultCode\": 200, \"resultMessage\": \"查询成功~~~\", \"resultData\": &#123; \"discoveryClients\": [&#123; \"order\": 0, \"services\": [\"provider-payment\"] &#125;, &#123; \"order\": 0, \"services\": [] &#125;], \"services\": [\"provider-payment\"], \"order\": 0 &#125;&#125; Eureka自我保护机制 概念：保护模式主要用于一组客户端和Eureka Server之间存在网络分区场景下的保。一旦进入保护模式，Eureka Server将会尝试保护其服务注册表的信息，不再删除服务注册表中的数据，也就是不会注销任何微服务如果在Eureka Server的首页看到以下这段提示，说明Eureka进入了保护模式通俗的话来说：某时刻某一个微服务不可用了，Eureka不会立刻清理，依旧会对该微服务的信息进行保存。这属于CAP里面的AP分支 导致原因：默认情况下，如果Eureka Server在一定时间内没有接收到某个微服务实例的心跳，Eureka Server将会注销该实例，默认为90秒。但当网络分区故障发生(延时、卡顿、拥挤)时，微服务与Eureka Server之间无法正常通信，以上行为可能变得非常危险了————因为微服务本身其实是健康的，此时不应该注销这个微服务。Eureka通过自我保护模式来解决这个问题，当Eureka Server节点在短时间丢失过多客户端，那么这个节点就会进入自我保护模式，这是一种高可用的机制在自我保护模式下，Eureka Server会保护服务注册表中的信息，不在注销任何服务实例综上，自我保护模式是一种应对网络异常的安全保护措施，它的架构哲学是宁可保留所有微服务，也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加健壮，稳定 禁止自我保护(Eureka默认开启自我保护)： Eureka服务端设置： 123456eureka: server: # 关闭自我保护机制 enable-self-preservation: false # 心跳时间默认90s,改为2000ms,即2s eviction-interval-timer-in-ms: 2000 Eureka客户端设置 123456eureka: instance: # Eureka客户端向服务端发送心跳的时间间隔,单位为秒,默认30秒 lease-renewal-interval-in-seconds: 1 # Eureka服务端在收到最后一次心跳后等待时间上限,单位为秒,默认90秒,超时将剔除服务 lease-expiration-duration-in-seconds: 2 设置完成后，只要服务宕机，会马上从服务注册列表中清楚 Eureka停更之后的替代者： Zookeeper Consul Nacos Eureka停更后的替换组件 Zookeeper Zookeeper介绍：一个分布式协调工具，可以实现注册中心功能 注册服务进Zookeeper： 引入pom依赖： 123456789101112131415161718&lt;!-- SpringBoot整合zookeeper客户端 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zookeeper-discovery&lt;/artifactId&gt; &lt;!-- 先排除自带的zookeeper3.5.3 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!-- 添加zookeeper3.6.2版本 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.6.2&lt;/version&gt;&lt;/dependency&gt; 修改yml配置文件： 1234567891011server: port: 8004spring: application: # 服务别名——注册进zookeeper注册中心的服务名 # 多个服务实例要设置同一个服务别名 name: provider-payment cloud: zookeeper: connect-string: test1:2181 修改主启动类： 12345678@SpringBootApplication// 该注解用于向使用consul或者zookeeper作为注册中心时注册服务@EnableDiscoveryClientpublic class ProviderPayment8004Application &#123; public static void main(String[] args) &#123; SpringApplication.run(ProviderPayment8004Application.class, args); &#125;&#125; 思考：服务已经成功注册到Zookeeper客户端，那么注册上去的节点为临时节点还是持久节点？首先Eureka有自我保护机制，也就是某个服务下线后，不会立刻清除该服务，而是将服务保留一段时间Zookeeper一样在服务下线后也会等待一段时间，之后才会把该节点删除，这就说明Zookeeper上的节点是临时节点 Consul Consul介绍 简介：Consul是一套开源的分布式服务发现和配置管理系统，由HashiCorp公司用Go语言开发提供了微服务系统中的服务治理、配置中心、控制总线等功能，这些功能中的每一个都可以根据需要单独使用，也可以一起使用构建全方位的服务网路，总之Consul提供了一种完整的服务网络解决方案具有很多优点，包括：基于raft协议，比较简洁；支持健康检查；同时支持HTTP和DNS协议；支持跨数据中心的WAN集群；提供图形化界面；跨平台，支持Linux、MAC、Windows 官网：https://www.consul.io/ 功能： 服务发现：提供HTTP和DNS两种发现方式 健康监测：支持多种方法，HTTP，TCP，Docker，Shell脚本定制化 KV存储：Key，Value的存储方式 多数据中心：Consul支持多数据中心 可视化Web界面 安装： 官网下载(可执行文件) 查看版本：consul --version 运行：consul agent -dev 测试：访问http://test1:8500进入consul可视化界面 注册服务进Consul： 引入pom依赖： 12345&lt;!-- consul --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 修改yml配置文件： 1234567891011server: port: 8006spring: application: name: provider-payment cloud: consul: host: test1 port: 8500 discovery: service-name: $&#123;spring.application.name&#125; 测试：访问http://test1:8500进入consul可视化界面，发现微服务已注册进consul 总结三个注册中心： 组件名 语言 健康检查 对外暴露接口 CAP Spring Cloud集成 Eureka Java 可配支持 HTTP AP 已集成 Consul Go 支持 HTTP/DNS CP 已集成 Zookeeper Java 支持 客户端 CP 已集成 CAP理论：C(Consistency)表示强一致性；A(Availability)表示高可用；P(Partition Tolerance)表示分区容错性。CAP理论关注粒度是数据，而不是整体系统设计的策略CAP理论的核心：一个分布式系统不可能同时很好的满足一致性、可用性和分区容错性这个三个需求。现在的微服务架构要么是CP要么是AP(即P一定需要保证)，最多只能较好地同时满足两个根据CAP原理可将一个分布式系统分成CA、CP和AP三大类： CA：单点集群，满足一致性、可用性的系统，通常在可扩展性上不太满足 CP：满足一致性、分区容忍性，通常性能不是特别高 AP：满足可用性、分区容忍性，通常对一致性要求低一些 AP架构(Eureka)：因为同步原因出现问题，而造成数据没有一致性当出现网络分区后，为了保证高可用，系统B可以返回旧值，保证系统的可用性结论：违背了一致性C的要求，只满足可用性和分区容错性，即AP CP架构(Zookeeper、Consul)：当出现网络分区后，为了保证一致性，就必须拒绝请求，否者无法保证一致性结论：违背了可用性A的要求，只满足一致性和分区容错性，即CP Ribbon负载均衡介绍 Ribbon介绍：Spring Cloud Ribbon是基于Netflix Ribbon实现的一套客户端负载均衡的工具Ribbon是NetFlix发布的开源项目。主要功能是提供客户端的软件负载均衡算法和服务调用。Ribbon客户端组件提供了一系列完善的配置项如连接超时、重试等。简单的说，就是在配置文件中列出Load Balancer(简称LB)后面所有的机器，Ribbon会自动地基于某种规则(简单轮询、随机连接等)去连接这些机器。很容易使用Ribbon实现自定义的负载均衡算法 负载均衡介绍：Load Balance，简单来说就是将用户的请求平摊地分配到多个服务上，从而达到系统的HA(高可用)。常见的负载均衡有软件Nginx、LVS等，硬件有F5等 集中式LB：在服务的消费方和提供方之间使用独立的LB设施(可以是硬件,如F5;也可以是软件,如Nginx)，由该设施负责把访问请求通过某种策略转发至服务的提供方 进程内LB：将LB逻辑集成到消费方————消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。Ribbon就属于进程内LB，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址 Ribbon和Nginx的对比： Nginx是服务器负载均衡，客户端所有的请求都会交给nginx，然后由nginx实现转发请求，即负载均衡是由服务端实现的 Ribbon是本地负载均衡，在调用微服务接口时会从注册中心上获取注册信息服务列表，再缓存到JVM本地，从而在本地实现RPC远程调用 Ribbon工作原理：Ribbon其实就是一个软负载均衡的客户端组件，它可以和其它所需请求的客户端结合使用，和Eureka结合只是其中的一个实例Ribbon在工作时分成两步： 首先选择Eureka Server，它优先选择在同一个区域内负载较少的Server 再根据用户指定的策略，从Server取到的服务注册列表中选择一个地址(Ribbon提供了多种策略：比如轮询、随机和根据响应时间加权) 具体实践： 引入Ribbon：新版Eureka已默认引入Ribbon，无需额外引入 核心组件IRule介绍： Ribbon默认使用轮询作为负载均衡算法 IRule根据特定算法从服务列表中选取一个要访问的服务，IRule是一个接口 12345public interface IRule&#123; public Server choose(Object key); public void setLoadBalancer(ILoadBalancer lb); public ILoadBalancer getLoadBalancer();&#125; Ribbon提供了多种IRule的默认实现 共有以下七种： RoundRobinRule：轮询 RandomRule：随机 RetryRule：先按照RoundRobinRule的策略获取服务，如果获取服务失败则在指定时间内会进行重试，获取可用服务 WeightedResponseTimeRule：对RoundRobinRule的扩展，响应速度越快的实例选择的权重越大，越容易被选择 BestAvailableRule：会先过滤掉由于多次访问故障而处于短路跳闸状态的服务，然后选择一个并发量最小的服务 AvailabilityFilteringRule：先过滤掉故障实例，在选择并发较小的实例 ZoneAvoidanceRule：默认规则，符合判断server所在区域的性能和server的可用性选择服务器 默认负载均衡算法替换 Ribbon的小bug：官网警告自定义的配置类不能放在@ComponentScanner所扫描的当前包以及子包下，否者自定义的这个配置类就会被所有的Ribbon客户端所共享，达不到特殊化定制的目的了(不能在SpringApplication主启动类的同级及子包下) 创建自定义Rule接口： 12345678@Configurationpublic class MyRibbonRule &#123; @Bean public IRule getRule()&#123; // 自定义为随机规则 return new RandomRule(); &#125;&#125; 在主启动类中设置新规则(@RibbonClient) 12345678@SpringBootApplication@EnableEurekaClient@RibbonClient(name = \"PROVIDER-PAYMENT\", configuration = MyRibbonRule.class)public class ConsumerOrder80Application &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerOrder80Application.class, args); &#125;&#125; 手写Ribbon负载均衡算法 原理：记实际调用服务器位置下标为serviceIndex，服务器集群总数量为serviceCount，rest接口请求次数为n，则有：serviceIndex = n % serviceCount(即轮询的原理) RoundRobinRule原理(发现采用思想一样,再加入了一些判断和CAS线程安全保证)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class RoundRobinRule extends AbstractLoadBalancerRule &#123; private AtomicInteger nextServerCyclicCounter; private static final boolean AVAILABLE_ONLY_SERVERS = true; private static final boolean ALL_SERVERS = false; public RoundRobinRule() &#123; nextServerCyclicCounter = new AtomicInteger(0); &#125; public RoundRobinRule(ILoadBalancer lb) &#123; this(); setLoadBalancer(lb); &#125; public Server choose(ILoadBalancer lb, Object key) &#123; if (lb == null) &#123; return null; &#125; Server server = null; int count = 0; while (server == null &amp;&amp; count++ &lt; 10) &#123; List&lt;Server&gt; reachableServers = lb.getReachableServers(); List&lt;Server&gt; allServers = lb.getAllServers(); int upCount = reachableServers.size(); int serverCount = allServers.size(); if ((upCount == 0) || (serverCount == 0)) &#123; return null; &#125; int nextServerIndex = incrementAndGetModulo(serverCount); server = allServers.get(nextServerIndex); if (server == null) &#123; /* Transient. */ Thread.yield(); continue; &#125; if (server.isAlive() &amp;&amp; (server.isReadyToServe())) &#123; return (server); &#125; // Next. server = null; &#125; return server; &#125; private int incrementAndGetModulo(int modulo) &#123; for (;;) &#123; int current = nextServerCyclicCounter.get(); int next = (current + 1) % modulo; if (nextServerCyclicCounter.compareAndSet(current, next)) return next; &#125; &#125; @Override public Server choose(Object key) &#123; return choose(getLoadBalancer(), key); &#125;&#125; 实现自己版本的负载均衡算法： 原理：从Eureka服务器获取实例地址信息 + 机器数取余 + JUC(CAS + 原子整型) 准备工作：删除RestTemplate上的@LoadBalance注解，防止Ribbon LB的干扰 仿造创建LoadBalanced接口： 123public interface LoadBalancer &#123; ServiceInstance instances(List&lt;ServiceInstance&gt; serviceInstances);&#125; 创建实现类：MyLoadBalancer 12345678910111213141516171819202122232425262728@Component@Slf4jpublic class MyLoadBalancer implements LoadBalancer &#123; // 原子整型 private final AtomicInteger atomicInteger = new AtomicInteger(0); // 获取Rest调用的次数 public final int getAndIncrement() &#123; int current, next; do &#123; // 获取当前值 current = atomicInteger.get(); // 计数达到最大值,重回0 next = current &gt;= Integer.MAX_VALUE ? 0 : current + 1; // CAS比较并交换 &#125; while (!atomicInteger.compareAndSet(current, next)); log.info(\"Next: &#123;&#125;\", next); return next; &#125; // 获取具体服务提供者实例信息 @Override public ServiceInstance instances(List&lt;ServiceInstance&gt; serviceInstances) &#123; // 获取当前访问机器的下标：调用次数 % 机器总数 int index = getAndIncrement() % serviceInstances.size(); return serviceInstances.get(index); &#125;&#125; 具体使用： 123456789101112// 注入自己声明为@Component的MyLoadBalancer@Autowiredprivate LoadBalancer loadBalancer;@GetMapping(\"/payment/getLb/&#123;id&#125;\")public Result getByIdInLoadBalance(@PathVariable(\"id\") Long id) &#123; List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"PROVIDER-PAYMENT\"); if (instances == null || instances.isEmpty()) return null; ServiceInstance instance = loadBalancer.instances(instances); URI uri = instance.getUri(); return restTemplate.getForObject(uri + \"/payment/get/\" + id, Result.class);&#125; Ribbon停更后的替代者：Spring Cloud自己提供的LoadBalancer","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"}]},{"title":"Effective Java","slug":"ProgrammingLanguage/Java/Effective_Java","date":"2020-11-15T08:28:20.000Z","updated":"2020-11-20T10:59:35.652Z","comments":true,"path":"2020/11/15/ProgrammingLanguage/Java/Effective_Java/","link":"","permalink":"https://sobxiong.github.io/2020/11/15/ProgrammingLanguage/Java/Effective_Java/","excerpt":"内容 引言 创建和销毁对象 对于所有对象都通用的方法","text":"内容 引言 创建和销毁对象 对于所有对象都通用的方法 引言Java语言支持四种类型：接口(包括注释)、类(包括enum)、数组和基本类型。前三种种类型通常被称为引用类型(reference type)，类实例和数组是对象(object)，而基本类型的值则不是对象。类的成员(member)由它的域(field)、方法(method)、成员类(member class)和成员接口(member interface)组成。方法的签名(signature)由它的名称和所有参数类型组成；签名不包括方法的返回类型 创建和销毁对象 第一条：用静态工厂方法代替构造器 第一大优势在于前者有名称。如果构造器的参数本身没有确切地描述正被返回的对象，那么具有适当名称的静态工厂会更容易使用，产生的客户端代码也更易于阅读 第二大优势在于不必在每次调用它们的时候都创建一个新对象。这使得不可变类可以使用预先构建好的实例，或者将构建好的实例缓存起来，进行重复利用，从而避免创建不必要的重复对象 第三大优势在于它们可以返回原返回类型的任何子类型的对象。这样在选择返回对象的类时就有了更大的灵活性。这种灵活性的一种应用是API可以返回对象，同时又不会使对对象的类变成公有的。以这种方式隐藏实现类会使API变得非常简洁 第四大优势在于所返回的对象的类可以随着每次调用而发生变化，这取决于静态工厂方法的参数值。只要是已声明的返回类型的子类型，都是允许的 第五大优势在于方法返回的对象所属的类，在编写包含该静态工厂方法的类时可以不存在(规范,可插拔,例如JDBC具体实现) 主要缺点在于类如果不含公有的或者受保护的构造器，就不能被子类化(Collections中便利的实现类子类化) 总结：静态工厂方法和公有构造器都各有用处，需要理解它们各自的长处。静态工厂经常更加合适，切忌第一反应就是提供公有的构造器，而不先考虑静态工厂 第二条：遇到多个构造器参数时要考虑使用构建器 静态工厂和构造器有个共同的局限性：它们都不能很好地扩展到大量的可选参数 方案1：重叠构造器(telescoping constructor)模式。在这种模式下，提供的第一个构造器只有必要的参数，第二个构造器有一个可选参数，第三个构造器有两个可选参数，依此类推。重叠构造器模式可行，但当有许多参数的时候，客户端代码会很难编写，并且仍然较难阅读 方案2：JavaBean模式。在这种模式下，先调用一个无参构造器来创建对象，然后再调用setter 方法来设置每个必要的参数，以及每个相关的可选参数。这种模式弥补了重叠构造器模式的不足：创建实例很容易，代码读起来也很容易 JavaBean模式的缺点：JavaBean模式自身有着很严重的缺点。因为构造过程被分到了几个调用中，在构造过程中JavaBean可能处于不一致的状态。类无法仅仅通过检验构造器参数的有效性来保证一致性。试图使用处于不一致状态的对象将会导致失败，这种失败与包含错误的代码大相径庭，因此调试起来十分困难。另一点不足在于JavaBean模式使得把类做成不可变的可能性不复存在，这就需要付出努力确保线程安全 不一致的解释：模拟多线程获取同一个JavaBean的场景，线程A获取Obj对象，对其属性进行set；同时线程B获取Obj对象对其进行get；这时可能会出现线程A中没有set完毕，线程B就开始get相应的属性 方案3：建造者(Buidler)模式。它既能保证重叠构造器模式那样的安全性，也能保证JavaBean模式那么好的可读性。它不直接生成想要的对象，而是让客户端利用所有必要的参数调用构造器(或静态工厂)，得到一个builder对象。然后客户端在builder对象上调用类似setter的方法来设置每个相关的可选参数。最后客户端调build()方法来生成通常是不可变的对象。这个builder通常是它构建的类的静态成员类 案例： 123456789101112131415161718192021222324252627282930313233343536373839public class MilkTea &#123; private final int ice; private final int sugar; private final boolean addPearl; private final int size; public static class Builder &#123; private int ice = NORMAL_ICE; private int sugar = NORMAL_SUGAR; private boolean addPearl = false; private final int size; public Builder(int size) &#123; this.size = size; &#125; public Builder ice(int ice) &#123; this.ice = ice; return this; &#125; public Builder sugar(int sugar) &#123; this.sugar = sugar; return this; &#125; public Builder addPearl(boolean addPearl) &#123; this.addPearl = addPearl; return this; &#125; public MilkTea build() &#123; return new MilkTea(this); &#125; &#125; private MilkTea(Builder builder) &#123; this.size = builder.size; this.sugar = builder.sugar; this.addPearl = builder.addPearl; this.ice = builder.ice; &#125;&#125; 优点： 客户端代码很容易编写、易于阅读。模拟了具名的可选参数 适用于类层次结构。 使用平行层次结构的builder时，各自嵌套在相应的类中。抽象类有抽象的builder，具体类有具体的builder 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public abstract class AbstractResource &#123; // 公共属性... // 泛型,递归参数类型,在子类中适当地进行方法链接,无需转换类型 abstract static class Builder&lt;T extends Builder&lt;T&gt;&gt; &#123; // 公共属性... public T setCommonFileds(...) &#123; // 公共属性赋值 return self(); &#125; abstract AbstractResource build(); // 模拟self类型 protected abstract T self(); &#125; AbstractResource(Builder&lt;?&gt; builder) &#123; // 设置公共属性... &#125;&#125;public class ConcreteResource extends AbstractResource &#123; // 公共属性... public static class Builder extends AbstractResource.Builder&lt;Builder&gt; &#123; // 公共属性... public Builder setFields(...)&#123; // 子类特有属性赋值 return this; &#125; // build返回都是子类的类型 @Override AbstractResource build() &#123; return new ConcreteResource(this); &#125; @Override protected Builder self() &#123; return this; &#125; &#125; private ConcreteResource(Builder builder) &#123; super(builder); // 设置公共属性... &#125;&#125; Builder模式十分灵活 缺点： 为了创建对象，必须先创建它的构建器 模式较冗长，但容易扩展 总结：如果类的构造器或者静态工厂中具有多个参数，Builder模式是一种不错的选择，特别是当大多数参数类型相同或是可选参数时 第三条：用私有构造器或者枚举类型强化Singleton属性 Singleton是指仅仅被实例化一次的类，通常被用来代表无状态的对象，比如那些本质上唯一的系统组件 方案1：饿汉式(静态常量),懒加载问题有时可忽略 12345678class Singleton &#123; // 1、构造器私有化(防止外部new) private Singleton() &#123;&#125; // 2、类内创建对象(静态常量) private final static Singleton instance = new Singleton(); // 3、向外暴露静态公共方法,返回单例instance public static Singleton getInstance() &#123; return instance; &#125;&#125; 缺陷：如果Singleton需要可序列化，仅仅加上implements Serializable是不够的。为了维护并保证Singleton，必须声明所有实例域都是瞬时(transient)的，并提供一个readResolve方法。否则每次反序列化一个序列化的实例时都会创建一个新的实例 方案2：声明一个包含单个元素的枚举类型 123public enum Singleton&#123; INSTANCE;&#125; 优点：该方式无偿提供了序列化机制，绝对防止多次实例化，即使是在面对复杂的序列化或者反射攻击的时候。虽然这种方法还没有广泛采用，但单枚举类型经常成为实现Singleton的最佳方法 注意：如果Singleton必须扩展一个超类，而不是扩展Enum时，则不宜使用这个方法 \b\b\b\b\b单例模式的其他介绍参见设计模式 第四条：通过私有构造器强化不可实例化的能力 有时可能需要编写只包含静态方法和静态域的类。这些工具类不希望被实例化，因为实例化对它没有任何意义。然而在缺少显式构造器的情况下，编译器会自动提供一个公有的、无参的缺省构造器。对于用户而言，这个构造器与其他的构造器没有任何区别 企图通过将类做成抽象类来强制该类不可被实例化是行不通的。该类可以被子类化并且该子类也可以被实例化。这种方式还会产生误导 有些简单方法可以确保类不可被实例化。由于只有当类不包含显式的构造器时，编译器才会生成缺省的构造器，因此只要让该类包含一个私有构造器，它就不能被实例化： 123456public class UtilClass &#123; private UtilClass() &#123; throw new AssertionError(); &#125; // ...&#125; 抛出异常能避免在类内部调用私有构造器；最好的做法是在私有构造器中标明注释以及抛出异常时显式指出异常信息。这种方法也有副作用————它使得此类不能被子类化(所有构造器都必须显式或隐式调用超类构造器) 第五条：优先考虑依赖注入来引用资源 有许多类会依赖一个或多个底层的资源，但静态工具类Singleton类不适合需要引用底层资源的类 依赖注入适用于任意数量的资源以及任意的依赖形式。依赖注入的对象资源具有不可变性，因此多个客户端可以共享依赖对象(假设客户端们想要的是同一个底层资源)。依赖注入也同样适用于构造器、静态工厂和构建器 依赖注入的最简单的模式是当创建一个新实例时就将该资源传到构造器中；另一种有用的变体是将资源工厂(factory)传给构造器，工厂是可以被重复调用来创建类型实例的一个对象，这类工厂具体表现为工厂方法模式(Java8增加的接口SupplierT&lt;&gt;最适合用于表示工厂)；另一种方式是采用依赖注入框架，如Dagger、Guice或Spring(设置成手动依赖注入的API,一般都适用) 总结：不要用Singleton和静态工具类来实现依赖一个或多个底层资源的类，且该资源的行为会影响到该类的行为；也不要直接用这个类来创建这些资源。应该将这些资源或者工厂传给构造器(静态工厂/构建器)，通过它们来创建类。该实践就被称作依赖注入，它极大地提升了类的灵活性、可重用性和可测试性 第六条：避免创建不必要的对象 一般来说最好能用单个对象，而不是在每次需要的时候就创建一个相同功能的新对象。重用方式既快速，又流行。如果对象是不可变的(immutable)，它就始终可以被重用 对于同时提供了静态工厂方法(static factory method)和构造器的不可变类，通常优先使用静态工厂方法而不是构造器，以避免创建不必要的对象 有些对象创建的成本比其他对象要高得多。如果重复地需要这类”昂贵的对象”，建议将它缓存下来重用 案例如下： 12345678910// 方式1static boolean isRomanNumeral(String s) &#123; return s.matches(\"xxxx\");&#125;// 方式2private static final Pattern pattern = Pattern.compile(\"xxxx\");static boolean isRomanNumeral(String s) &#123; return pattern.matcher(s).matches();&#125; 说明：String.matches()方法易于查看一个字符串是否与正则表达式相匹配，但并不适合在注重性能的情形中重复使用。问题在于，它在内部为正则表达式创建了一个Pattern实例，却只使用了一次，之后就可以进行垃圾回收了。创建Pattern的实例的成本很高，因为需要将正则表达式编译成一个有限状态机(finite state machine)。为了提升性能，应该显式地将正则表达式编译成一个Pattern实例(不可变)，让它成为类初始化的一部分，并将它缓存起来 如果一个对象是不变的，那么它显然能够被安全地重用，但其他有些情形则并不总是这么明显。考虑虑适配器(adapter)的情形，有时也叫作视图(view)。适配器是指这样一个对象：它把功能委托给一个后备对象(backing object)，从而为后备对象提供一个可以替代的接口。由于适配器除了后备对象之外没有其他的状态信息，所以针对某个给定对象的特定适配器而言，它不需要创建多个适配器实例 例如Map接口的keySet()方法返回该Map对象的Set视图，其中包含该Map中所有的键(key)。乍看之下，好像每次调用keySet()都应该创建一个新的Set实例，但对于一个给定的Map对象，实际上每次调用keySet()都返回同样的Set实例。虽然被返回的Set实例一般是可改变的，但所有返回的对象在功能上是等同的；当其中一个返回对象发生变化的时候，所有其他的返回对象也要发生变化，因为它们是由同一个Map实例支撑的 另一种创建多余对象的方法，称作自动装箱(autoboxing)。它允许程序员将基本类型和装箱基本类型(Boxed Primitive Type)混用，按需要自动装箱和拆箱。自动装箱使基本类型和装箱基本类型之间的差别变得模糊起来，但并没有完全消除。它们在语义上还有着微妙的差别，在性能上也有着比较明显的差别 案例如下 12345private static long sum() &#123; Long sum = 0L; for (long i = 0; i &lt;= Integer.MAX_VALUE; i++) sum += i; return sum;&#125; 说明：这段程序算出的答案是正确的，但是比实际情况要更慢一些，只因为声明为装箱类型。变量sum 被声明成Long而不是long意味着程序构造了大约2^31个多余的Long实例(大约每次往Long sum中增加long时构造一个实例)。因此，要优先使用基本类型而不是装箱基本类型，要当心无意识的自动装箱 不要错误地认为本条目所介绍的内容暗示着”创建对象的代价非常昂贵,应尽可能避免创建对象”。相反，由于小对象的构造器只做很少量的显式工作，小对象的创建和回收动作是非常廉价的，特别是在现代的JVM 实现上更是如此。通过创建附加的对象，提升程序的清晰性、简洁性和功能性，这通常是件好事 反之通过维护自己的对象池(object pool)来避免创建对象并不是一种好的做法，除非池中的对象是非常重量级的。正确使用对象池的典型对象示例就是数据库连接池。建立数据库连接的代价是非常昂贵的，因此重用这些对象非常有意义。而且，数据库的许可可能限制只能使用一定数量的连接。但一般而言，维护自己的对象池必定会把代码弄得很乱，同时增加内存占用(footprint)，并且还可能会损害性能。现代的JVM实现具有高度优化的垃圾回收器，其性能很容易就会超过轻量级对象池的性能 第七条：消除过期的对象引用 简单案例： 12345678910111213141516171819202122232425public class Stack &#123; private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() &#123; elements = new Object[DEFAULT_INITIAL_CAPACITY]; &#125; public void push(Object e) &#123; ensureCapacity(); elements[size++] = e; &#125; public Object pop() &#123; if (size == 0) throw new EmptyStackException(); return elements[--size]; &#125; private void ensureCapacity() &#123; if (elements.length == size) elements = Arrays.copyOf(elements, size + 1); &#125;&#125; 案例解读：没有很明显的错误。无论如何测试，它都会成功地通过每一项测试，但是当中隐藏着一个问题。不严格地讲，这段程序有一个”内存泄漏”，随着垃圾回收器活动的增加，或者由于内存占用的不断增加，程序性能的降低会逐渐表现出来。在极端的情况下，这种内存泄漏会导致磁盘交换(Disk Paging)，甚至导致程序失败(OutOfMemoryError错误)，但这种失败情形相对比较少见 案例中哪里发生了内存泄漏呢？如果一个栈先是增长然后再收缩，那么从栈中弹出来的对象将不会被当作垃圾回收，即使使用栈的程序不再引用这些对象，它们也不会被回收。这是因为栈内部维护着对这些对象的过期引用(obsolete reference)。所谓的过期引用，是指永远也不会再被解除的引用。在本例中，凡是在elements数组的”活动部分”(active portion)之外的任何引用都是过期的。活动部分是指elements中下标小于size的那些元素 在支持垃圾回收的语言中，内存泄漏是很隐蔽的(称这类内存泄漏为”无意识的对象保持”(unintentional object retention)更为恰当)。如果一个对象引用被无意识地保留起来了，那么垃圾回收机制不仅不会处理这个对象，而且也不会处理被这个对象所引用的所有其他对象。即使只有少量的几个对象引用被无意识地保留下来，也会有许许多多的对象被排除在垃圾回收机制之外，从而对性能造成潜在的重大影响 这类问题的修复方法很简单：一旦对象引用已经过期，只需清空这些引用即可。对于上述案例中的Stack类而言，只要一个单元被弹出栈，指向它的引用就过期了。pop()方法的修订版本如下所示： 1234567public Object pop() &#123; if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; return result;&#125; 清空过期引用的另一个好处是，如果它们以后又被错误地解除引用，程序就会立即抛出NullPointerException异常，而不是悄悄地错误运行下去。尽快地检测出程序中的错误总是有益的 当第一次被类似这样的问题困扰的时候，往往会过分小心：对于每一个对象引用，一旦程序不再用到它，就把它清空。其实这样做既没必要，也不是我们所期望的，因为这样做会把程序代码弄得很乱。清空对象引用应该是一种例外，而不是一种规范行为。消除过期引用最好的方法是让包含该引用的变量结束其生命周期。如果是在最紧凑的作用域范围内定义每一个变量，这种情形就会自然地发生 一般来说，只要类是自己管理内存，就应该警惕内存泄漏问题。一旦元素被释放掉，则该元素中包含的任何对象引用都应该被清空 内存泄漏的另一个常见来源是缓存。一旦把对象引用放到缓存中，它就很容易被遗忘掉，从而使得它不再有用之后很长一段时间内仍然留在缓存中。一种有效的方案如下：只要在缓存之外存在对某个项的键的引用，该项就有意义，那么就可以用WeakHashMap代表缓存。当缓存中的项过期之后，它们就会自动被删除。但只有当所要的缓存项的生命周期是由该键的外部引用而不是由值决定时，WeakHashMap才有用处 更为常见的情形则是，”缓存项的生命周期是否有意义”并不是很容易确定，随着时间的推移，其中的项会变得越来越没有价值。在这种情况下，缓存应该时不时地清除掉没用的项。这项清除工作可以由一个后台线程(可能是ScheduledThreadPoolExecutor)来完成，或者也可以在给缓存添加新条目的时候顺便进行清理。LinkedHashMap类利用它removeEldestEntry()方法可以很容易地实现后一种方案。对于更复杂的缓存，可以使用java.lang.ref 内存泄漏的第三个常见来源是监昕器和其他回调。如果你实现了一个API，客户端在这个API中注册了回调，却没有显式地取消注册，那么除非你采取某些动作，否则它们就会不断地堆积起来。确保回调立即被当作垃圾回收的最佳方法是只保存它们的弱引用(weak reference)，例如，只将它们保存成WeakHashMap中的键 由于内存泄漏通常不会表现成明显的失败，所以它们可以在一个系统中存在很多年。往往只有通过仔细检查代码，或者借助于Heap剖析工具(Heap Profiler)才能发现内存泄漏问题。因此如果能够在内存泄漏发生之前就知道如何预测此类问题并阻止它们发生，那是最好不过的 第8条：避免使用终结方法和清除方法 终结方法(finalizer)通常是不可预测的，也是很危险的，一般情况下是不必要的。用终结方法会导致行为不稳定、性能降低，以及可移植性问题。当然，终结方法也有其可用之处；但是根据经验，应该避免使用终结方法。在Java9中使用清除方法(cleaner)代替了终结方法。清除方法没有终结方法那么危险，但仍然不可预测、运行缓慢，一般情况下也是不必要的 在C++中，析构器是回收一个对象所占用资源的常规方法，是构造器所必需的对应物。而在Java中，当一个对象变得不可到达的时候，垃圾回收器会回收与该对象相关联的存储空间，并不需要程序员做专门的工作。C++的析构器也可以被用来回收其他的非内存资源，而在Java中一般用try-finally块来完成类似的工作 终结方法和清除方法的缺点在于不能保证会被及时执行。从一个对象变得不可到达开始，到它的终结方法被执行，所花费的这段时间是任意长的。这意味着，注重时间(time-critical)的任务不应该由终结方法或者清除方法来完成。例如用终结方法或者清除方法来关闭已经打开的文件，这就是个严重的错误，因为打开文件的描述符是一种很有限的资源。如果系统无法及时运行终结方法或者清除方法就会导致大量的文件仍然保留在打开状态，于是当一个程序再也不能打开文件的时候，它可能会运行失败 及时地执行终结方法和清除方法正是垃圾回收算法的一个主要功能，这种算法在不同的JVM实现中会大相径庭。如果程序依赖于终结方法或者清除方法被执行的时间点，那么这个程序的行为在不同的JVM中运行的表现可能就会截然不同 延迟终结过程并不只是一个理论问题。在很少见的情况下，为类提供终结方法可能会随意地延迟其实例的回收过程。Java语言规范并不保证哪个线程将会执行终结方法，所以除了不使用终结方法之外，并没有很轻便的办法能够避免这样的问题。在这方面，清除方法比终结方法稍好一些，因为类的设计者可以控制自己的清除线程，但清除方法仍然在后台运行，处于垃圾回收器的控制之下，因此不能确保及时清除 Java语言规范不仅不保证终结方法或者清除方法会被及时地执行，而且根本就不保证它们会被执行。当一个程序终止的时候，某些已经无法访问的对象上的终结方法却根本没有被执行，这是完全有可能的。结论是：永远不应该依赖终结方法或者清除方法来更新重要的持久状态。例如，依赖终结方法或者清除方法来释放共享资源(比如数据库)上的永久锁，这很容易让整个分布式系统垮掉 不要被System.gc()和System.runFinalization()这两个方法所诱惑，它们确实增加了终结方法或者清除方法被执行的机会，但是它们并不保证终结方法或者清除方法会被执行。唯一声称保证它们会被执行的两个方法是System.runFinalizersOnExit()及其臭名昭著的孪生兄弟Runtime.runFinalizersOnExit()。这两个方法都有致命的缺陷，井且已经被废弃很久了 使用终结方法和清除方法有非常严重的性能损失 终结方法有一个严重的安全问题：它们为终结方法攻击(finalizer attack)打开了类的大门。终结方法攻击背后的思想很简单：如果从构造器或者它的序列化对等体(readObject和readResolve方法)抛出异常，恶意子类的终结方法就可以在构造了部分的应该已经半途夭折的对象上运行。这个终结方法会将对该对象的引用记录在一个静态域中，阻止它被垃圾回收。一旦记录到异常的对象，就可以轻松地在这个对象上调用任何原本永远不允许在这里出现的方法。从构造器抛出的异常，应该足以防止对象继续存在；有了终结方法的存在，这一点就做不到了。这种攻击可能造成致命的后果。final类不会受到终结方法攻击，因为没有人能够编写出final类的恶意子类。为了防止非final类受到终结方法攻击，要编写一个空的final的finalize()方法 如果类的对象中封装的资源(例如文件或者线程)确实需要终止，应该怎么做才能不用编写终结方法或者清除方法呢？只需让类实现AutoCloseable，并要求其客户端在每个实例不再需要的时候调用close()方法，一般是利用try-with-resources确保终止，即使遇到异常也是如此。值得提及的一个细节是，该实例必须记录下自己是否已经被关闭了————close()方法必须在一个私有域中记录下”该对象已经不再有效”。如果这些方法是在对象已经终止之后被调用，其他的方法就必须检查这个域，并抛出IllegalStateException异常 学finalize()… 总而言之，除非是作为安全网，或者是为了终止非关键的本地资源，否则请不要使用清除方法，对于在Java9之前的发行版本，则尽量不要使用终结方法。若使用了终结方法或者清除方法，则要注意它的不确定性和性能后果 第9条：try-with-resources优先于try-finally Java类库中包括许多必须通过调用close()方法来手工关闭的资源。例如InputStream、OutputStrea以及java.sql.Connection。客户端经常会忽略资源的关闭，造成严重的性能后果也就可想而知了。虽然这其中的许多资源都是用终结方法作为安全网，但效果并不理想 根据经验，try-finally语句是确保资源会被适时关闭的最佳方法，就算发生异常或者返回也一样 一个try-finally的示例 12345678static String firstLineOfFile(String path) throws IOException &#123; BufferedReader reader = new BufferedReader(new FileReader(path)); try &#123; return reader.readLine(); &#125; finally &#123; reader.close(); &#125;&#125; 即使用try-finally语句正确地关闭了资源，也存在着些许不足。因为在try块和finally块中的代码，都可能会抛出异常。例如在firstLineOfFile()方法中，如果底层的物理设备异常，那么调用readLine()方法就会抛出异常。基于同样的原因，调用close()方法也会出现异常。在这种情况下，第二个异常完全抹除了第一个异常。在异常堆枝轨迹中，完全没有关于第一个异常的记录，这在现实的系统中会导致调试变得非常复杂，因为通常需要看到第一个异常才能诊断出问题何在。虽然可以通过编写代码来禁止第一个异常，保留第一个异常，但事实上没有人会这么做，因为实现起来太烦琐了 当Java7引人try-with-sources语句时，所有这些问题一下子就全部解决。要使用这个构造的资源，必须先实现AutoCloseable接口，其中包含了声明返回值void的close()方法。Java类库与第三方类库中的许多类和接口，现在都实现或扩展了AutoCloseable接口。如果编写了一个类且它代表的是必须被关闭的资源，那么该类也应该实现AutoCloseable firstLineOfFile的try-with-sources示例 12345static String firstLineOfFile(String path) throws IOException &#123; try (BufferedReader reader = new BufferedReader(new FileReader(path))) &#123; return reader.readLine(); &#125;&#125; 使用try-with-resources不仅使代码变得更简洁易懂，也更容易进行诊断。以firstLineOfFile()方法为例，如果调用readLine()和(不可见的)close()方法都抛出异常，后一个异常就会被禁止，以保留第一个异常。事实上，为了保留想要看到的那个异常，即便多个异常都可以被禁止。这些被禁止的异常并不是简单地被抛弃了，而是会被打印在堆栈轨迹中，并注明它们是被禁止的异常。通过编程调用getSuppressed()方法还可以访问到它们，getsuppressed()方法已经添加到Java7的Throwable类中 在try-with-resources语句中还可以使用catch子句，就像在平时的try-finally语句一样 结论：在处理必须关闭的资源时，始终要优先考虑用try-with-resources，而不是用try-finally。这样得到的代码将更加简洁、清晰，产生的异常也更有价值。有了try-with-resources语句，在使用必须关闭的资源时，就能更轻松地正确编写代码了。实践证明，这个用try-finally是不可能做到的 对于所有对象都通用的方法","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"}]},{"title":"Java日志","slug":"ProgrammingLanguage/Java/Java日志","date":"2020-10-29T14:06:04.000Z","updated":"2020-11-02T01:32:08.222Z","comments":true,"path":"2020/10/29/ProgrammingLanguage/Java/Java日志/","link":"","permalink":"https://sobxiong.github.io/2020/10/29/ProgrammingLanguage/Java/Java%E6%97%A5%E5%BF%97/","excerpt":"内容 日志介绍 日志实现 日志门面 SpringBoot中的日志","text":"内容 日志介绍 日志实现 日志门面 SpringBoot中的日志 日志介绍 日志文件是用于记录系统操作事件的文件集合，可分为事件日志和消息日志。具有处理历史数据、诊断问题的追踪以及理解系统的活动等重要作用。在计算机中，日志文件是记录在操作系统或其他软件运行中发生的事件或在通信软件的不同用户之间的消息的文件 日志的价值、好处： 记录系统中硬件、软件和系统问题的信息，监视系统中发生的事件 检查错误发生的原因 发现一些之前从未意识到的问题 现有Java日志框架介绍： 日志实现： JUL(java util logging) logback log4j log4j2 日志门面： JCL(Jakarta Commons Logging) slf4j(Simple Logging Facade for Java) log4j2 为什么需要日志框架： 集中精力完成系统的业务逻辑设计 框架一般是成熟，稳健的，可以处理系统很多细节问题 经过实践检验，结构很好，扩展性强，可以不断升级 Java日志框架主要解决的问题： 控制日志输出的内容和格式 控制日志输出的位置(file,console) 日志优化：异步日志，日志文件的归档和压缩 日志系统的维护 面向接口开发——日志的门面(适配性能更高的日志框架) 日志实现 JUL入门 JUL介绍：全称Java util Logging，是java原生的日志框架，使用时不需要另外引用第三方类库。相对其他日志框架使用方便，学习简单，能够在小型应用中灵活使用 架构介绍：Loggers：记录器，应用程序通过获取Logger对象调用其API来发布日志信息。Logger通常是应用程序访问日志系统的入口Appenders：也称为Handlers，每个Logger都会关联一组Handlers。Logger将日志交给关联Handlers处理，由Handlers负责将日志做记录。Handlers是一个抽象，其具体的实现决定了日志记录的位置，可以是控制台、文件、网络上的其他日志服务或操作系统日志等Layouts：也称为Formatters，负责对日志事件中的数据进行转换和格式化。Layouts决定了数据在一条日志记录中的最终形式Level：每条日志消息都有一个关联的日志级别。该级别粗略指导了日志消息的重要性和紧迫，可以将Level和Loggers，Appenders关联以便于过滤消息Filters：过滤器，根据需要定制哪些信息会被记录，哪些信息会被放行 入门案例： 123456789101112// 1、获取日志记录器Logger logger = Logger.getLogger(getClass().getName());// 2、日志记录输出logger.info(\"Hello JUL~~~\");// 通用方法进行日志记录logger.log(Level.INFO, \"Hello SOBXiong~~~\");// 通过占位符方式输出变量值String name = \"SOBXiong\";int age = 23;logger.log(Level.INFO, \"Hello &#123;0&#125;,&#123;1&#125;\", new Object[]&#123;name, age&#125;); 日志级别： SEVERE(最高值) WARNING INFO(默认级别) CONFIG FINE FINER FINEST(最低值) OFF(关闭日志记录) ALL(启用所有消息的日志记录) 自定义日志级别案例： 1234567891011121314151617181920212223242526272829303132333435// 1、获取日志记录器Logger logger = Logger.getLogger(getClass().getName());// 关闭系统默认配置logger.setUseParentHandlers(false);// 自定义日志级别// 创建ConsoleHandlerConsoleHandler consoleHandler = new ConsoleHandler();// 创建简单格式转换对象SimpleFormatter simpleFormatter = new SimpleFormatter();// 进行关联consoleHandler.setFormatter(simpleFormatter);logger.addHandler(consoleHandler);// 配置日志具体级别logger.setLevel(Level.ALL);consoleHandler.setLevel(Level.ALL);// FileHandler文件输出FileHandler fileHandler = new FileHandler(\"jul.log\");fileHandler.setFormatter(simpleFormatter);// 进行关联(一个logger可以设置多个handler)logger.addHandler(fileHandler);// 2、日志记录输出logger.severe(\"Hello SOBXiong\");logger.warning(\"Hello SOBXiong\");// jul默认日志级别为infologger.info(\"Hello SOBXiong\");logger.config(\"Hello SOBXiong\");logger.fine(\"Hello SOBXiong\"); Logger之间父子关系案例： 123456789101112131415161718192021222324252627/*JUL中Logger之间存在父子关系，这种父子关系通过树状结构存储。JUL在初始化时会创建一个顶层RootLogger作为所有Logger的父Logger，存储上作为树状结构的根节点。父子关系通过路径来关联*/Logger logger1 = Logger.getLogger(\"com.xiong\");Logger logger2 = Logger.getLogger(\"com\");// 测试logger1.log(Level.INFO, \"logger1 == logger2 &#123;0&#125;\", logger1.getParent() == logger2);System.out.println(logger2.getParent());// 所有日志记录器的顶级父元素: LogManager$RootLogger, name为空System.out.println(logger2.getParent().getName());// 关闭默认配置logger2.setUseParentHandlers(false);// 设置logger2日志级别ConsoleHandler consoleHandler = new ConsoleHandler();SimpleFormatter simpleFormatter = new SimpleFormatter();consoleHandler.setFormatter(simpleFormatter);logger2.addHandler(consoleHandler);logger2.setLevel(Level.ALL);consoleHandler.setLevel(Level.ALL);logger1.severe(\"Hello SOBXiong\");logger1.warning(\"Hello SOBXiong\");logger1.info(\"Hello SOBXiong\");logger1.config(\"Hello SOBXiong\");logger1.fine(\"Hello SOBXiong\"); 日志配置文件案例： 1234567891011121314151617181920212223242526272829303132333435# RootLogger顶级父元素指定的默认处理器为ConsoleHandlerhandlers=java.util.logging.ConsoleHandler,java.util.logging.FileHandler# RootLogger顶级父元素默认日志级别为ALL,名次为空.level=ALL# 自定义Loggercom.xiong.logger.handlers=java.util.logging.ConsoleHandlercom.xiong.logger.level=CONFIG# 关闭默认配置com.xiong.logger.useParentHandlers=false# 向日志文件输出的handler对象# 指定日志文件路径java.util.logging.FileHandler.pattern=java%u.log# 指定日志文件内容大小java.util.logging.FileHandler.limit=50000# 指定日志文件数量java.util.logging.FileHandler.count=1# 指定handler对象日志消息格式对象java.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter# 指定以追加方式添加日志内容java.util.logging.FileHandler.append=true# 向控制台输出的handler对象# 指定handler对象的日志级别java.util.logging.ConsoleHandler.level=ALL# 指定handler对象的日志消息格式对象java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter# 指定handler对象的字符集java.util.logging.ConsoleHandler.encoding=UTF-8# 指定日志消息格式java.util.logging.SimpleFormatter.format=%4$s: %5$s [%1$tc]%n 123456789101112131415// 读取配置文件,通过类加载器InputStream is = getClass().getClassLoader().getResourceAsStream(\"logging.properties\");// 创建LogManagerLogManager logManager = LogManager.getLogManager();// 通过LogManager加载配置文件logManager.readConfiguration(is);// 创建日志记录器Logger logger = Logger.getLogger(getClass().getName());logger.severe(\"Hello SOBXiong1\");logger.warning(\"Hello SOBXiong2\");logger.info(\"Hello SOBXiong3\");logger.config(\"Hello SOBXiong4\");logger.fine(\"Hello SOBXiong5\"); 日志原理解析： 初始化LogManager LogManager加载logging.properties配置 添加Logger到LogManager 从单例LogManager获取Logger 设置级别Level，并指定日志记录LogRecord Filter提供了日志级别之外更细粒度的控制 Handler用来处理日志输出位置 Formatter用来格式化LogRecord log4j入门 log4j介绍：Log4j是Apache下的一款开源的日志框架，通过在项目中使用Log4J，可控制日志信息输出到控制台、文件、甚至是数据库中。可控制每一条日志的输出格式；通过定义日志的输出级别，可以更灵活地控制日志的输出过程；方便项目的调试。官网：http://logging.apache.org/log4j/1.2/ 入门案例： 123456&lt;!-- log4j依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920// 初始化配置信息,当前暂使用代码不使用配置文件BasicConfigurator.configure();// 获取日志记录器对象Logger logger = Logger.getLogger(Log4jTest.class);// 日志记录输出logger.info(\"Hello Log4j\");// 日志级别// 严重错误,一般会造成系统崩溃并终止运行logger.fatal(\"fatal\");// 错误信息,不会影响系统运行logger.error(\"error\");// 警告信息,可能会发生问题logger.warn(\"error\");// 运行信息,数据连接、网络连接、IO操作等等logger.info(\"info\");// 调试信息,一般在开发中使用,记录程序变量参数传递信息等等logger.debug(\"debug\");// 追踪信息,记录程序所有的流程信息logger.trace(\"trace\"); 日志级别： fatal：指出每个将会导致应用程序退出的严重的错误事件 error：指出虽然发生错误事件，但仍然不影响系统继续运行 warn：表明会出现潜在的错误情形 info：一般用于粗粒度级别上，强调应用程序的运行全程 debug：一般用于细粒度级别上，对调试应用程序非常有帮助 trace：程序追踪，可以用于输出程序运行中的变量，显示执行的流程 OFF：用来关闭日志记录 ALL：启用所有消息的日志记录 组件：log4j主要由Loggers(日志记录器)、Appenders(输出端)和Layout(日志格式化器)组成。其中Loggers控制日志的输出级别与日志是否输出；Appenders指定日志的输出方式(输出到控制台、文件等)；Layout控制日志信息的输出格式 Loggers：日志记录器，负责收集处理日志记录。实例的命名就是类的全限定名，Logger的名字大小写敏感，其命名有继承机制。例如：name为org.apache.commons的logger会继承name为org.apache的loggerlog4j中有一个特殊的logger叫”root”，它是所有logger的根，也意味着其他所有的logger都会直接或者间接地继承自root。root logger可以用Logger.getRootLogger()方法获取 Appenders：用来指定日志输出到哪个地方，可以同时指定日志的输出目的地。常用有以下5种： 输出端类型 作用 ConsoleAppender 将日志输出到控制台 FileAppender 将日志输出到文件中 DailyRollingFileAppender 将日志输出到一个日志文件,并且每天输出到一个新的文件 RollingFileAppender 将日志信息输出到一个日志文件,并且指定文件的最大尺寸;当文件大小达到指定尺寸时,会自动把文件改名,同时产生一个新的文件 JDBCAppender 把日志信息保存到数据库中 Layouts：布局器Layouts用于控制日志输出内容的格式，可使用各种指定的格式输出日志。常用的Layouts有以下3种： 格式化器类型 作用 HTMLLayout 格式化日志输出为HTML表格形式 SimpleLayout 简单的日志输出格式化,打印的日志格式为(info - message) PatternLayout 最强大的格式化器,可以根据自定义格式输出日志;如果没有指定转换格式,就是用默认的转换格式 Layout格式： 1234567891011121314151617# log4j采用类似C语言printf()函数的打印格式格式化日志信息,具体的占位符及其含义如下： %m 输出代码中指定的日志信息%p 输出优先级,即DEBUG、INFO等%n 换行符%r 输出自应用启动到输出该log信息耗费的毫秒数%c 输出打印语句所属的类的全名%t 输出产生该日志的线程全名%d 输出服务器当前时间,默认为ISO8601;也可以指定格式,如：%d&#123;yyyy年MM月dd日 HH:mm:ss&#125;%l 输出日志时间发生的位置,包括类名、线程、及在代码中的行数。如：Test.main(Test.java:10)%F 输出日志消息产生时所在的文件名称%L 输出代码中的行号%% 输出一个'%'字符# 可以在'%'与字符之间加上修饰符来控制最小宽度、最大宽度和文本的对其方式：%5c 输出category名称,最小宽度是5;category&lt;5,默认的情况下右对齐%-5c 输出category名称,最小宽度是5;category&lt;5,\"-\"号指定左对齐,会有空格%.5c 输出category名称,最大宽度是5;category&gt;5,就会将左边多出的字符截掉;&lt;5不会有空格%20.30c 输出category名称,&lt;20补空格,并且右对齐;&gt;30就从左边多出的字符截掉 配置文件案例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 默认类路径下的log4j.properties文件# 指定RootLogger顶级父元素默认配置# 指定日志级别为trace,使用appender为console、filelog4j.rootLogger = trace,console# 自定义logger对象设置,不显式声明appender则使用rootLogger的appenderlog4j.logger.com.xiong.logger = info,file# 指定控制台日志输出的appenderlog4j.appender.console = org.apache.log4j.ConsoleAppender# 指定消息格式layoutlog4j.appender.console.layout = org.apache.log4j.PatternLayout# 指定消息格式内容(OGNL表达式)log4j.appender.console.layout.conversionPattern = [%-10p]%r %l %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %m%n# 日志文件输出的appender对象log4j.appender.file = org.apache.log4j.FileAppender# 指定消息格式layoutlog4j.appender.file.layout = org.apache.log4j.PatternLayout# 指定消息格式内容(OGNL表达式)log4j.appender.file.layout.conversionPattern = [%-10p]%r %l %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %m%n# 指定日志文件保存路径log4j.appender.file.file = log4j.log# 指定日志文件的字符集log4j.appender.file.encoding = UTF-8# 按照文件大小拆分的appender对象log4j.appender.rollingFile = org.apache.log4j.RollingFileAppender# 指定消息格式layoutlog4j.appender.rollingFile.layout = org.apache.log4j.PatternLayout# 指定消息格式内容(OGNL表达式)log4j.appender.rollingFile.layout.conversionPattern = [%-10p]%r %l %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %m%n# 指定日志文件保存路径log4j.appender.rollingFile.file = log4j.log# 指定日志文件的字符集log4j.appender.rollingFile.encoding = UTF-8# 指定日志文件内容的大小log4j.appender.rollingFile.maxFileSize = 1MB# 指定日志文件的数量log4j.appender.rollingFile.maxBackupIndex = 10# 按照时间规则拆分的appender对象log4j.appender.dailyFile = org.apache.log4j.DailyRollingFileAppender# 指定消息格式layoutlog4j.appender.dailyFile.layout = org.apache.log4j.PatternLayout# 指定消息格式内容(OGNL表达式)log4j.appender.dailyFile.layout.conversionPattern = [%-10p]%r %l %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %m%n# 指定日志文件保存路径log4j.appender.dailyFile.file = log4j.log# 指定日志文件的字符集log4j.appender.dailyFile.encoding = UTF-8# 指定日期的拆分规则log4j.appender.dailyFile.datePattern = '.'yyyy-MM-dd-HH-mm-ss logback入门： logback介绍：Logback是由log4j创始人设计的另一个开源日志组件，性能比log4j要好。官网：https://logback.qos.ch/index.htmllogback主要分为三个模块： logback-core：基础核心模块 logback-classic：log4j的一个改良版本，并完整实现了slf4j API logback-access：访问模块与Servlet容器集成提供通过Http来访问日志的功能 入门案例： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; 123456789101112public static final Logger logger = LoggerFactory.getLogger(LogbackTest.class);@Testpublic void testQuick() &#123; // 由于logback和slf4j是同一作者,API设置差不多 // 日志输出 logger.error(\"error\"); logger.warn(\"warn\"); logger.info(\"info\"); logger.debug(\"debug\"); logger.trace(\"trace\");&#125; logback配置： 读取配置流程(依次去读)： logback.groovy logback-test.xml logback.xml 默认配置 logback组件间的关系： Logger：日志记录器，关联到应用的对应的context上后，主要用于存放日志对象，也可以定义日志类型、级别 Appender：用于指定日志输出的目的地，可以是控制台、文件、数据库等等 Layout：负责把事件转换成字符串(格式化的日志信息)。在logback中Layout对象被封装在encoder中 基本配置信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;configuration&gt; &lt;!-- 日志输出格式： %-5level: 级别,左对齐,5个字符 %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;: 格式化日期 %c: 类的完整名称 %M: method方法名 %L: 行号 %thread: 线程名称 %m/%msg: 信息 %n: 换行 --&gt; &lt;!-- 定义打印pattern --&gt; &lt;property name=\"pattern\" value=\"%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %c [%thread] %-5level %msg%n\"/&gt; &lt;!-- 定义日志文件保存路径属性 --&gt; &lt;property name=\"logDir\" value=\"logback\"/&gt; &lt;!-- 控制台日志输出的appender --&gt; &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!-- 控制输出流对象,默认System.out,改为System.err --&gt; &lt;target&gt;System.err&lt;/target&gt; &lt;!-- 日志消息格式配置 --&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;pattern&gt;$&#123;pattern&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 日志文件输出的appender --&gt; &lt;appender name=\"file\" class=\"ch.qos.logback.core.FileAppender\"&gt; &lt;!-- 日志文件保存路径 --&gt; &lt;file&gt;$&#123;logDir&#125;/tmp.log&lt;/file&gt; &lt;!-- 日志消息格式配置 --&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;pattern&gt;$&#123;pattern&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- HTML格式日志文件输出appender --&gt; &lt;appender name=\"htmlFile\" class=\"ch.qos.logback.core.FileAppender\"&gt; &lt;!-- 日志文件保存路径 --&gt; &lt;file&gt;$&#123;logDir&#125;/tmp.html&lt;/file&gt; &lt;!-- html日志消息格式配置 --&gt; &lt;encoder class=\"ch.qos.logback.core.encoder.LayoutWrappingEncoder\"&gt; &lt;layout class=\"ch.qos.logback.classic.html.HTMLLayout\"&gt; &lt;pattern&gt;$&#123;pattern&#125;&lt;/pattern&gt; &lt;/layout&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 日志拆分和归档压缩的appender对象 --&gt; &lt;appender name=\"rollFile\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!-- 日志文件保存路径 --&gt; &lt;file&gt;$&#123;logDir&#125;/roll_logback.log&lt;/file&gt; &lt;!-- 日志消息格式配置 --&gt; &lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;pattern&gt;$&#123;pattern&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;!-- 指定拆分规则 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;!-- 按照时间和压缩格式声明拆分的文件名 --&gt; &lt;!-- %i根据文件排序 --&gt; &lt;fileNamePattern&gt;$&#123;logDir&#125;/rolling.%d&#123;yyyy-MM-dd-HH-mm-ss&#125;.%i.log.gz&lt;/fileNamePattern&gt; &lt;!-- 按照文件大小拆分 --&gt; &lt;maxFileSize&gt;1MB&lt;/maxFileSize&gt; &lt;/rollingPolicy&gt; &lt;!-- 日志级别过滤器 --&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;!-- 日志过滤规则 --&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 异步日志 --&gt; &lt;appender name=\"async\" class=\"ch.qos.logback.classic.AsyncAppender\"&gt; &lt;!-- 指定某个具体的appender --&gt; &lt;appender-ref ref=\"rollFile\"/&gt; &lt;/appender&gt; &lt;!-- root logger配置 --&gt; &lt;root level=\"ALL\"&gt; &lt;!-- &lt;appender-ref ref=\"console\"/&gt; &lt;appender-ref ref=\"file\"/&gt; &lt;appender-ref ref=\"htmlFile\"/&gt; --&gt; &lt;appender-ref ref=\"async\"/&gt; &lt;/root&gt; &lt;!-- 自定义logger对象 additivity: 自定义logger对象是否继承rootLogger --&gt; &lt;logger name=\"com.xiong.logger\" level=\"info\" additivity=\"false\"&gt; &lt;appender-ref ref=\"console\"/&gt; &lt;/logger&gt;&lt;/configuration&gt; logback-access使用：logback-access模块与Servlet容器(如Tomcat、Jetty)集成以提供HTTP访问日志功能。可使用logback-access模块来替换tomcat的访问日志 将logback-access.jar与logback-core.jar复制到$TOMCAT_HOME/lib/目录下 修改$TOMCAT_HOME/conf/server.xml中的Host元素中添加： 1&lt;Valve className=\"ch.qos.logback.access.tomcat.LogbackValve\" /&gt; logback默认会在$TOMCAT_HOME/conf下查找文件logback-access.xml 123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration&gt; &lt;!-- 参考自http://logback.qos.ch/access.html#configuration --&gt; &lt;!-- always a good activate OnConsoleStatusListener --&gt; &lt;statusListener class=\"ch.qos.logback.core.status.OnConsoleStatusListener\"/&gt; &lt;property name=\"LOG_DIR\" value=\"$&#123;catalina.base&#125;/logs\"/&gt; &lt;appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;LOG_DIR&#125;/access.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;access.%d&#123;yyyy-MM-dd&#125;.log.zip&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;!-- 访问日志的格式 --&gt; &lt;pattern&gt;combined&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender-ref ref=\"FILE\"/&gt;&lt;/configuration&gt; log4j2入门(即是日志门面也是日志实现)： log4j2介绍：Apache Log4j2是对Log4j的升级版，参考了logback的一些优秀的设计，并且修复了一些问题，带来了一些重大的提升，主要有： 异常处理：在logback中，Appender中的异常不会被应用感知到；在log4j2中提供了一些异常处理机制 性能提升：log4j2相较于log4j和logback具有明显的性能提升 自动重载配置：参考了logback的设计，提供了自动刷新参数配置，在生产上可以动态地修改日志的级别而不需要重启应用 无垃圾机制：log4j2在大部分情况下都可以使用其设计的一套无垃圾机制，避免频繁的日志收集导致的JVM GC官网：https://logging.apache.org/log4j/2.x/ 入门案例： 123456789101112&lt;!-- Log4j2门面API--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.11.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Log4j2日志实现 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.11.1&lt;/version&gt;&lt;/dependency&gt; 12345678910111213// 定义日志记录器对象public static final Logger logger = LogManager.getLogger(Log4j2Test.class);// 快速入门@Testpublic void testQuick() &#123; // 日志消息输出 logger.error(\"error\"); logger.warn(\"warn\"); logger.info(\"info\"); logger.debug(\"debug\"); logger.trace(\"trace\");&#125; log4j2配置：log4j2默认加载classpath下的log4j2.xml文件中的配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!--status=\"debug\": warn为日志框架本身的输出日志级别monitorInterval=\"5\": 自动加载配置文件的间隔时间,不低于5s--&gt;&lt;Configuration status=\"debug\" monitorInterval=\"5\"&gt; &lt;!-- 集中配置属性进行管理,可以使用$&#123;&#125;获取 --&gt; &lt;properties&gt; &lt;property name=\"LOG_HOME\"&gt;log4j2&lt;/property&gt; &lt;/properties&gt; &lt;!-- 日志处理器 --&gt; &lt;Appenders&gt; &lt;!-- 控制台输出appender --&gt; &lt;Console name=\"Console\" target=\"SYSTEM_OUT\"&gt; &lt;PatternLayout pattern=\"%d&#123;HH:mm:ss.SSS&#125; [%t] [%-5level] %c&#123;36&#125;:%L - -- %m%n\"/&gt; &lt;/Console&gt; &lt;!-- 日志文件输出appender --&gt; &lt;File name=\"file\" fileName=\"$&#123;LOG_HOME&#125;/file.log\"&gt; &lt;PatternLayout pattern=\"[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%-5level] %l %c&#123;36&#125; - %m%n\"/&gt; &lt;/File&gt; &lt;!-- 随机读写流输出appender,性能提高 --&gt; &lt;RandomAccessFile name=\"accessFile\" fileName=\"$&#123;LOG_HOME&#125;/accessFile.log\"&gt; &lt;PatternLayout pattern=\"[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%-5level] %l %c&#123;36&#125; - %m%n\"/&gt; &lt;/RandomAccessFile&gt; &lt;!-- 按照一定的规则拆分日志文件appender filePattern拆分规则 --&gt; &lt;RollingFile name=\"rollingFile\" fileName=\"$&#123;LOG_HOME&#125;/rollingFile.log\" filePattern=\"log4j2/$$&#123;date:yyyy-MM-dd&#125;/rollingFile-%d&#123;yyyy- MM-dd-HH-mm&#125;-%i.log\"&gt; &lt;!-- 日志级别过滤器 --&gt; &lt;ThresholdFilter level=\"debug\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/&gt; &lt;!-- 日志消息格式 --&gt; &lt;PatternLayout pattern=\"[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%-5level] %l %c&#123;36&#125; - %msg%n\"/&gt; &lt;!-- 策略 --&gt; &lt;Policies&gt; &lt;!-- 在系统启动时触发拆分规则,产生一个新日志文件 --&gt; &lt;OnStartupTriggeringPolicy/&gt; &lt;!-- 按照文件大小进行拆分,产生一个新日志文件 --&gt; &lt;SizeBasedTriggeringPolicy size=\"10 MB\"/&gt; &lt;!-- 按照时间节点拆分,规则由filePattern定义 --&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;/Policies&gt; &lt;!-- 在同一个目录下,文件个数限定,超过进行覆盖 --&gt; &lt;DefaultRolloverStrategy max=\"30\"/&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt; &lt;!-- logger定义 --&gt; &lt;Loggers&gt; &lt;!-- 使用rootLogger配置以及level --&gt; &lt;Root level=\"trace\"&gt; &lt;AppenderRef ref=\"Console\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; log4j2异步日志：log4j2最大的特点就是异步日志，其性能的提升也主要在异步日志Log4j2提供了两种实现异步日志的方式： 通过AsyncAppender，对应Appender组件 通过AsyncLogger，对应Logger组件 导入依赖(异步日志需要另外依赖) 123456&lt;!--异步日志依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.lmax&lt;/groupId&gt; &lt;artifactId&gt;disruptor&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt;&lt;/dependency&gt; AsyncAppender方式： 123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;Configuration status=\"warn\"&gt; &lt;properties&gt; &lt;property name=\"LOG_HOME\"&gt;logs&lt;/property&gt; &lt;/properties&gt; &lt;Appenders&gt; &lt;File name=\"file\" fileName=\"$&#123;LOG_HOME&#125;/log4j2.log\"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;%d %p %c&#123;1.&#125; [%t] %m%n&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;/File&gt; &lt;Async name=\"Async\"&gt; &lt;AppenderRef ref=\"file\"/&gt; &lt;/Async&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level=\"error\"&gt; &lt;AppenderRef ref=\"Async\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; AsyncLogger方式：AsyncLogger才是log4j2的重头戏，也是官方推荐的异步方式。它可使调用Logger.log返回地更快。有两种选择：全局异步和混合异步 全局异步：所有的日志都异步的记录，在配置文件上不用做任何改动，只需要添加一个log4j2.component.properties配置 1Log4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector 混合异步：可在应用中同时使用同步日志和异步日志，这使得日志的配置方式更加灵活 123456789101112131415161718192021222324&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;Configuration status=\"WARN\"&gt; &lt;properties&gt; &lt;property name=\"LOG_HOME\"&gt;logs&lt;/property&gt; &lt;/properties&gt; &lt;Appenders&gt; &lt;File name=\"file\" fileName=\"$&#123;LOG_HOME&#125;/log4j2.log\"&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;%d %p %c&#123;1.&#125; [%t] %m%n&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;/File&gt; &lt;Async name=\"Async\"&gt; &lt;AppenderRef ref=\"file\"/&gt; &lt;/Async&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;AsyncLogger name=\"com.xiong\" level=\"trace\" includeLocation=\"false\" additivity=\"false\"&gt; &lt;AppenderRef ref=\"file\"/&gt; &lt;/AsyncLogger&gt; &lt;Root level=\"info\" includeLocation=\"true\"&gt; &lt;AppenderRef ref=\"file\"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 使用异步日志需要注意的问题： 如果使用异步日志，AsyncAppender、AsyncLogger和全局日志不要同时出现。性能会和AsyncAppender一致，降至最低(木桶原理) 需设置includeLocation=false，打印位置信息会急剧降低异步日志的性能，比同步日志还要慢 log4j2无垃圾记录：垃圾收集暂停是延迟峰值的常见原因。对于许多系统而言，需要花费大量精力来控制这些暂停许多日志库(包括以前版本的Log4j)在稳态日志记录期间分配临时对象，如日志事件对象、字符串、字符数组、字节数组等。这会对垃圾收集器造成压力并增加GC暂停发生的频率从版本2.6开始，默认情况下Log4j以”无垃圾”模式运行，重用对象和缓冲区，并且尽可能不分配临时对象。还有一个”低垃圾”模式，它不是完全无垃圾，但不使用ThreadLocal字段Log4j版本2.6中的无垃圾日志记录部分通过重用ThreadLocal字段中的对象来实现，部分通过在将文本转换为字节时重用缓冲区来实现有两个单独的系统属性可用于手动控制Log4j避免创建临时对象的机制： log4j2.enableThreadlocals：true(非Web应用程序的默认值)，则对象存储在ThreadLocal字段中并重新使用，否则将为每个日志事件创建新对象 log4j2.enableDirectEncoders：tru(默认)，则将日志事件转换为文本，此文本转换为字节而不创建临时对象。注意：由于共享缓冲区上的同步，在此模式下多线程应用程序的同步日志记录性能可能更差。如果应用程序是多线程的并且日志记录性能很重要，请考虑使用异步记录器 日志门面 背景：当系统变得更加复杂时，日志就容易发生混乱。随着系统开发的进行，可能会更新不同的日志框架，这会造成当前系统中存在不同的日志依赖，难以统一地管理和控制。就算强制要求所有的模块使用相同的日志框架，系统中也难以避免使用其他类似Spring、MyBatis等其他的第三方框架，它们依赖于不同的日志框架，而且它们自身的日志系统有着不一致性，依然会导致日志体系的混乱借鉴JDBC的思想，可为日志系统也提供一套门面，那么就可以面向这些接口规范来开发，免去直接依赖具体的日志实现框架 日志框架出现顺序：log4j -&gt; JUL -&gt; JCL -&gt; slf4j -&gt; logback -&gt; log4j2 具体日志门面框架： JCL入门： 介绍：JCL(Jakarta Commons Logging)是Apache提供的一个通用日志API。它为”所有的Java日志实现”提供一个统一的接口，它自身也提供一个简单日志的实现(SimpleLog,但功能非常常弱)。它允许使用不同的具体日志实现框架：Log4j、JDK自带的日志(JUL)以及SimpleLog。JCL有两个基本的抽象类：Log(基本记录器)和LogFactory(负责创建Log实例) 入门案例： 12345&lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt; 1234567891011public void testQuick()&#123; // 获取log日志记录器对象 Log log = LogFactory.getLog(JCLTest.class); // 日志记录输出 log.info(\"Hello JCL\"); log.fatal(\"fatal\"); log.error(\"error\"); log.warn(\"warn\"); log.info(\"info\"); log.debug(\"debug\");&#125; JCL原理： 通过LogFactory动态加载Log实现类 获取具体的日志实现： 12345678private static final String[] classesToDiscover = new String[]&#123; \"org.apache.commons.logging.impl.Log4JLogger\", \"org.apache.commons.logging.impl.Jdk14Logger\", \"org.apache.commons.logging.impl.Jdk13LumberjackLogger\", \"org.apache.commons.logging.impl.SimpleLog\"&#125;;// ...// 默认加载第一个发现的日志实现框架for(int i = 0; i &lt; classesToDiscover.length &amp;&amp; result == null; ++i) &#123; result = this.createLogFromClass(classesToDiscover[i], logCategory, true);&#125; slf4j入门： 介绍：简单日志门面slf4j((Simple Logging Facade For Java))主要是为了给Java日志访问提供一套标准、规范的API框架。主要意义在于提供接口，具体的实现交由其他日志框架，如log4j2和logback等slf4j自身也提供功能较为简单的实现，但基本不使用。slf4j是目前市面上最流行的日志门面。现在项目中基本上都使用slf4j作为日志门面，配上具体的实现框架(log4j2、logback等)，中间使用桥接器完成桥接slf4j日志门面主要提供两大功能：日志框架的绑定和桥接官方网站：https://www.slf4j.org/ 入门案例： 123456789101112&lt;!--slf4j core 使用slf4j必須添加--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt;&lt;!--slf4j 自带的简单日志实现 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223public static final Logger LOGGER = LoggerFactory.getLogger(Slf4jTest.class);@Testpublic void test01() &#123; // 日志输出 LOGGER.error(\"error\"); LOGGER.warn(\"warn\"); LOGGER.info(\"info\"); LOGGER.debug(\"debug\"); LOGGER.trace(\"trace\"); // 使用占位符输出日志信息 String name = \"SOBXiong\"; Integer age = 23; LOGGER.info(\"Name: &#123;&#125; , age: &#123;&#125;\", name, age); // 异常信息输出 try &#123; int i = 1 / 0; &#125; catch (Exception e) &#123; LOGGER.error(\"Error Occurs: \", e); &#125;&#125; slf4j的好处： 使用slf4j框架，可在部署时迁移到所需的日志记录框架 slf4j提供了对所有流行的日志框架的绑定，例如log4j，JUL，Simple logging和NOP等。因此可在部署时切换到任何这些框架 无论使用哪种绑定，slf4j都支持参数化日志记录消息。由于slf4j将应用程序和日志记录框架分离，因此可以轻松编写独立于日志记录框架的应用程序，而无需担心用于编写应用程序的日志记录框架 slf4j提供了一个简单的Java工具，称为迁移器。使用此工具，可以迁移现有项目到slf4j 绑定日志实现(Bind)：slf4j支持各种日志框架。slf4j发行版附带了几个称为”slf4j绑定”的jar包，每个绑定对应一个受支持的框架 绑定流程： 添加slf4j-api的依赖 使用slf4j的API在项目中进行统一的日志记录 绑定具体的日志实现框架 绑定已经实现了slf4j的日志框架：直接添加对应依赖 绑定没有实现slf4j的日志框架：先添加日志的适配器，再添加实现类的依赖 slf4j有且仅有一个日志实现框架的绑定(如果出现多个默认使用第一个依赖日志实现) 常用的绑定日志实现pom依赖： 123456789101112131415161718192021222324252627282930313233343536&lt;!-- slf4j core,使用slf4j必须添加 --&gt;&lt;!-- logback默认支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- log4j --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt;&lt;!-- jul --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-jdk14&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- jcl --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-jcl&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- nop --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt; 绑定原理：要切换日志框架只需替换类路径上的slf4j绑定。例如从java.util.logging切换到log4j，只需将slf4j-jdk14.jar替换为slf4j-log4j12.jar即可slf4j不依赖于任何特殊的类装载。实际上每个slf4j绑定在编译时都是硬连线以使用一个且只有一个特定的日志记录框架。以下是一般概念的图解说明： 桥接旧日志框架(Bridge)通常，依赖的某些组件依赖于slf4j以外的日志记录API。假设这些组件在不久的将来不会切换到slf4j。为解决该情况，slf4j附带了几个桥接模块，这些模块将对log4j、JCL和java.util.logging的API调用重定向，就好像它们是对slf4j API操作一样桥接解决的是项目中日志的遗留问题，当系统中存在之前的日志API，可通过桥接转换到slf4j的实现： 先去除之前老的日志框架的依赖 添加slf4j提供的桥接组件 为项目添加slf4j的具体实现常用的桥接依赖： 123456789101112131415161718&lt;!-- log4j --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- jul --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jul-to-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt;&lt;!-- jcl --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.27&lt;/version&gt;&lt;/dependency&gt; 注意： jcl-over-slf4j.jar和slf4j-jcl.jar不能同时部署。前一个jar文件将导致JCL将日志系统的选择委托给slf4j，后一个jar文件将导致slf4j将日志系统的选择委托给JCL，从而导致无限循环(其余同理) 所有的桥接都只对Logger日志记录器对象有效，如果程序中调用了内部的配置类或者是Appender、Filter等对象，将无法产生效果 slf4j原理： slf4j通过LoggerFactory加载日志具体的实现对象 LoggerFactory在初始化的过程中，通过performInitialization()方法绑定具体的日志实现 在绑定具体实现时通过类加载器加载org/slf4j/impl/StaticLoggerBinder.class 因此只要是一个日志实现框架，且在org.slf4j.impl包中提供一个自己的StaticLoggerBinder类，提供具体日志实现的LoggerFactory，就可以被slf4j加载 SpringBoot中的日志 基本介绍：SpringBoot框架在企业中的使用越来越普遍，SpringBoot日志也是开发中常用的日志系统。SpringBoot默认采用slf4j作为日志门面，logback作为日志实现来记录日志 SpringBoot中的日志设计 1234&lt;dependency&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;/dependency&gt; 总结： 默认使用slf4j作为日志门面，logback作为日志实现 将jul和log4j转为slf4j \bSpringBoot中日志的使用： 修改默认日志配置： 123456logging.level.com.itheima=trace# 在控制台输出的日志的格式,同logbacklogging.pattern.console=%d&#123;yyyy-MM-dd&#125; [%thread] [%-5level] %logger&#123;50&#125; - %msg%n# 指定文件中日志输出的格式logging.file=logs/springboot.loglogging.pattern.file=%d&#123;yyyy-MM-dd&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n 指定配置：在类路径下添加每个日志框架的配置文件时，SpringBoot就不使用默认配置 日志框架 配置文件 logback logback-spring.xml,logback.xml(直接被日志框架识别) log4j2 log4j2-spring.xml,log4j2.xml JUL logging.properties 使用SpringBoot解析日志配置 123456789&lt;!-- logback-spring.xml --&gt;&lt;encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\"&gt; &lt;springProfile name=\"dev\"&gt; &lt;pattern&gt;$&#123;pattern&#125;&lt;/pattern&gt; &lt;/springProfile&gt; &lt;springProfile name=\"prod\"&gt; &lt;pattern&gt;%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level %msg%n&lt;/pattern&gt; &lt;/springProfile&gt;&lt;/encoder&gt; 12# application.propertiesspring.profiles.active = dev 将日志切换为log4j2(log4j2 + slf4j是趋势) 12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;!-- 排除logback --&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!-- 添加log4j2 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;&lt;/dependency&gt;","categories":[],"tags":[{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"}]},{"title":"设计模式","slug":"BasicSkill/DesignPattern","date":"2020-10-09T14:41:27.000Z","updated":"2020-11-07T08:41:41.524Z","comments":true,"path":"2020/10/09/BasicSkill/DesignPattern/","link":"","permalink":"https://sobxiong.github.io/2020/10/09/BasicSkill/DesignPattern/","excerpt":"内容 七大原则 UML类图 设计模式概述 单例模式 工厂模式 原型模式 建造者模式 适配器模式 桥接模式 装饰者模式 组合模式 外观模式 享元模式 代理模式 模板方法模式 命令模式 访问者模式 迭代器模式 观察者模式 中介者模式 备忘录模式 解释器模式 状态模式 策略模式 职责链模式","text":"内容 七大原则 UML类图 设计模式概述 单例模式 工厂模式 原型模式 建造者模式 适配器模式 桥接模式 装饰者模式 组合模式 外观模式 享元模式 代理模式 模板方法模式 命令模式 访问者模式 迭代器模式 观察者模式 中介者模式 备忘录模式 解释器模式 状态模式 策略模式 职责链模式 七大原则 设计模式目的：编写软件过程中，程序员面临着来自耦合性、内聚性、可维护性、可扩展性、重用性和灵活性等多方面的挑战，设计模式是为了让程序(软件)具有更好的 代码重用性(相同功能的代码,不用多次编写) 可读性(编程规范性,便于其他程序员的阅读和理解) 可扩展性(当需要增加新的功能时非常的方便,也称为可维护性) 可靠性(当我们增加新的功能后对原来的功能没有影响) 使程序呈现高内聚，低耦合的特性 设计模式原则其实就是程序员在编程时应当遵守的原则，也是各种设计模式的基础 设计原则核心思想： 找出应用中可能需要变化之处，把它们独立出来，不和不需要变化的代码混在一起 面向接口编程，而不是面向实现编程 为了交互对象之间的松耦合设计而努力 七大原则： 单一职责原则：一个类和方法只做一件事，一个类应该也只有一个引起它修改的原因 接口隔离原则：客户端不应依赖它不需要的接口 依赖倒转(倒置)原则：细节依赖抽象，下层依赖上层 里氏替换原则：子类应该可以完全替换父类。在使用继承时只扩展新功能而不破坏父类原有的功能 开闭原则：一个软件实体如类、模块和函数应该对修改封闭，对扩展开放 迪米特原则：最小知道原则，一个类不应知道自己操作的类的细节 合成复用原则 单一职责原则 基本介绍：对类来说一个类应该只负责一项职责。如类A负责两个不同职责——职责1和2。当职责1需求变更而改变A时，可能造成职责2执行错误，所以需要将类A的粒度分解为A1和A2 案例分析： 12345678910111213141516171819202122232425262728293031323334353637383940414243// 原始方案// 交通工具类class Vehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \" 在公路上运行....\");&#125;&#125;Vehicle vehicle = new Vehicle();vehicle.run(\"摩托车\");vehicle.run(\"汽车\");vehicle.run(\"飞机\");// 改进方案1// 1、遵守单一职责原则// 2、但是这样做的改动很大,将类分解同时修改客户端class RoadVehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \"公路运行\");&#125;&#125;class AirVehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \"天空运行\");&#125;&#125;class WaterVehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \"水中运行\");&#125;&#125;RoadVehicle roadVehicle = new RoadVehicle();roadVehicle.run(\"摩托车\");roadVehicle.run(\"汽车\");AirVehicle airVehicle = new AirVehicle();airVehicle.run(\"飞机\");// 改进方案2// 1、没有对原来的类做大的修改,只是增加方法// 2、虽然没有在类这个级别上遵守单一职责原则,但在方法级别上仍然是遵守单一职责class Vehicle &#123; public void run(String vehicle) &#123; System.out.println(vehicle + \" 在公路上运行....\");&#125; public void runAir(String vehicle) &#123; System.out.println(vehicle + \" 在天空上运行....\");&#125; public void runWater(String vehicle) &#123; System.out.println(vehicle + \" 在水中行....\");&#125;&#125;Vehicle vehicle = new Vehicle();vehicle.run(\"摩托车\");vehicle.run(\"汽车\");vehicle.runAir(\"飞机\"); 注意事项和细节 降低类的复杂度，一个类只负责一项职责(尽量) 提高类的可读性、可维护性 降低变更引起的风险 通常情况下，我们应当遵守单一职责原则。只有逻辑足够简单时才可以在代码级违反单一职责原则；只有类中方法数量足够少时才可以在方法级别保持单一职责原则 接口隔离原则 基本介绍：客户端不应该依赖它不需要的接口(一个类对另一个类的依赖应该建立在最小的接口上) 案例分析： 改进措施：将接口Interface1拆分为独立的几个接口，类A和C分别与它们需要的接口建立依赖关系(也就是采用接口隔离原则) 改进结果： 123456789101112131415161718interface interface1 &#123; void operation1();&#125;interface interface2 &#123; void operation2(); void operation3();&#125;interface interface3 &#123; void operation4(); void operation5();&#125;class A implements interface1, interface2 &#123; // ...&#125;class B implements interface1, interface3 &#123; // ...&#125; 依赖倒转原则 基本介绍： 高层模块不应该依赖低层模块，二者都应该依赖其抽象 抽象不应该依赖细节，细节应该依赖抽象 依赖倒转(倒置)的中心思想是面向接口编程 设计理念：相对于细节的多变，抽象的东西要稳定的多。以抽象为基础搭建的架构比以细节为基础的架构要稳定的多。在java中抽象指的是接口或抽象类，细节是具体的实现类 使用接口或抽象类的目的是制定好规范，而不涉及任何具体的操作，把展现细节的任务交给他们的实现类去完成 案例分析： 12345678910111213141516171819202122232425262728293031323334// 原始版本// 如果我们getInfo需要微信、短信等消息// 则新增类同时Person类也要增加相应的接收方法class Email &#123; public String getInfo() &#123; return \"电子邮件信息: hello,world\";&#125;&#125;class Person &#123; public void receive(Email email) &#123; System.out.println(email.getInfo());&#125;&#125;Person person = new Person();person.receive(new Email());// 改进方案：引入一个抽象的接口IReceiver表示接收者// 这样Person类与接口IReceiver发生依赖// Email、WeiXin等属于接收者的范围,各自实现IReceiver接口,遵循了依赖倒转原则interface IReceiver &#123; public String getInfo();&#125;class Email implements IReceiver &#123; public String getInfo() &#123; return \"电子邮件信息: hello,world\";&#125;&#125;class WeiXin implements IReceiver &#123; public String getInfo() &#123; return \"微信信息: hello,ok\";&#125;&#125;// 新增接收者不需要对Person类进行改动class Person &#123; // 改为对接口IReceiver的依赖 public void receive(IReceiver receiver ) &#123; System.out.println(receiver.getInfo());&#125;&#125;Person person = new Person();person.receive(new Email());person.receive(new WeiXin()); 依赖关系传递的三种方式： 接口依赖 12345678910111213interface IRemote &#123; void closeTv(ITv tv);&#125;interface ITv &#123; void close(); &#125;class HuaweiTv implements ITv &#123; @Override public void close() &#123; System.out.println(\"Close Huawei TV~~~\");&#125;&#125;class Mate30Pro implements IRemote &#123; @Override public void closeTv(ITv tv) &#123; System.out.println(\"Mate30Pro close tv~~~\"); tv.close(); &#125;&#125; 构造方法传递 12345678910interface ITv &#123; void close();&#125;class HuaweiTv implements ITv &#123; @Override public void close() &#123; System.out.println(\"Close Huawei TV~~~\");&#125;&#125;class Mate30Pro &#123; private ITv tv; public Mate30Pro(ITv tv) &#123; this.tv = tv;&#125; public void close() &#123; this.tv.close();&#125;&#125; setter方式传递 12345678910interface ITv &#123; void close();&#125;class HuaweiTv implements ITv &#123; @Override public void close() &#123; System.out.println(\"Close Huawei TV~~~\");&#125;&#125;class Mate30Pro &#123; private ITv tv; public setTv(ITv tv) &#123; this.tv = tv;&#125; public void close() &#123; this.tv.close();&#125;&#125; 注意事项和细节： 低层模块尽量都要有抽象类或接口，或者两者都有，程序稳定性更好 变量的声明类型尽量是抽象类或接口，这样变量引用和实际对象间存在一个缓冲层，利于程序扩展和优化 继承时遵循里氏替换原则 里氏替换原则 OO(Object Oriented,面向对象)继承性的思考和说明： 继承包含这一层含义：父类中凡是已经实现好的方法，实际上是在设定规范和契约。虽然它不强制要求所有的子类必须遵循这些契约，但是如果子类对这些已经实现的方法任意修改，就会对整个继承体系造成破坏 继承在给程序设计带来便利的同时，也带来了弊端。如使用继承会给程序带来侵入性、程序的可移植性降低、增加对象间的耦合性；如果一个类被其他的类所继承，则当这个类需要修改时，必须考虑到所有的子类，并且父类修改后，所有涉及子类的功能都有可能产生错误 问题提出：在编程中，如何正确的使用继承?——遵循里氏替换原则 基本介绍： 里氏替换原则(Liskov Substitution Principle)在1988年由麻省理工学院的以为姓里的女士提出 如果每个类型为T1的对象o1都有类型为T2的对象o2，使得以T1定义的所有程序P在所有的对象o1都代换成o2时程序P的行为没有发生变化，那么类型T2是类型T1的子类型。换句话说，所有引用基类的地方必须都能透明地使用其子类的对象 在使用继承时，遵循里氏替换原则，在子类中尽量不重写父类的方法 里氏替换原则告诉我们继承实际上让两个类耦合性增强了，在适当的情况下，可通过聚合、组合和依赖来解决问题 开闭原则 基本介绍： 开闭原则(Open Closed Principle)是编程中最基础、最重要的设计原则 一个软件实体如类、模块和函数应该对扩展开放(对提供方)，对修改关闭(对使用方)。用抽象构建框架，用实现扩展细节 当软件需要变化时，尽量通过扩展软件实体的行为来实现变化，而不是通过修改已有的代码来实现变化 编程中设计模式和其他原则的基础就是开闭原则 案例分析： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 原始版本class GraphicEditor &#123; public void drawShape(Shape s) &#123; if (s.type == 1) drawRectangle(s); else if (s.type == 2) drawCircle(s); // 新增三角形 else if (s.type == 3) drawTriangle(s); &#125; public void drawRectangle(Shape r) &#123; System.out.println(\"Draw Rectangle\");&#125; public void drawCircle(Shape r) &#123; System.out.println(\"Draw Circle\");&#125; // 新增三角形 public void drawTriangle(Shape r) &#123; System.out.println(\"Draw Triangle\");&#125;&#125;class Shape &#123; int type;&#125;class Rectangle extends Shape &#123; Rectangle() &#123; super.type = 1;&#125;&#125;class Circle extends Shape &#123; Circle() &#123; super.type = 2;&#125;&#125;// 新增三角形class Triangle extends Shape &#123; Triangle() &#123; super.type = 3;&#125;&#125;GraphicEditor graphicEditor = new GraphicEditor();graphicEditor.drawShape(new Rectangle());graphicEditor.drawShape(new Circle());graphicEditor.drawShape(new Triangle());// 改进版本class GraphicEditor &#123; public void drawShape(Shape s) &#123; s.draw();&#125;&#125;interface Shape&#123; void draw();&#125;class Rectangle implements Shape &#123; @Override public void draw()&#123; System.out.println(\"Draw Rectangle\");&#125;&#125;class Circle implements Shape &#123; @Override public void draw()&#123; System.out.println(\"Draw Circle\");&#125;&#125;// 新增三角形class Triangle implements Shape &#123; @Override public void draw()&#123; System.out.println(\"Draw Triangle\");&#125;&#125;// 一样的调用方式GraphicEditor graphicEditor = new GraphicEditor();graphicEditor.drawShape(new Rectangle());graphicEditor.drawShape(new Circle());graphicEditor.drawShape(new Triangle()); 迪米特法则 基本介绍： 一个对象应该对其他对象保持最少的了解 类与类关系越密切，耦合度越大 迪米特法则(Demeter Principle)又叫最少知识原则，即一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类不管多么复杂，都尽量将逻辑封装在类的内部。对外除了提供public方法，不对外泄露任何信息 迪米特法则还有个更简单的定义：只与直接的朋友通信 直接的朋友：每个对象都会与其他对象有耦合关系，只要两个对象之间有耦合关系，我们就说这两个对象之间是朋友关系。耦合的方式很多：依赖、关联、组合和聚合等。其中称出现成员变量、方法参数、方法返回值中的类为直接的朋友，而出现在局部变量中的类不是直接的朋友。也就是说，陌生的类最好不要以局部变量的形式出现在类的内部 注意事项和细节： 迪米特法则的核心是降低类之间的耦合 注意：迪米特法则只是要求降低类间(对象间)耦合关系，并不是要求完全没有依赖关系 合成复用原则(Composite Reuse Principle)：尽量使用合成/聚合的方式，而不是使用继承 UML类图 UML基本介绍： UML——Unified modeling language(统一建模语言)是一种用于软件系统分析和设计的语言工具，用于帮助软件开发人员进行思考和记录思路的结果 UML本身是一套符号的规定，就像数学符号和化学符号一样。这些符号用于描述软件模型中的各个元素和它们之间的关系，比如类、接口、实现、泛化、依赖、组合、聚合等 UML图分类： 用例图(use case) 静态结构图：类图(描述类与类之间的关系,是UML图中最核心的)、对象图、包图、组件图、部署图 动态行为图：交互图(时序图与协作图)、状态图、活动图 UML类图基本介绍： 用于描述系统中的类(对象)本身的组成和类(对象)之间的各种静态关系 类之间的关系：依赖、泛化(继承)、实现、关联、聚合与组合 类之间的关系： 依赖关系(Dependence)： 基本介绍：只要是在类中用到了对方，那么他们之间就存在依赖关系。如果没有对方，连编绎都通过不了 具体体现： 类的成员属性 类的成员方法的返回类型 类的成员方法接收的参数类型 类的成员方法中使用到 存在继承、实现等多态关系 泛化关系(Generalization)：泛化关系实际上就是继承关系，是依赖关系的特例 实现关系(Implementation)：实现关系实际上就是实现(接口)关系，是依赖关系的特例 关联关系(Association)： 基本介绍：关联关系实际上就是类与类之间的联系，是依赖关系的特例(引用数) 性质： 具有导航性：即双向或单向关系 具有多重性：如”1”(表示有且仅有一个)、”0…”(表示0个或者多个)、”0,1”(表示0个或1个),”n…m”(表示n到m个),”m…”(表示至少m个) 聚合关系(Aggregation)：表示的是整体和部分的关系，整体与部分可以分开。聚合关系是关联关系的特例，所以具有关联的导航性与多重性 组合关系(Composition)：也是整体与部分的关系，但是整体与部分不可以分开(逻辑或者代码层面上) 设计模式概述 基本介绍： 设计模式是程序员在面对同类软件工程设计问题所总结出来的有用的经验。设计模式不是代码，而是某类问题的通用解决方案，设计模式(Design pattern)代表了最佳的实践。这些解决方案是众多软件开发人员经过相当长的一段时间的试验总结出来的 设计模式的本质：提高软件的维护性、通用性和扩展性，降低软件的复杂度 设计模式并不局限于某种语言，Java，PHP，C++都有设计模式 类型： 创建型模式：单例模式、抽象工厂模式、原型模式、建造者模式、工厂模式 结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式 行为型模式：模版方法模式、命令模式、访问者模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式(Interpreter模式)、状态模式、策略模式、职责链模式(责任链模式) 单例模式 介绍：保证一个类只有一个实例，哪怕多线程同时访问，并提供一个全局访问此实例的方法 典型场景： 数据库连接池 Spring中的单例Bean 单例设计模式的八种实现方式 饿汉式(静态常量) 案例代码： 12345678class Singleton &#123; // 1、构造器私有化(防止外部new) private Singleton() &#123;&#125; // 2、类内创建对象(静态常量) private final static Singleton instance = new Singleton(); // 3、向外暴露静态公共方法,返回单例instance public static Singleton getInstance() &#123; return instance;&#125;&#125; 优缺点总结： 优点：写法较简单，在类装载的时候就完成了实例化。避免了线程同步问题 缺点：在类装载时就完成了实例化，没有达到Lazy Loading(懒加载)的效果。如果从始至终未使用过这个实例，则会造成内存的浪费 该方式基于类加载机制避免了多线程的同步问题，instance在类装载时就实例化。虽然导致类装载的原因有很多种，在单例模式中大多数时候都是调用getInstance()方法。但不能确定有其他的方式(或者其他的静态方法)导致类装载，这时候初始化instance就没有达到Lazy Loading的效果 结论：该单例模式可用，但可能造成内存浪费(一般可忽略) 饿汉式(静态代码块) 案例代码： 123456789101112class Singleton &#123; // 1、私有化构造器 private Singleton() &#123;&#125; // 2、声明静态变量实例 private static Singleton instance; // 3、在静态代码块中,创建单例对象 static &#123; instance = new Singleton(); &#125; // 4、提供一个公有静态方法,返回实例对象 public static Singleton getInstance() &#123; return instance;&#125;&#125; 优缺点总结： 该方式和方式一类似，过将类实例化的过程放在静态代码块中。在类装载时执行静态代码块中的代码，初始化类的实例。优缺点同方式一 结论：该单例模式可用，但可能造成内存浪费(一般可忽略) 懒汉式(线程不安全) 案例代码： 1234567891011class Singleton &#123; // 1、私有化构造器 private Singleton() &#123;&#125; // 2、声明静态变量实例 private static Singleton instance; // 3、提供一个公有静态方法,当调用该方法时才去创建instance实例 public static Singleton getInstance() &#123; if(instance == null) instance = new Singleton(); return instance; &#125;&#125; 优缺点总结： 起到了Lazy Loading的效果，但只能在单线程下使用 在多线程下，当一个线程进入了if判断语句块，还未来得及往下执行；同时另一个线程也通过了这个判断语句，这时便会产生多个实例。所以在多线程环境下不可使用这种方式 结论：在实际开发中，不使用这种方式 懒汉式(线程安全,同步方法) 案例代码： 1234567891011class Singleton &#123; // 1、私有化构造器 private Singleton() &#123;&#125; // 2、声明静态变量实例 private static Singleton instance; // 3、提供一个公有静态方法,当调用该方法时才去创建instance实例(声明为同步方法,解决线程安全问题) public static synchronized Singleton getInstance() &#123; if(instance == null) &#123; instance = new Singleton();&#125; return instance; &#125;&#125; 优缺点总结： 解决了线程安全问题 效率太低了，每个线程在想获得该类的实例时执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return即可。方法进行同步效率太低 结论：在实际开发中，不推荐使用这种方式 懒汉式(线程不安全,同步代码块) 案例代码： 123456789101112131415class Singleton &#123; // 1、私有化构造器 private Singleton() &#123;&#125; // 2、声明静态变量实例 private static Singleton instance; // 3、提供一个公有静态方法,当调用该方法时才去创建instance实例(加入synchronized同步代码块,还存在线程安全问题) public static Singleton getInstance() &#123; if(instance == null) &#123; synchronized (Singleton.class) &#123; instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 优缺点总结： 还存在线程安全问题(都进入到判断后,未开始同步,会实例化两次) 结论：在实际开发中，不使用这种方式 双重检查(Double-Check) 案例代码： 12345678910111213141516class Singleton &#123; // 1、私有化构造器 private Singleton() &#123;&#125; // 2、声明静态变量实例(volatile声明,内存可见) private static volatile Singleton instance; // 3、提供一个公有静态方法,设置双重检查,解决线程安全问题,同时解决懒加载问题,保证了效率,推荐使用 // 几乎解决了线程安全问题 public static Singleton getInstance() &#123; if(instance == null) &#123; synchronized (Singleton.class) &#123; if(instance == null) &#123; instance = new Singleton();&#125; &#125; &#125; return instance; &#125;&#125; 优缺点总结： Double-Check概念是多线程开发中常使用到的。进行了两次if检查，这几乎可以保证线程安全 实例化代码只执行一次，再次访问时直接返回实例化对象。也避免反复进行方法同步 线程安全、延迟加载、效率较高 结论：在实际开发中推荐使用该方式 静态内部类 案例代码： 123456789101112class Singleton &#123; // 1、私有化构造器 private Singleton() &#123;&#125; // 2、声明静态内部类,该类中有一个静态常量Singleton private static class SingletonInstance &#123; private static final Singleton INSTANCE = new Singleton(); &#125; // 3、提供一个公有静态方法,直接返回静态内部类中的静态常量INSTANCE public static synchronized Singleton getInstance() &#123; return SingletonInstance.INSTANCE; &#125;&#125; 优缺点总结： 该方式在Singleton类被装载时并不会立即实例化，而是在首次调用getInstance()方法时装载SingletonInstance类，从而完成Singleton的实例化 类的静态属性只会在第一次加载类的时候初始化，在这里JVM帮助我们保证了线程的安全性。在类进行初始化时其他线程是无法进入的 优点：避免了线程不安全，利用静态内部类特点实现延迟加载，效率高 结论：推荐使用 枚举 案例代码： 1234567// 1、声明枚举类enum Singletion&#123; // 2、声明一个实例 INSTANCE; // 测试方法 public void sayOK() &#123; System.out.println(\"ok~\");&#125;&#125; 优缺点总结： 借助JDK1.5中添加的枚举来实现单例模式。不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象 在有继承的场景下不适用 结论：推荐使用(Effective Java作者推荐) 源码中的应用： 1234567// Runtime.class// 采用饿汉式创建,因为其他地方需要用到,不会产生内存浪费public class Runtime &#123; private Runtime() &#123;&#125; private static Runtime currentRuntime = new Runtime(); public static Runtime getRuntime() &#123; return currentRuntime;&#125;&#125; 单例模式注意事项和细节说明： 单例模式保证了系统内存中该类只存在一个对象，节省了系统资源。对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能 当想实例化一个单例类的时候，要使用相应的获取对象的方法(通常是getInstance())，而不是使用new创建 单例模式使用的场景：需要频繁地进行创建和销毁的对象、创建对象耗时过多或耗费资源过多(即重量级对象)但又经常用到的对象、工具类对象、频繁访问数据库或文件的对象(比如数据源、session工厂等) 工厂模式 问题背景：在平时编程中，构建对象最常用的方式是new一个对象。乍一看这种做法没什么不好，而实际上这也属于一种硬编码。每new一个对象，相当于调用者多知道了一个类，增加了类与类之间的联系，不利于程序的松耦合。其实构建过程可以被封装起来，工厂模式便是用于封装对象的设计模式 简单工厂模式 基本介绍：让一个工厂类继承构建所有对象的职责(将构建工作封装到一个工厂类中) 案例代码： 123456789101112131415161718192021222324// 只是对传统方式做了一层简单的封装class PhoneFactory &#123; // 调用处代码修改较小,大多只需在工厂类中修改 public Phone create(String brand) &#123; // 如果某个构造方式相当复杂,可以大大减少代码重复 switch (brand) &#123; case \"apple\": return new IPhone(); case \"huawei\": &#123; Phone phone = new Mate() // 具体设置工作... return phone; &#125; default: throw new IllegalArgumentException(\"No other brands now~~~\"); &#125; &#125;&#125;// 调用PhoneFactory factory = new PhoneFactory();// 直接new需要知道mate和iphone的具体构造细节Phone mate = factory.create(\"huawei\");Phone iphone = factory.create(\"apple\");mate.takePhoto();iphone.takePhoto(); 弊端： 如果需生产的产品过多会导致工厂类过于庞大，承担过多的职责，变成超级类。每个产品生产过程的修改都需要修改工厂类(不止一个引起修改的原因)。违背了单一职责原则 当要生产新的产品时，必须在工厂类中添加新的判断分支。而开闭原则告诉我们：类应该对修改封闭。即添加新功能时最好只需增加新的类，而不是修改既有的类 工厂方式模式： 由来：为了解决简单工厂的两处弊端 基本介绍：定义一个创建对象的抽象方法，由子类决定要实例化的类。工厂方法模式将对象的实例化推迟到子类 案例代码： 123456789101112131415161718192021222324252627/*1、产品种类增加时不会变成超级工厂,工厂类会变多,保持灵活2、改变只需改变对应工厂的方法3、新增产品,无需修改已有的工厂,只需添加新工厂*/// 注意：factory接口必须声明慎重,更改接口需要更改所有的工厂类interface PhoneFactory &#123; Phone create();&#125;public class AppleFactory implements PhoneFactory &#123; public Phone create() &#123; return new IPhone(); &#125;&#125;public class HuaweiFactory implements PhoneFactory&#123; public Phone create() &#123; return new Mate(); &#125;&#125;// 调用PhoneFactory appleFactory = new AppleFactory();Phone iphone = appleFactory.create();PhoneFactory huaweiFactory = new HuaweiFactory();Phone mate = huaweiFactory.create();iphone.takePhoto();mate.takePhoto(); 抽象工厂模式： 基本介绍：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。简单来说工厂方法是创建出一种产品，抽象工厂则是创建一类产品 案例代码： 1234567891011121314151617interface TerminalFactory &#123; Phone create(); Pad create(); Watch ceate();&#125;public class AppleFactory implements TerminalFactory &#123; public Phone create() &#123; return new IPhone(); &#125; public Pad create() &#123; return new IPad(); &#125; public Watch create() &#123; return new Watch(); &#125;&#125;// ... 源码中的应用(简单工厂)： 123456789101112131415161718192021222324252627Calendar.getInstance();// Calendar.classpublic static Calendar getInstance() &#123; return createCalendar(TimeZone.getDefault(), Locale.getDefault(Locale.Category.FORMAT));&#125;private static Calendar createCalendar(TimeZone zone, Locale aLocale) &#123; // ... Calendar cal = null; if (aLocale.hasExtensions()) &#123; String caltype = aLocale.getUnicodeLocaleType(\"ca\"); if (caltype != null) &#123; switch (caltype) &#123; case \"buddhist\": cal = new BuddhistCalendar(zone, aLocale); break; case \"japanese\": cal = new JapaneseImperialCalendar(zone, aLocale); break; case \"gregory\": cal = new GregorianCalendar(zone, aLocale); break; &#125; &#125; &#125; // ...&#125; 总结： 工厂模式的意义：将实例化对象的代码提取出来放到一个类中统一管理和维护，达到解耦的目的，从而提高项目的扩展和维护性 设计模式的依赖抽象原则： 创建对象实例时尽量不要直接new，而是封装到工厂的方法中 尽量不要让类继承具体类，而是继承抽象类或者是实现接口 尽量不要覆盖基类中已经实现的方法 原型模式 传统方式 案例代码： 12345678910111213141516// 以买周董同款奶茶为例public class MilkTea &#123; private String type; private boolean isCold;&#125;MilkTea jZhouMilkTea = new MilkTea();jZhouMilkTea.type = \"珍珠奶茶\";jZhouMilkTea.isCold = true;// 假复制(和周董喝的是同一杯奶茶)MilkTea xiongMilkTea = jZhouMilkTea;// 真复制MilkTea xiongMilkTea = new MilkTea();xiongMilkTea.type = jZhouMilkTea.type;xiongMilkTea.isCold = jZhouMilkTea.isCold; 优缺点： 优点：比较好理解，简单易操作 如果对象的属性较多或需要复制的数量较多，那么会造成大量的重复 如果对象改变，使用复制的代码也需进行较大的改动 原型模式 基本介绍： 原型模式(Prototype)指用原型实例指定创建对象的细节，通过拷贝原型创建新的对象 原型模式是一种创建型设计模式，允许通过一个对象再创建另外一个可定制的对象而无需知道如何创建的细节 Java中原生支持——Object的clone()方法 案例代码： 123456789101112131415// Java自带的clone()方法是浅拷贝,只有基本类型的属性会被拷贝,类类型几乎都是传递引用(String等除外)public class MilkTea implements Cloneable&#123; private String type; private boolean isCold; @Override public MilkTea clone() throws CloneNotSupportedException &#123; return (MilkTea) super.clone(); &#125;&#125;MilkTea jZhouMilkTea = new MilkTea();jZhouMilkTea.type = \"珍珠奶茶\";jZhouMilkTea.isCold = true;MilkTea xiongMilkTea = jZhouMilkTea.clone(); 注意事项和细节： 创建新的对象比较复杂时可以利用原型模式简化对象的创建过程，同时也能够提高效率 不用重新初始化对象，而是动态地获得对象运行时的状态 如果原始对象发生变化(增加或者减少属性)，其它克隆对象的也会发生相应的变化，无需修改代码 在实现深克隆的时候可能需要比较复杂的代码 源码中的应用：Java中的Object的clone()方法，采用C++ Cloneable接口，否则运行时出错 浅拷贝和深拷贝： 浅拷贝介绍： 对于数据类型是基本数据类型的成员变量，浅拷贝会直接进行值传递(将该属性值复制一份给新的对象) 对于数据类型是引用数据类型的成员变量(某个数组、某个类的对象等)，那么浅拷贝会进行引用传递(实际也是值拷贝,引用值拷贝)，即只是将该成员变量的引用值(内存地址)复制一份给新的对象。因此实际上两个对象的该成员变量都指向同一个实例。在该情况下，在一个对象中修改该成员变量会影响到另一个对象的该成员变量值 Java Object的clone()方法默认就是浅拷贝 深拷贝介绍： 复制对象的所有基本数据类型的成员变量值 为所有引用数据类型的成员变量申请存储空间，复制每个引用数据类型成员变量所引用的对象(该对象可达的所有对象)。即对象进行深拷贝要对整个对象(包括对象的引用类型)进行拷贝 在Java中可通过以下两种方式实现深拷贝： 自定义类中实现cloneable接口重写clone()方法 通过对象序列化 案例代码： 123456789101112131415161718192021222324252627282930313233public class Test implements Serializable, Cloneable &#123; private static final long serialVersionUID = 1L; private String value;&#125;public class DeepCloneObject implements Serializable, Cloneable &#123; public String name; public Test test; @Override protected Object clone() throws CloneNotSupportedException &#123; Object obj = null; obj = super.clone(); DeepCloneObject deepObj = (DeepCloneObject) obj; deepObj.test = (Test) test.clone(); return deepObj; &#125; public Object deepClone() &#123; // 序列化 try (ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.write(this); ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis)) &#123; // 反序列化 DeepCloneObject deepObj = (DeepCloneObject) ois.readObject(); return deepObj; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 建造者模式 基本介绍： 建造者模式(Builder Pattern)是一种对象构建模式。将一个复杂的构建与其表示想分离，使同样的构建过程可以创建不同的表示 传统的建造者模式采用”建造者-指挥者”方式 现在的建造者模式主要用来链式调用生成不同的配置 案例代码： 123456789101112131415161718192021222324252627282930313233343536373839public class MilkTea &#123; private String name; private double price; private boolean addIce; private MilkTea() &#123;&#125; // ...getter/setter public static class Builder &#123; private MilkTea milkTea = new MilkTea(); public Builder() &#123; milkTea.setName(\"珍珠奶茶\"); milkTea.setPrice(12.5d); milkTea.setAddIce(true); &#125; public Builder setName(String name) &#123; milkTea.setName(name); return this; &#125; public Builder setPrice(double price) &#123; milkTea.setPrice(price); return this; &#125; public Builder addIce(boolean addIce) &#123; milkTea.setAddIce(addIce); return this; &#125; public MilkTea build() &#123; return milkTea; &#125; &#125;&#125;MilkTea milkTea = new MilkTea.Builder().setName(\"波霸奶茶\").addIce(true).build(); 源码中的应用：OkHttp、Retrofit等 适配器模式(待完善) 基本介绍：适配器模式(Adapter Pattern)是结构型模式，将某个类的接口转换成客户端期望的另一个接口表示，目的是兼容性，让原本因接口不匹配不能一起工作的两个类可以协同工作。其别名为包装器(Wrapper) 具体介绍： 类适配器模式： 基本介绍：Adapter类，通过继承src类，实现dst类接口，完成src到dst的适配 注意事项和细节： Java是单继承机制，类适配器需要继承src类。这就要求dst必须是接口，有一定局限性 src类的方法在Adapter中都会暴露出来，增加了使用的成本 由于继承了src类，可根据需求重写src类的方法，使得Adapter的灵活性增强 对象适配器模式： 基本介绍： 基本思路和类适配器模式相同，Adapter类不继承src类，而持有src类的实例以解决兼容性的问题——即持有src类对象，实现dst类接口，完成src到dst的适配 根据”合成复用原则”，在系统中尽量使用关联关系(聚合)来替代继承关系 对象适配器模式是适配器模式常用的一种 注意事项和细节： 对象适配器和类适配器算是同一种思想，只不过实现方式不同 根据合成复用原则，使用组合替代继承。解决了类适配器必须继承src的局限性问题，也不再要求dst必须是接口 使用成本更低，更灵活 接口适配器模式： 一些书籍称为缺省适配器模式 核心思想：当无需实现接口提供的全部方法时，可先设计一个抽象类实现接口，并为该接口中每个方法提供一个默认实现(空实现)。则该抽象类的子类可有选择地覆盖父类的某些方法来实现需求 适用于一个接口不使用其所有的方法的情况 源码中的应用：通过jdbc访问SQLServer(jdbc-odbc) 桥接模式 基本介绍： 桥接模式(Bridge Pattern)是一种结构型设计模式，指将实现与抽象放在两个不同的类层次中，使两个层次可以独立改变 桥接模式基于类的最小设计原则，通过使用封装、聚合及继承等行为让不同的类承担不同的职责。主要特点是把抽象(Abstraction)与行为实现(Implementation)分离开来，从而保持各部分的独立性以及应对他们的功能扩展 传统模式： 需求1：绘三种图案：矩形、圆形和三角形 解决方案： 12345678910111213141516// 依据OOP思想,三个具体实现类一个抽象接口public interface Shape&#123; void draw();&#125;class Rectangle implements Shape&#123; @Override public void draw()&#123; System.out.println(\"Draw Rectangle~~~\"); &#125;&#125;class Circle implements Shape&#123; @Override public void draw()&#123; System.out.println(\"Draw Circle~~~\"); &#125;&#125;class Triangle implements Shape&#123; @Override public void draw()&#123; System.out.println(\"Draw Triangle~~~\"); &#125;&#125; 需求2：在1的基础上添加颜色选择，每种颜色都需有四种不同颜色 解决方案： 复用形状，将具体形状定义为父类，每种不同颜色的图形继承其形状父类，共12个类 复用颜色，将每种颜色定义为父类，每种不同颜色的图形继承其颜色父类，共12个类 采用桥接模式，将形状与颜色分离，根据需要对颜色和形状组合，不会产生类爆炸问题 1234567891011121314public interface Color&#123; String getColor();&#125;public class Red implements Color&#123; @Override public String getColor()&#123; return \"red\"; &#125;&#125;// Yellow Blue Green...class Rectange implements Shape()&#123; private Color color; void setColor(Color color)&#123; this.color = color; &#125; @Override public void draw()&#123; System.out.println(\"Draw \" + color.getColor() + \"Triangle~~~\"); &#125;&#125; 注意事项和细节： 实现了抽象和实现部分的分离，从而极大地提高了系统的灵活性。让抽象部分和实现部分独立开来，这有助于系统进行分层设计，从而产生更好的结构化系统 桥接模式替代多层继承方案，可以减少子类的个数，降低系统的管理和维护成本 桥接模式要求正确识别出系统中两个独立变化的维度(抽象、和实现)，其使用范围有一定的局限 对于那些不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统，桥接模式尤为适用 装饰者模式 基本介绍： 装饰者模式动态地将新功能附加到对象上。在对象功能扩展方面比继承更有弹性，体现了开闭原则(OCP) 主要作用： 增强一个类原有的功能 为一个类添加新的功能 案例代码： 12345678910111213141516171819public interface Condiment&#123; void getCondiment();&#125;public class Milk implements Condiment&#123; @Override public void getCondiment()&#123; System.out.println(\"Milk~~~\"); &#125;&#125;public class IceDecorator implements Condiment&#123; private final Condiment origin; public IceDecorator(Condiment condiment)&#123; origin = condiment; &#125; @Override public void getCondiment()&#123; System.out.println(\"Add Ice~~~\"); origin.getCondiment(); &#125;&#125;Condiment condiment = new IceDecorator(new Milk());Condiment.getCondiment(); 源码中的应用： 1234567// FilterInputStream.classpublic class FilterInputStream extends InputStream &#123; protected volatile InputStream in; protected FilterInputStream(InputStream in) &#123; this.in = in; &#125;&#125; 组合模式 基本介绍：组合模式(Composite Pattern)又叫部分整体模式，属于结构型模式。用于把一组相似的对象当作一个单一的对象。组合模式依据树形结构来组合对象来表示部分以及整体层次 案例代码： 123456789101112131415161718192021public abstract class File &#123; private String name; private Date createTime; abstract public void printInfo();&#125;public class ConcreteFile extends File &#123; @Override public void printInfo() &#123; System.out.println(\"ConcreteFile name: \" + name + \" , createTime: \" + createTime); &#125;&#125;public class Folder extends File&#123; private List&lt;File&gt; childFiles = new ArrayList&lt;&gt;(); public void addFile(File file)&#123; childFiles.add(file);&#125; @Override public void printInfo() &#123; System.out.println(\"Folder name: \" + name + \" , createTime: \" + createTime); for (File file : childFiles) &#123; file.printInfo(); &#125; &#125;&#125; 注意事项和细节： 能简化客户端操作，客户端只需要面对一致的对象而不用考虑整体部分或者节点叶子的问题 具有较强的扩展性，当要更改组合对象时只需要调整内部的层次关系，客户端不用做出任何改动 方便创建出复杂的层次结构。客户端不用理会组合里的组成细节，易添加节点从而创建出复杂的树形结构 需要遍历组织机构或处理的对象具有树形结构时，非常适合使用组合模式 要求较高的抽象性，如果非叶节点和叶节点有很多差异性的话，比如很多方法和属性都不一样，不适合使用组合模式 外观模式 基本介绍：外观模式(Facade Pattern)又名门面模式。外部与一个子系统的通信必须通过一个统一的外观对象进行，为子系统的一组接口提供一个一致的界面，外观模式定义了一个高层接口，使得子系统更易使用 注意事项和细节： 外观模式对外屏蔽了子系统的细节，降低了客户端使用子系统的复杂性；使客户端与子系统解耦，子系统内部的模块更易维护和扩展 当系统需进行分层设计时可考虑使用Facade模式 维护一个遗留的大型系统时，可能该系统已变得非常难以维护和扩展。此时可考虑为新系统开发一个Facade类来提供遗留系统较清晰简单的接口，让新系统与Facade类交互，提高复用性 合理地使用外观模式可以更好地划分访问层次，不能过多或不合理地使用外观模式。使用外观模式好还是直接调用模块好取决于问题复杂度和实际情况。要以让系统有层次和利于维护为目的 享元模式 基本介绍： 通过运用共享技术有效地支持大量细粒度的对象 常用于系统底层开发，解决系统的性能问题。像数据库连接池，里面都是创建好的连接对象，需要时可以直接使用，避免重新创建；如果没有线程再创建 能够解决对象重复创建销毁的资源耗费。当系统中有大量相似对象需要缓冲池时，不需总是创建新对象，而是从缓冲池里拿 经典的应用场景就是池技术——String常量池、数据库连接池、缓冲池等；享元模式是池技术的重要实现方式 内部状态和外部状态 享元模式提出了两个要求：细粒度和共享对象。这就涉及到内部状态和外部状态了，即将对象的信息分为两个部分——内部状态和外部状态 内部状态指对象共享出来的信息，存储在享元对象内部且不会随环境的改变而改变 外部状态指对象得以依赖的一个标记，是随环境改变而改变、不可共享的状态 源码中的应用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 测试案例// 第二次把127全替换为200Integer[] integers = new Integer[]&#123; Integer.valueOf(127), new Integer(127), Integer.valueOf(127), new Integer(127)&#125;;for (int i = 0; i &lt; integers.length; i++) &#123; for (int j = i + 1; j &lt; integers.length; j++) &#123; System.out.println(String.format(\"i%d.equals(i%d)[%s], i%d == i%d[%s]\", i, j, integers[i].equals(integers[j]), i, j, integers[i] == integers[j])); &#125;&#125;/* 第一次结果： i0.equals(i1)[true], i0 == i1[false] i0.equals(i2)[true], i0 == i2[true] i0.equals(i3)[true], i0 == i3[false] i1.equals(i2)[true], i1 == i2[false] i1.equals(i3)[true], i1 == i3[false] i2.equals(i3)[true], i2 == i3[false]*/// Integer.classpublic final class Integer extends Number implements Comparable&lt;Integer&gt; &#123; // 如果i在[low, high]区间,在cache中获取,否则返回新创建的对象 public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; // 内部缓存,使用享元模式 private static class IntegerCache &#123; // 最低-128 static final int low = -128; static final int high; static final Integer cache[]; static &#123; // 最高127,但可以通过VM设置 // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; // 在静态初始化时把[-128, 127]全加入缓存 for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125; &#125;&#125; 注意事项和细节 “享”表示共享，”元”表示对象 系统中有大量对象且这些对象消耗大量内存并且对象的状态大部分可以外部化时，可考虑选用享元模式 用唯一标识码判断，如果在内存中有，则返回这个唯一标识码所标识的对象。大多用HashMap/HashTable存储 大大减少了对象的创建开销，降低了程序内存的占用，提高效率 提高了系统的复杂度，需剥离出内部状态和外部状态。外部状态具有固化特性，不随内部状态的改变而改变。此为使用享元模式需要注意的地方 要注意划分内部状态和外部状态，并且通常需要一个工厂类加以控制 代理模式 基本介绍： 代理模式：为一个对象提供一个替身以控制对该对象的访问。通过代理对象访问目标对象的好处是——可在目标对象实现的基础上，增强额外的功能操作——即扩展目标对象的功能 被代理的对象可以是远程对象、创建开销大的对象或需要安全控制的对象 代理模式有不同的形式，主要有三种：静态代理、JDK代理(又名接口代理,底层采用asm)和Cglib代理(可在内存动态地创建对象,无需实现接口,属于动态代理的范畴,底层采用asm) 静态代理 基本介绍：静态代理在使用时需要定义接口或父类，被代理对象(即目标对象)与代理对象一起实现相同的接口或是继承相同父类(形如装饰模式,但重在控制) 案例代码： 1234567891011121314151617181920212223public interface ITestDao &#123; void test();&#125;public class TestDao implements ITestDao &#123; @Override public void test() &#123; System.out.println(\"TestDao test()~~~\"); &#125;&#125;public class TestDaoProxy implements ITestDao&#123; private ITestDao target; public TestDaoProxy(ITestDao target) &#123; this.target = target; &#125; @Override public void test() &#123; System.out.println(\"Enter TestDaoProxy test()~~~\"); target.test(); System.out.println(\"Leave TestDaoProxy test()~~~\"); &#125;&#125;ITestDao testDao = new TestDao();TestDaoProxy testDaoProxy = new TestDaoProxy(testDao);testDaoProxy.test(); 优缺点总结： 优点：在不修改目标对象功能前提下能通过代理对象对目标功能进行扩展 缺点：因代理对象需要与目标对象实现一样的接口，因此会有很多代理类，一旦接口增加方法，目标对象与代理对象都要维护 JDK代理： 基本介绍： 代理对象不需要实现接口，但目标对象要实现接口，否则不能用动态代理 代理对象的生成是利用JDK的API，动态地在内存中构建代理对象 案例代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243public interface ITestDao &#123; void test(); void testWithParam(String value);&#125;public class TestDao implements ITestDao &#123; @Override public void test() &#123; System.out.println(\"TestDao test()~~~\"); &#125; @Override public void testWithParam(String value) &#123; System.out.println(\"TestDao testWithParam(\" + value + \")~~~\"); &#125;&#125;public class ProxyFactory &#123; // 维护一个目标对象 private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; // 生成代理对象 public Object getProxyInstance() &#123; // ClassLoader loader：指定当前目标对象使用的类加载,获取加载器的方法是固定的 // Class&lt;?&gt;[] interfaces：目标对象实现的接口类型,使用泛型方式确定类型 // InvocationHandler h：事件处理,执行目标对象方法时会触发事件处理器方法,会把当前目标对象方法作为参数传入 return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), (Object proxy, Method method, Object[] args) -&gt; &#123; System.out.println(\"Jdk Proxy start~~~\"); // 反射机制调用目标对象方法 Object result = method.invoke(target, args); System.out.println(\"Jdk Proxy end~~~\"); return result; &#125;); &#125;&#125;// 给目标对象创建代理对象ITestDao proxyInstance = (ITestDao)new ProxyFactory(new TestDao()).getProxyInstance();proxyInstance.test();System.out.println(proxyInstance.testWithParam(\"Hello Proxy\"));System.out.println(proxyInstance);System.out.println(proxyInstance.getClass()); Cglib代理 基本介绍： 静态代理和JDK代理模式都要求目标对象实现一个接口，但有时候目标对象只是一个单独的对象且并没有实现任何的接口，这时候可使用目标对象子类来实现代理——Cglib代理 Cglib代理也叫作子类代理，它在内存中构建一个子类对象从而实现对目标对象功能的扩展，一些资料也将Cglib代理归属到动态代理 Cglib是一个强大的高性能的代码生成包，它可在运行期扩展java类与实现java接口。它广泛地被许多AOP框架使用，如Spring AOP，用于实现方法拦截 在AOP编程中如何选择代理模式： 目标对象需要实现接口，用JDK代理 目标对象不需要实现接口，用Cglib代理 Cglib包的底层是通过使用字节码处理框架ASM来转换字节码并生成新的类 注意事项： 需要引入cglib的jar文件(Spring中集成了Cglib的使用) 注意代理的类不能为final，否则报错——java.lang.IllegalArgumentException 目标对象的方法如果为final/static方法，那么就不会被拦截——即不会执行目标对象额外的业务方法 案例代码： 123456789101112131415161718192021222324252627282930313233public class TestDao &#123; public void test() &#123; System.out.println(\"TestDao test()~~~\"); &#125;&#125;public class ProxyFactory implements MethodInterceptor &#123; // 维护一个目标对象 private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; // 生成代理对象 public Object getProxyInstance() &#123; // 1、创建一个工具类 Enhancer enhancer = new Enhancer(); // 2、设置父类 enhancer.setSuperclass(target.getClass()); // 3、设置回调函数 enhancer.setCallback(this); // 4、创建子类对象即代理对象 return enhancer.create(); &#125; // 重写intercept方法,会调用目标对象的方法 @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; System.out.println(\"Cglib Proxy start~~~\"); Object result = method.invoke(target, args); System.out.println(\"Cglib Proxy end~~~\"); return result; &#125;&#125;TestDao proxyInstance = (TestDao) new ProxyFactory(new TestDao()).getProxyInstance();proxyInstance.test();System.out.println(proxyInstance);System.out.println(proxyInstance.getClass()); 几种常见的代理模式(常见变体) 缓存代理：如当请求图片文件等资源时，先到缓存代理取，如果取不到资源再到公网或者数据库取，然后缓存 远程代理：通过远程对象的本地代表可以把远程对象当本地对象来调用。远程代理通过网络和真正的远程对象沟通信息 同步代理：主要使用在多线程编程中完成多线程间同步工作 模板方法模式 基本介绍： 模板方法模式(Template Method Pattern)又叫模板模式，属于行为型模式。在一个抽象类公开定义执行方法的模板。子类可按需重写方法实现，但调用将以抽象类中定义的方式进行 模板方法模式定义一个操作中算法的骨架，而将一些步骤延迟到子类，这使得子类可以不改变一个算法的结构就可以重定义算法的某些特定步骤 注意事项和细节： 基本思想：算法只存在于一个地方也就是在父类中，容易修改。需要修改算法时只需修改父类的模板方法或已经实现的某些步骤，子类就会继承这些修改 实现了代码复用的最大化。父类的模板方法和已实现的某些步骤会被子类继承直接使用 既统一了算法也提供了很大的灵活性。父类的模板方法确保算法结构保持不变，同时由子类提供部分步骤的实现 模式的不足之处：每一个不同的实现都需要一个子类实现，这会导致类的个数增加，使得系统更加庞大 不希望子类覆写的方法(模版方法)用final修饰；要求子类必须覆写的方法用abstract修饰 模板方法模式使用场景：当要完成在某个过程，该过程要执行一系列步骤且这一系列的步骤基本相同，但其个别步骤在实现时可能不同。这种情况下通常考虑用模板方法模式来处理 命令模式 基本介绍：命令模式(Command Pattern)将一个请求封装为一个对象，可参数化请求对象，支持对请求排队、记录和撤销的操作 案例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public interface ICommand &#123; void execute(); void undo();&#125;public class LightOpenCommand implements ICommand &#123; Light light; public LightOpenCommand(Light light) &#123; this.light = light;&#125; @Override public void execute() &#123; System.out.println(\"Monitor light turn on~~~\"); light.open(); &#125; @Override public void undo() &#123; System.out.println(\"Monitor light turn off~~~\"); light.close(); &#125;&#125;public class LightCloseCommand implements ICommand &#123; Light light; public LightCloseCommand(Light light) &#123; this.light = light; &#125; @Override public void execute() &#123; System.out.println(\"Monitor light turn off~~~\"); light.close(); &#125; @Override public void undo() &#123; System.out.println(\"Monitor light turn on~~~\"); light.open(); &#125;&#125;public class TelevisionOpenCommand implements ICommand &#123; Television television; public TelevisionOpenCommand(Television television) &#123; this.television = television; &#125; @Override public void execute() &#123; System.out.println(\"Monitor television turn on~~~\"); television.open(); &#125; @Override public void undo() &#123; System.out.println(\"Monitor television turn off~~~\"); television.close(); &#125;&#125;public class TelevisionCloseCommand implements ICommand &#123; Television television; public TelevisionCloseCommand(Television television) &#123; this.television = television; &#125; @Override public void execute() &#123; System.out.println(\"Monitor television turn off~~~\"); television.close(); &#125; @Override public void undo() &#123; System.out.println(\"Monitor television turn on~~~\"); television.open(); &#125;&#125;public class MicroCommand implements ICommand &#123; List&lt;ICommand&gt; iCommands; public MicroCommand(List&lt;ICommand&gt; iCommands) &#123; this.iCommands = iCommands;&#125; @Override public void execute() &#123; System.out.println(\"MicroCommand execute~~~\"); for (ICommand iCommand : iCommands) &#123; iCommand.execute(); &#125; &#125; @Override public void undo() &#123; System.out.println(\"MicroCommand undo~~~\"); for (ICommand iCommand : iCommands) &#123; iCommand.undo(); &#125; &#125;&#125;Light light = new Light();ICommand lightOpenCommand = new LightOpenCommand(light);ICommand lightCloseCommand = new LightCloseCommand(light);Television television = new Television();ICommand televisionOpenCommand = new TelevisionOpenCommand(television);ICommand televisionCloseCommand = new TelevisionCloseCommand(television);ICommand microCommand = new MicroCommand(Arrays.asList(lightCloseCommand, lightOpenCommand, televisionOpenCommand, televisionCloseCommand));lightOpenCommand.execute();lightOpenCommand.undo();lightCloseCommand.execute();lightCloseCommand.undo();televisionOpenCommand.execute();televisionOpenCommand.undo();televisionCloseCommand.execute();televisionCloseCommand.undo();microCommand.execute();microCommand.undo(); 注意事项和细节： 容易设计一个命令队列。只要把命令对象放到列队就可以多线程地执行命令 容易实现请求的撤销和重做 可能导致某些系统有过多的具体命令类，增加了系统的复杂度 空命令也是一种设计模式，它省去了判空的操作 经典应用场景：界面的一个按钮都是一条命令、模拟CMD(DOS命令)、订单的撤销/恢复、触发-反馈机制 访问者模式 基本介绍： 访问者模式(Visitor Pattern)封装一些作用于某种数据结构各元素的操作，使得可在不改变数据结构的前提下定义作用于这些元素的新操作 核心思想：将数据结构与数据操作分离，解决数据结构和操作耦合性问题 基本工作原理：在被访问的类中加一个对外提供访问的接口(accept) 主要应用场景：需要对一个对象结构中的对象进行很多不同操作(这些操作彼此没有关联)，同时需避免这些操作”污染”这些对象的类 案例代码： 12345678910111213141516171819202122232425262728293031323334353637public class Chip &#123; private String name; private double value; public void visit(Visitor visitor)&#123; visitor.accept(this); &#125;&#125;public interface Visitor &#123; void accept(Chip chip);&#125;public class AMD implements Visitor &#123; @Override public void accept(Chip chip) &#123; chip.setName(\"Ryzen 9 5950X Design\"); chip.setValue(4000); &#125;&#125;public class TSMC implements Visitor &#123; @Override public void accept(Chip chip) &#123; chip.setName(\"Ryzen 9 5950X Original Entrusted Manufacture\"); chip.setValue(4500); &#125;&#125;public class TMALL implements Visitor &#123; @Override public void accept(Chip chip) &#123; chip.setName(\"Ryzen 9 5950X Sell\"); chip.setValue(5500); &#125;&#125;Chip chip = new Chip();List&lt;Visitor&gt; capitalists = Arrays.asList(new AMD(), new TSMC(), new TMALL());for (Visitor capitalist : capitalists) &#123; chip.visit(capitalist); logger.info(\"Visit By &#123;&#125; , Chip -&gt; &#123;&#125; , &#123;&#125;\", capitalist.getClass().getSimpleName(), chip.getName(), chip.getValue());&#125; 注意事项和细节： 优点： 符合单一职责原则、使程序具有优秀的扩展性、灵活性非常高 可对功能进行统一，可应用于报表、UI、拦截器与过滤器等场景，适用于数据结构相对稳定的系统 缺点 具体元素对访问者公布细节，即访问者关注了其他类的内部细节，违背了迪米特法则且具体元素变更比较困难 违背了依赖倒转原则。访问者依赖的是具体元素，而不是抽象元素 总结：如果一个系统有比较稳定的数据结构，又有经常变化的功能需求，那么访问者模式就比较合适 迭代器模式 基本介绍：迭代器模式(Iterator Pattern)属于行为型模式，它提供一种方法访问一个容器对象中各个元素，而又无需暴露该对象的内部细节 源码中的应用：JDK(Collection.class, Iterator.class) 注意事项和细节： 优点 提供一个统一的方法遍历对象 隐藏了对象内部细节 隐藏了一种设计思想：一个类应只有一个引起变化的原因(单一责任原则)。剥离迭代器，把管理对象集合和遍历对象集合的责任分开 当要展示一组相似对象或遍历一组相同对象时，适合使用迭代器模式 缺点：当遍历细节不同时，会生成多个具体迭代器类 观察者模式 基本介绍：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新 源码中的应用：JDK中的Observable和Observer类 中介者模式 基本介绍：中介者模式(Mediator Pattern)属于行为型模式，定义一个中介对象来封装一系列对象之间的交互。使原来有对象的耦合松散，且可以独立地改变它们之间的交互 注意事项和细节： 多个类相互耦合会形成网状结构，使用中介者模式将网状结构分离为星型结构，进行解耦 减少类间依赖，降低了耦合，符合迪米特原则 中介者承担了较多的责任，需要处理所有类之间的协调工作，一旦中介者出现了问题，整个系统就会受到影响 如果设计不当，中介者对象本身会变得过于复杂 备忘录模式 基本介绍：备忘录模式(Memento Pattern)属于行为型模式，在不破坏封装性的条件下，通过备忘录对象存储另一个对象内部状态的快照，在将来合适时把这个对象还原到存储时的状态 注意事项和细节： 为用户提供了一种可以恢复状态的机制，可使用户能比较方便地回到某个历史的状态 实现了信息的封装，使得用户无需关心状态的保存细节 消耗资源：如果类的成员变量过多，势必会占用比较大的资源，且每一次保存都会消耗一定的内存 适用的应用场景： 打游戏时的存档 Windows里的Ctrl + z 浏览器中的后退 数据库的事务管理 解释器模式 基本介绍： 在编译原理中，一个表达式通过词法分析器形成词法单元，而后这些词法单元再通过语法分析器构建语法分析树，最终形成一颗抽象的语法分析树。这里词法分析器和语法分析器都可以看做是解释器 解释器模式(Interpreter Pattern)：给定一门语言(表达式)，定义它的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中的句子(表达式) 将不可拆分的最小单元称之为终结表达式，可被拆分的表达式称之为非终结表达式 注意事项和细节： 应用场景：编译器、运算表达式计算、正则表达式、机器人等 可能带来的问题：解释器模式会引起类膨胀；解释器模式通常采用递归，这将会导致调试复杂、运行效率降低 状态模式 基本介绍： 状态模式(State Pattern)主要用来解决对象在多种状态转换时需要对外输出不同的行为的问题。状态和行为是一一对应的，状态之间可以相互转换 当一个对象的内在状态改变时允许改变其行为，这个对象看起来像是改变了其类 注意事项和细节： 代码有很强的可读性。状态模式将每个状态的行为封装到对应的一个类中 方便维护。将容易产生问题的if-else语句删除了，如果把每个状态的行为都放到一个类中，每次调用方法时都要判断当前是什么状态，不但会产出很多if-else语句，而且容易出错 符合”开闭原则”，容易增删状态 会产生很多类。每个状态都需要一个对应的类，当状态过多时会产生很多类，加大维护难度 应用场景：当一个事件或者对象有多种状态，状态之间会相互转换，对不同的状态要求有不同的行为时可考虑状态模式 策略模式 基本介绍：定义了一系列算法，并将每一个算法封装起来，且使它们可以相互替换。策略模式让算法独立于使用它的客户而独立变化。简单来说，即殊途同归——当我们做同一件事有多种方式时可将每种方法封装起来，在不同的场景选择不同的策略，调用不同的方法 源码中的应用：图片加载框架(Glide,picaso等)缓存策略 注意事项和细节： 关键是分析项目中变化部分与不变部分 注意多用组合/聚合、少用继承 体现了OCP原则，客户端增加行为不用修改原有代码，只要添加一种策略(或者行为)即可，避免使用多重if-else 每添加一个策略就要增加一个类，当策略过多时会导致类数目庞大 更好的实践：与工厂模式结合，将不同的策略对象封装到工厂类中，只需传递不同的策略类型从工厂中获取对应的策略对象 职责链模式 基本介绍： 职责链模式(Chain of Responsibility Pattern)又叫责任链模式，属于行为型模式。它为请求创建了一个处理者对象的链，对请求的发送者和接收者进行解耦 通常每个处理者都包含对另一个处理者的引用。如果一个处理者不能处理该请求，那么它会把该请求传给下一个处理者，依此类推 源码中的应用：javax包下的FilterChain、SpringMVC包装的FilterChain 注意事项和细节： 将请求和处理分开，实现解耦，提高系统的灵活性 责任分担，每个处理者只处理自身该处理的任务，其余交由下一个处理者完成或提前返回 性能会受到影响，特别是在链比较长的时候。因此需控制链中最大节点数量 采用了类似递归的方式，调试不方便","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BasicSkill","slug":"BasicSkill","permalink":"https://sobxiong.github.io/tags/BasicSkill/"}]},{"title":"Docker","slug":"BasicSkill/Docker","date":"2020-10-09T07:22:59.000Z","updated":"2020-10-16T08:40:44.809Z","comments":true,"path":"2020/10/09/BasicSkill/Docker/","link":"","permalink":"https://sobxiong.github.io/2020/10/09/BasicSkill/Docker/","excerpt":"内容 Docker简介 Docker安装 Docker常用命令 Docker镜像 Docker容器数据卷 Dockerfile Docker安装步骤 Docker镜像发布","text":"内容 Docker简介 Docker安装 Docker常用命令 Docker镜像 Docker容器数据卷 Dockerfile Docker安装步骤 Docker镜像发布 Docker简介 Docker是什么： Docker出现背景： 一款产品从开发到上线，从操作系统，到运行环境，再到应用配置。开发/运维之间的协作需要我们关心很多东西，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验 Docker之所以发展如此迅速，就是因为它对此给出了一个标准化的解决方案 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。能不能从根本上解决问题，软件可以带环境安装？也就是说，安装的时候，把原始环境一模一样地复制过来。利用Docker可以消除协作编码时”在我的机器上可正常工作”的问题 之前在服务器配置一个应用的运行环境，要安装各种软件，有时还不能跨平台，移植应用也非常麻烦 传统上认为，软件编码开发和测试环节结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等。为了让这些程序可以顺利执行，开发团队也得准备完整的部署文件。开发需要清楚地告诉运维部署团队用的全部配置文件和所有软件环境，即便如此仍然会发生部署失败的状况。Docker镜像的设计打破过去”程序即应用”的观念。透过镜像(images)将作业系统核心除外地运行应用程序所需要的系统环境，由下而上打包，达到应用程序跨平台间的无缝接轨运行 Docker理念： Docker是基于Go语言实现的云开源项目 Docker的主要目标是”Build, Ship and Run Any App, Anywhere”。即通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的APP(可以是一个WEB应用或数据库应用等)及其运行环境能够做到”一次封装，到处运行“ Linux容器技术的出现就解决了这样一个问题，而Docker就是在它的基础上发展过来的。将应用运行在Docker容器上面，Docker容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作 总结：解决了运行环境和配置问题软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术 Docker与传统虚拟机技术 虚拟机技术： 虚拟机(virtual machine)是带环境安装的一种解决方案。 可以在一种操作系统里面运行另一种操作系统(Windows系统里面运行Linux系统)，应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样。而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美地运行了另一套系统，能够使应用程序、操作系统和硬件三者之间的逻辑不变 虚拟机的缺点： 资源占用多 冗余步骤多 启动慢 容器虚拟化技术： 由于前面虚拟机存在这些缺点，Linux发展出了另一种虚拟化技术——Linux容器(Linux Containers,缩写为LXC) Linux容器并不模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行 Docker和传统虚拟化方式的不同之处： 传统虚拟机技术是虚拟出一套硬件后在其上运行一个完整操作系统，再在该系统上再运行所需的应用进程 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便 每个容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源 Docker能干嘛： 开发/运维(DevOps,一次构建、随处运行)： 一次构建、随处运行：传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测试验证时间 更便捷的升级和扩缩容：随着微服务架构和Docker的发展，大量的应用会通过微服务方式构建。应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成一块”积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级 更简单的系统运维：应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度一致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复 更高效的计算资源利用：Docker是内核级虚拟化，其不像传统的虚拟化技术一样需要额外的Hypervisor支持，所以在一台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率 Docker去哪下： 官网：https://www.docker.com/ 镜像仓库：https://hub.docker.com/ Docker安装 前提：Docker只能运行在CentOS-6.5或更高的版本的CentOS上，要求系统为64位、系统内核版本为2.6.32-431或者更高版本(以CentOS为例) Docker的基本组成： 镜像(image)：Docker镜像就是一个只读的模板。镜像可以用来创建容器，一个镜像可以创建多个容器 容器(container)： Docker利用容器独立运行的一个或一组应用。容器是用镜像创建的运行实例 它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的Linux环境(包括root用户权限、进程空间、用户空间和网络空间等)和运行在其中的应用程序 容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的 仓库(repository)： 仓库是集中存放镜像文件的场所 仓库(Repository)和仓库注册服务器(Registry)是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签(tag) 仓库分为公开仓库(Public)和私有仓库(Private)两种形式 最大的公开仓库是Docker Hub，里面存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云等 总结： 区分并理解仓储/镜像/容器这些概念 Docker本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是image镜像文件。只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模板。Docker根据image文件生成容器的实例。同一个image文件可以生成多个同时运行的容器实例 image文件生成的容器实例本身也是一个文件，称为镜像文件 一个容器运行一种服务，当我们需要的时候就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 仓储是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以 安装步骤(CentOS7) 参考网址：https://docs.docker.com/engine/install/centos/ 具体步骤： 卸载老版本依赖 12345678sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 设置yum仓库(repository) 1234sudo yum install -y yum-utilssudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 下载最新版本Docker(ce版本——免费,ee版本——商业收费) 1sudo yum install docker-ce docker-ce-cli containerd.io 启动Docker服务 123sudo systemctl start docker# 设置docker服务开机自启动sudo systemctl enable docker 卸载Docker： 123456# 关闭docker服务sudo systemctl stop docker# 删除docker包和依赖sudo yum remove docker-ce docker-ce-cli containerd.io# 删除docker libsudo rm -rf /var/lib/docker 测试docker：docker version 配置docker阿里云镜像加速：控制台 -&gt; 容器镜像服务 -&gt; 镜像加速器 -&gt; 查看操作文档 永远的hello world： 运行：docker run hello-world run hello-world的流程： 底层原理 Docker是怎么工作的：Docker是一个CS结构的系统，Docker守护进程运行在主机上，通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。容器是一个运行时环境，就是”🐳背上的集装箱” 为什么Docker比虚拟机(VM)快： docker有着比虚拟机更少的抽象层。由于docker不需要Hypervisor实现硬件资源虚拟化，运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker会在效率上有明显优势 docker利用的是宿主机的内核，而不需要Guest OS。因此当新建一个容器时，docker不需要和虚拟机一样重新加载一个操作系统内核。从而避免引寻、加载操作系统内核等比较费时费资源的过程，当新建一个虚拟机时，虚拟机软件需要加载Guest OS，整个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统、省略了新建过程，新建一个docker容器只需要几秒钟 Docker常用命令 帮助命令： docker version docker info docker –help 镜像命令： docker images [Options] 介绍：列出本地主机上的镜像 显示参数介绍： REPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小同一仓库源可以有多个TAG，代表这个仓库源的不同个版本，我们使用REPOSITORY:TAG来定义不同的镜像如果不指定一个镜像的版本标签，docker将默认使用centos:latest镜像(系统为CentOS) 常用Options说明： -a：列出本地所有的镜像(含中间映像层) -q：只显示镜像id –digests：显示镜像的摘要信息 –no-trunc：显示完整的镜像信息 docker search [Options] 镜像名 默认搜索源：https://hub.docker.com 常用Options说明： –no-trunc：显示完整的镜像描述 -s：列出收藏数不小于指定值的镜像 –automated：只列出automated build类型的镜像 docker pull 镜像名[:标签]：下载指定镜像 docker rmi：删除镜像 删除单个：docker rmi -f 镜像id/镜像名[:标签] 删除多个：docker rmi -f 镜像名1[:标签] 镜像名2[:标签] 删除全部：docker rmi -f $(docker images -qa) 容器命令： 新建并启动容器(有镜像才能创建容器)：docker run [Options] image [Command] [args…] 常用Options说明： –name dockerName：为容器指定一个名称 -d：后台运行容器，并返回容器ID(也即启动守护式容器) -i：以交互模式运行容器，通常与-t同时使用 -t：为容器重新分配一个伪输入终端，通常与-i同时使用 -P: 随机端口映射 -p: 指定端口映射，有以下四种格式: ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 举例： 12# 使用镜像centos:latest以交互模式启动一个容器,在容器内执行/bin/bash命令docker run -it centos /bin/bash 说明： 操作：docker run -d centos 现象：然后使用docker ps -a进行查看运行的容器，会发现容器已经退出 原因：Docker容器后台运行，就必须有一个前台进程。容器运行的命令如果不是那些一直挂起的命令(比如运行top、tail)，就会自动退出。这是docker的机制问题，比如web容器。以nginx为例，正常情况下，我们配置启动服务只需要启动响应service即可，例如service nginx start。但是这样做，nginx以后台进程模式运行，就导致docker前台没有运行的应用。这样的容器后台启动后，会立即自杀。因为它觉得无事可做。因此最佳的解决方案是将你要运行的程序以前台进程的形式运行 列出当前所有正在运行的容器：docker ps [OPTIONS] 常用Options说明： -a：列出当前所有正在运行、历史上运行过的容器 -l：显示最近创建的容器 -n：显示最近n个创建的容器 -q：静默模式，只显示容器编号 –no-trunc：不截断输出 退出容器(两种方式) 容器停止退出：exit 容器不停止退出：ctrl+P+Q 启动容器：docker start 容器id/容器名 停止容器：docker stop 容器id/容器名 强制停止容器：docker kill 容器id/容器名 删除已停止的容器： 删除单个容器：docker rm 容器id 一次性删除多个容器(-f——force强制,关闭已启动的)： docker rm -f 容器id1 容器id2 … docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm 查看容器日志：docker logs -f -t –tail 条数 容器id 参数说明： -t：加入时间戳 -f：跟随最新的日志打印 –tail num：显示最后多少条 查看容器内运行的进程：docker top 容器id 查看容器内部细节(资源、配置)：docker inspect 容器id 进入正在运行的容器并以命令行交互： 两种方式： docker exec -it 容器id /bin/bash docker attach 容器id 两种方式区别： attach：直接进入容器启动命令的终端，不会启动新的进程 exec：是在容器中打开新的终端，并且可以启动新的进程 从容器内拷贝文件到主机上：docker cp 容器id:容器文件路径 主机文件路径 命令一览： 命令 介绍 attach Attach to a running container(当前shell下attach连接指定运行镜像) build Build an image from a Dockerfile(通过Dockerfile定制镜像) commit Create a new image from a container changes(提交当前容器为新的镜像) cp Copy files/folders from the containers filesystem to the host path(从容器中拷贝指定文件或者目录到宿主机中) create Create a new container(创建一个新的容器,同run,但不启动容器) diff Inspect changes on a container’s filesystem(查看docker容器变化) events Get real time events from the server(从docker服务获取容器实时事件) exec Run a command in an existing container(在已存在的容器上运行命令) export Stream the contents of a container as a tar archive(导出容器的内容流作为一个tar归档文件,对应import) history Show the history of an image(展示一个镜像形成历史) images List images(列出系统当前镜像) import Create a new filesystem image from the contents of a tarball(从tar包中的内容创建一个新的文件系统映像,对应export) info Display system-wide information(显示系统相关信息) inspect Return low-level information on a container(查看容器详细信息) kill Kill a running container(kill指定docker容器) load Load an image from a tar archive(从一个tar包中加载一个镜像,对应save) login Register or Login to the docker registry server(注册或者登陆一个 docker源服务器) logout Log out from a Docker registry server(从当前Docker registry退出) logs Fetch the logs of a container(输出当前容器日志信息) port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT(查看映射端口对应的容器内部源端口) pause Pause all processes within a container(暂停容器) ps List containers(列出容器列表) pull Pull an image or a repository from the docker registry server(从docker镜像源服务器拉取指定镜像或者库镜像) push Push an image or a repository to the docker registry server(推送指定镜像或者库镜像至docker源服务器) restart Restart a running container(重启运行的容器) rm Remove one or more containers(移除一个或者多个容器) rmi Remove one or more images(移除一个或多个镜像——无容器使用该镜像才可删除,否则需删除相关容器才可继续或-f强制删除) run Run a command in a new container(创建一个新的容器并运行一个命令) save Save an image to a tar archive(保存一个镜像为一个tar包,对应load) search Search for an image on the Docker Hub(在docker hub中搜索镜像) start Start a stopped containers(启动容器) stop Stop a running containers(停止容器) tag Tag an image into a repository(给源中镜像打标签) top Lookup the running processes of a container(查看容器中运行的进程信息) unpause Unpause a paused container(取消暂停容器) version Show the docker version information(查看docker版本号) wait Block until a container stops, then print its exit code(截取容器停止时的退出状态值) Docker镜像 是什么：镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件。它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件 UnionFS(联合文件系统,类比花卷)： 介绍：Union文件系统(UnionFS)是一种分层、轻量级并且高性能的文件系统。它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像(没有父镜像)，可以制作各种具体的应用镜像 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统。联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理： docker的镜像实际上由一层一层的文件系统(UnionFS)组成 bootfs(boot file system)主要包含bootloader和kernel，bootloader主要是引导加载kernel。Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs rootfs(root file system)在bootfs之上。包含典型Linux系统中的/dev、/proc、/bin和/etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等 为什么Docker中CentOS镜像出奇的小？(平时安装进虚拟机的CentOS都是好几个G,docker里才200M)对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了。因为底层直接用Host的kernel，自己只需要提供rootfs就行。由此可见对于不同的linux发行版，bootfs基本是一致的，rootfs会有差别，因此不同的发行版可以公用bootfs Docker镜像采用分层结构的理由：最大好处就是共享资源。比如有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像，同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享 特点：Docker镜像都是只读的。当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作”容器层”，”容器层”之下的都叫”镜像层” commit命令补充 命令介绍：提交容器副本使之成为一个新的镜像 命令格式：docker commit -m=”提交的描述信息” -a=”作者” 容器id 要创建的目标镜像名:[标签] Docker容器数据卷 是什么：Docker的理念是将软件与运行的环境打包形成容器运行，运行可以伴随着容器。但对数据的要求希望是持久化的、容器之间希望有可能共享数据。Docker容器产生的数据如果不通过docker commit生成新的镜像使得数据作为镜像的一部分保存下来，那么当容器删除后，数据自然也就没有了。为了能保存数据在docker中我们使用卷。卷类似Redis里面的rdb和aof文件 能干嘛： 容器的持久化 容器间继承+共享数据 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union File System提供一些用于持续存储或共享数据的特性。卷的设计目的就是数据的持久化，它完全独立于容器的生存周期，Docker不会在容器删除时删除其挂载的数据卷 特点： 数据卷可在容器之间共享或重用数据 卷中的更改可以直接生效 数据卷中的更改不会包含在镜像的更新中 数据卷的生命周期一直持续到没有容器使用它为止 数据卷添加 直接命令添加：docker run -it -v /主机目录路径:/容器目录路径 镜像名 查看是否成功：docker inspect 容器id 容器停止退出后，主机修改后数据依然同步 设置只读(read only,容器)：docker run -it -v /主机目录路径:/容器目录路径:ro 镜像名 Dockerfile添加： 在新建Dockerfile添加volume指令： 1234# 说明: 处于可移植和分享的考虑# 使用-v 主机目录:容器目录这种方式不能直接在Dockerfile中实现# 宿主机目录是依赖于特定宿主机的,并不能够保证在所有的宿主机上都存设定的特定目录VOLUME [\"/dataVolumeContainer\"] 完整Dockerfile： 12345# volume testFROM centosVOLUME [\"/dataVolumeContainer1\", \"/dataVolumeContainer2\"]CMD echo \"Build Docker with Volume succeed~~~\"CMD /bin/bash 完整流程： 编写上述Dockerfile build生成镜像：docker build -f Dockerfile -t sobxiong/centos . run运行容器：docker run -it sobxiong/centos /bin/bash 查看目录是否存在，测试创建文件并写入内容 查看宿主机对应目录：docker inspect 容器id 前往对应目录查看内容 数据卷容器 是什么：命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享。挂载数据卷的容器称为数据卷容器 前提：以sobxiong/centos为模板运行容器test1，它具有/dataVolumeContainer1和/dataVolumeContainer2容器卷 容器间传递共享： 先启动父容器test1，并在dataVolumeContainer2中新增内容 以继承方式启动test2和test3：docker run -it –name test2 –volumes-from test1 sobxiong/centos test2和test3分别在dataVolumeContainer2中新增内容 test1中可以看到新增的内容 删除test1后，test2修改的内容test3依旧可读 删除test2后，test3依旧可读之前内容 新建test4继承test3再删除test3，之前内容依旧可见 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止 Dockerfile 是什么：用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本 构建三步骤： 编写Dockerfile文件 docker build docker run Dockerfile具体实例 123456# 以CentOS镜像为例# https://github.com/CentOS/sig-cloud-instance-images/blob/12a4f1c0d78e257ce3d33fe89092eee07e6574da/docker/DockerfileFROM scratchADD centos-8-x86_64.tar.xz /LABEL org.label-schema.schema-version=\"1.0\" org.label-schema.name=\"CentOS Base Image\" org.label-schema.vendor=\"CentOS\" org.label-schema.license=\"GPLv2\" org.label-schema.build-date=\"20200809\"CMD [\"/bin/bash\"] Dockerfile基础知识： 每条保留字指令都必须为大写字母且后面要跟随至少一个参数 指令按照从上到下，顺序执行 ‘#’表示注释 每条指令都会创建一个新的镜像层，并对镜像进行提交 执行Dockerfile的大致流程 docker从基础镜像(scratch)运行一个容器 执行一条指令并对容器作出修改 执行类似docker commit的操作提交一个新的镜像层 docker再基于刚提交的镜像运行一个新容器 执行Dockerfile中的下一条指令直到所有指令都执行完成 小总结： 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段： Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件的运行态 Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石： Dockerfile：定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务、内核进程打交道时,需要考虑如何设计namespace的权限控制)等等 Docker镜像：在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行Docker镜像时真正开始提供服务 Docker容器：直接提供服务 Dockerfile体系结构(保留字指令) FROM：基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER：镜像维护者的姓名和邮箱地址 RUN：容器构建时需要运行的命令 EXPOSE：当前容器对外暴露出的端口 WORKDIR：指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点 ENV：用来在构建镜像过程中设置环境变量 123# 环境变量可以在后续的任何RUN指令中使用,就如同在命令前面指定了环境变量前缀一样# 也可以在其它指令中直接使用这些环境变量,如：WORKDIR $MY_PATHENV MY_PATH /usr/mytest ADD：将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY：类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中源路径的文件/目录复制到新的一层的镜像内的目标路径位置 COPY src dest COPY [“src”, “dest”] VOLUME：容器数据卷，用于数据保存和持久化工作 CMD：指定一个容器启动时要运行的命令；Dockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被docker run之后的参数替换 ENTRYPOINT：指定一个容器启动时要运行的命令；ENTRYPOINT的目的和CMD一样，都是在指定容器启动程序及参数(不像CMD,不会被替换,都生效) ONBUILD：当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发 小总结： 案例： Base镜像(scratch)：Docker Hub中99%的镜像都是通过在base镜像中安装和配置需要的软件构建出来的 自定义镜像mycentos： 编写： 目标： 基于centos镜像 登陆后默认路径为/ 安装vim编辑器 安装net-tools(支持ifconfig) 内容： 12345678910FROM centosMAINTAINER sobxiongENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo \"success--------------ok\"CMD /bin/bash 构建：docker build -f Dockerfile -t 新镜像名:Tag . 运行：docker run -it 新镜像名:Tag 列出镜像变更历史：docker history 镜像名 CMD/ENTRYPOINT：均指定一个容器启动时要运行的命令 CMD：Dockerfile中可以有多个CMD 指令，但只有最后一个生效，CMD会被docker run之后的参数替换 ENTRYPOINT：docker run之后的参数会被当做参数传递给ENTRYPOINT，之后形成新的命令组合 总结： Docker安装步骤 搜索镜像 拉取镜像 查看镜像 启动镜像 停止容器 移除容器 Docker镜像发布 发布流程(阿里云) 生成镜像： 从Dockerfile构建 从容器创建一个新镜像：docker commit [Options] 容器id [Repository[:Tag]] 将本地镜像推送到阿里云 登陆阿里云 创建仓库镜像：命名空间、仓库名称 根据提示推送镜像到registery(此后可查看可下载)","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BasicSkill","slug":"BasicSkill","permalink":"https://sobxiong.github.io/tags/BasicSkill/"}]},{"title":"Flume","slug":"BigData/Flume","date":"2020-10-09T07:21:16.000Z","updated":"2020-10-22T15:05:23.143Z","comments":true,"path":"2020/10/09/BigData/Flume/","link":"","permalink":"https://sobxiong.github.io/2020/10/09/BigData/Flume/","excerpt":"内容 Flume概述 Flume入门 Flume进阶 Flume知识点","text":"内容 Flume概述 Flume入门 Flume进阶 Flume知识点 Flume概述 Flume基本介绍：Flume是Cloudera提供的一个高可用的、高可靠的、分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单 Flume基础架构： Agent：Agent是一个JVM进程，以事件的形式将数据从源头送至目的Agent主要有3个部分组成：Source、Channel、Sink Source：Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型和格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy Sink：Sink不断轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者发送到另一个Flume AgentSink组件目的地包括hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义 Channel：Channel是位于Source和Sink之间的缓冲区。Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作Flume自带两种Channel：Memory Channel和File Channel Memory Channel是内存中的队列。Memory Channel适用于无需关心数据丢失的情景。如果关心数据丢失，那么Memory Channel就不应该使用。程序死亡、机器宕机或者重启都会导致数据丢失 File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据 Event：Event是Flume数据传输的基本单元，Flume以Event的形式将数据从源头送至目的地。Event由Header和Body两部分组成，Header用来存放该event的一些属性，为K-V结构；Body用来存放该条数据，为字节数组 Flume入门 Flume安装部署： 主要资料来源： Flume官网地址：http://flume.apache.org/ 文档地址：http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html 下载地址：http://flume.apache.org/download.html 安装部署： 上传并解压：tar zxvf apache-flume-1.9.0-bin.tar.gz -C /opt/module/ 删除lib文件夹下的guava-11.0.2.jar以兼容Hadoop3.1.3：rm -rf guava-11.0.2.jar 设置环境变量(方便使用)： 12345# vim /etc/profile## FLUME_HOMEexport FLUME_HOME=/opt/module/flume-1.9.0export PATH=$PATH:$FLUME_HOME/bin## 记得source /etc/profile Flume入门案例： 监控端口数据： 案例需求：使用Flume监听一个端口，收集该端口数据，并打印到控制台 案例分析： 实现步骤： 安装netcat工具：sudo yum install -y nc 查看44444端口是否被占用：sudo netstat -tunlp | grep 44444 创建并编写Agent配置文件 1234567891011121314151617181920212223# netcat_memory_logger.conf# 设置当前agent、sources、channels、sinks的名字a1、s1、c1、s1a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1# source配置a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 44444# channel配置a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000# sink配置a1.sinks.k1.type &#x3D; logger# 声明source,sink和channel关系a1.sources.r1.channels &#x3D; c1# 注意:一个sink只能对应一个channel# 一个channel可以对应多个sinka1.sinks.k1.channel &#x3D; c1 开启flume agent监听端口： 1234# 写法1flume-ng agent --conf conf/ --name a1 --conf-file agent_conf/netcat_memory_logger.conf -Dflume.root.logger=INFO,console# 写法2flume-ng agent -c conf/ -n a1 -f agent_conf/netcat_memory_logger.conf -Dflume.root.logger=INFO,console 参数说明： –conf/-c：表示配置文件存储在conf/目录 –name/-n：表示给agent起名为a1 –conf-file/-f：flume本次启动读取的配置文件是在agent_conf文件夹下的netcat_memory_logger.conf文件 -Dflume.root.logger=INFO,console：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error 使用netcat工具向本机44444端口发送内容：nc localhost 44444(之后键入内容) 在flume监听终端观察接受数据情况 实时监控单个追加文件 案例需求：实时监控文件，并上传到HDFS中 案例分析： 实现步骤： Flume要想将数据输出到HDFS需要依赖Hadoop相关jar包，检查/etc/profile，确认Hadoop和Java环境变量配置正确 12345678## JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_251export PATH=$PATH:$JAVA_HOME/bin## HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-3.1.3export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin 创建并编写Agent配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# exec_hdfs.conf# 设置当前agent名,sources、channels、sinks的名字a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1# source配置# 要想读取Linux系统中的文件就得按照Linux命令的规则执行命令。# exec即execute——执行。表示执行Linux命令来读取文件a1.sources.r1.type &#x3D; execa1.sources.r1.command &#x3D; tail -F &#x2F;opt&#x2F;data&#x2F;test.log# channel配置a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000# sink配置a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop101:9000&#x2F;flume&#x2F;%Y%m%d&#x2F;%H# 是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true# 上传文件的前缀a1.sinks.k1.hdfs.filePrefix &#x3D; logs-# 是否按照时间滚动文件夹a1.sinks.k1.hdfs.round &#x3D; true# 多少单位时间创建一个新文件夹a1.sinks.k1.hdfs.roundValue &#x3D; 1# 定义时间单位a1.sinks.k1.hdfs.roundUnit &#x3D; hour# 积攒多少哥Event才flush到HDFSa1.sinks.k1.hdfs.batchSize &#x3D; 100# 设置文件类型,可支持压缩a1.sinks.k1.hdfs.fileType &#x3D; DataStream# 多久生成一个新文件a1.sinks.k1.hdfs.rollInterval &#x3D; 600# 设置每个文件的滚动大小a1.sinks.k1.hdfs.rollSize &#x3D; 134217700# 文件滚动与Event数量无关a1.sinks.k1.hdfs.rollCount &#x3D; 0# 声明source,sink和channel关系a1.sources.r1.channels &#x3D; c1# 注意:一个sink只能对应一个channel# 一个channel可以对应多个sinka1.sinks.k1.channel &#x3D; c1 开启HDFS准备接受文件： 12# hadoop101start-dfs.sh 开启flume agent监听文件： 1flume-ng agent -c conf/ -n a1 -f agent_conf/exec_hdfs.conf 在HDFS的web监控界面查看文件 实时监控目录下多个新文件 案例需求：使用Flume监听整个目录的文件，并上传至HDFS 案例分析： 实现步骤： 创建并编写Agent配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142# spooldir_hdfs.confa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1# Spooling Directory Source:用来监听一个目录进行自动收集目录中内容# 1.当目录中某个log文件内容被读取完毕后,该文件有两种处理方案(取决于deletePolicy配置):# 1、删除 2、更改扩展名.COMPLETED# 2.更改扩展名目的就是为了标示该文件已被读取完毕# 注意:该目录中的文件名不能相同,如果相同会抛异常a1.sources.r1.type &#x3D; spooldira1.sources.r1.spoolDir &#x3D; &#x2F;opt&#x2F;dataa1.sources.r1.fileHeader &#x3D; true# 忽略所有以.tmp结尾的文件,不上传a1.sources.r1.ignorePattern &#x3D; ([^ ]*\\.tmp)# channel配置a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000# sink配置a1.sinks.k1.type &#x3D; hdfsa1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop101:9000&#x2F;flume2&#x2F;%Y%m%d&#x2F;%Ha1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; truea1.sinks.k1.hdfs.filePrefix &#x3D; logs-a1.sinks.k1.hdfs.round &#x3D; truea1.sinks.k1.hdfs.roundValue &#x3D; 1a1.sinks.k1.hdfs.roundUnit &#x3D; houra1.sinks.k1.hdfs.batchSize &#x3D; 100a1.sinks.k1.hdfs.fileType &#x3D; DataStreama1.sinks.k1.hdfs.rollInterval &#x3D; 600a1.sinks.k1.hdfs.rollSize &#x3D; 134217700a1.sinks.k1.hdfs.rollCount &#x3D; 0# 声明source,sink和channel关系a1.sources.r1.channels &#x3D; c1# 注意:一个sink只能对应一个channel# 一个channel可以对应多个sinka1.sinks.k1.channel &#x3D; c1 开启flume agent监听文件夹： 12# 说明：在使用Spooling Directory Source时,不要在监控目录中创建并持续修改文件;上传完成的文件会以.COMPLETED结尾;被监控文件夹每500毫秒扫描一次文件变动flume-ng agent -c conf/ -n a1 -f agent_conf/spooldir_hdfs.conf 向data文件夹添加文件： 12echo sobxiong &gt;&gt; data/test1.txtecho testxixi &gt;&gt; data/test2.txt 在HDFS的web监控界面查看文件 等待1s，查看data目录中文件后缀名变化 实时监控目录下的多个追加文件 监控文件比对：Exec source适用于监控一个实时追加的文件，不能实现断点续传；Spooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；而Taildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传 案例需求：使用Flume监听整个目录的实时追加文件，并上传至HDFS 案例分析： 实现步骤： 创建并编写Agent配置文件： 12345678910111213141516171819202122232425262728a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; TAILDIR# Taildir说明：# Taildir Source维护了一个json格式的position File,其会定期地往positionFile中更新每个文件读取到的最新的位置因此能够实现断点续传# Position File的格式如下：# &#123;&quot;inode&quot;:2496272,&quot;pos&quot;:12,&quot;file&quot;:&quot;&#x2F;opt&#x2F;data&#x2F;demo&#x2F;test.txt&quot;&#125;# 注：Linux中储存文件元数据的区域就叫做inode,每个inode都有一个号码# 操作系统用inode号码来识别不同的文件,Unix&#x2F;Linux系统内部不使用文件名,而使用inode号码来识别文件# 该文件中记录了source读取到的内容的位置a1.sources.r1.positionFile &#x3D; &#x2F;opt&#x2F;module&#x2F;flume-1.9.0&#x2F;taildir_position.jsona1.sources.r1.filegroups &#x3D; f1a1.sources.r1.filegroups.f1 &#x3D; &#x2F;opt&#x2F;data&#x2F;demo&#x2F;*.txt# channel配置a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000# sink配置a1.sinks.k1.type &#x3D; logger# 声明source,sink和channel关系a1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 开启flume agent监听文件夹： 1flume-ng agent -c conf/ -n a1 -f agent_conf/taildir_logger.conf 向demo文件夹添加文件： 12echo sobxiong &gt;&gt; test1.txtecho testxixi &gt;&gt; test2.txt 在HDFS的web监控界面查看文件 Flume进阶 Flume事务 Flume Agent内部原理重要组件： ChannelSelectorChannelSelector的作用就是选出Event将要被发往哪个Channel。其共有两种类型，分别是Replicating(复制)和Multiplexing(多路复用)ReplicatingSelector会将同一个Event发往所有的Channel；Multiplexing会根据相应的原则，将不同的Event发往不同的Channel SinkProcessorSinkProcessor共有三种类型，分别是DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessorDefaultSinkProcessor对应的是单个的Sink，LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group。LoadBalancingSinkProcessor可以实现负载均衡的功能；FailoverSinkProcessor可以错误恢复的功能 Flume拓扑结构 简单串联介绍：该模式将多个flume顺序连接起来，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume，flume数量过多不仅会影响传输速率，且一旦传输过程中某个节点flume宕机，会影响整个传输系统 复制和多路复用Flume支持将事件流向一个或者多个目的地。该模式可将相同数据复制到多个channel中，或将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地 负载均衡和故障转移Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能 聚合最常见的也非常实用的模式，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等再进行日志分析等后续操作 Flume开发案例 使用Flume提供的时间戳拦截器\b： 案例需求：使用netcat向监听的Flume的发送信息并设置Flume提供的时间戳拦截器 案例配置： 1234567891011121314151617181920# agent1(hadoop101) netcat -&gt; memory -&gt; loggera1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 44444# 设置一个拦截器(用来在headers中添加时间戳)a1.sources.r1.interceptors &#x3D; i1a1.sources.r1.interceptors.i1.type &#x3D; timestampa1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 串联： 案例需求：通过netcat向hadoop101的flume发送信息，hadoop101的flume将收到的消息通过arvo sink向hadoop102的flume发送，最终由hadoop102打印日志 案例配置： 1234567891011121314151617181920212223242526272829303132333435363738# agent1(hadoop101) netcat -&gt; memory -&gt; arvoa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 44444a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avro# hostname是写出到哪台机器a1.sinks.k1.hostname &#x3D; hadoop102a1.sinks.k1.port &#x3D; 33333a1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1# ------------------------------------------# agent1(hadoop102) avro -&gt; memory -&gt; loggera1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; avroa1.sources.r1.bind &#x3D; hadoop102a1.sources.r1.port &#x3D; 33333a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 复制： 案例需求：通过hadoop101监听本地文件的变化，将新增的内容打包成消息通过arvo sink发送给hadoop102和hadoop103，通过复制的方式，如此hadoop102和hadoop103各自都有一份消息副本；hadoop102和hadoop103打印日志显示结果 案例配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# agent1(hadoop101) exec -&gt; memory -&gt; arvoa1.sources &#x3D; r1a1.channels &#x3D; c1 c2a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; execa1.sources.r1.command &#x3D; tail -F &#x2F;opt&#x2F;data&#x2F;test.log# 配置channel selector(replicating默认,不配置也成)a1.sources.r1.selector.type &#x3D; replicatinga1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c2.type &#x3D; memorya1.channels.c2.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; hadoop102a1.sinks.k1.port &#x3D; 33333a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; hadoop103a1.sinks.k2.port &#x3D; 33333a1.sources.r1.channels &#x3D; c1 c2a1.sinks.k1.channel &#x3D; c1a1.sinks.k2.channel &#x3D; c2# ------------------------------------------# agent1(hadoop102) avro -&gt; memory -&gt; loggera1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; avroa1.sources.r1.bind &#x3D; hadoop102a1.sources.r1.port &#x3D; 33333a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1# ------------------------------------------# agent1(hadoop103) avro -&gt; memory -&gt; loggera1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; avroa1.sources.r1.bind &#x3D; hadoop103a1.sources.r1.port &#x3D; 33333a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 多路复用： 案例需求：通过hadoop101监听本地文件的变化，将新增的内容打包成消息通过arvo sink发送给hadoop102和hadoop103。由于采用多路复用的方式，sink会依据消息event头部header中的key和value对数据进行分类，将各类分到各自对应的channel；如此hadoop102和hadoop103得到消息的不同部分，hadoop102和hadoop103打印日志显示结果 案例配置： 123456789101112131415161718192021222324252627282930313233343536373839# agent1(hadoop101) exec -&gt; memory -&gt; arvoa1.sources &#x3D; r1a1.channels &#x3D; c1 c2a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; execa1.sources.r1.command &#x3D; tail -F &#x2F;opt&#x2F;data&#x2F;test.log# 设置为复用a1.sources.r1.selector.type &#x3D; multiplexing# event(headers, body),根据headers中的key和value进行数据的发送# header指的是headers中的key值a1.sources.r1.selector.header &#x3D; state# mapping.x指的是value对应的值,&#x3D;y指的是分发到y channela1.sources.r1.selector.mapping.CZ &#x3D; c1a1.sources.r1.selector.mapping.US &#x3D; c2# 设置拦截器,给event添加headers内容a1.sources.r1.interceptors &#x3D; i1a1.sources.r1.interceptors.i1.type &#x3D; statica1.sources.r1.interceptors.i1.key &#x3D; statea1.sources.r1.interceptors.i1.value &#x3D; CZa1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c2.type &#x3D; memorya1.channels.c2.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; hadoop102a1.sinks.k1.port &#x3D; 33333a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; hadoop103a1.sinks.k2.port &#x3D; 33333a1.sources.r1.channels &#x3D; c1 c2a1.sinks.k1.channel &#x3D; c1a1.sinks.k2.channel &#x3D; c2# hadoop102和hadoop103同复制案例 故障转移： 案例需求：通过hadoop101监听本地文件的变化，将新增的内容打包成消息通过arvo sink发送给hadoop102。hadoop103是hadoop102的备份机，设置hadoop102的优先级大于hadoop103，如此在hadoop102正常时消息event都会发送给hadoop102；当hadoop102错误或宕机时，消息event会发送给hadoop103 案例配置： 12345678910111213141516171819202122232425262728293031323334# agent1(hadoop101) exec -&gt; memory -&gt; arvoa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 33333a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; hadoop102a1.sinks.k1.port &#x3D; 33333a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; hadoop103a1.sinks.k2.port &#x3D; 33333# 一个channel对应多个sink时要设置一个sinkgroupsa1.sinkgroups &#x3D; g1# 该sink组有哪些sink实例a1.sinkgroups.g1.sinks &#x3D; k1 k2# 配置sinkProcessor类型：failover&#x2F;load_balancea1.sinkgroups.g1.processor.type &#x3D; failover# 配置sink优先级(数值越大优先级越高)a1.sinkgroups.g1.processor.priority.k1 &#x3D; 5a1.sinkgroups.g1.processor.priority.k2 &#x3D; 10a1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1a1.sinks.k2.channel &#x3D; c1# hadoop102和hadoop103同复制案例 负载均衡： 案例需求：通过hadoop101监听44444端口，并把监听到的字符串打包成消息通过arvo sink发送给hadoop102和hadoop103。由于设置为负载均衡，故hadoop102和hadoop103都会得到消息event的一部分 案例配置： 12345678910111213141516171819202122232425262728# agent1(hadoop101) netcat -&gt; memory -&gt; avroa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 33333a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; hadoop102a1.sinks.k1.port &#x3D; 33333a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; hadoop103a1.sinks.k2.port &#x3D; 33333a1.sinkgroups &#x3D; g1a1.sinkgroups.g1.sinks &#x3D; k1 k2a1.sinkgroups.g1.processor.type &#x3D; load_balancea1.sinkgroups.g1.processor.selector &#x3D; round_robina1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1a1.sinks.k2.channel &#x3D; c1# hadoop102和hadoop103同复制案例 聚合： 案例需求：hadoop101和hadoop102通过netcat各自监听本机的44444端口，并把监听到的字符串打包成event消息都发送给hadoop103；hadoop103显示到日志中 案例配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# agent1(hadoop101) netcat -&gt; memory -&gt; arvoa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 44444a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avro# hostname是写出到哪台机器a1.sinks.k1.hostname &#x3D; hadoop103a1.sinks.k1.port &#x3D; 33333a1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1# agent1(hadoop102) netcat -&gt; memory -&gt; arvoa1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop102a1.sources.r1.port &#x3D; 44444a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avro# hostname是写出到哪台机器a1.sinks.k1.hostname &#x3D; hadoop103a1.sinks.k1.port &#x3D; 33333a1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1# agent1(hadoop103) avro -&gt; memory -&gt; loggera1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1a1.sources.r1.type &#x3D; avroa1.sources.r1.bind &#x3D; hadoop103a1.sources.r1.port &#x3D; 33333a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 自定义Interceptor： 案例需求：使用hadoop101主机监听本地netcat，根据消息内容的不同(数字和字母开头)发往不同的主机(数字和字母分别发往hadoop102和hadoop103) 实现步骤： Maven项目导入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt;&lt;/dependency&gt; 编写自定义拦截器MyInterceptor——继承Flume的Interceptor，重写方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445/*** 自定义拦截器* 作用：根据body中的内容在headers中添加指定的kv*/public class MyInterceptor implements Interceptor &#123; // 初始化 public void initialize() &#123;&#125; // 拦截单个消息 // channelProcessor调用拦截器时会调用该方法并将event传过来为每个event中的header添加kv public Event intercept(Event event) &#123; // 1、获取body中内容 byte[] body = event.getBody(); // 2、判断数据中内容 // 判断内容是否为数字 if (body[0] &gt;= '0' &amp;&amp; body[0] &lt;= '9') &#123; event.getHeaders().put(\"type\", \"number\"); &#125; else if ((body[0] &gt;= 'A' &amp;&amp; body[0] &lt;= 'Z') || (body[0] &gt;= 'a' &amp;&amp; body[0] &lt;= 'z')) &#123; event.getHeaders().put(\"type\", \"letter\"); &#125; return event; &#125; // 拦截多个消息(batch) public List&lt;Event&gt; intercept(List&lt;Event&gt; list) &#123; // 从集合中遍历每个event for (Event event : list) &#123; intercept(event); &#125; return list; &#125; public void close() &#123; &#125; /** * 内部类(静态内部类) * 作用：返回MyInterceptor的实例 * 注意：1、静态内部类 2、权限是public */ public static class MyBuilder implements Interceptor.Builder &#123; // 返回一个interceptor接口实例 public Interceptor build() &#123; return new MyInterceptor(); &#125; public void configure(Context context) &#123; &#125; &#125;&#125; 将Maven工程打包分发到服务器集群中flume的lib目录下 编写配置文件 123456789101112131415161718192021222324252627282930313233a1.sources &#x3D; r1a1.channels &#x3D; c1 c2a1.sinks &#x3D; k1 k2a1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 44444a1.sources.r1.selector.type &#x3D; multiplexinga1.sources.r1.selector.header &#x3D; typea1.sources.r1.selector.mapping.letter &#x3D; c1a1.sources.r1.selector.mapping.number &#x3D; c2a1.sources.r1.interceptors &#x3D; i1a1.sources.r1.interceptors.i1.type &#x3D; com.xiong.flume.MyInterceptor$MyBuildera1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.channels.c2.type &#x3D; memorya1.channels.c2.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; avroa1.sinks.k1.hostname &#x3D; hadoop102a1.sinks.k1.port &#x3D; 33333a1.sinks.k2.type &#x3D; avroa1.sinks.k2.hostname &#x3D; hadoop103a1.sinks.k2.port &#x3D; 33333a1.sources.r1.channels &#x3D; c1 c2a1.sinks.k1.channel &#x3D; c1a1.sinks.k2.channel &#x3D; c2# hadoop102和hadoop103同复制案例 前后在hadoop102、103和101上启动Flume进程，进行测试 自定义Source： 案例需求：Source是负责接受数据到Flume Agent的组件。通过自定义Source发送随机数据(实际使用场景：读取MySQL数据或其他文件系统) 实现步骤： Maven项目导入依赖： 编写自定义数据源MySource——继承Flume的AbstractSource类，实现Configurable和PollableSource接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/*** 自定义source* 需求：使用flume接受数据,并给每条数据添加前缀,输出到控制台。前缀可从flume配置文件中配置*/public class MySource extends AbstractSource implements Configurable, PollableSource &#123; private String prefix; /** * 核心方法 - 获取数据并封装成event,将创建的event放入到channel中 * 该方法会被循环调用 * @return Status —— 一个枚举类,表示向ChannelProcessor中添加数据是否成功 * READY：成功; BACKOFF：失败 * @throws EventDeliveryException */ public Status process() throws EventDeliveryException &#123; List&lt;Event&gt; events = new ArrayList&lt;&gt;(); // 设置数据 for (int i = 0; i &lt; 2; i++) &#123; // 创建event SimpleEvent event = new SimpleEvent(); event.setBody((prefix + UUID.randomUUID().toString().substring(0, 8)).getBytes()); events.add(event); &#125; try &#123; // 获取channelProcessor ChannelProcessor channelProcessor = getChannelProcessor(); // 将数据放入到channel中(ChannelProcessor) // channelProcessor.processEvent(event); channelProcessor.processEventBatch(events); &#125; catch (Exception e) &#123; e.printStackTrace(); return Status.BACKOFF; &#125; return Status.READY; &#125; // 当source没数据可封装时,让source所在线程休息 public long getBackOffSleepIncrement() &#123; return 2000L; &#125; // 当source没数据可封装时,让source所在线程休息的最大时间 public long getMaxBackOffSleepInterval() &#123; return 5000L; &#125; // 获取上下文,可以读取配置文件中内容 public void configure(Context context) &#123; prefix = context.getString(\"prefix\", \"sobxiong\"); &#125;&#125; 将Maven工程打包分发到服务器集群中flume的lib目录下 编写配置文件 123456789101112131415a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1# 自定义sourcea1.sources.r1.type &#x3D; com.xiong.flume.MySourcea1.sources.r1.prefix &#x3D; sobxionga1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; loggera1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 启动Flume进程，进行测试 自定义Sink 案例需求：Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储、索引系统或者发送到另一个Flume AgentSink是完全事务性的。在Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件本案例中只简单地将channel传来的数据进行打印(实际使用场景：读取Channel数据写入MySQL或其他文件系统) 实现步骤： Maven项目导入依赖： 编写自定义MySink——继承Flume的AbstractSink类，实现Configurable接口 123456789101112131415161718192021222324252627282930313233343536373839404142public class MySink extends AbstractSink implements Configurable &#123; private String suffix; // 以日志方式输出 private static final Logger logger = LoggerFactory.getLogger(MySink.class); // 核心方法,用来处理sink逻辑(将channel中内容写出) public Status process() throws EventDeliveryException &#123; Status status = Status.BACKOFF; // 1、获取Channel Channel channel = getChannel(); // 2、从Channel中获取事务 Transaction transaction = channel.getTransaction(); // 开启事务 transaction.begin(); try &#123; // 3、获取数据 Event event = channel.take(); // 没有消息就阻塞等待 while (event == null) &#123; TimeUnit.SECONDS.sleep(2); event = channel.take(); &#125; // 4、将数据写出 logger.info(\"headers: &#123;&#125; , body: &#123;&#125;\", event.getHeaders(), new String(event.getBody()) + suffix); // 5、提交事务 transaction.commit(); status = Status.READY; &#125; catch (Exception e) &#123; e.printStackTrace(); // 事务回滚 transaction.rollback(); &#125; finally &#123; transaction.close(); &#125; return status; &#125; // 读取配置文件中内容 public void configure(Context context) &#123; suffix = context.getString(\"suffix\", \" &gt; &gt; &gt; hello\"); &#125;&#125; 将Maven工程打包分发到服务器集群中flume的lib目录下 编写配置文件 1234567891011121314151617a1.sources &#x3D; r1a1.channels &#x3D; c1a1.sinks &#x3D; k1# 自定义sourcea1.sources.r1.type &#x3D; netcata1.sources.r1.bind &#x3D; hadoop101a1.sources.r1.port &#x3D; 44444a1.channels.c1.type &#x3D; memorya1.channels.c1.capacity &#x3D; 1000a1.sinks.k1.type &#x3D; com.xiong.flume.MySinka1.sinks.k1.subffix &#x3D; &gt; &gt; &gt; testa1.sources.r1.channels &#x3D; c1a1.sinks.k1.channel &#x3D; c1 启动Flume进程，进行测试 Flume知识点 如何实现Flume数据传输的监控：使用第三方框架Ganglia Flume的Source，Sink，Channel的作用？Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacyChannel组件对采集到的数据进行缓存，可以存放在Memory或File中Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义 Flume参数调优 Source增加Source个数(使用Tair Dir Source时可增加FileGroups个数)可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source以保证Source有足够的能力获取到新产生的数据batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能 Channeltype选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能Capacity参数决定Channel可容纳最大的event条数。transactionCapacity参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数transactionCapacity需要大于Source和Sink的batchSize参数 Sink增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好，够用就行，过多的Sink会占用系统资源，造成系统资源的浪费batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能 Flume的事务机制Flume的事务机制(类似数据库的事务机制)：Flume使用两个独立的事务分别负责从Source到Channel以及从Channel到Sink的事件传递。比如spooling directory source为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Source就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递 Flume采集数据会丢失吗?根据Flume的架构原理，Flume是不可能丢失数据的，其内部有完善的事务机制。Source到Channel是事务性的，Channel到Sink是事务性的，因此这两个环节不会出现数据的丢失。唯一可能丢失数据的情况是Channel采用memory channel，agent宕机导致数据丢失；或者Channel存储数据已满，导致Source不再写入，未写入的数据丢失Flume不会丢失数据，但是有可能造成数据的重复。例如数据已经成功由Sink发出，但是没有接收到响应，Sink会再次发送数据，此时可能会导致数据的重复","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Linux常用命令","slug":"BasicSkill/Linux/Linux常用命令","date":"2020-10-09T02:59:50.000Z","updated":"2020-10-09T07:23:31.356Z","comments":true,"path":"2020/10/09/BasicSkill/Linux/Linux常用命令/","link":"","permalink":"https://sobxiong.github.io/2020/10/09/BasicSkill/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"内容 top命令 free命令 df命令 vmstat命令 iostat命令","text":"内容 top命令 free命令 df命令 vmstat命令 iostat命令 top命令 查看整机性能：top(主要cpu) 按数字1查看cpu各核心情况： us user sy system id idle(空闲率,越高越好) load average：a b c(系统1、5、15分钟的系统平均负载量,如果abc平均大于0.6说明系统负担重,大于0.8说明系统快宕机) q退出 低配版：uptime free命令 查看内存：free(默认kb) free -m(MB)、free -g(GB) df命令 查看磁盘：df(disk free,默认kb) df -h(以MB为单位) vmstat命令 查看简单的系统性能：vmstat -n 2 3(2代表每两秒采集一次,3代表共采集三次) 重要参数： r：runtime process(运行进程数) b：blocking process(阻塞进程数,越少越好) iostat命令 查看磁盘IO情况：iostat -xdk 2 3(2,3同上) 重要参数： r/s：每秒读 w/s：每秒写 %util：一秒中有百分之多少的时间用于I/O操作，或者说一秒中有多少时间I/O队列是非空的","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BasicSkill","slug":"BasicSkill","permalink":"https://sobxiong.github.io/tags/BasicSkill/"},{"name":"Linux","slug":"Linux","permalink":"https://sobxiong.github.io/tags/Linux/"}]},{"title":"Java基础知识","slug":"ProgrammingLanguage/Java/Java基础知识","date":"2020-10-08T15:17:04.000Z","updated":"2020-11-15T08:33:25.193Z","comments":true,"path":"2020/10/08/ProgrammingLanguage/Java/Java基础知识/","link":"","permalink":"https://sobxiong.github.io/2020/10/08/ProgrammingLanguage/Java/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"内容 基础语法 进阶语法","text":"内容 基础语法 进阶语法 基础语法进阶语法 自动资源管理：Java7增加了一个新特性：该特性提供了另外一种管理资源的方式，这种方式能自动关闭文件(或资源)。这个特性有时被称为自动资源管理(Automatic Resource Management,ARM)，该特性以try语句的扩展版为基础。当不再需要文件(或其他资源)时，可以防止无意中忘记释放它们 123456789101112131415/*自动资源管理基于try语句的扩展形式：当try代码块结束,自动释放资源。不需要显式调用close()方法。该形式也称为\"带资源的try语句\"注意:1、try语句中声明的资源被隐式声明为final,资源的作用局限于带资源的try语句2、可以在一条try语句中管理多个资源,每个资源以';'隔开即可3、需要关闭的资源必须实现AutoCloseable接口或其子接口Closeable*/try(需要关闭的资源声明)&#123; // 可能发生异常的语句&#125; catch(异常类型变量名) &#123; // 异常的处理语句finally&#123; // 一定执行的语句&#125;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://sobxiong.github.io/tags/Java/"}]},{"title":"NIO","slug":"ProgrammingLanguage/Java/NIO","date":"2020-10-05T15:01:22.000Z","updated":"2020-11-14T04:01:45.549Z","comments":true,"path":"2020/10/05/ProgrammingLanguage/Java/NIO/","link":"","permalink":"https://sobxiong.github.io/2020/10/05/ProgrammingLanguage/Java/NIO/","excerpt":"内容 NIO简介 缓冲区(Buffer) 通道(Channel) 选择器(Selector) 其他","text":"内容 NIO简介 缓冲区(Buffer) 通道(Channel) 选择器(Selector) 其他 NIO简介 NIO简介：Java NIO(New I0)是从Java 1.4版本开始引入的一个新的I0 API，可以替代标准的Java I0 API。NIO与原来的I0有同样的作用和目的，但是使用的方式完全不同，NIO支持面向缓冲区的、基于通道的I0操作。NIO将以更加高效的方式进行文件的读写操作 NIO系统的核心：通道(Channel)和缓冲区(Buffer)。通道表示打开到I0设备(例如：文件、套接字)的连接。若需要使用NIO系统，需要获取用于连接I0设备的通道以及用于容纳数据的缓冲区。然后操作缓冲区，对数据进行处理。简言之，Channel负责传输，Buffer负责存储 NIO与IO的主要区别： IO NIO 面向流(Stream Oriented) 面向缓冲区(Buffer Oriented) 阻塞IO(Blocking IO) 非阻塞IO(Non Blocking IO) (无) 选择器(Selectors) 阻塞和非阻塞介绍： 阻塞：传统的I0流都是阻塞式的。也就是说，当一个线程调用read()或write()时，该线程被阻塞，直到有一些数据被读取或写入，该线程在此期间不能执行其他任务。因此，在完成网络通信进行I0操作时，由于线程会阻塞，所以服务器端必须为每个客户端都提供一个独立的线程进行处理，当服务器端需要处理大量客户端时，性能急剧下降 非阻塞：NIO是非阻塞模式的。当线程从某通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务。线程通常将非阻塞I0的空闲时间用于在其他通道上执行I0操作，所以单独的线程可以管理多个输入和输出通道。因此，NIO可以让服务器端使用一个或有限几个线程来同时处理连接到服务器端的所有客户端 NIO主要有三大核心组件：通道(Channel)、缓冲区(Buffer)以及选择器(Selector) 缓冲区(Buffer) 介绍： 一个用于特定基本数据类型的容器的，在java.nio包中定义，所有缓冲区都是Buffer抽象类的子类。NIO中的Buffer主要用于与NIO通道进行交互，数据是从通道读入缓冲区，从缓冲区写入通道中的 Buffer就像一个数组，可以保存多个相同类型的数据。根据数据类型不同(boolean除外)，有以下Buffer常用子类：ByteBuffer、CharBuffer、ShortBuffer、IntBuffer、LongBuffer、FloatBuffer以及DoubleBuffer。上述Buffer类它们都采用相似的方法进行管理数据，只是各自管理的数据类型不同而已。都是通过如下方法获取一个Bufferd对象： 12// 创建一个容量为capacity的XxxBuffer对象static XxxBuffer allocate(int capacity); 基本属性(标记、位置、限制、容量遵守以下不变式：0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity)： 容量(capacity)：表示Buffer最大数据容量，capacity不能为负，并且创建后不能更改 限制(limit)：第一个不应该读取或写入的数据的索引，即位于limit后的数据不可读写。limit不能为负，并且不能大于其容量capacity 位置(position)：下一个要读取或写入的数据的索引。position不能为负，并且不能大于其限制limit 标记(mark)与重置(reset)：标记是一个索引，通过Buffer中的mark()方法指定Buffer中一个特定的position，之后可以通过调用reset()方法恢复到这个position 常用方法 方法 描述 Buffer clear() 清空缓冲区并返回对缓冲区的引用 Buffer flip() 将缓冲区的界限设置为当前位置,并将当前位置重置为0 int capacity() 返回Buffer的capacity大小 boolean hasRemaining() 判断缓冲区中是否还有元素 int limit() 返回Buffer的界限(limit)的位置 Buffer limit(int n) 将设置缓冲区界限为n,并返回一个具有新limit的缓冲区对象 Buffer mark() 对缓冲区设置标记 int position() 返回缓冲区的当前位置position Buffer position(int n) 将设置缓冲区的当前位置为n,并返回修改后的Buffer对象 int remaining() 返回position和limit之间的元素个数 Buffer reset() 将位置position转到以前设置的mark所在的位置 Buffer rewind() 将位置设为为0,取消设置的mark 数据操作：Buffer所有子类提供了两个用于数据操作的方法——get()与put()方法 获取Buffer中的数据 get()：读取单个字节 get(byte[] dst)：批量读取多个字节到dst中 get(int index)：读取指定索引位置的字节(不会移动position) 放入数据到Buffer中 put(byte b)：将给定单个字节写入缓冲区的当前位置 put(byte[]src)：将src中的字节写入缓冲区的当前位置 put(int index, byte b)：将指定字节写入缓冲区的索引位置(不会移动position) 直接与非直接缓冲区 字节缓冲区要么是直接的，要么是非直接的。如果为直接字节缓冲区，则Java虚拟机会尽最大努力直接在此缓冲区上执行本机I/0操作。也就是说，在每次调用基础操作系统的一个本机I/0操作之前(或之后)，虚拟机都会尽量避免将缓冲区的内容复制到中间缓冲区中(或从中间缓冲区中复制内容) 直接字节缓冲区可以通过调用此类的allocateDirect()工厂方法来创建。此方法返回的缓冲区进行分配和取消分配所需成本通常高于非直接缓冲区。直接缓冲区的内容可以驻留在常规的垃圾回收堆之外，因此，它们对应用程序的内存需求量造成的影响可能并不明显。所以，建议将直接缓冲区主要分配给那些易受基础系统的本机I/0操作影响的大型、持久的缓冲区。-般情况下，最好仅在直接缓冲区能在程序性能方面带来明显好处时分配它们 直接字节缓冲区还可以通过FileChannel的map()方法将文件区域直接映射到内存中来创建。该方法返回MappedByteBuffer。Java平台的实现有助于通过JNI从本机代码创建直接字节缓冲区。如果以上这些缓冲区中的某个缓冲区实例指的是不可访问的内存区域，则试图访问该区域不会更改该缓冲区的内容，并且将会在访问期间或稍后的某个时间导致抛出不确定的异常 字节缓冲区是直接缓冲区还是非直接缓冲区可通过调用其isDirect()方法来确定。提供此方法是为了能够在性能关键型代码中执行显式缓冲区管理 通道(Channel) 介绍：由java.nio.channels包定义。Channel表示I0源与目标打开的连接。Channel类似于传统的”流”。只不过Channel本身不能直接访问数据，Channel只能与Buffer进行交互 主要实现类 FileChannel：用于读取、写入、映射和操作文件的通道 DatagramChannel：通过UDP读写网络中的数据通道 SocketChannel：通过TCP读写网络中的数据 ServerSocketChannel：可以监听新进来的TCP连接，对每一个新进来的连接都会创建一个Socke tChannel 获取通道： 对支持通道的对象调用getChannel()方法。支持通道的类包括FileInputStream、FileOutputStream、RandomAccessFile、DatagramSocket、Socket、ServerSocket 使用Files类的静态方法newByteChannel()获取字节通道、或者通过通道的静态方法open()打开并返回指定通道 数据传输： 将Buffer数据写入Channel： 1int writeLen = channel.write(buffer); 从Channel读取数据到Buffer 1int readLen = channel.read(buffer); 分散与聚集 分散读取(Scattering Reads)：从Channel中读取的数据”分散”到多个Buffer中注意：按照缓冲区的顺序,从Channel中读取的数据依次将Buffer填满 聚集写入(Gathering Writes)：将多个Buffer中的数据”聚集”到Channel注意：按照缓冲区的顺序,写入position和limit之间的数据到Channel 数据通道相互传输：将数据从源通道传输到其他Channel中 12toChannel.transferFrom(fromChannel, count, position);fromChannel.transforTo(position, count, toChannel); FileChannel的常用方法： 方法 描述 int read(ByteBuffer dst) 从Channel中读取数据到ByteBuffer long read(ByteBuffer[] dsts) 将Channel中的数据”分散”到ByteBuffer[] int write(ByteBuffer src) 将ByteBuffer中的数据写入到Channel long write(ByteBuffer[] srcs) 将ByteBuffer[]中的数据”聚集”到Channel long position() 返回此通道的文件位置 FileChannel position(long p) 设置此通道的文件位置 long size() 返回此通道的文件的当前大小 FileChannel truncate(long s) 将此通道的文件截取为给定大小 void force(boolean metaData) 强制将所有对此通道的文件更新写入到存储设备中 选择器(Selector) 介绍：选择器(Selector)是SelectableChannle对象的多路复用器，Selector可以同时监控多个SelectableChannel的I0状况，也就是说，利用Selector可使一个单独的线程管理多个Channel。Selector是非阻塞I0的核心 常见方法： 方法 描述 Set&lt;SelectionKey&gt; keys() 所有的SelectionKey集合。代表注册在该Selector上的Channel selectedKeys() 被选择的SelectionKey集合。返回此Selector的已选择键集 int select() 监控所有注册的Channel,当它们中间有需要处理的I0操作时,该方法返回,并将对应得的SelectionKey加入被选择的SelectionKey集合中,该方法返回这些Channel的数量 int select(long timeout) 可以设置超时时长的select()操作 int selectNow() 执行一个立即返回的select()操作,该方法不会阻塞线程 Selector wakeup() 使一个还未返回的select()方法立即返回 void close() 关闭该选择器 SelectableChannle继承结构 选择器使用： 创建Selector：通过调用Selector.open()方法创建一个Selector 向选择器注册通道：SelectableChannel.register(Selector sel,int ops) 当调用register()向选择器注册通道时，选择器对通道的监听事件通过第二个参数ops指定 可以监听的事件类型(可使用SelectionKey的四个常量表示)： 读：SelectionKey.OP_READ(1) 写：SelectionKey.OP_WRITE(4) 连接：SelectionKey.OP_CONNECT(8) 接收：SelectionKey.OP_ ACCEPT(16) 若注册时不止监听一个事件，则可以使用”位或”操作符连接 SelectionKey： 介绍：表示SelectableChannel和Selector之间的注册关系。每次向选择器注册通道时就会选择一个事件(选择键)。选择键包含两个表示为整数值的操作集。操作集的每一位都表示该键的通道所支持的一类可选择操作 常用方法： 方法 描述 int interestOps() 获取感兴趣事件集合 int readyOps() 获取通道已经准备就绪的操作的集合 SelectableChannel channel() 获取注册通道 Selector selector() 返回选择器 boolean isReadable() 检测Channal中读事件是否就绪 boolean isWritable() 检测Channal中写事件是否就绪 boolean isConnectable() 检测Channel中连接是否就绪 boolean isAcceptable() 检测Channel中接收是否就绪 其他 管道(pipe)：NIO管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取 Path与Paths： java.nio.file.Path接口代表-一个平台无关的平台路径，描述了目录结构中文件的位置 Paths提供get()方法用来获取Path对象：Path get(String first, String … more)：用于将多个字符串串连成路径 Path常用方法: 方法 描述 boolean endsWith(String path) 判断是否以path路径结束 boolean startsWith(String path) 判断是否以path路径开始 boolean isAbsolute() 判断是否是绝对路径 Path getFileName() 返回与调用Path对象关联的文件名 Path getName(int idx) 返回的指定索引位置idx的路径名称 int getNameCount() 返回Path根目录后面元素的数量 Path getParent() 返回Path对象包含整个路径,不包含Path对象指定的文件路径 Path getRoot() 返回调用Path对象的根路径 Path resolve(Path p) 将相对路径解析为绝对路径 Path toAbsolutePath() 作为绝对路径返回调用Path对象 String toString() 返回调用Path对象的字符串表示形式 Files： java.nio.file.Files用于操作文件或目录的工具类 Files常用方法： 方法 描述 Path copy(Path src, Path dest, CopyOption … how) 文件的复制 Path createDirectory(Path path, FileAttribute&lt;?&gt; … attr) 创建一个目录 Path createFile(Path path, FileAttribute&lt;?&gt; … arr) 创建一个文件 void delete(Path path) 删除一个文件 Path move(Path src, Path dest, CopyOption … how) 将src移动到dest位置 long size(Path path) 返回path指定文件的大小 Files常用方法(判断)： 方法 描述 boolean exists(Path path, LinkOption … opts) 判断文件是否存在 boolean isDirectory(Path path, LinkOption … opts) 判断是否是目录 boolean isExecutable(Path path) 判断是否是可执行文件 boolean isHidden(Path path) 判断是否是隐藏文件 boolean isReadable(Path path) 判断文件是否可读 boolean isWritable(Path path) 判断文件是否可写 boolean notExists(Path path, LinkOption … opts) 判断文件是否不存在 public static &lt;A extends BasicFileAttributes&gt; A readAttributes(Path path, Class&lt;A&gt; type, LinkOption … options) 获取与path指定的文件相关联的属性 Files常用方法(操作内容)： 方法 描述 SeekableByteChannel newByteChannel(Path path, OpenOptin … how) 获取与指定文件的连接,how指定打开方式 DirectoryStream newDirectoryStream(Path path) 打开path指定的目录 InputStream newInputStream(Path path, OpenOptin … how) 获取InputStream对象 OutputStream newOutputStream(Path path, OpenOptin … how) 获取OutputStream对象","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"}]},{"title":"Kafka","slug":"BigData/Kafka","date":"2020-09-27T12:01:28.000Z","updated":"2020-10-24T11:56:14.204Z","comments":true,"path":"2020/09/27/BigData/Kafka/","link":"","permalink":"https://sobxiong.github.io/2020/09/27/BigData/Kafka/","excerpt":"内容 Kafka概述 Kafka快速入门 Kafka架构深入 Kafka_API","text":"内容 Kafka概述 Kafka快速入门 Kafka架构深入 Kafka_API Kafka概述 消息队列(Message Queue) 传统消息队列的应用场景 消息队列的两种模式 点对点模式(1对1,消费者主动拉取数据,消息收到后消息清除)消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费 发布/订阅模式(一对多,消费者消费数据之后不会清除消息)消息生产者(发布)将消息发布到topic中，同时有多个消息消费者(订阅)消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费 Kafka定义：Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域 Kafka基础架构： Producer：消息生产者，就是向kafka broker发消息的客户端 Consumer：消息消费者，向kafka broker取消息的客户端 Consumer Group(CG)：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者 Broker：一台kafka服务器就是一个broker，一个集群由多个broker组成，一个broker可以容纳多个topic Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker(即服务器)上，一个topic可以分为多个partition，每个partition是一个有序的队列 Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower leader：每个分区多个副本的”主”，生产者发送数据的对象以及消费者消费数据的对象都是leader follower：每个分区多个副本中的”从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader Kafka快速入门 安装部署 集群规划：Hadoop101、hadoop102、hadoop103各自都运行zookeeper和kafka 安装包下载：http://kafka.apache.org/downloads 集群部署 解压安装包：tar -zxvf kafka_2.12-2.6.0.tgz -C /opt/module 修改配置文件： 123456789# 需要修改的地方# broker(主机)的全局唯一编号,不能重复broker.id=x# 设置允许删除topic功能delete.topic.enable=true# 设置kafka运行日志存放地址log.dirs=/opt/module/kafka_2.12-2.6.0/logs# 配置链接zookeeper集群地址zookeeper.connect=hadoop101:2181,hadoop102:2181,hadoop103:2181 配置环境变量(vim + source) 123# KAFKA_HOMEexport KAFKA_HOME=/opt/module/kafka_2.12-2.6.0export PATH=$PATH:$KAFKA_HOME/bin 分发安装包(修改环境变量和配置文件的broker.id)：xsync kafka_2.12-2.6.0 启动集群： 123# 依次启动hadoop101、hadoop102、hadoop103的zookeeper# 该命令hadoop101、hadoop102、hadoop103均需使用kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties kafka群起脚本 123456for i in `cat $HADOOP_HOME/etc/hadoop/workers`doecho \"========== $i ==========\"ssh $i 'kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties'echo $?done Kafka命令行操作(直接使用,不加参数可以查看用法) 查看当前服务器中的所有topic： 1kafka-topics.sh --zookeeper hadoop101:2181 --list 创建topic： 1kafka-topics.sh --zookeeper hadoop101:2181 --create --replication-factor 3 --partitions 1 --topic first 选项说明： –topic：定义topic名 –replication-factor：定义副本数 –partitions：定义分区数 删除topic 123# 需要server.properties中设置delete.topic.enable=true# 否则只是标记删除kafka-topics.sh --zookeeper hadoop101:2181 --delete --topic first 发送消息： 12# 出现 &gt; ,可输入消息字符kafka-console-producer.sh --broker-list hadoop101:9092 --topic first 消费消息： 12# --from-beginning会把topic以往所有的消息数据都取出来kafka-console-consumer.sh --bootstrap-server hadoop101:9092 --from-beginning --topic first 修改分区数： 1kafka-topics.sh --zookeeper hadoop101:2181 --alter --topic first --partitions 6 Kafka架构深入 Kafka工作流程及文件存储机制 topicKafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时从上次的位置继续消费 Kafka文件存储机制由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——.index文件和.log文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称 + 分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。index和log文件以当前segment的第一条消息的offset命名 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log .index和.log文件详解.index文件存储大量的索引信息，.log文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址 Kafka生产者 分区策略 分区原因 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以由多个Partition组成，因此整个集群就可以适应任意大小的数据了 可以提高并发能力，因为可以以Partition为单位读写了 分区原则：我们需要将producer发送的数据封装成一个ProducerRecord对象 指明partition的情况下，直接将指明的值直接作为partiton值 没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值 既没有partition值又没有key值的情况下，第一次调用时随机生成一个整数(后面每次调用在这个整数上自增)，将这个值与topic可用的partition总数取余得到partition值，也就是常说的 round-robin(轮询)算法 数据可靠性保证为保证producer发送的数据能可靠地发送到指定的topic，topic的每个partition收到producer发送的数据后都需要向producer发送ack(acknowledgement确认收到)，如果producer收到ack，就会进行下一轮的发送，否则重新发送数据 副本数据同步策略 方案 优点 缺点 半数以上完成同步,就发送ack 延迟低 选举新的leader时,容忍n台节点的故障,需要2n+1个副本 全部完成同步,才发送ack 选举新的leader时,容忍n台节点的故障,需要n+1个副本 延迟高 Kafka选择了第二种方案，原因如下： 同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余 虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小 ISR采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据。但有一个follower因为某种故障迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步才能发送ack。这个问题怎么解决呢？Leader维护了一个动态的in-sync replica set(ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader ack应答机制对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的acks参数配置： 0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据 1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据 -1(all)：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复 故障处理细节 follower故障follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了 leader故障leader发生故障之后，会从ISR中选出一个新的leader。之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复 Exactly Once语义将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响0.11版本的Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：At Least Once + 幂等性 = Exactly Once要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once Kafka消费者 消费方式consumer采用pull(拉)模式从broker中读取数据push(推)模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息pull模式的不足之处：如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout 分区分配策略一个consumer group中有多个consumer，一个topic有多个partition，所以必然会涉及到partition的分配问题——即确定哪个partition由哪个consumer来消费Kafka有两种分配策略： roundrobin：轮询 range：按顺序均分 offset维护由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费在0.9版本之前，consumer默认将offset保存在Zookeeper中；从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets Kafka高效读写数据 顺序写磁盘Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间 应用PagecacheKafka数据持久化是直接持久化到Pagecache中，这样会产生以下几个好处： I/O Scheduler会将连续的小块写组装成大块物理写从而提高性能 I/O Scheduler会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间 充分利用所有空闲内存(非JVM内存)。如果使用应用层Cache(即JVM堆内存)，会增加GC负担 读操作可直接在Page Cache内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘(直接通过Page Cache)交换数据 如果进程重启，JVM内的Cache会失效，但Page Cache仍然可用尽管持久化到Pagecache上可能会造成宕机丢失数据的情况，但这可以被Kafka的Replication机制解决。如果为了保证这种情况下数据不丢失而强制将Page Cache中的数据Flush到磁盘，反而会降低性能 零复制(拷贝)技术 Zookeeper在Kafka中的作用Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线、所有topic的分区副本分配和leader选举等工作Controller的管理工作依赖于Zookeeper以下为partition的leader选举过程： Kafka事务Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败 Producer事务为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID为了管理Transaction，Kafka引入了一个新的组件Transaction Coordinator。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行 Consumer事务(精准一次性消费)上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其是无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况如果想完成Consumer端的精准一次性消费，那么需要kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将kafka的offset保存到支持事务的自定义介质中(比如mysql) Kafka_API Producer API 消息发送流程Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker相关参数：batch.size：只有数据积累到batch.size之后，sender才会发送数据linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据 发送API 导入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt;&lt;/dependency&gt; 编写测试代码 异步发送 1234567891011121314151617181920212223242526272829303132333435363738394041424344// ProducerConfig封装了一系列配置参数名的常量// 1、实例化kafka集群Properties properties = new Properties();properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");properties.put(\"acks\", \"all\");properties.put(\"bootstrap.servers\", \"hadoop101:9092\");// 重试次数properties.put(\"retries\", 1);// 批次大小properties.put(\"batch.size\", 16384);// 等待时间properties.put(\"linger.ms\", 1);// RecordAccumulator缓冲区大小properties.put(\"buffer.memory\", 33554432);// KafkaProducer,生产者对象,用来发送数据KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties);// 2、用集群对象发送数据for (int i = 0; i &lt; 1000; i++) &#123; // 封装ProducerRecord(发送的消息封装类) // 同步 // producer.send(new ProducerRecord&lt;&gt;(\"test\",Integer.toString(i),\"value\" + i); // 异步 // 回调函数会在producer收到ack时调用,为异步调用 // 该方法有两个参数,分别是RecordMetadata和Exception;如果Exception为null,说明消息发送成功。如果Exception不为null,说明消息发送失败 // 注意：消息发送失败会自动重试,不需要我们在回调函数中手动重试 producer.send(new ProducerRecord&lt;&gt;( \"test\", Integer.toString(i), \"value\" + i ), (metadata, exception) -&gt; &#123; if (null == exception) System.out.println(\"metadata = \" + metadata); else exception.printStackTrace(); &#125;); System.out.println(\"发送了第\" + i + \"条\");&#125;// 3、关闭资源producer.close(); 同步发送 12345// 同步发送：一条消息发送之后,会阻塞当前线程直至返回ack// 由于send()方法返回的是一个Future对象,根据Futrue对象的特点,也可实现同步发送效果,只需调用Future对象的get()方法即可for (int i = 0; i &lt; 100; i++) &#123; producer.send(new ProducerRecord&lt;String, String&gt;(\"test\", Integer.toString(i), Integer.toString(i))).get();&#125; Consumer APIConsumer消费数据时的可靠性是容易保证的，因为数据在Kafka中是持久化的，不用担心数据丢失问题。但由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需从故障前的位置继续消费，因此consumer需要实时记录消费到了哪个offset以便故障恢复后继续消费。offset的维护是Consumer消费数据必须考虑的问题 提交offset 导入依赖，同producer 编写测试代码 自动提交offset 12345678910111213141516171819202122232425// ConsumerConfig封装了一系列配置参数名的常量// 1、新建一个Consumer对象Properties props = new Properties();props.put(\"bootstrap.servers\", \"hadoop101:9092\");props.put(\"group.id\", \"test\");// 是否开启自动提交offsetprops.put(\"enable.auto.commit\", \"true\");// 自动提交offset时间间隔props.put(\"auto.commit.interval.ms\", \"1000\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");// KafkaConsumer,消费者对象,用于消费数据KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);// 2、用这个对象接受消息consumer.subscribe(Collections.singleton(\"first\"));// 从订阅的话题中拉取数据ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(2));// 消费拉取的数据// ConsumerRecord(接受的消息封装)for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123; System.out.println(\"record = \" + record);&#125;// 3、关闭Consumerconsumer.close(); 手动提交offset虽然自动提交offset十分简洁便利，但由于是基于时间提交的，难以把握offset提交的时机。Kafka还提供了手动提交offset的API手动提交offset的方法有两种：commitSync(同步提交)和commitAsync(异步提交)两者相同点：将本次poll的一批数据最高的偏移量提交两者不同点：commitSync阻塞当前线程一直到提交成功，并且会自动失败重试(由不可控因素导致,也会出现提交失败)；commitAsync则没有失败重试机制，故有可能提交失败 123456789101112131415161718192021222324252627282930// 同步提交offset// 有失败重试机制,更加可靠while (true) &#123; // 消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(record); &#125; // 同步提交,当前线程会阻塞直到offset提交成功 consumer.commitSync();&#125;// 异步提交offset// 比同步吞吐量高while (true) &#123; // 消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(record); &#125; // 异步提交 consumer.commitAsync(new OffsetCommitCallback() &#123; @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (exception != null) &#123; System.err.println(\"Commit failed -&gt; \" + offsets); &#125; &#125; &#125;);&#125; 数据漏消费和重复消费分析无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"JVM","slug":"ProgrammingLanguage/Java/JVM","date":"2020-09-26T12:10:10.000Z","updated":"2020-11-15T07:44:12.180Z","comments":true,"path":"2020/09/26/ProgrammingLanguage/Java/JVM/","link":"","permalink":"https://sobxiong.github.io/2020/09/26/ProgrammingLanguage/Java/JVM/","excerpt":"内容 JVM体系结构概述 堆体系结构概述 堆参数调优","text":"内容 JVM体系结构概述 堆体系结构概述 堆参数调优 JVM体系结构概述 JVM位置：运行与操作系统之上(可以认为是一种中间件)，与硬件没有直接的交互 \bJVM结构 类装载器ClassLoader 作用：负责加载class文件，class文件在文件开头有特定的文件标示，将class文件字节码内容加载到内存中，并将这些内容转换成方法区中的运行时数据结构。ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定 种类： 虚拟机自带的加载器 启动类加载器(Bootstrap) C++($JAVA_HOME/jre/lib/rt.jar) 扩展类加载器(Extension) Java($JAVA_HOME/jre/lib/ext/*.jar) 应用程序类加载器(AppClassLoader)，Java中也叫系统类加载器(System Class Loader)，加载当前应用的classpath的所有类 用户自定义加载器：java.lang.ClassLoader的子类，用户可以定制类的加载方式 种类案例： 123456789101112131415Object obj = new Object();// null// Object定义在rt.jar中,采用C++的Bootstrap加载器System.out.println(obj.getClass().getClassLoader());MyObject mObj = new MyObject();// sun.misc.Launcher$AppClassLoader@18b4aac2// MyObject是用户自定义类,采用系统类加载器AppClassLoaderSystem.out.println(mObj.getClass().getClassLoader());// sun.misc.Launcher$ExtClassLoader@61bbe9ba// 系统类加载器的父类即为扩展类加载器ExtClassLoaderSystem.out.println(mObj.getClass().getClassLoader().getParent());// null// 扩展类加载器ExtClassLoader的父类即为C++的Bootstrap加载器System.out.println(mObj.getClass().getClassLoader().getParent().getParent()); 类加载机制(双亲委派)：一个类收到了类加载请求，它首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成，每一个层次类加载器都是如此，因此所有的加载请求都应该传送到启动类加载器中，只有当父类加载器反馈自己无法完成这个请求的时候(在它的加载路径下没有找到所需加载的Class)，子类加载器才会尝试自己去加载。采用双亲委派的一个好处是比如加载位于rt.jar包中的类java.lang.Object，不管是哪个加载器加载这个类，最终都是委托给顶层的启动类加载器进行加载，这样就保证了使用不同的类加载器最终得到的都是同样一个Object对象(也防止用户自定义了系统预先定义的类[包名和类名完全相同]造成的类加载冲突——编译器不报错,运行时报错) Execution Engine：执行引擎负责解释命令，提交操作系统执行 Native Interface(本地接口)： 本地接口的作用是融合不同的编程语言为Java所用，它的初衷是融合C/C++程序(Java诞生的时候是C/C++横行的时候,要想立足,必须能够调用C/C++程序)，于是就在内存中专门开辟了一块区域处理标记为native的代码，它的具体做法是Native Method Stack中登记native方法，在Execution Engine执行时加载native libraies 目前该方法使用得越来越少了，除非是与硬件有关的应用，在企业级应用中已经比较少见。因为现在的异构领域间的通信很发达，比如可以使用Socket通信、Web Service等 12345678910111213141516171819202122232425// Test.javapublic static void main(String[] args) &#123; Thread t1 = new Thread(); t1.start();&#125;// Thread.javapublic synchronized void start() &#123; // 同一个thread不能start()两次 if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; &#125; &#125;&#125;private native void start0(); Native Method Stack：登记native方法，在Execution Engine执行时加载本地方法库 PC寄存器(类似汇编)： 每个线程都有一个程序计数器，是线程私有的，就是一个指针，指向方法区中的方法字节码(用来存储指向下一条指令的地址,也即将要执行的指令代码)，由执行引擎读取下一条指令 这块内存区域(空间)很小，几乎可以忽略不记，它是当前线程所执行的字节码的行号指示器，字节码解释器通过改变这个计数器的值来选取下一条需要执行的字节码指令 如果执行的是一个Native方法，那这个计数器是空的 用以完成分支、循环、跳转、异常处理、线程恢复等基础功能。不会发生内存溢出(OOM：Out Of Memory)错误 Method Area(方法区)： 是供各线程共享的运行时内存区域。它存储了每一个类的结构信息，例如运行时常量池(Runtime Constant Pool)、字段和方法数据、构造函数和普通方法的字节码内容 上面讲的是规范，在不同虚拟机中实现是不一样的，最典型的就是Java7的永久代(PermGen space)和Java8的元空间(Metaspace) 实例变量存在堆内存中，和方法区无关 Stack(栈)： 介绍：栈也叫栈内存，主管Java程序的运行。在线程创建时创建，它的生命期是跟随线程的生命期，线程结束栈内存也就释放，对于栈来说不存在垃圾回收问题，只要线程一结束该栈就Over，生命周期和线程一致，是线程私有的。8种基本类型的变量、对象的引用变量、实例方法都是在函数的栈内存中分配 栈存储什么(主要保存3类数据)： 本地变量(Local Variables)：输入参数、输出参数以及方法内的变量 栈操作(Operand Stack)：记录出栈、入栈的操作 栈帧数据(Frame Data)：包括类文件、方法等 栈运行原理：栈中的数据都是以栈帧(Stack Frame)格式存在，栈帧是一个内存区块、一个数据集、一个有关方法(Method)和运行期数据的数据集当一个方法A被调用时就产生了一个栈帧F1，并被压入到栈中；A方法又调用了B方法，于是产生的栈帧F2也被压入栈；B方法又调用了C方法，于是产生的栈帧F3也被压入栈…..方法相继执行完毕后，先弹出F3栈帧，再弹出F2栈帧，再弹出F1栈帧……遵循“先进后出”/“后进先出”原则每个方法执行的同时都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完毕的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程。栈的大小和具体JVM的实现有关，通常在256K~756K之间，约等于1Mb左右 栈/堆/方法区的交互关系HotSpot(Java8)是使用指针的方式来访问对；Java堆中会存在访问类元数据的地址；reference存储的就是对象的地址 堆体系结构概述 Heap堆 介绍：一个JVM实例只存在一个堆内存，堆内存的大小是可以调节的。类加载器读取了类文件后，需要把类、方法、常变量放到堆内存中，保存所有引用类型的真实信息，以方便执行器执行 组成部分(逻辑上划分)： Young Generation Space(新生区 Young/New) Tenure Generation Space(老年区 Old/Tenure) Permanent Space(永久区 Perm) GC过程： 新生区是类的诞生、成长和消亡的区域，一个类在这里产生、应用、最后被垃圾回收器收集结束生命。新生区又分为两部分：伊甸区(Eden Space)和幸存者区(Survivor pace)，所有的类都是在伊甸区被new创建出来的。幸存区有两个：0区(Survivor 0 Space)和1区(Survivor 1 Space) 当伊甸区的空间用完时，程序又需要创建对象，JVM的垃圾回收器将对伊甸区进行垃圾回收(Minor GC)，将伊甸区中的不再被其他对象所引用的对象进行销毁。然后将伊甸区中的剩余对象移动到幸存0区。若幸存0区也满了，再对该区进行垃圾回收，然后移动到1区，如果1区也满了再移动到老年区 若老年区也满了，那么这时候将发生Major GC(Full GC)进行老年区的内存清理。若老年区执行了Full GC之后发现依然无法进行对象的保存，就会产生OOM异常(OutOfMemoryError) Minor GC过程(复制 -&gt; 清空 -&gt; 互换)： Eden、Survivor From区复制到Survivor To区，年龄+1：首先，当Eden区满的时候会触发首次GC——把还活着的对象拷贝到Survivor From区。当Eden区再次触发GC时会扫描Eden区和From区，对这两个区域进行垃圾回收，经过这次回收后还存活的对象，则直接复制到To区域(如果有对象的年龄已经达到了老年的标准则赋复制老年区)，同时把这些对象的年龄+1 清空Eden、Survivor From区：然后，清空Eden和Survivor From区中的对象 Survivor To区和Survivor From区互换：最后，Survivor To和Survivor From互换，原Survivor To区成为下一次GC时的Survivor From区。部分对象会在From和To区域中复制来复制去，如此交换15次(由JVM参数MaxTenuringThreshold决定,默认为15)最终如果还是存活，就存入到老年区 方法区(Method Area) 介绍：实际而言，方法区(Method Area)和堆一样是各个线程共享的内存区域，它用于存储虚拟机加载的类信息、普通常量、静态常量和编译器编译后的代码等。虽然JVM规范将方法区描述为堆的一个逻辑部分，但它却还有一个别名叫做Non-Heap(非堆)，目的就是要和堆分开 方法区的实现：对于HotSpot虚拟机，很多开发者习惯将方法区称之为永久代(Parmanent Gen)，但严格本质上说两者不同，永久代是方法区的一个实现。jdk1.7的版本中，已经将原本放在永久代的字符串常量池移走(永久带是1.7版本的叫法,1.8则为元空间Metaspace) Java7永久区(7及之前)：永久存储区是一个常驻内存区域，用于存放JDK自身所携带的Class、Interface的元数据，也就是说它存储的是运行环境必须的类信息。被装载进此区域的数据是不会被垃圾回收器回收掉的，关闭JVM才会释放此区域所占用的内存 堆参数调优 Java堆： 在Java8中永久代已经被移除，被一个称为元空间的区域所取代。元空间的本质和永久代类似 元空间与永久代之间最大的区别：永久带使用的JVM的堆内存；Java8以后的元空间并不在虚拟机中，而是使用本机物理内存 默认情况下，元空间的大小仅受本地内存限制。类的元数据放入Native Memory，字符串池和类的静态变量放入Java堆中，这样可以加载多少类的元数据就不再由MaxPermSize控制而由系统的实际可用空间来控制 堆参数(在VM options中指定) 参数 解释 -Xms 设置JVM初始内存大小,默认为物理内存的1/64 -Xmx 设置JVM最大分配内存,默认为物理内存的1/4 -XX:+PrintGCDetails 输出详细的GC处理日志 123456// 返回Java虚拟机试图使用的最大内存量long maxMemory = Runtime.getRuntime().maxMemory();// 返回Java虚拟机中的内存总量long totalMemory = Runtime.getRuntime().totalMemory();System.out.println(\"-Xmx:maxMemory = \" + maxMemory + \"Byte , \" + (maxMemory / (double) 1024 / 1024) + \"MB\");System.out.println(\"-Xms:totalMemory = \" + totalMemory + \"Byte , \" + (totalMemory / (double) 1024 / 1024) + \"MB\"); 12345678910111213141516171819202122232425262728293031323334353637// java.lang.OutOfMemoryError: Java heap space// byte[] bytes = new byte[40 * 1024 * 1024];// VM Options: -Xms8m -Xmx8m -XX:+PrintGCDetailsString str = \"www.sobxiong.com\";while (true) &#123; str += str + new Random().nextInt(88888888) + new Random().nextInt(99999999);&#125;/*[GC (Allocation Failure) [PSYoungGen: 1508K-&gt;496K(2048K)] 1508K-&gt;535K(7680K), 0.0017728 secs] [Times: user=0.01 sys=0.00, real=0.01 secs][GC (Allocation Failure) [PSYoungGen: 1882K-&gt;505K(2048K)] 1921K-&gt;797K(7680K), 0.0053175 secs] [Times: user=0.01 sys=0.00, real=0.00 secs][GC (Allocation Failure) [PSYoungGen: 2041K-&gt;352K(2048K)] 3331K-&gt;1891K(7680K), 0.0014598 secs] [Times: user=0.00 sys=0.00, real=0.01 secs][Full GC (Ergonomics) [PSYoungGen: 1488K-&gt;0K(2048K)] [ParOldGen: 5531K-&gt;1366K(5632K)] 7019K-&gt;1366K(7680K), [Metaspace: 3036K-&gt;3036K(1056768K)], 0.0054889 secs] [Times: user=0.01 sys=0.00, real=0.00 secs][GC (Allocation Failure) [PSYoungGen: 1074K-&gt;96K(2048K)] 4436K-&gt;3458K(7680K), 0.0015233 secs] [Times: user=0.01 sys=0.00, real=0.01 secs][GC (Allocation Failure) [PSYoungGen: 96K-&gt;96K(2048K)] 3458K-&gt;3458K(7680K), 0.0017007 secs] [Times: user=0.00 sys=0.00, real=0.00 secs][Full GC (Allocation Failure) [PSYoungGen: 96K-&gt;0K(2048K)] [ParOldGen: 3362K-&gt;3363K(5632K)] 3458K-&gt;3363K(7680K), [Metaspace: 3055K-&gt;3055K(1056768K)], 0.0047922 secs] [Times: user=0.02 sys=0.00, real=0.00 secs][GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2048K)] 3363K-&gt;3363K(7680K), 0.0009790 secs] [Times: user=0.00 sys=0.00, real=0.00 secs][Full GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2048K)] [ParOldGen: 3363K-&gt;3344K(5632K)] 3363K-&gt;3344K(7680K), [Metaspace: 3055K-&gt;3055K(1056768K)], 0.0045256 secs] [Times: user=0.02 sys=0.00, real=0.01 secs]HeapPSYoungGen total 2048K, used 66K [0x00000007bfd80000, 0x00000007c0000000, 0x00000007c0000000) eden space 1536K, 4% used [0x00000007bfd80000,0x00000007bfd90978,0x00000007bff00000) from space 512K, 0% used [0x00000007bff80000,0x00000007bff80000,0x00000007c0000000) to space 512K, 0% used [0x00000007bff00000,0x00000007bff00000,0x00000007bff80000)ParOldGen total 5632K, used 3344K [0x00000007bf800000, 0x00000007bfd80000, 0x00000007bfd80000) object space 5632K, 59% used [0x00000007bf800000,0x00000007bfb44040,0x00000007bfd80000)Metaspace used 3109K, capacity 4496K, committed 4864K, reserved 1056768K class space used 338K, capacity 388K, committed 512K, reserved 1048576KException in thread \"main\" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3332) at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124) at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:674) at java.lang.StringBuilder.append(StringBuilder.java:208) at com.xiong.jvm.Test2.main(Test2.java:12)*/// 不是立刻执行,禁止使用// System.gc();","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"}]},{"title":"JUC","slug":"ProgrammingLanguage/Java/JUC","date":"2020-09-18T11:28:07.000Z","updated":"2020-11-15T08:36:30.162Z","comments":true,"path":"2020/09/18/ProgrammingLanguage/Java/JUC/","link":"","permalink":"https://sobxiong.github.io/2020/09/18/ProgrammingLanguage/Java/JUC/","excerpt":"内容 JUC是什么 Lock接口 线程间通信 线程间定制化调用通信 线程八锁 线程不安全集合 Callable接口 JUC辅助类 BlockingQueue阻塞队列 ThreadPool线程池 Java8流式计算 Java8分支合并 异步回调 volatile CAS 值传递和引用传递 Java锁的类型 死锁及定位分析","text":"内容 JUC是什么 Lock接口 线程间通信 线程间定制化调用通信 线程八锁 线程不安全集合 Callable接口 JUC辅助类 BlockingQueue阻塞队列 ThreadPool线程池 Java8流式计算 Java8分支合并 异步回调 volatile CAS 值传递和引用传递 Java锁的类型 死锁及定位分析 JUC是什么 JUC介绍：JDK1.5时Java引入的并发编程工具包——java.util.concurrent 基础知识回顾： 进程/线程是什么： 进程：进程是一个具有一定独立功能的程序关于某个数据集合的一次运行活动。它是操作系统动态执行的基本单元，在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元 线程：通常在一个进程中可以包含若干个线程，当然一个进程中至少有一个线程，不然没有存在的意义。线程可以利用进程所拥有的资源，在引入线程的操作系统中，通常都是把进程作为分配资源的基本单位，而把线程作为独立运行和独立调度的基本单位，由于线程比进程更小，基本上不拥有系统资源，故对它的调度所付出的开销就会小得多，能更高效的提高系统多个程序间并发执行的程度 进程/线程例子： 进程：QQ.ext、word.exe 线程：word检查拼写、word容灾备份 线程的状态 123456789// Thread.javapublic enum State &#123; NEW, // 创建 RUNNABLE, // 准备就绪(还需等待OS),Thread实例start()后并不是马上运行,只是进入就绪状态,等待OS BLOCKED, // 阻塞 WAITING, // 等待(一直等下去——不见不散) TIMED_WAITING, // 等待(有时限的等待——过时不候) TERMINATED; // 终止&#125; wait/sleep的区别： wait/sleep都可以使当前线程暂停 wait放开手睡眠，放开手里的锁 sleep握紧手睡眠，唤醒后手里还有锁 并发/并行各自都是什么： 并发：同一时刻多个线程在访问同一个资源(例子：抢车票) 并行：多项工作同时执行，之后在汇合(例子：泡脚玩手机) Lock接口 复习Synchronized： 多线程口诀1、2： 高内聚低耦合 线程、操作、资源类 实现步骤： 创建资源类 资源类里创建同步方法(代码块) 创建线程，访问资源 卖票实例： 1234567891011121314151617181920212223242526272829303132333435/*** 题目：三个售票员,卖100张票* 多线程编程的企业级套路 + 模版* 1、高内聚低耦合* 2、线程 操作(对外暴露的调用方法) 资源类*/public class SaleTicket &#123; public static void main(String[] args) &#123; final Ticket ticket = new Ticket(); // 创建线程不能直接继承Thread类,因为Java是单继承,资源宝贵,要使用接口方式 // 如果方法体简单,可以不用继承Runnable接口,而直接采用匿名内部类/lambda表达式 // 创建线程要使用两个参数Thread(runnable, name)的方式 new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"C\").start(); &#125;&#125;// 资源类class Ticket &#123; private int number = 100; public synchronized void saleTicket() &#123; if (number &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + \"\\t卖出第\" + (number--) + \"张票\\t,还剩下\" + number + \"张票\"); &#125; &#125;&#125; Lock接口： Lock介绍(摘录自JDK1.8)：Lock implementations provide more extensive locking operations than can be obtained using synchronized methods and statements. They allow more flexible structuring, may have quite different properties, and may support multiple associated Condition objects —— 锁实现提供了比使用同步方法和语句可以获得的更广泛的锁操作。它们允许更灵活的结构，可能具有非常不同的属性，并且可能支持多个关联的条件对象 Lock的常用实现类ReentrantLock(可重入锁) 1234567891011121314151617181920212223// Lock使用模版class SharedResource &#123; private final ReentrantLock lock = new ReentrantLock(); /** * synchronized与Lock的区别 * 1、首先synchronized是java内置关键字,在jvm层面;Lock是个java类 * 2、synchronized无法判断是否获取锁的状态,Lock可以判断是否获取到锁 * 3、synchronized会自动释放锁(a:线程执行完同步代码会释放锁;b:线程执行过程中发生异常会释放锁);Lock需在finally中手动释放锁(unlock()方法释放锁),否则容易造成线程死锁 * 4、用synchronized关键字的两个线程1和线程2,如果当前线程1获得锁,线程2线程等待。如果线程1阻塞,线程2则会一直等待下去;而Lock锁就不一定会等待下去,如果尝试获取不到锁,线程可以不用一直等待就结束了 * 5.synchronized的锁可重入、不可中断、非公平,而Lock锁可重入、可判断、可公平(默认非公平,二者皆可) * 6.Lock锁适合大量同步的代码的同步问题,synchronized锁适合代码少量的同步问题 */ public void competitionMethod() &#123; // block until condition holds lock.lock(); try &#123; // ... method body &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; Lock方式卖票实例： 123456789101112131415161718192021222324252627282930313233public class SaleTicket &#123; public static void main(String[] args) &#123; final Ticket ticket = new Ticket(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt;= 120; i++) ticket.saleTicket(); &#125;, \"C\").start(); &#125;&#125;// 资源类class Ticket &#123; private int number = 100; private final Lock lock = new ReentrantLock(); public void saleTicket() &#123; lock.lock(); try &#123; if (number &gt; 0) &#123; System.out.println(Thread.currentThread().getName() + \"\\t卖出第\" + (number--) + \"张票\\t,还剩下\" + number + \"张票\"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 线程间通信 题目：两个线程来操作初始值为零的一个变量，实现一个线程对该变量加1,另一个线程对该变量减1。实现交替10个轮次,变量初始值为0 线程间通信： 生产者/消费者模型 通知等待唤醒机制 多线程口诀3： 判断 干活 通知 老版本synchronized实现： 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Cake &#123; private int number = 0; public synchronized void increment() throws InterruptedException &#123; // 1、判断 if (number != 0) &#123; this.wait(); &#125; // 2、干活 number++; System.out.println(Thread.currentThread().getName() + \"\\t生产,剩余\" + number); // 3、通知 this.notifyAll(); &#125; public synchronized void decrement() throws InterruptedException &#123; // 1、判断 if (number == 0) &#123; this.wait(); &#125; // 2、干活 number--; System.out.println(Thread.currentThread().getName() + \"\\t消费,剩余\" + number); // 3、通知 this.notifyAll(); &#125;&#125;/*** 题目：两个线程来操作初始值为零的一个变量,实现一个线程对该变量加1,另一个线程对该变量减1;实现交替10个轮次,变量初始值为0* 1、高聚合低耦合前提下,线程操作资源类* 2、判断/干活/通知*/public class ThreadWaitNotify &#123; public static void main(String[] args) &#123; Cake cake = new Cake(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.increment(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ProducerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.decrement(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ConsumerA\").start(); &#125;&#125; 结果：符合要求 如果换成4个线程(2消费者,2生产者) 12345678910111213141516171819202122232425262728293031323334353637383940414243// 只改变mainpublic static void main(String[] args) &#123; Cake cake = new Cake(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.increment(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ProducerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.decrement(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ConsumerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.increment(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ProducerB\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; cake.decrement(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"ConsumerB\").start();&#125; 结果：出现错误，有可能生产出大于1的cake来 原因：换成4个线程会导致错误——虚假唤醒，因为在Java多线程判断时，不能用if。错误出在了判断上面：如果突然有一个增加cake的线程进入到if里面了，但突然中断了并交出控制权。等到唤醒后由于是if，不需要再次进行验证，而是直接走下去了，所以进行了错误的增加 解决方法：把所有的资源类的increment()和decrement()方法中的if判断变为while判断 多线程口诀4：注意多线程之间的虚假唤醒 新版本Lock实现： 新老版本对标： synchronized - Lock wait - await notify - signal Lock示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Cake1 &#123; private int number = 0; private final Lock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); public void increment() &#123; lock.lock(); try &#123; // 1、判断 while (number != 0) &#123; condition.await(); System.out.println(Thread.currentThread().getName() + \"\\t生产,剩余\" + number); &#125; // 2、干活 number++; // 3、通知 condition.signalAll(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void decrement() &#123; lock.lock(); try &#123; // 1、判断 while (number == 0) &#123; condition.await(); System.out.println(Thread.currentThread().getName() + \"\\t消费,剩余\" + number); &#125; // 2、干活 number--; // 3、通知 condition.signalAll(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;/*** 题目：两个线程来操作初始值为零的一个变量,实现一个线程对该变量加1,另一个线程对该变量减1;实现交替10个轮次,变量初始值为0* 1、高聚合低耦合前提下,线程操作资源类* 2、判断/干活/通知* 3、多线程交互中,必须要防止多线程的虚假唤醒,也即(判断只能用while,不能用if)*/public class ThreadAwaitSignal &#123; public static void main(String[] args) &#123; Cake1 cake = new Cake1(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.increment(); &#125; &#125;, \"ProducerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.decrement(); &#125; &#125;, \"ConsumerA\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.increment(); &#125; &#125;, \"ProducerB\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; cake.decrement(); &#125; &#125;, \"ConsumerB\").start(); &#125;&#125; 线程间定制化调用通信 题目：多线程之间按顺序调用，实现A -&gt; B -&gt; C，三个线程启动,要求如下：AAAAA打印5次，BBBBB打印10次，CCCCC打印15次，以上操作进行10轮 多线程口诀5：标志位 实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class ShareResource &#123; private int number = 1; // 1 -&gt; A, 2 -&gt; B, 3 -&gt; C // 一把锁lock private final Lock lock = new ReentrantLock(); // 三把钥匙condition private final Condition conditionA = lock.newCondition(); private final Condition conditionB = lock.newCondition(); private final Condition conditionC = lock.newCondition(); public void printFromA() &#123; lock.lock(); try &#123; // 1、判断 while (number != 1) &#123; conditionA.await(); &#125; // 2、干活 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"AAAAA ~~~\"); &#125; // 3、通知(修改标志位,通知下一个) number = 2; conditionB.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printFromB() &#123; lock.lock(); try &#123; // 1、判断 while (number != 2) &#123; conditionB.await(); &#125; // 2、干活 for (int i = 0; i &lt; 10; i++) &#123; System.out.println(\"BBBBB ~~~\"); &#125; // 3、通知(修改标志位,通知下一个) number = 3; conditionC.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void printFromC() &#123; lock.lock(); try &#123; // 1、判断 while (number != 3) &#123; conditionC.await(); &#125; // 2、干活 for (int i = 0; i &lt; 15; i++) &#123; System.out.println(\"CCCCC ~~~\"); &#125; // 3、通知(修改标志位,通知下一个) number = 1; conditionA.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;/*** 题目：多线程之间按顺序调用,实现A -&gt; B -&gt; C,三个线程启动,要求如下：AAAAA打印5次,BBBBB打印10次,CCCCC打印15次,以上操作进行10轮* 1、高聚合低耦合前提下,线程操作资源类* 2、判断/干活/通知* 3、多线程交互中,必须要防止多线程的虚假唤醒,也即(判断只能用while,不能用if)* 4、标志位*/public class ThreadOrderAccess &#123; public static void main(String[] args) &#123; ShareResource shareResource = new ShareResource(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) shareResource.printFromA(); &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) shareResource.printFromB(); &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) shareResource.printFromC(); &#125;, \"C\").start(); &#125;&#125; 线程八锁 八锁示例： 情况1(标准访问) 12345678910111213141516171819202122232425262728293031class Phone &#123; public synchronized void sendEmail() throws Exception &#123; System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 1、标准访问,请问先打印邮件还是短信? 邮件public class Lock8 &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况2(其一线程sleep) 1234567891011121314151617181920212223242526272829303132class Phone &#123; public synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 2、邮件方法暂停4秒,请问先打印邮件还是短信? 邮件public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况3(新增一个普通方法) 123456789101112131415161718192021222324252627282930313233343536class Phone &#123; public synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125; public void hello() &#123; System.out.println(\"Hello ~~~\"); &#125;&#125;// 3、在2的基础上,新增并使用一个普通方法hello(),请问先打印邮件还是hello? hellopublic class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.hello(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况4(两个对象) 123456789101112131415161718192021222324252627282930313233class Phone &#123; public synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 4、两部手机,请问先打印邮件还是短信? 短信public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); Phone phone1 = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone1.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况5(改为静态同步方法) 1234567891011121314151617181920212223242526272829303132class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public static synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 5、两个静态同步方法,同一部手机,请问先打印邮件还是短信? 邮件public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况6(两个对象调用静态同步方法) 123456789101112131415161718192021222324252627282930313233class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public static synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 6、两个静态同步方法,两部手机,请问先打印邮件还是短信? 邮件public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); Phone phone1 = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone1.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况7(一个静态同步方法,一个普通同步方法) 1234567891011121314151617181920212223242526272829303132class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 7、一个普通同步方法,一个静态同步方法,一部手机,请问先打印邮件还是短信? 短信public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 情况8(一个静态同步方法,一个普通同步方法,两个对) 123456789101112131415161718192021222324252627282930313233class Phone &#123; public static synchronized void sendEmail() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"Send Email ~~~\"); &#125; public synchronized void sendMessage() throws Exception &#123; System.out.println(\"Send Message ~~~\"); &#125;&#125;// 8、一个普通同步方法,一个静态同步方法,两部手机,请问先打印邮件还是短信? 短信public class Test &#123; public static void main(String[] args) throws Exception &#123; Phone phone = new Phone(); Phone phone1 = new Phone(); new Thread(() -&gt; &#123; try &#123; phone.sendEmail(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; try &#123; phone1.sendMessage(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"B\").start(); &#125;&#125; 八锁分析： 情况1、2：如果一个对象有多个synchronized方法，某个时刻内只要有一个线程去调用当前对象的一个synchronized方法，那么其它线程只能等待。换句话说，某个时刻内只能有唯一一个线程去访问这些synchronized方法。锁作用的是当前对象this，this被锁定后其它的线程都不能进入到当前对象的其它的synchronized方法(情况1、2都进入了sendEmail()方法,因此不响应sendMessage()方法) 情况3、4：普通方法和同步锁无关，一个线程调用了synchronized方法，另一个线程可以同时调用普通方法；当换成两个对象后，synchronized锁的是对象实例，而当前有两个实例，锁的就不是同一把锁了，因此sendMessage先打印 情况5、6：对于静态同步方法，锁是当前类的Class对象，对于同一个Phone类锁是相同的(Phone.class)，因此进入sendEmail()方法后不会响应sendMessage()方法，而是等待sendEmail()方法执行完毕 情况7、8：对于普通同步方法和静态同步方法，他们锁的对象不同，普通同步方法锁的是当前对象实例(Phone的一个实例对象)，而静态同步方法锁的是当前类的Class对象(Phone.class)。他们锁的对象不同，不会相互影响，因此先打印sendMessage 线程八锁总结： synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现为以下3种形式： 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的Class对象 对于同步方法块，锁是Synchonized括号里配置的对象 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。也就是说如果一个实例对象的普通同步方法获取锁后，该实例对象的其他普通同步方法必须等待已获取锁的方法释放锁后才能获取锁 其他实例对象的普通同步方法跟当前实例对象的普通同步方法用的是不同的锁(不同的实例对象)，所以无须等待当前实例对象已获取锁的普通同步方法释放锁就可以获取他们自己的锁 所有静态同步方法用的是同一把锁——类对象本身，普通同步方法的锁和静态同步方法的锁是两个不同的对象，所以静态同步方法与普通同步方法之间是不会有竞态条件的。但一旦一个静态同步方法获取锁后，其他静态同步方法都必须等待该方法释放锁后才能获取锁(而不管是同一个实例对象的静态同步方法之间，还是不同实例对象的静态同步方法之间，只要它们是同一个类的实例对象) 线程不安全集合 线程不安全的集合 ArrayList 情况1：3个线程同时读写ArrayList(结果：运行基本不报错,但是会出现List中有时内容为null或者集合元素个数不等于3的情况) 情况2：30个线程同时读写ArrayList(结果：运行报错——java.util.ConcurrentModificationException并发修改异常) 123456789public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 3 or 30; i++) &#123; new Thread(() -&gt; &#123; list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(list); &#125;, \"list\" + i).start(); &#125;&#125; 出错原因：ArrayList本身就是线程不安全的(为了性能考虑,不加锁性能提升但会出错误) 123456// ArrayList.javapublic boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 解决方案： 改用Vector(线程安全,加了synchronized,加锁数据一致但性能下降;性能较差,不要使用) 1234567// Vector.javapublic synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; Collections工具类 少量数据可以使用，通过在ArrayList外包装一层同步机制 Collections.synchronizedList(new ArrayList&lt;&gt;()) CopyOnWriteArrayList(推荐使用) CopyOnWrite容器即写时复制的容器。往一个容器添加元素的时候，不直接往当前容器Object[]添加，而是先将当前容器Object[]进行copy，复制出一个新的容器Object[] newElements，然后向新的容器Object[] newElements里添加元素。添加元素后，再将原容器的引用指向新的容器setArray(newElements) 这样做的好处是可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器是一种读写分离的思想，读和写不同的容器 1234567891011121314151617181920// CopyOnWriteArrayList.javapublic boolean add(E e) &#123; final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; // copy原始数据 Object[] newElements = Arrays.copyOf(elements, len + 1); // 新增数据 newElements[len] = e; // 引用更新 setArray(newElements); return true; &#125; finally &#123; // 解锁 lock.unlock(); &#125;&#125; HashSet(HashSet底层就是HashMap) 示例： 123456789101112public static void main(String[] args) &#123; // new HashSet&lt;&gt;() // Collections.synchronizedSet(new HashSet&lt;&gt;()) // new CopyOnWriteArraySet&lt;&gt;() Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(() -&gt; &#123; set.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(set); &#125;, \"set\" + i).start(); &#125;&#125; 解决方案： Collections工具类 Collections.synchronizedSet(new HashSet&lt;&gt;()) CopyOnWriteArraySet 底层还是CopyOnWriteArrayList 123456789101112public class CopyOnWriteArraySet&lt;E&gt; extends AbstractSet&lt;E&gt; implements java.io.Serializable &#123; private static final long serialVersionUID = 5457747651344034263L; private final CopyOnWriteArrayList&lt;E&gt; al; /** * Creates an empty set. */ public CopyOnWriteArraySet() &#123; al = new CopyOnWriteArrayList&lt;E&gt;(); &#125; // ...&#125; HashMap 示例： 123456789101112public static void main(String[] args) &#123; // new HashMap&lt;&gt;() // Collections.synchronizedMap(new HashMap&lt;&gt;()) // new ConcurrentHashMap&lt;&gt;() Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); for (int i = 0; i &lt; 300; i++) &#123; new Thread(() -&gt; &#123; map.put(Thread.currentThread().getName(), UUID.randomUUID().toString().substring(0, 8)); System.out.println(map); &#125;, \"map\" + i).start(); &#125;&#125; 解决方案： Collections工具类 Collections.synchronizedMap(new HashMap&lt;&gt;()) ConcurrentHashMap Callable接口 获得多线程的方法有几种? 继承Thread类(不建议使用) 实现Runnale接口 实现Callable接口 从线程池获取 Callable是什么：一个JDK1.5推出的线程接口，比Runnable更强大。是一个函数式接口，可用作lambda表达式。能在线程执行完成后返回结果(应用场景一般在于批处理业务,如转账时需要返回结果的状态码,代表本次操作的成功与否) 与Runnable的区别： 是否有返回值 是否会抛出异常 落地方法不同(run()/call()) 123456789class MyThread implements Runnable &#123; @Override public void run() &#123;&#125;&#125;class MyThread2 implements Callable&lt;String&gt; &#123; @Override public String call() throws Exception &#123; return null; &#125;&#125; 怎么使用： 直接替换runnable：不可行，Thread的构造方法传参都是Runnable接口，没有Callable接口 找中间人FutureTask：FutureTask类实现了Runnable接口，并且接收一个Callable接口作为构造函数 12345678910111213141516171819202122232425262728293031323334353637383940// FutureTask.classpublic class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; // ... public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable &#125; // ...&#125;class MyThread2 implements Callable&lt;String&gt; &#123; @Override public String call() throws Exception &#123; TimeUnit.SECONDS.sleep(4); System.out.println(\"~~~\" + Thread.currentThread().getName() + \" Come in call() ~~~\"); return \"1024\"; &#125;&#125;/*** 多线程第3种创建多线程的方式* get()方法一般请放在最后一行,它会阻塞线程*/public class CallableDemo &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(new MyThread2()); new Thread(futureTask, \"A\").start(); new Thread(futureTask, \"B\").start(); // System.out.println(Thread.currentThread().getName() + \"计算中~~~\"); // 使用类似自旋锁的方式判断是否运行完毕 while (!futureTask.isDone())&#123; TimeUnit.MILLISECONDS.sleep(500); System.out.println(Thread.currentThread().getName() + \"计算中~~~\"); &#125; System.out.println(futureTask.get()); System.out.println(futureTask.get()); System.out.println(Thread.currentThread().getName() + \"计算完成~~~\"); &#125;&#125; 运行结果如下 123456789101112main计算中~~~main计算中~~~main计算中~~~main计算中~~~main计算中~~~main计算中~~~main计算中~~~~~~A Come in call() ~~~main计算中~~~10241024main计算完成~~~ 说明：多个线程执行一个FutureTask时只会计算一次，结果缓存，因此Come in call()方法只打印一次。如果需要两个线程同时计算任务时，需要定义两个futureTask FutureTask/Callable应用场景：在主线程中需要执行比较耗时的操作但又不想阻塞主线程时，可把这些操作交给FutureTask对象在后台完成。当主线程将来需要操作结果时可以通过FutureTask对象获得后台作业的计算结果或者执行状态。一般FutureTask多用于耗时的计算任务，主线程可在完成自己的任务后再去获取结果。仅在计算完成时才能检索结果；如果计算尚未完成，则会阻塞get()方法。get()方法获取结果只有在计算完成时获取，否则会阻塞直到任务转入完成状态，最后返回结果或者抛出异常。一旦计算完成，就不会再重新开始或取消计算，如果再次调用结果方法，会将缓存的结果直接返回 JUC辅助类 CountDownLatch(减少计数) 概念：让一些线程阻塞直到另一些线程完成一系列操作才被唤醒 原理：CountDownLatch主要有两个方————countDown()以及await()。当一个或多个线程调用await()方法时，这些线程会阻塞。其它线程调用countDown()方法会将计数器减1(调用countDown()方法的线程不会阻塞)，当计数器的值变为0时，因await()方法阻塞的线程会被唤醒，继续执行 例子： 场景：假设一个自习室里有7个人，其中有一个是班长，班长的主要职责就是在其它6个同学走了后关灯、锁教室门然后走人，因此班长是需要最后一个走的。需要一种方法能够控制班长这个线程最后一个执行，而其它线程是随机执行的 代码： 12345678910111213141516171819202122232425262728// 要求：所有子线程完成后(所有同学离开教室),主线程退出(班长离开教室)public class CountDownLatchDemo &#123; public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0; i &lt; 6; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(finalI); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \" 离开教室~~~\"); countDownLatch.countDown(); &#125;, UUID.randomUUID().toString().substring(0, 8)).start(); &#125; countDownLatch.await(); System.out.println(Thread.currentThread().getName() + \" 班长关门走人~~~\"); &#125; // 常规方法无法完成,会有乱序 private static void closeDoor() &#123; for (int i = 0; i &lt; 6; i++) &#123; new Thread(() -&gt; System.out.println(Thread.currentThread().getName() + \" 离开教室~~~\"), i + \"\").start(); &#125; System.out.println(Thread.currentThread().getName() + \" 班长关门走人~~~\"); &#125;&#125; 运行结果如下 1234567891011121314151617# 普通版本0 离开教室~~~2 离开教室~~~3 离开教室~~~1 离开教室~~~main 班长关门走人~~~4 离开教室~~~5 离开教室~~~# CountDownLatch版本a4055b7f 离开教室~~~816b8658 离开教室~~~1876e706 离开教室~~~9657fae5 离开教室~~~4833f0f9 离开教室~~~6d229a84 离开教室~~~main 班长关门走人~~~ CyclicBarrier(循环栅栏) 概念：让一些线程阻塞直到另一些线程完成一系列操作才被唤醒。方式与CountDownLatch相反，做加法，开始为0，加到某个值时才开始执行 原理：CyclicBarrier的字面意思是可循环(Cyclic)使用的屏障(Barrier)。它做的事情是让一组线程到达一个屏障(也可以叫同步点)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门；此时所有被屏障拦截的线程才会继续干活。线程进入屏障通过CyclicBarrier的await()方法 例子： 场景：集齐7颗龙珠召唤神龙 代码： 123456789101112131415161718192021222324// 要求：子线程全部完成后再运行指定方法(集齐七棵龙珠召唤神龙)public class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; System.out.println(Thread.currentThread().getName() + \" 召唤神龙~~~\")); for (int i = 1; i &lt;= 7; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(finalI); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \"收集到第\" + finalI + \"颗龙珠~~~\"); try &#123; cyclicBarrier.await(); // 在7个线程中最后一个线程到达await()屏障,之后下面的语句和cyclicBarrier中设定的动作才会被调度执行 System.out.println(Thread.currentThread().getName() + \"收集第\" + finalI + \"颗龙珠完毕~~~\"); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;, i + \"\").start(); &#125; &#125;&#125; 运行结果如下 1234567891011121314151 收集到第1颗龙珠~~~2 收集到第2颗龙珠~~~3 收集到第3颗龙珠~~~4 收集到第4颗龙珠~~~5 收集到第5颗龙珠~~~6 收集到第6颗龙珠~~~7 收集到第7颗龙珠~~~7 召唤神龙~~~7 收集第7颗龙珠完毕~~~1 收集第1颗龙珠完毕~~~3 收集第3颗龙珠完毕~~~2 收集第2颗龙珠完毕~~~6 收集第6颗龙珠完毕~~~5 收集第5颗龙珠完毕~~~4 收集第4颗龙珠完毕~~~ Semaphore(信号量) 概念：信号量，用于两个目的： 用于共享资源的互斥使用 用于并发线程数的控制 原理：在信号量上了定义两种操作：acquire()————获取，当一个线程调用acquire()操作时，它要么成功并获取信号量(信号量减1)，要么一直等下去直到有线程释放信号量或超时；release()————释放，会将信号量的值加1，然后唤醒等待的线程 例子： 场景：抢车位，假设有6辆车，3个停车位 代码： 12345678910111213141516171819202122232425// 要求：只有三个线程，但是希望六个线程都能够运行(有3个空闲车位,共有6辆车,一开始3辆车抢到,之后开走1辆另外的车占1个车位)public class SemaphoreDemo &#123; public static void main(String[] args) &#123; // 模拟资源类,有3个空车位 // false表示非公平锁,默认也是非公平锁 Semaphore semaphore = new Semaphore(3, false); for (int i = 0; i &lt; 6; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; try &#123; // 获取信号量(车位) semaphore.acquire(); System.out.println(Thread.currentThread().getName() + \" 抢到了车位 ~~~\"); TimeUnit.SECONDS.sleep(finalI); System.out.println(Thread.currentThread().getName() + \" 离开了车位 ~~~\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; // 释放信号量(车位) semaphore.release(); &#125; &#125;, i + \"号车\").start(); &#125; &#125;&#125; 运行结果如下 1234567891011120号车 抢到了车位 ~~~2号车 抢到了车位 ~~~1号车 抢到了车位 ~~~0号车 离开了车位 ~~~3号车 抢到了车位 ~~~1号车 离开了车位 ~~~4号车 抢到了车位 ~~~2号车 离开了车位 ~~~5号车 抢到了车位 ~~~3号车 离开了车位 ~~~4号车 离开了车位 ~~~5号车 离开了车位 ~~~ BlockingQueue阻塞队列 阻塞队列介绍：首先是一个队列，大致的数据结构如下图所示： 线程1往阻塞队列里添加元素，线程2从阻塞队列里移除元素 当队列是空的，从队列中获取元素的操作将会被阻塞 当队列是满的，从队列中添加元素的操作将会被阻塞 试图从空的队列中获取元素的线程将会被阻塞，直到其他线程往空的队列插入新的元素 试图向已满的队列中添加新元素的线程将会被阻塞，直到其他线程从队列中移除一个或多个元素，或者完全清空使队列变得空闲起来，再进行后续新增 阻塞的解释：在多线程领域，所谓的阻塞就是指在某些情况下线程会被挂起，但一旦条件满足，被挂起的线程又会自动被唤起 阻塞队列的好处： 不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，为这一切BlockingQueue都一手包办了 在concurrent包发布以前，多线程环境下每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给程序开发带来不小的复杂度 阻塞队列种类： 继承图： 各实现类介绍： ArrayBlockingQueue：由数组结构组成的有界阻塞队列 LinkedBlockingQueue：由链表结构组成的有界(但大小默认值为integer.MAX_VALUE)的阻塞队列 PriorityBlockingQueue：支持优先级排序的无界阻塞队列 DelayQueue：使用优先级队列实现的延迟无界阻塞队列 SynchronousQueue：不存储元素的阻塞队列，也即单个元素的队列(生产一个,消费一个,不存储元素,不消费不生产) LinkedTransferQueue：由链表组成的无界阻塞队列 LinkedBlockingDeque：由链表组成的双向阻塞队列 阻塞队列核心方法： 方法类型 解释 抛出异常 当阻塞队列满时，再往队列里add插入元素会抛出异常IllegalStateException:Queue full；当阻塞队列空时，再往队列里remove移除元素会抛出异常NoSuchElementException 特殊值 插入方法————成功ture，失败false；移除方法————成功返回出队列的元素，队列里没有就返回null 一直阻塞 当阻塞队列满时，生产者线程继续往队列里put元素，队列会一直阻塞生产者线程直到put数据成功(元素被消费)或响应中断退出；当阻塞队列空时，消费者线程试图从队列里take元素，队列会一直阻塞消费者线程直到队列可用(元素被生产) 超时退出 当阻塞队列满时，队列会阻塞生产者线程一定时间，超过限时后生产者线程会退出 用处 线程池 消息中间件 生产者消费者模式 \b需求：一个初始值为0的变量，两个线程对其交替操作，一个加1，一个减1，来五轮 多线程口诀： 线程、操作、资源类 判断、干活、通知 防止虚假唤醒机制 传统版： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class ShareData &#123; private int num = 0; private final Lock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); public void increment() throws Exception &#123; // 加锁 lock.lock(); try &#123; // 循环判断 while (num != 0) &#123; // 等待,不生产 condition.await(); &#125; // 干活 num++; System.out.println(Thread.currentThread().getName() + \"\\t\" + num); // 通知唤醒 condition.signalAll(); &#125; finally &#123; // 解锁 lock.unlock(); &#125; &#125; public void deIncrement() throws Exception &#123; // 加锁 lock.lock(); try &#123; // 判断 while (num == 0) &#123; // 等待,不生产 condition.await(); &#125; // 干活 num--; System.out.println(Thread.currentThread().getName() + \"\\t\" + num); // 通知唤醒 condition.signalAll(); &#125; finally &#123; // 解锁 lock.unlock(); &#125; &#125;&#125;public class ProdConsumerTraditionDemo &#123; public static void main(String[] args) &#123; ShareData shareData = new ShareData(); new Thread(() -&gt; &#123; for (int i = 1; i &lt; 5; i++) &#123; try &#123; shareData.increment(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"t1\").start(); new Thread(() -&gt; &#123; for (int i = 1; i &lt; 5; i++) &#123; try &#123; shareData.deIncrement(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;, \"t2\").start(); &#125;&#125; 运行结果如下 12345678t1 1t2 0t1 1t2 0t1 1t2 0t1 1t2 0 阻塞队列版： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class MyResource &#123; // 默认开启,进行生产/消费 // volatile修饰,保证多线程内存可见性 private volatile boolean flag = true; private final AtomicInteger atomicInteger = new AtomicInteger(); // 依赖注入,定义接口,而非实现 private final BlockingQueue&lt;String&gt; blockingQueue; public MyResource(BlockingQueue&lt;String&gt; blockingQueue) &#123; this.blockingQueue = blockingQueue; System.out.println(blockingQueue.getClass().getName()); &#125; public void myProduce() throws Exception &#123; String data; boolean returnValue; // 多线程环境的判断一定要使用while进行,防止出现虚假唤醒 while (flag) &#123; data = atomicInteger.incrementAndGet() + \"\"; returnValue = blockingQueue.offer(data, 2L, TimeUnit.SECONDS); if (returnValue) &#123; System.out.println(Thread.currentThread().getName() + \" insert \" + data + \" succeed~~~\"); &#125; else &#123; System.out.println(Thread.currentThread().getName() + \" insert \" + data + \" fail~~~\"); &#125; TimeUnit.SECONDS.sleep(1); &#125; System.out.println(Thread.currentThread().getName() + \" myProduce() finish~~~\"); &#125; public void myConsume() throws Exception &#123; String result; // 多线程环境的判断一定要使用while进行,防止出现虚假唤醒 while (flag) &#123; result = blockingQueue.poll(2L, TimeUnit.SECONDS); if (null == result || result.isEmpty()) &#123; flag = false; System.out.println(Thread.currentThread().getName() + \" over 2 seconds\"); return; &#125; System.out.println(Thread.currentThread().getName() + \" get \" + result + \" succeed~~~\"); &#125; System.out.println(Thread.currentThread().getName() + \" myConsume() finish~~~\"); &#125; public void stop() &#123; this.flag = false; &#125;&#125;public class ProduceConsumeBlockQueueDemo &#123; public static void main(String[] args) &#123; MyResource myResource = new MyResource(new ArrayBlockingQueue&lt;&gt;(10)); new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \" produce thread start~~~\"); try &#123; myResource.myProduce(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"Produce Thread\").start(); new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \" Consume thread start~~~\"); try &#123; myResource.myConsume(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;, \"Consume Thread\").start(); try &#123; TimeUnit.SECONDS.sleep(5); myResource.stop(); System.out.println(\"Main Stop()~~~\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果如下 12345678910111213141516java.util.concurrent.ArrayBlockingQueueProduce Thread produce thread start~~~Consume Thread Consume thread start~~~Produce Thread insert 1 succeed~~~Consume Thread get 1 succeed~~~Produce Thread insert 2 succeed~~~Consume Thread get 2 succeed~~~Produce Thread insert 3 succeed~~~Consume Thread get 3 succeed~~~Produce Thread insert 4 succeed~~~Consume Thread get 4 succeed~~~Produce Thread insert 5 succeed~~~Consume Thread get 5 succeed~~~Main Stop()~~~Produce Thread myProduce() finish~~~Consume Thread over 2 seconds ThreadPool线程池 为什么使用线程池：主要特点有线程复用、控制最大并发数、管理线程线程池做的主要工作就是控制运行的线程的数量，处理过程中将任务放入到队列中，然后线程创建后，启动这些任务，如果线程数量超过了最大数量则排队等候，等其它线程执行完毕，再从队列中取出任务来执行 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的销耗 提高响应速度。当任务到达时，任务可以不需要等待线程创建就能立即执行 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会销耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 线程池如何使用： 线程池类继承图： 获取线程池的几种方式： Executors.newFixedThreadPool(int)：创建一个具有n个固定线程的线程池 执行长期的任务，性能较好 创建一个定长线程池，可控制线程数最大并发数，超出的线程会在队列中等待 corePoolSize和MaxmumPoolSize是相等的，为输入的n；使用的底层阻塞队列是LinkedBlockingQueue Executors.newSingleThreadExecutor()：创建一个只有1个线程的单线程池 一个任务一个任务执行的场景 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序执行 corePoolSize和MaxmumPoolSize都设置为1；使用的底层阻塞队列是LinkedBlockingQueue Executors.newCachedThreadPool()：创建一个可扩容的线程池 使用执行很多短期异步的小程序或负载较轻的服务器 创建一个可缓存线程池，如果线程长度超过处理需要，可灵活回收空闲线程。如无线程可回收，则新建新线程 corePoolSize设置为0，maxmumPoolSize设置为Integer.MAX_VALUE；使用的底层阻塞队列是SynchronousQueue；来了任务就创建线程运行，如果线程空闲超过传入指定的keepAliveTime时间，就销毁线程 Executors.newScheduledThreadPool(int)：创建周期性执行任务的线程池 线程池支持定时以及周期性执行任务，创建一个corePoolSize为传入参数，最大线程数为整形的最大数的线程池 使用的底层阻塞队列为DelayedWorkQueue Executors创建线程原理： 线程池参数解释： 12345678910111213141516171819202122public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; corePoolSize：核心线程数，线程池中的常驻核心线程数 在创建线程池后，当有请求任务就会安排池中的线程去执行请求任务 当线程池中的线程数目达到corePoolSize后，就会把到达的请求任务放到缓存队列中 maximumPoolSize：线程池中能够容纳同时执行的最大线程数，此值必须大于等于1 keepAliveTime：多余的空闲线程的存活时间， 若当前池中线程数量超过corePoolSize，且当空闲时间达到keepAliveTime时，多余线程会被销毁，直到只剩下corePoolSize个线程为止 默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用 unit：keepAliveTime的时间单位 workQueue：任务队列，用于存储被提交但尚未被执行的任务(也即上面介绍的阻塞队列) LinkedBlockingQueue：链表阻塞队列 SynchronousBlockingQueue：同步阻塞队列 threadFactory：表示生成线程池中工作线程的线程工厂，用于创建线程，一般默认即可 handler：拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数(maximumPoolSize)时，如何来拒绝请求执行的Runnable的策略 线程池底层工作原理： 在创建了线程池后，开始等待提交过来的任务请求 当调用execute()方法添加一个请求任务时，线程池会做出如下判断： 如果正在运行的线程数量小于corePoolSize，那么马上创建线程运行这个任务 如果正在运行的线程数量大于或等于corePoolSize，那么将这个任务放入队列 如果这个时候队列满了，且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务 如果队列满了且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会启动饱和拒绝策略来执行 当一个线程完成任务时，它会从队列中取下一个任务来执行 当一个线程无事可做超过一定的时间(keepAliveTime)时，线程会判断： 如果当前运行的线程数大于corePoolSize，那么该线程就被停掉 当线程池的所有任务完成后，线程池线程数目最终会收缩到corePoolSize 线程池拒绝策略 拒绝策略是什么：等待队列已经排满了，再也塞不下新任务了。同时，线程池中也达到了最大线程数，无法继续为新任务服务。这时就需要一种拒绝策略机制合理地处理这个问题 JDK内置的拒绝策略(内置拒绝策略均实现了RejectedExecutionHandle接口) AbortPolicy(默认)：直接抛出RejectedExecutionException异常阻止系统正常运行 CallerRunsPolicy：”调用者运行”调节机制，该策略既不会抛弃任务也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量 DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务 DiscardPolicy：该策略默默地丢弃无法处理的任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的一种策略 线程池的选用 在工作中单一的/固定数的/可变的三种创建线程池的方法哪个用的多：一个都不用如Alibaba Java开发手册规定所示 自定义线程池示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class MyThreadPoolDemo &#123; public static void main(String[] args) &#123; System.out.println(Runtime.getRuntime().availableProcessors()); // RejectedExecutionException AbortPolicy // CallerRunsPolicy() // DiscardPolicy() // DiscardOldestPolicy() ExecutorService threadPool = new ThreadPoolExecutor( 2, 5, 2L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.DiscardOldestPolicy()); try &#123; // 模拟有10个顾客来银行办理业务 for (int i = 1; i &lt;= 10; i++) &#123; threadPool.execute(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"办理业务~~~\"); &#125;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 线程池必须要关闭,重量资源 threadPool.shutdown(); &#125; &#125; private static void useSystemPool() &#123; // 1池5个工作线程 // ExecutorService threadPool = Executors.newFixedThreadPool(5); // 1池1个工作线程 // ExecutorService threadPool = Executors.newSingleThreadExecutor(); // 1池N个工作线程 ExecutorService threadPool = Executors.newCachedThreadPool(); try &#123; // 模拟有10个顾客来银行办理业务 for (int i = 1; i &lt;= 10; i++) &#123; threadPool.execute(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"办理业务~~~\"); &#125;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 线程池必须要关闭,重量资源 threadPool.shutdown(); &#125; &#125;&#125; 线程池的合理参数配置生产环境中如何配置corePoolSize和maximumPoolSize大小：根据具体业务来配置，分为CPU密集型和IO密集型 CPU密集型CPU密集的意思是该任务需要大量的运算，而没有阻塞，CPU一直全速运行CPU密集任务只有在真正的多核CPU上才可能得到加速(通过多线程)；而在单核CPU上，无论开几个模拟的多线程该任务都不可能得到加速，因为CPU总的运算能力就那些CPU密集型任务配置尽可能少的线程数量：一般为————CPU核数 + 1 IO密集型由于IO密集型任务线程并不是一直在执行任务，则应配置尽可能多的线程，如CPU核数 * 2IO密集型即该任务需要大量的IO操作，即大量的阻塞在单线程上运行IO密集型的任务会导致浪费大量的CPU运算能力花费在等待上；IO密集型任务中使用多线程可以大大地加速程序的运行，即使在单核CPU上。这种加速主要就是利用了被浪费掉的阻塞时间IO密集时，大部分线程都被阻塞，故需要多配置线程数：参考————CPU核数 / (1 - 阻塞系数)通常阻塞系数在0.8 ~ 0.9左右例如：8核CPU：8 / (1 - 0.9) = 80个线程数 Java8流式计算 函数式接口(只有一个方法(除default和static方法)的接口)： Lambda表达式口诀：拷贝小括号，写死右括号，落地大括号 Lambda案例： 123456789101112131415161718192021222324252627282930interface Foo &#123; void sayHello();&#125;interface Foo1 &#123; int add(int x, int y); default double div(double x, double y) &#123; return x / y; &#125; static int multiply(int x, int y) &#123; return x * y; &#125;&#125;/*** Lambda表达式* 口诀：拷贝小括号(参数),写死右括号,落地大括号* 注释@FunctionalInterface(只有一个方法的接口,函数式接口)* default方法允许接口有默认实现(jdk1.8),可以有多个* 静态方法实现*/public class LambdaExpress &#123; public static void main(String[] args) &#123; Foo foo = () -&gt; System.out.println(\"Hello LambdaExpress\"); foo.sayHello(); Foo1 foo1 = (x, y) -&gt; 2 * x + y; System.out.println(foo1.add(1, 2)); System.out.println(foo1.div(1.0, 0.3333)); System.out.println(Foo1.multiply(2, 5)); &#125;&#125; Java内置核心四大函数式接口 Stream流 是什么：流(Stream)是数据渠道，用于操作数据源(集合、数组等)所生成的元素序列。集合讲的是数据，流讲的是计算 特点： Stream自己不会存储元素 Stream不会改变源对象。相反，他们会返回一个持有结果的新Stream Stream操作是延迟执行的。这意味着他们会等到需要结果的时候才执行(懒加载) 怎么用： 创建一个Stream：一个数据源(数组、集合) 中间操作：一个中间操作，处理数据源数据 终止操作：一个终止操作，执行中间操作链，产生结果 案例： 12345678910111213141516171819202122232425262728293031@Data@NoArgsConstructor@AllArgsConstructorclass User &#123; private Integer id; private String userName; private int age;&#125;/*** 题目：请按照给出数据，找出同时满足* 偶数ID且年龄大于24且用户名转为大写且用户名字母倒排序* 最后只输出一个用户名字*/public class StreamDemo &#123; public static void main(String[] args) &#123; User u1 = new User(11,\"a\",23); User u2 = new User(12,\"b\",24); User u3 = new User(13,\"c\",22); User u4 = new User(14,\"d\",28); User u5 = new User(16,\"e\",26); List list = Arrays.asList(u1,u2,u3,u4,u5); list.stream().filter(p -&gt; p.getId() % 2 == 0) .filter(p -&gt; p.getAge() &gt; 24) .map(f -&gt; f.getUserName().toUpperCase()) .sorted((o1, o2) -&gt; o2.compareTo(o1)) .limit(1) .forEach(System.out::println); &#125;&#125; Java8分支合并 原理(类同MapReduce)：Fork：把一个复杂任务进行分拆，大事化小Join：把分拆任务的结果进行合并 相关类： ForkJoinPool(分支合并线程池) ForkJoinTask(类似FutureTask) RecursiveTask(递归任务,继承自ForkJoinTask,递归自己) 案例： 123456789101112131415161718192021222324252627282930313233343536373839class MyTask extends RecursiveTask&lt;Long&gt; &#123; private static final Integer ADJUST_VALUE = 20; private final long begin; private final long end; private long result = 0; public MyTask(long begin, long end) &#123; this.begin = begin; this.end = end; &#125; @Override protected Long compute() &#123; if ((end - begin) &lt;= ADJUST_VALUE) &#123; for (long i = begin; i &lt;= end; i++) &#123; result += i; &#125; &#125; else &#123; long middle = (end + begin) / 2; MyTask leftTask = new MyTask(begin, middle); MyTask rightTask = new MyTask(middle + 1, end); leftTask.fork(); rightTask.fork(); result = leftTask.join() + rightTask.join(); &#125; return result; &#125;&#125;public class ForkJoinDemo &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; MyTask myTask = new MyTask(0, 1023423423L); ForkJoinPool threadPool = new ForkJoinPool(); ForkJoinTask&lt;Long&gt; forkJoinTask = threadPool.submit(myTask); System.out.println(forkJoinTask.get()); threadPool.shutdown(); &#125;&#125; 异步回调 案例： 123456789101112131415161718192021public static void main(String[] args) throws ExecutionException, InterruptedException &#123; // 同步 CompletableFuture&lt;Void&gt; completableFuture = CompletableFuture.runAsync(() -&gt; System.out.println(Thread.currentThread().getName() + \"无返回!\")); completableFuture.get(); // 异步 CompletableFuture&lt;Integer&gt; completableFuture2 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"有返回!\"); int result = 1 / 0; return 1024; &#125;); // 异步回调 System.out.println(completableFuture2.whenComplete((t, u) -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"：t = \" + t); System.out.println(Thread.currentThread().getName() + \"：u = \" + u); &#125;).exceptionally(f -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"：f = \" + f.getMessage()); return 4444; &#125;).get());&#125; volatile 介绍：volatile是Java虚拟机提供的轻量级的同步机制 保证可见性 不保证原子性 禁止指令重排 JMM介绍：JMM(Java内存模型Java Memory Model,简称JMM)本身是一种抽象的概念，并不真实存在。它描述的是一组规则或规范，通过这组规范定义了程序中各个变量(包括实例字段、静态字段和构成数组对象的元素)的访问方式JMM关于同步规定： 线程解锁前，必须把共享变量的值刷新回主内存 线程加锁前，必须读取主内存的最新值到自己的工作内存 加锁解锁是同一把锁 由于JVM运行程序的实体是线程，而每个线程创建时JVM都会为其创建一个工作内存(有些地方成为栈空间)。工作内存是每个线程的私有数据区域，而Java内存模型中规定所有变量都存储在主内存。主内存是共享内存区域，所有线程都可访问，但线程对变量的操作(读取赋值等)必须在工作内存中进行。首先要将变量从主内存拷贝到自己的工作空间，然后对变量进行操作，操作完成再将变量写回主内存。不能直接操作主内存中的变量，各个线程中的工作内存储存着主内存中的变量副本拷贝，因此不同的线程无法访问对方的工作内存，线程间的通讯(传值)必须通过主内存来完成，其简要访问过程如下图所示： 可见性：通过JMM的介绍，可知各个线程对主内存中共享变量的操作都是各个线程各自拷贝到自己的工作内存操作后再写回主内存中的。这将导致可能存在一个线程A修改了共享变量X的值，该值还未写回主内存中；此时另外一个线程B又对内存中的共享变量X进行操作，但此时线程A工作内存中的共享变量X对线程B来说并不不可见。这种工作内存与主内存同步延迟现象就造成了可见性问题 12345678910111213141516171819202122232425262728293031323334353637383940414243// 资源类class MyData &#123; // volatile int number = 0; int number = 0; public void add() &#123; number++; &#125;&#125;// 验证volatile可见性// 假如int number = 0;number变量之前没有volatile关键字修饰,没有可见性// 添加了volatile,可解决可见性问题public class VolatileDemo &#123; public static void main(String[] args) &#123; MyData myData = new MyData(); new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"\\t come in\"); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; myData.add(); System.out.println(Thread.currentThread().getName() + \"\\t update number\"); &#125;, \"Test\").start(); // 循环等待,直到number被更新 while (myData.number == 0) ; System.out.println(Thread.currentThread().getName() + \"\\t mission is over , number = \" + myData.number); &#125;&#125;/*当number不加volatile关键字时,主内存不更新,一直在循环中打印:Test come inTest update number当number加volatile关键字时,主内存可见,循环能正常退出打印:Test come inTest update numbermain mission is over , number = 1*/ 原子性：操作不可分割，完整性。即某个线程正在做某个具体业务时，中间不可以被加塞或被分割。需整体完整，要么同时成功要么同时失败 12345678910111213141516171819202122232425262728293031323334353637383940// 资源类class MyData &#123; // AtomicInteger number = new AtomicInteger(); volatile int number = 0; // public void add() &#123; number.getAndIncrement(); &#125; // public synchronized void add() &#123; number++; &#125; public void add() &#123; number++; &#125;&#125;// 验证volatile不保证原子性// 如何保证原子性：// 1、加synchronized关键字()// 2、使用AtomicInteger原子整型类替换intpublic class VolatileDemo &#123; public static void main(String[] args) &#123; MyData myData = new MyData(); for (int i = 0; i &lt; 20; i++) &#123; new Thread(() -&gt; &#123; for (int j = 1; j &lt;= 1000; j++) &#123; myData.add(); &#125; &#125;, \"Thread \" + i).start(); &#125; // 等待20个线程全部即算完成后,主线程继续执行(当还有子线程时,主线程算一个) while (Thread.activeCount() &gt; 2) &#123; // 线程让步,让出CPU执行时间给其他线程 Thread.yield(); &#125; // 打印最终的结果值 System.out.println(myData.number); &#125;&#125;/*当add()不加synchronized关键字时打印小于20000的数当add()加synchronized关键字时打印20000*/ 12345678910111213141516171819202122// add()方法对应的字节码反编译public class MyData &#123; volatile int number = 0; public void add() &#123; number++; &#125;&#125;public void add();Code: 0: aload_0 1: dup 2: getfield #2 // Field number:I 5: iconst_1 6: iadd 7: putfield #2 // Field number:I 10: return/*n++被拆分成三个指令：1、执行getfield从主内存拿到原始number2、执行iadd进行加1操作3、执行putfield把累加后的值写回主内存可能会出现写覆盖,因此小于20000*/ 数值丢失的原因：假设线程1和2同时修改各自工作空间的内容，因此可见性，因此需要重新写入主内存。但在线程1写入时，线程2也同时写入，这导致线程1的写入操作被挂起；线程2写完后线程1继续写入，这样线程1写入的值覆盖了线程2写入的值，造成数据丢失 有序性：计算机在执行程序时为了提高性能，编译器和处理器常常会做指令重排，一般分为以下3种：源代码 -&gt; 编译器优化的重排 -&gt; 指令并行的重排 -&gt; 内存系统的重排 -&gt; 最终执行指令单线程环境里确保程序最终执行结果和代码顺序执行的结果一致处理器在进行重新排序是必须要考虑指令之间的数据依赖性多线程环境中线程交替执行，由于编译器优化重排的存在，两个线程使用的变量能否保持一致性是无法确定的，结果无法预测 指令重排案例： 案例一： 123456public void testSort()&#123; int x = 11; // 语句1 int y = 12; // 语句2 x = x + 5; // 语句3 y = x * x; // 语句4&#125; 正常单线程情况：1234 多线程环境可能的情况：2134、1324 上述的过程就可以当做是指令的重排，即内部执行顺序和我们的代码顺序不一样。但是指令重排也是有限制的，即不会此情况：4321 因为处理器在进行重排时候，必须考虑到指令之间的数据依赖性。语句4依赖于y以及x的申明，因为存在数据依赖，无法首先执行 案例二： int a = 0, b = 0, x = 0, y = 0; 线程1 线程2 x = a; y = b; b = 1; a = 2; x = 0; y = 0; 如果编译器对这段代码进行执行重排优化后，可能出现下列情况： 线程1 线程2 b = 1; a = 2; x = a; y = b; x = 2; y = 1; 这也就说明在多线程环境下，由于编译器优化重排的存在，两个线程使用的变量能否保持一致是无法确定的 案例三： 12345678910111213141516public class ResortSeqDemo &#123; int a = 0; boolean flag = false; public void method01() &#123; a = 1; flag = true; &#125; public void method02() &#123; if(flag) &#123; a = a + 5; System.out.println(\"Value: \" + a); &#125; &#125;&#125; 按照正常的顺序分别调用method01()和method02()，那么最终输出就是a = 6 但如果在多线程环境下，因为method01()和method02()之间不能存在数据依赖的问题，因此原先的顺序可能是 1234a = 1;flag = true;a = a + 5;System.out.println(\"Value:\" + a); 但是在经过编译器，指令，或者内存的重排后，可能会出现这样的情况： 1234flag = true;a = a + 5;System.out.println(\"Value:\" + a);a = 1; 先执行flag = true后，另外一个线程马上调用方法2，满足flag的判断，最终a = a + 5，结果为5，这样出现了数据不一致的问题 多线程环境中线程交替执行，由于编译器优化重排的存在，两个线程中使用的变量能否保证一致性是无法确定的，结果无法预测 这就需要通过volatile来修饰变量，来保证线程安全性 volatile对指令重排作用总结：volatile实现禁止指令重排优化，从而避免了多线程环境下程序出现乱序执行的现象内存屏障(Memory Barrier)又称内存栅栏，是一个CPU指令，作用有两个： 保证特定操作的顺序 保证某些变量的内存可见性(利用该特性实现volatile的内存可见性)由于编译器和处理器都能执行指令重排的优化，如果在指令间插入一条Memory Barrier则会告诉编译器和CPU，不管什么指令都不能和这条Memory Barrier指令重排序，即通过插入内存屏障禁止在内存屏障前后的指令执行重排序优化。内存屏障另外一个作用是刷新出各种CPU的缓存数，因此任何CPU上的线程都能读取到这些数据的最新版本 线程安全保证： 工作内存与主内存同步延迟现象导致的可见性问题： 可通过synchronized或volatile关键字解决，它们都可使一个线程修改后的变量立即对其它线程可见 对于指令重排导致的可见性问题和有序性问题： 可以使用volatile关键字解决，volatile关键字的另一个作用就是禁止重排序优化 volatile的应用：单例模式DCL 12345678910111213141516171819202122232425262728293031323334353637383940// version1：普通方式/加synchronized关键字public class SingletonDemo &#123; private static SingletonDemo instance = null; private SingletonDemo () &#123; System.out.println(Thread.currentThread().getName() + \"\\t Constructor\"); &#125; public static SingletonDemo getInstance() &#123; if(instance == null) &#123; instance = new SingletonDemo(); &#125; return instance; &#125; public static void main(String[] args) &#123; // 比较内存地址(单线程) 正确 System.out.println(SingletonDemo.getInstance() == SingletonDemo.getInstance()); // 查看打印信息(多线程) -&gt; 打印多次构造器,错误 // 改进：getInstance()方法添加关键字synchronized for (int i = 0; i &lt; 10; i++) &#123; new Thread( () -&gt; SingletonDemo.getInstance(), \"Thread\" + i).start(); &#125; &#125;&#125;/*通过引入synchronized关键字,能够解决高并发环境下的单例模式问题但synchronized属于重量级的同步机制,它只允许一个线程同时访问获取实例的方法;为了保证数据一致性而减低了并发性,因此采用的比较少*/// version2：DCL(Double Check Lock:双端检锁机制)public static SingletonDemo getInstance() &#123; if(instance == null) &#123; // 同步代码段的时候，进行检测 synchronized (SingletonDemo.class) &#123; if(instance == null) &#123; instance = new SingletonDemo(); &#125; &#125; &#125; return instance;&#125; 上面的DCL(双端检锁)机制不一定线程安全，原因是由于指令重排的存在；加入volatile可以禁止指令重排原因在于某一个线程在执行到第一次检测，读取到的instance不为null时，instance的引用对象可能没有完成初始化：instance = new SingletonDemo();可以分为以下步骤(伪代码)memory = allocate(); // 1、分配对象内存空间instance(memory); // 2、初始化对象instance = memory; // 3、设置instance指向刚分配的内存地址,此时instance != null步骤2和步骤3不存在数据依赖关系。而且无论重排前还是重排后程序执行的结果在单线程中并没有改变，因此这种重排优化是允许的memory = allocate(); // 1、分配对象内存空间instance = memory; // 3、设置instance指向刚分配的内存地址,此时instance != null;但对象还没有初始化完instance(memory); // 2、初始化对象当我们执行到重排后的步骤2试图获取instance的时候会得到null，因为对象的初始化还没有完成，而在重排后的步骤3才完成。因此执行单例模式的代码时候，就会重新再创建一个instance实例指令重排只会保证串行语义的执行一致性(单线程),并不会关心多线程间的语义一致性所以当一条线程访问instance不为null时，由于instance实例未必完成初始化,也就造成了线程安全问题。因此需要引入volatile来保证出现指令重排的问题，从而保证单例模式的线程安全性 CAS 概念：CAS的全称是Compare-And-Swap(比较并交换)，它是一条CPU并发原语它的功能是判断内存某个位置的值是否为预期值，如果是则更改为新的值，这个过程是原子性的CAS并发原语体现在Java语言中就是sun.misc.Unsafe类的各个方法。调用UnSafe类中的CAS方法，JVM会帮我们实现CAS汇编指令，这是一种完全依赖于硬件的功能，通过它实现了原子操作。CAS是一种系统原语(属于操作系统原语范畴)，由若干条指令组成，用于完成某个功能的一个过程。原语的执行必须是连续的，在执行过程中不允许被中断，也就是说CAS是一条CPU的原子指令，不会造成所谓的数据不一致的问题，也就是说CAS是线程安全的 案例： 1234567// 创建了一个AtomicInteger实例,并初始化为5AtomicInteger atomicInteger = new AtomicInteger(5);// 调用CAS方法,企图更新成2019// 两个参数：前者为5，表示期望值;后者为2019,是要求更新的值System.out.println(atomicInteger.compareAndSet(5, 2019) + \"\\t current data = \" + atomicInteger.get());// 再次调用CAS方法,企图更新成1024System.out.println(atomicInteger.compareAndSet(5, 1024) + \"\\t current data = \" + atomicInteger.get()); 第一次执行CAS方法时，期望值和原本值满足，因此修改成功，返回true和2019；但第二次执行主内存值已修改为2019，不满足期望值，本次写入失败，因此返回了false和2019类似于SVN或者Git的版本号，如果没有人更改过，就能够正常提交，否者需要先将代码pull下来，合并代码后，然后提交 CAS底层原理： 1234567891011/*** Atomically increments by one the current value.** @return the previous value*/// atomicInteger.getAndIncrement()方法源码// this：当前atomicInteger对象// valueOffset：内存偏移量,即内存地址public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1);&#125; 大致意思是通过valueOffset直接利用内存地址获取到值然后进行加1的操作底层又调用了一个unsafe类的getAndAddInt()方法AtomicInteger底层部分代码： 123456789public class AtomicInteger extends Number implements java.io.Serializable &#123; // ... // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; // ... private volatile int value; // ...&#125; Unsafe类： Unsafe是CAS的核心类，由于Java方法无法直接访问底层系统，需要通过本地(Native)方法来访问，Unsafe相当于一个后门，基于该类可以直接操作特定的内存数据。Unsafe类存在于sun.misc包中，其内部方法操作可以像C指针一样直接操作内存。Java中的CAS操作的执行依赖于Unsafe类的方法 注意Unsafe类的所有方法都是native修饰的，也就是说unsafe类中的方法都直接调用操作系统底层资源执行相应的任务 为什么Atomic修饰的包装类，能够保证原子性，依靠的就是底层的unsafe类 变量valueOffset：表示该变量值(Atomic修饰的值)在内存中的偏移地址，Unsafe就是根据内存偏移地址获取数据的 变量value用volatile修饰：保证多线程之间内存的可见性 具体的unsafe.getAndAddInt()方法： 12345678public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; // getIntVolatile()和compareAndSwapInt()都是native方法 var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 解释： 参数解释： val1：AtomicInteger对象本身 var2：值的引用内存地址 var4：需要增加的int值 var5：用var1和var2找到的主内存中的真实值——即从主内存中拷贝到工作内存中的值(每次都要从主内存拿到最新的值并拷贝到当前线程的本地内存，然后执行compareAndSwapInt()方法再和主内存的值进行比较——在高并发环境下可能主内存值又被更新。线程不可以直接越过高速缓存直接操作主内存，所以需要执行比较方法，值统一后再执行加1操作——底层原子绑定) 实现思路：操作时需要比较当前线程工作内存中的值和主内存中的值，假设执行compareAndSwapInt()方法返回false，那么就一直执行while循环体的内容，一直刷新主内存中的值到当前线程的工作内存，直到期望的值和真实值一样 用该对象当前的值与var5比较 如果相同，更新为var5 + var4并返回true 如果不同，继续从主内存中取最新的值然后再比较，重复流程1，直到值相同，更新完成该处没有使用synchronized加锁机制，而使用CAS，这能提高并发性，同样也能实现一致性。这是因为每个线程进来后，进入do while循环体，不断地获取内存中的值，判断是否为最新的值，为最新值后再进行更新操作 具体案例：假设线程A和线程B同时执行getAndInt()方法(分别跑在不同的CPU上) AtomicInteger里面的value原始值为3(即主内存中AtomicInteger的value为3)，根据JMM模型，线程A和线程B各自持有一份值为3的AtomicInteger副本，并分别存储在各自的工作内存中 线程A通过getIntVolatile()方法拿到最新value值3，此时线程A被挂起(该线程失去CPU执行权) 线程B此时通过getIntVolatile()方法获取到最新value值，也是3。此时线程B没有被挂起，并执行了compareAndSwapInt()方法，主内存的值也是3，成功修改主内存值为4，线程B退出循环 此时线程A恢复，执行compareAndSwapInt()方法，比较发现工作内存的值3和主内存中的值4不一致，说明该值已被其它线程抢先一步修改过。因此A线程本次修改失败，再次进入循环体 线程A重新获取主内存最新的value值，因为变量value被volatile修饰，所以其它线程对它的修改线程A总能够看到，线程A继续执行compareAndSwapInt()方法进行比较并交换直到成功 Unsafe类的CAS思想：自旋(自我旋转,直到成功) 底层实现：Unsafe类中的compareAndSwapInt是一个本地方法，该方法的实现位于unsafe.cpp中 12345678UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) // 先想办法拿到变量value在内存中的地址 // 通过Atomic::cmpxchg实现比较替换,其中参数X是即将更新的值,参数e是原内存的值 UnsafeWrapper(\"Unsafe_CompareAndSwapInt\"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_END CAS缺点： 循环时间长，CPU开销大(因为执行的是do-while语句,如果比较不一致会一直循环;最差的情况就是某个线程一直取到的值和预期值都不一样,导致无限循环) 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，可以通过CAS方式来保证原子操作 当对多个共享变量执行操作时，CAS方式就无法保证操作的原子性，此时只能用锁来保证原子性 引出ABA问题 ABA问题： 介绍：CAS算法实现的一个重要前提：需要取出内存中某时刻的数据，并在当下时刻比较并交换，在这个时间差上可能会发生数据的变化比如线程1从内存位置V中取出值为A，此时另一线程2也从内存V中取出A，并且线程2进行了一些操作将值更新为B之后又将值更新为A；这时线程1进行CAS操作发现内存中仍然是A，然后线程1操作成功尽管线程1的CAS操作成功，但不代表该过程就是没有问题的 原子引用AtomicReference(能操作对象)： 1234567891011121314class User &#123; String userName; int age;&#125;User user1 = new User(\"zhangsan\", 22);User user2 = new User(\"lisi\", 23);AtomicReference&lt;User&gt; atomicReference = new AtomicReference&lt;&gt;();atomicReference.set(user1);System.out.println(atomicReference.compareAndSet(user1, user2) + \"\\t\" + atomicReference.get().toString());System.out.println(atomicReference.compareAndSet(user1, user2) + \"\\t\" + atomicReference.get().toString());// 结果：// true User&#123;userName='lisi', age=23&#125;// false User&#123;userName='lisi', age=23&#125; ABA问题的解决(AtomicStampedReference,新增版本号(类似时间戳)标记)： 123456789101112131415161718// ABA问题的模拟static AtomicReference&lt;Integer&gt; atomicReference = new AtomicReference&lt;&gt;(100);new Thread(() -&gt; &#123; System.out.println(atomicReference.compareAndSet(100, 101) + \"\\t\" + atomicReference.get()); System.out.println(atomicReference.compareAndSet(101, 100) + \"\\t\" + atomicReference.get());&#125;, \"ABA A\").start();new Thread(() -&gt; &#123; // 暂停1秒,保证t1线程完成ABA操作 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(atomicReference.compareAndSet(100, 2019) + \"\\t\" + atomicReference.get());&#125;, \"ABA B\").start();// 结果：// true 101// true 100// true 2019 1234567891011121314151617181920212223242526272829// ABA问题的解决AtomicStampedReference// 新增版本号标记数据版本static AtomicStampedReference&lt;Integer&gt; atomicStampedReference = new AtomicStampedReference&lt;&gt;(100, 1);new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + \"\\t Version 1st: \" + atomicStampedReference.getStamp()); // 等待线程B获取版本号 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; atomicStampedReference.compareAndSet(100, 101, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t Version 2nd: \" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101, 100, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"\\t Version 3rd: \" + atomicStampedReference.getStamp());&#125;, \"ABA Solution A\").start();new Thread(() -&gt; &#123; int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + \"\\t Version 1st: \" + stamp); // 等待线程A完成一次ABA操作 try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \"\\t Result = \" + atomicStampedReference.compareAndSet(100, 2019, stamp, stamp + 1) + \"\\t Current Version = \" + atomicStampedReference.getStamp() + \"\\t Value = \" + atomicStampedReference.getReference());&#125;, \"ABA Solution B\").start();// 结果：// ABA Solution A Version 1st: 1// ABA Solution B Version 1st: 1// ABA Solution A Version 2nd: 2// ABA Solution A Version 3rd: 3// ABA Solution B Result = false Current Version = 3 Value = 100 CAS总结： CAS是compare and swap的缩写，表示比较当前工作内存中的值和主物理内存中的值；如果相同则执行规定操作，否则继续比较直到主内存和工作内存的值一致为止 CAS有3个操作数，内存值V，旧的预期值A，要修改的更新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否者什么都不做 值传递和引用传递 举例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Person &#123; private Integer id; private String personName; public Person(String personName) &#123; this.personName = personName; &#125;&#125;public class TransferValueDemo &#123; public void changeValue1(int age) &#123; age = 30; &#125; public void changeValue2(Person person) &#123; person.setPersonName(\"XXXX\"); &#125; public void changeValue3(String str) &#123; str = \"XXX\"; &#125; public static void main(String[] args) &#123; TransferValueDemo test = new TransferValueDemo(); // 定义基本数据类型 int age = 20; test.changeValue1(age); System.out.println(\"age: \" + age); // 实例化person类 Person person = new Person(\"SOBXiong\"); test.changeValue2(person); System.out.println(\"personName: \" + person.getPersonName()); // String String str = \"SOBXiong\"; test.changeValue3(str); System.out.println(\"string: \" + str); &#125;&#125;/*输出结果：age: 20personName: XXXXstring: SOBXiong*/ ChangeValue1()执行过程八种基本数据类型，在栈里面分配内存，属于值传递栈管运行，堆管存储执行changeValue1()时，int是基本数据类型，所以传递的是int = 20这个值，传递的是一个副本，main方法里面的age并没有改变，因此输出的结果age还是20，属于值传递 ChangeValue2()执行过程Person是属于对象，作为方法参数时也是值传递，只不过传递的是引用——在堆空间的内存地址。执行changeValue2()时复制了内存地址，两个值都是指向同一个地址 ChangeValue3()执行过程String不属于基本数据类型，但是为什么执行完成后值未改变？这时因为String的特殊性，当执行String str = “SOBXiong”的时候，JVM会检查常量池中是否已存在”SOBXiong”，如果存在则不会创建，会生成一个引用指向常量池中的”SOBXiong”并把str赋值为引用的值；如果不存在，则会新建”SOBXiong”并把引用指向它并将str赋值为引用的值 Java锁的类型 公平锁/非公平锁 概念： 公平锁：多个线程按照申请锁的顺序来获取锁，先来先服务，就是公平的，也即队列 非公平锁：多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁，在高并发环境下有可能造成优先级翻转，或者饥饿线程(某个线程一直得不到锁) 如何创建： 并发包中ReentrantLock的创建可以指定构造函数的boolean值来得到公平锁/非公平锁，默认是非公平锁，因为非公平锁有吞吐量比公平锁高的优点 Lock lock = new ReentrantLock(true); synchronized也是一种非公平锁 两者区别： 公平锁：很公平。在并发环境中每个线程在获取锁时会先查看此锁维护的等待队列，如果为空或当前线程是等待队列中的第一个，就占用锁，否则就会加入到等待队列中，此后按照FIFO的规则 非公平锁：较粗鲁，一上来就直接尝试占有锁，如果尝试失败，再采用类似公平锁的方式 可重入锁(递归锁) 概念：可重入锁就是递归锁指同一线程外层函数获得锁之后，内层递归函数仍然能获取到该锁的代码，同一线程在外层方法获取锁时，进入内层方法会自动获取锁也即————线程可以进入任何一个它已拥有的锁所同步的代码块ReentrantLock/Synchronized是典型的可重入锁 作用：可避免死锁 可重入锁验证： 案例一：验证Synchronized 1234567891011121314151617class Phone &#123; public synchronized void sendSMS() &#123; System.out.println(Thread.currentThread().getName() + \" invoked sendSMS()\"); // 在同步方法中调用另外一个同步方法 sendEmail(); &#125; public synchronized void sendEmail() &#123; System.out.println(Thread.currentThread().getName() + \" invoked sendEmail()\"); &#125;&#125;public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); new Thread(phone::sendSMS, \"t1\").start(); new Thread(phone::sendSMS, \"t2\").start(); &#125;&#125; 运行结果如下 1234t1 invoked sendSMS()t1 invoked sendEmail()t2 invoked sendSMS()t2 invoked sendEmail() 说明：sendSMS()同步方法调用sendEmail()同步方法，synchronized锁的是对象，假如不是重入锁，那么sendSMS()方法要获取一次锁，sendEmail()又要获取一次锁，就会造成死锁；由于是同一线程，再次获取锁时就是sendSMS()获取的那个锁，没有新锁产生。同时由于synchronized锁的是对象，线程t1得到锁后，线程t2只能等t1运行完sendSMS()同步方法释放完锁 案例二：验证ReentrantLock 123456789101112131415161718192021222324252627282930313233343536class Phone implements Runnable&#123; Lock lock = new ReentrantLock(); public void getLock() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \" get Lock\"); setLock(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void setLock() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \" set Lock\"); &#125; finally &#123; lock.unlock(); &#125; &#125; @Override public void run() &#123; getLock(); &#125;&#125;public class ReenterLockDemo &#123; public static void main(String[] args) &#123; Phone phone = new Phone(); Thread t3 = new Thread(phone, \"t3\"); Thread t4 = new Thread(phone, \"t4\"); t3.start(); t4.start(); &#125;&#125; 运行结果如下 1234t3 get Lockt3 set Lockt4 get Lockt4 set Lock 说明：与加synchronized锁一样，多次获取锁后，锁是重入的，并不会锁死自己 案例三：案例二变形 1234567891011public void getLock() &#123; lock.lock(); lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \" get Lock\"); setLock(); &#125; finally &#123; lock.unlock(); lock.unlock(); &#125;&#125; 运行结果如下 1234t3 get Lockt3 set Lockt4 get Lockt4 set Lock 说明：结果与案例二一样，因此不管有几把锁，实际都是同一把锁，用同一把钥匙都能打开 案例四：案例三变形 12345678910public void getLock() &#123; lock.lock(); lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \" get Lock\"); setLock(); &#125; finally &#123; lock.unlock(); &#125;&#125; 运行结果如下 12t3 get Lockt3 set Lock 说明：加锁两次，解锁一次。程序在输出上述两句后阻塞(线程阻塞)，也就是说ReentrantLock申请加锁几次就要解锁几次 案例五：案例三变形 12345678910public void getLock() &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + \" get Lock\"); setLock(); &#125; finally &#123; lock.unlock(); lock.unlock(); &#125;&#125; 运行结果如下 123456789101112131415161718t3 get Lockt3 set Lockt4 get Lockt4 set LockException in thread \"t3\" Exception in thread \"t4\" java.lang.IllegalMonitorStateException at java.util.concurrent.locks.ReentrantLock$Sync.tryRelease(ReentrantLock.java:151) at java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1261) at java.util.concurrent.locks.ReentrantLock.unlock(ReentrantLock.java:457) at com.xiong.interview.Phone.getLock(ReenterLockDemo.java:16) at com.xiong.interview.Phone.run(ReenterLockDemo.java:31) at java.lang.Thread.run(Thread.java:748)java.lang.IllegalMonitorStateException at java.util.concurrent.locks.ReentrantLock$Sync.tryRelease(ReentrantLock.java:151) at java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1261) at java.util.concurrent.locks.ReentrantLock.unlock(ReentrantLock.java:457) at com.xiong.interview.Phone.getLock(ReenterLockDemo.java:16) at com.xiong.interview.Phone.run(ReenterLockDemo.java:31) at java.lang.Thread.run(Thread.java:748) 说明：加锁次数小于解锁次数，直接抛异常 自旋锁： 概念：SpinLock，指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁。好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU资源上文提及的比较并交换(CAS)底层使用的就是自旋 手写自旋锁 123456789101112131415161718192021222324252627282930313233343536373839404142public class SpinLockDemo &#123; // 原子引用线程 AtomicReference&lt;Thread&gt; atomicReference = new AtomicReference&lt;&gt;(); public void myLock() &#123; Thread thread = Thread.currentThread(); while (!atomicReference.compareAndSet(null, thread)) &#123; &#125; System.out.println(thread.getName() + \" Come in ~~~\"); &#125; public void myUnlock() &#123; Thread thread = Thread.currentThread(); atomicReference.compareAndSet(thread, null); System.out.println(thread.getName() + \" Come out ~~~\"); &#125; public static void main(String[] args) &#123; SpinLockDemo spinLockDemo = new SpinLockDemo(); new Thread(() -&gt; &#123; spinLockDemo.myLock(); // 暂停一会 try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; spinLockDemo.myUnlock(); &#125;,\"Thread A\").start(); // 暂停一会 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(() -&gt; &#123; spinLockDemo.myLock(); spinLockDemo.myUnlock(); &#125;,\"Thread B\").start(); &#125;&#125; 运行结果如下 1234Thread A Come in ~~~Thread A Come out ~~~Thread B Come in ~~~Thread B Come out ~~~ 说明：打印第1句5s后打印后三句话，而且out紧接第一句话。A线程调用myLock()方法自己持有锁5s，线程B进入方法后发现有线程持有锁，因此只能通过自旋等待，A线程调用myUnlock()方法释放锁后，线程B才能继续执行 独占锁(写锁)/共享锁(读锁)/互斥锁 概念：独占锁：指该锁一次只能被一个线程所持有。ReentrantLock和Synchronized都是独占锁共享锁：指该锁可以被多个线程锁持有ReentrantReadWriteLock其读锁是共享，写锁是独占。写的时候只能一个人写，但是读的时候可以多个人同时读。读锁的共享锁可保证并发读是非常高效的。读写/写写的过程是互斥的 为什么会有写锁和读锁：独占锁一次只能一个线程访问，但有一个读写分离的场景，读时想同时进行，原来独占锁的并发性就没这么好了。读锁并不会造成数据不一致的问题，因此可以多个人共享读多个线程同时读一个资源类没有任何问题，为了满足并发，读取共享资源应该可以同时进行。但若有一个线程去写共享资源，就不应该再有其他线程对该资源进行读或写操作 案例： 1234567891011121314151617181920212223242526272829303132333435363738394041public class MyCache &#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); public void put(String key, Object value) &#123; System.out.println(Thread.currentThread().getName() + \"\\t 正在写入：\" + key); try &#123; // 模拟网络拥堵 TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + \"\\t 写入完成\"); &#125; public void get(String key) &#123; System.out.println(Thread.currentThread().getName() + \"\\t 正在读取:\"); try &#123; // 模拟网络拥堵 TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; Object value = map.get(key); System.out.println(Thread.currentThread().getName() + \"\\t 读取完成：\" + value); &#125; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); // 线程操作资源类，5个线程写 for (int i = 0; i &lt; 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; myCache.put(tempInt + \"\", tempInt + \"\"), String.valueOf(i)).start(); &#125; // 线程操作资源类， 5个线程读 for (int i = 0; i &lt; 5; i++) &#123; final int tempInt = i; new Thread(() -&gt; myCache.get(tempInt + \"\"), String.valueOf(i)).start(); &#125; &#125;&#125; 运行结果如下 12345678910111213141516171819200 正在写入：03 正在写入：32 正在写入：21 正在写入：14 正在写入：40 正在读取:1 正在读取:2 正在读取:3 正在读取:4 正在读取:0 写入完成4 读取完成：43 读取完成：33 写入完成2 写入完成2 读取完成：24 写入完成0 读取完成：null1 写入完成1 读取完成：null 案例说明：在写入时，写线程被其他线程打断，这就造成还没写完就被挂起，其他线程开始写，这可能会造成数据的不一致。写操作需要原子 + 独占，整个过程必须是一个完整的统一体，中间不许被分割、打断。结果的打印顺序与真实顺序有出入，因为也是多线程的，实际读取到null的应该在写入完成之前执行 \b解决方案：加读写锁 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 增量部分// 创建一个读写锁,是一个读写一体的锁private ReentrantReadWriteLock lock = new ReentrantReadWriteLock();public void put(String key, Object value) &#123; // 写锁加锁 lock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + \"\\t 正在写入：\" + key); try &#123; // 模拟网络拥堵 TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; map.put(key, value); System.out.println(Thread.currentThread().getName() + \"\\t 写入完成\"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 写锁释放 lock.writeLock().unlock(); &#125;&#125;public void get(String key) &#123; // 读锁加锁 lock.readLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + \"\\t 正在读取:\"); try &#123; // 模拟网络拥堵 TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + \"\\t 读取完成：\" + map.get(key)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 读锁释放 lock.readLock().unlock(); &#125;&#125; 运行结果如下 12345678910111213141516171819200 正在写入：00 写入完成1 正在写入：11 写入完成2 正在写入：22 写入完成3 正在写入：33 写入完成4 正在写入：44 写入完成0 正在读取:1 正在读取:2 正在读取:3 正在读取:4 正在读取:1 读取完成：13 读取完成：32 读取完成：24 读取完成：40 读取完成：0 解决方案说明：写锁一次只允许一个线程进入执行写操作，而读锁允许多个线程同时进入执行读取的操作。结果证明写入操作中间不会被打断，而读操作可以同时进入 死锁及定位分析 死锁概念：死锁是指两个或多个以上的进程在执行过程中，因争夺资源而造成一种互相等待的现象。若无外力干涉，那它们都将无法继续推进下去。如果资源充足，进程的资源请求都能得到满足，死锁出现的可能性很低，否则就会因争夺有限的资源而陷入死锁 产生原因： 系统资源不足 进程运行推进的顺序不对 资源分配不当 死锁产生的四个必要条件： 互斥条件：一个资源每次只能被一个进程使用 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系 死锁案例： 1234567891011121314151617181920212223242526272829303132class HoldLockThread implements Runnable &#123; private final Object lockA; private final Object lockB; public HoldLockThread(Object lockA, Object lockB) &#123; this.lockA = lockA; this.lockB = lockB; &#125; @Override public void run() &#123; synchronized (lockA) &#123; System.out.println(Thread.currentThread().getName() + \" 自己持有&#123;\" + lockA + \"&#125; , 尝试获得&#123;\" + lockB + '&#125;'); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (lockB) &#123; System.out.println(Thread.currentThread().getName() + \" 自己持有&#123;\" + lockB + \"&#125; , 尝试获得&#123;\" + lockA + '&#125;'); &#125; &#125; &#125;&#125;public class DeadLockDemo &#123; public static void main(String[] args) &#123; Object lockA = new Object(), lockB = new Object(); new Thread(() -&gt; new HoldLockThread(lockA, lockB).run(), \"Thread A\").start(); new Thread(() -&gt; new HoldLockThread(lockB, lockA).run(), \"Thread B\").start(); &#125;&#125; 运行结果如下 12Thread B 自己持有&#123;java.lang.Object@1b77d269&#125; , 尝试获得&#123;java.lang.Object@42f5503f&#125;Thread A 自己持有&#123;java.lang.Object@42f5503f&#125; , 尝试获得&#123;java.lang.Object@1b77d269&#125; 说明：打印上述信息后，主线程阻塞，无法结束 排查死锁： 使用jps命令定位进程编号：jps -l 使用jstack命令查看堆栈信息：jstack pid 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.251-b08 mixed mode):\"Attach Listener\" #14 daemon prio=9 os_prio=31 tid=0x00007fcd65818000 nid=0x5903 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"DestroyJavaVM\" #13 prio=5 os_prio=31 tid=0x00007fcd6607d000 nid=0xe03 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"Thread B\" #12 prio=5 os_prio=31 tid=0x00007fcd67815000 nid=0x5803 waiting for monitor entry [0x0000700006488000] java.lang.Thread.State: BLOCKED (on object monitor) at com.xiong.interview.HoldLockThread.run(DeadLockDemo.java:24) - waiting to lock &lt;0x000000076ac374a8&gt; (a java.lang.Object) - locked &lt;0x000000076ac374b8&gt; (a java.lang.Object) at com.xiong.interview.DeadLockDemo.lambda$main$1(DeadLockDemo.java:34) at com.xiong.interview.DeadLockDemo$$Lambda$2/1595428806.run(Unknown Source) at java.lang.Thread.run(Thread.java:748)\"Thread A\" #11 prio=5 os_prio=31 tid=0x00007fcd638b8000 nid=0x5703 waiting for monitor entry [0x0000700006385000] java.lang.Thread.State: BLOCKED (on object monitor) at com.xiong.interview.HoldLockThread.run(DeadLockDemo.java:24) - waiting to lock &lt;0x000000076ac374b8&gt; (a java.lang.Object) - locked &lt;0x000000076ac374a8&gt; (a java.lang.Object) at com.xiong.interview.DeadLockDemo.lambda$main$0(DeadLockDemo.java:33) at com.xiong.interview.DeadLockDemo$$Lambda$1/2065951873.run(Unknown Source) at java.lang.Thread.run(Thread.java:748)\"Service Thread\" #10 daemon prio=9 os_prio=31 tid=0x00007fcd64056800 nid=0xa903 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"C1 CompilerThread3\" #9 daemon prio=9 os_prio=31 tid=0x00007fcd66057800 nid=0x4203 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"C2 CompilerThread2\" #8 daemon prio=9 os_prio=31 tid=0x00007fcd65052800 nid=0x4303 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"C2 CompilerThread1\" #7 daemon prio=9 os_prio=31 tid=0x00007fcd63843000 nid=0x4403 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"C2 CompilerThread0\" #6 daemon prio=9 os_prio=31 tid=0x00007fcd6604e800 nid=0x4603 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"Monitor Ctrl-Break\" #5 daemon prio=5 os_prio=31 tid=0x00007fcd6780e800 nid=0x3f03 runnable [0x0000700005c70000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284) at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326) at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178) - locked &lt;0x000000076ac839b8&gt; (a java.io.InputStreamReader) at java.io.InputStreamReader.read(InputStreamReader.java:184) at java.io.BufferedReader.fill(BufferedReader.java:161) at java.io.BufferedReader.readLine(BufferedReader.java:324) - locked &lt;0x000000076ac839b8&gt; (a java.io.InputStreamReader) at java.io.BufferedReader.readLine(BufferedReader.java:389) at com.intellij.rt.execution.application.AppMainV2$1.run(AppMainV2.java:61)\"Signal Dispatcher\" #4 daemon prio=9 os_prio=31 tid=0x00007fcd6701b000 nid=0x4803 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE\"Finalizer\" #3 daemon prio=8 os_prio=31 tid=0x00007fcd65813800 nid=0x3103 in Object.wait() [0x0000700005964000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x000000076ab08ee0&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144) - locked &lt;0x000000076ab08ee0&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)\"Reference Handler\" #2 daemon prio=10 os_prio=31 tid=0x00007fcd66813000 nid=0x2f03 in Object.wait() [0x0000700005861000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x000000076ab06c00&gt; (a java.lang.ref.Reference$Lock) at java.lang.Object.wait(Object.java:502) at java.lang.ref.Reference.tryHandlePending(Reference.java:191) - locked &lt;0x000000076ab06c00&gt; (a java.lang.ref.Reference$Lock) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)\"VM Thread\" os_prio=31 tid=0x00007fcd6580f000 nid=0x4f03 runnable\"GC task thread#0 (ParallelGC)\" os_prio=31 tid=0x00007fcd6500c800 nid=0x2007 runnable\"GC task thread#1 (ParallelGC)\" os_prio=31 tid=0x00007fcd6500d000 nid=0x1b03 runnable\"GC task thread#2 (ParallelGC)\" os_prio=31 tid=0x00007fcd6500e000 nid=0x1d03 runnable\"GC task thread#3 (ParallelGC)\" os_prio=31 tid=0x00007fcd65012800 nid=0x2a03 runnable\"GC task thread#4 (ParallelGC)\" os_prio=31 tid=0x00007fcd65013800 nid=0x2c03 runnable\"GC task thread#5 (ParallelGC)\" os_prio=31 tid=0x00007fcd65014000 nid=0x5303 runnable\"GC task thread#6 (ParallelGC)\" os_prio=31 tid=0x00007fcd65014800 nid=0x2e03 runnable\"GC task thread#7 (ParallelGC)\" os_prio=31 tid=0x00007fcd65015000 nid=0x5103 runnable\"VM Periodic Task Thread\" os_prio=31 tid=0x00007fcd65053000 nid=0xa803 waiting on conditionJNI global references: 319Found one Java-level deadlock:=============================\"Thread B\": waiting to lock monitor 0x00007fcd6504d758 (object 0x000000076ac374a8, a java.lang.Object), which is held by \"Thread A\"\"Thread A\": waiting to lock monitor 0x00007fcd6504d6a8 (object 0x000000076ac374b8, a java.lang.Object), which is held by \"Thread B\"Java stack information for the threads listed above:===================================================\"Thread B\": at com.xiong.interview.HoldLockThread.run(DeadLockDemo.java:24) - waiting to lock &lt;0x000000076ac374a8&gt; (a java.lang.Object) - locked &lt;0x000000076ac374b8&gt; (a java.lang.Object) at com.xiong.interview.DeadLockDemo.lambda$main$1(DeadLockDemo.java:34) at com.xiong.interview.DeadLockDemo$$Lambda$2/1595428806.run(Unknown Source) at java.lang.Thread.run(Thread.java:748)\"Thread A\": at com.xiong.interview.HoldLockThread.run(DeadLockDemo.java:24) - waiting to lock &lt;0x000000076ac374b8&gt; (a java.lang.Object) - locked &lt;0x000000076ac374a8&gt; (a java.lang.Object) at com.xiong.interview.DeadLockDemo.lambda$main$0(DeadLockDemo.java:33) at com.xiong.interview.DeadLockDemo$$Lambda$1/2065951873.run(Unknown Source) at java.lang.Thread.run(Thread.java:748)Found 1 deadlock. 说明：查看最后一行，看到Found 1 deadlock，即存在一个死锁，上面的部分信息指示了出现问题的文件和行数","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"}]},{"title":"Nginx","slug":"Middleware/Nginx","date":"2020-09-06T03:39:08.000Z","updated":"2020-10-18T12:43:35.696Z","comments":true,"path":"2020/09/06/Middleware/Nginx/","link":"","permalink":"https://sobxiong.github.io/2020/09/06/Middleware/Nginx/","excerpt":"内容 Nginx简介 Nginx安装 Nginx常用的命令和配置文件 Nginx配置实例-反向代理 Nginx配置实例-负载均衡 Nginx配置实例-动静分离 Nginx搭建高可用集群 Nginx原理简述","text":"内容 Nginx简介 Nginx安装 Nginx常用的命令和配置文件 Nginx配置实例-反向代理 Nginx配置实例-负载均衡 Nginx配置实例-动静分离 Nginx搭建高可用集群 Nginx原理简述 Nginx简介 Nginx概述：是一个高性能的HTTP和反向代理服务器,特点是占有内存少，并发能力强，事实上Nginx的并发能力确实在同类型的网页服务器中表现较好 Nginx的作用： 作为Web服务器：Nginx可以作为静态页面的web服务器，同时还支持CGI协议的动态语言，比如perl、php等。但是不支持java。Java程序只能通过与tomcat配合完成 正向代理：如果把局域网外的Internet想象成一个巨大的资源库，则局域网中的客户端要访问Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理(需要设置代理地址) 反向代理：客户端对代理是无感知的，因为客户端不需要任何配置就可以访问。我们只需要将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后再返回给客户端。此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器地址，隐藏了真实服务器IP地址 负载均衡：单个服务器解决不了并发需求，可以增加服务器的数量，然后将请求分发到各个服务器上，将原先请求集中到单个服务器上的情况变为将请求分发到多个服务器上，将负载分发到不同的服务器。这就是所说的负载均衡 动静分离：为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速度。降低原来单个服务器的压力 Nginx安装 进入Nginx官网http://nginx.org/的download板块下载 具体安装Nginx 安装所需的第三方库：pcre、openssl、zlib 1yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 安装nginx 1234567# 解压缩nginx的tar.gz包tar -zxvf nginx-1.18.0.tar.gz -C /opt/module# 进入目录,执行./configurecd /opt/module/nginx-1.18.0sudo ./configure# make安装sudo make &amp;&amp; make install 测试(注意Linux的防火墙设置) 123456# 进入/usr/local/nginx/sbin目录cd /usr/local/nginx/sbin# 启动nginxsudo ./nginx# 输入当前虚拟机的地址的80端口,查看是否能看到Welcome to nginx# http://sobxiong.com Nginx常用的命令和配置文件 Nginx常用的命令(当前路径:/usr/local/nginx/sbin) 查看nginx版本号：./nginx -v 启动命令：./nginx 关闭命令：./nginx -s stop 重新加载命令(重新加载配置文件)：./nginx - s reload nginx.conf配置文件 介绍：Nginx默认的配置文件都放在主目录下的conf目录，主配置文件nginx.conf也在其中，后续对nginx的使用基本上都是对此配置文件进行相应的修改 配置文件示例： 1234567891011121314151617181920212223242526272829worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application&#x2F;octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name localhost; location &#x2F; &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 &#x2F;50x.html; location &#x3D; &#x2F;50x.html &#123; root html; &#125; &#125;&#125; 配置文件解释：根据上述文件，可以明显地将nginx.conf配置文件分为三部分 全局块： 123# Nginx服务器并发处理服务的关键配置,worker_processes值越大,可以支持的并发处理量也越多# 但是会受到硬件、软件等设备的制约,一般设置为当前计算机的CPU核心数worker_processes 1; 从配置文件开始到events块之间的内容，主要会设置一些影响nginx服务器整体运行的配置指令，主要包括配置运行Nginx服务器的用户(组)、允许生成的worker process数、进程pid存放路径、日志存放路径和类型以及配置文件的引入等 events块： 1234events &#123; # 每个work process支持的最大连接数为1024 worker_connections 1024;&#125; events块涉及的指令主要影响Nginx服务器与用户的网络连接。常用的设置包括是否开启对多work process下的网络连接进行序列化、是否允许同时接收多个网络连接、选取哪种事件驱动模型来处理连接请求、每个work process可以同时支持的最大连接数等。这部分的配置对Nginx的性能影响较大，在实际中应该灵活配置 http块： 1234567891011121314151617181920212223http &#123; include mime.types; default_type application&#x2F;octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name localhost; location &#x2F; &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 &#x2F;50x.html; location &#x3D; &#x2F;50x.html &#123; root html; &#125; &#125;&#125; 这算是Nginx服务器配置中修改最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里http块还可以包括http全局块、server块 http全局块：配置的指令包括文件引入、MIME-TYPE定义、日志自定义、连接超时时间、单链接请求数上限等 server块：server块和虚拟主机有密切关系。虚拟主机从用户角度看和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器硬件成本每个http块可以包括多个server块，而每个server块就相当于一个虚拟主机而每个server块也分为全局server块以及可以同时包含多个locaton块 全局server块：最常见的配置是本虚拟机主机的监听配置和本虚拟主机的名称或IP配置 location块：一个server块可以配置多个location块该块的主要作用是基于Nginx服务器接收到的请求字符串(例如server_name/uri-string)，对虚拟主机名称(也可以是IP别名)之外的字符串(例如前面的/uri-string)进行匹配，对特定的请求进行处理。地址定向、数据缓存和应答控制等功能，还有许多第三方模块的配置也在这里进行 Nginx配置实例-反向代理 反向代理实例1 最终需求：使用nginx反向代理，访问 www.123.com 跳转到虚拟机的8080端口 实现步骤： 测试端口8080准备： 虚拟机安装Tomcat并启动(启动命令bin/startup.sh) 在mac中通过浏览器访问虚拟机Tomcat主页：http://172.16.85.201:8080 修改host文件：在mac中修改hosts文件，将 www.123.com 映射到虚拟机地址172.16.85.201。此时可以通过 www.123.com:8080 访问到测试端口 修改Nginx配置： 12345678server &#123; listen 80; server_name 172.16.85.201; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:8080; &#125;&#125; 最终测试 反向代理实例2 最终需求：使用Nginx反向代理，根据访问的路径跳转到不同端口的服务中。其中Nginx监听端口为 9001，要求访问http://172.16.85.201:9001/edu/直接跳转到172.16.85.201:8080，访问http://172.16.85.201:9001/vod/直接跳转到172.16.85.201:8081 实现步骤： 准备两个Tomcat服务器，一个8080端口，一个8081端口；创建文件夹和测试页面(在8080的Tomcat目录下创建edu目录，其内创建一个a.html测试页面;同理在8081的Tomcat目录下创建vod目录，其内创建一个b.html测试页面) 修改Nginx配置： 123456789101112server &#123; listen 9001; server_name 172.16.85.201; location ~ &#x2F;edu&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:8080; &#125; location ~ &#x2F;vod&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:8081; &#125;&#125; 分别测试网址&lt;172.16.85.201:9001/edu/a.html&gt;和&lt;172.16.85.201:9001/vod/b.html&gt;，验证结果 Location指令说明 用途：用于匹配URL 语法： 123location [ &#x3D; | ~ | ~* | ^~] uri &#123;&#125; 参数说明： = ：用于不含正则表达式的uri前，要求请求字符串与uri严格匹配，如果匹配成功，就停止继续向下搜索并立即处理该请求 ~：用于表示uri包含正则表达式，并且区分大小写 ~*：用于表示uri包含正则表达式，并且不区分大小写 ^~：用于不含正则表达式的uri前，要求Nginx服务器找到标识uri和请求字符串匹配度最高的 location后，立即使用此location处理请求，而不再使用location块中的正则uri和请求字符串做匹配 如果uri包含正则表达式，则必须要有或者*标识 Nginx配置实例-负载均衡 负载均衡示例 最终效果：浏览器访问http://172.16.85.201/edu.a.html，能够将请求负载均衡到8080端口和8081端口 准备工作： 准备一台虚拟机，装上两个Tomcat服务器，端口为8080和8081 在两个Tomcat服务器的webapps目录中，创建edu文件夹和其中的a.html用于测试 在Nginx的配置文件中进行负载均衡的配置 1234567891011121314upstream myserver&#123; server 172.16.85.201:8080; server 172.16.85.201:8081;&#125;server &#123; listen 80; server_name 172.16.85.201; location / &#123; proxy_pass http://myserver; root html; index index.html index.htm;&#125; Nginx分配服务器策略 轮询(默认)：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除 weight(权重)：weight代表权重，默认为1，权重越高被分配的客户端越多 1234upstream myserver&#123; server 172.16.85.201:8080 weight=10; server 172.16.85.201:8081 weight=20;&#125; ip_hash(适用于session)：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器 12345upstream myserver&#123; ip_hash; server 172.16.85.201:8080; server 172.16.85.201:8081;&#125; fair(第三方,公平)：按后端服务器的响应时间来分配请求，响应时间短的优先分配 12345upstream myserver&#123; server 172.16.85.201:8080; server 172.16.85.201:8081; fair;&#125; Nginx配置实例-动静分离 基本介绍：Nginx动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用Nginx处理静态页面，Tomcat处理动态页面。动静分离从目前实现角度来讲大致分为两种：一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案；另外一种方法就是动态跟静态文件混合在一起发布，通过nginx来分开 动静分离示例 准备工作：在虚拟机linux系统的本地文件系统中准备静态资源，用于进行访问 Nginx进行动静分离的配置 123456789101112131415server &#123; listen 80; server_name 172.16.85.201; location /text/ &#123; root /opt/data/; index index.html index.htm; &#125; location /image/ &#123; root /opt/data/; # 开启该配置后,访问/image/页面会展示当前目录下的文件基本信息列表 autoindex on; &#125;&#125; Nginx搭建高可用集群 准备工作： 准备两台Linux虚拟机，各自都装上Nginx 在两台服务器安装keepalived：yum install keepalived -y 修改/etc/keepalived下的keepalived.conf配置： 1234567891011121314151617181920212223242526272829303132333435363738global_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 172.16.85.201 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;vrrp_script chk_http_port &#123; script \"/opt/data/nginx_check.sh\" #(检测脚本执行的间隔) interval 2 weight 2&#125;vrrp_instance VI_1 &#123; # 备份服务器上将MASTER改为BACKUP state MASTER # 网卡 interface ens33 # 主、备机的virtual_router_id必须相同 virtual_router_id 51 # 主、备机取不同的优先级,主机值较大,备份机值较小 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; # VRRP H 虚拟地址 172.16.85.250 &#125;&#125; 在/opt/data下添加检测脚本文件nginx_check.sh： 123456789#!/bin/bashA=`ps -C nginx –no-header |wc -l`if [ $A -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then killall keepalived fifi 启动两台Linux虚拟机上的nginx和keepalived 1234# 启动nginx(/usr/local/nginx/sbin)./nginx# 启动keepalived服务systemctl start keepalived.service 输入ifconfig可以查看到虚拟ip——172.16.85.250 测试 在主机上访问172.16.85.250，nginx主页显示正常 把主服务器(172.16.85.250)的nginx和keepalived停止 再次访问172.16.85.250，主页依旧正常 Nginx原理简述 Nginx主要采用master-worker模式 一个master和多个worker的好处： 可以使用nginx –s reload热部署 每个worker是独立的进程，如果有其中的一个worker出现问题，其他worker可继续进行争抢，实现请求过程，不会造成服务中断 worker个数：和服务器cpu数相等 发送请求，占用了多少worker的连接数：2/4 Nginx有一个master，有四个worker，每个worker支持最大的连接数1024，那么支持的最大并发数是多少?普通的静态访问最大并发数是worker_connections * worker_processes / 2；如果作为反向代理，最大并发数量应该是worker_connections * worker_processes / 4","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Middleware","slug":"Middleware","permalink":"https://sobxiong.github.io/tags/Middleware/"}]},{"title":"Spark","slug":"BigData/Spark","date":"2020-09-04T04:22:21.000Z","updated":"2020-10-08T15:33:33.766Z","comments":true,"path":"2020/09/04/BigData/Spark/","link":"","permalink":"https://sobxiong.github.io/2020/09/04/BigData/Spark/","excerpt":"内容 Spark概述 Spark快速上手 Spark运行环境 Spark核心编程","text":"内容 Spark概述 Spark快速上手 Spark运行环境 Spark核心编程 Spark概述 Spark是什么：一种基于内存的快速、通用、可扩展的大数据分析计算引擎(unified analytics engine for large-scale data processing) Spark And Hadoop 从时间节点上看： Hadoop 2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发 2008年1月，Hadoop成为Apache顶级项目 2011年1.0正式发布 2012年3月稳定版发布 2013年10月发布2.X(Yarn)版本 Spark 2009年，Spark诞生于伯克利大学的AMPLab实验室 2010年，伯克利大学正式开源了Spark项目 2013年6月，Spark成为了Apache基金会下的项目 2014年2月，Spark以飞快的速度成为了Apache的顶级项目 2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark 从功能上看： Hadoop Hadoop是由Java编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架 作为Hadoop分布式文件系统，HDFS处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的The Google File System这篇论文，是GFS的开源实现 MapReduce是一种编程模型，Hadoop根据Google的MapReduce论文将其实现。作为Hadoop的分布式计算模型，MapReduce是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易 HBase是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop中非常重要的组件 Spark Spark是一种由Scala开发的快速、通用、可扩展的大数据分析引擎 Spark Core中提供了Spark最基础与最核心的功能 Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据 Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API 综合以上，Spark出现的时间相对较晚，并且主要功能主要是用于数据计算。因此Spark一直被认为是Hadoop MapReduce框架的升级版 Spark Or Hadoop Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景(如：机器学习、图挖掘算法、交互式数据挖掘算法)中存在诸多计算效率等问题。因此Spark应运而生，Spark就是在传统的MapReduce计算框架的基础上，优化其计算过程，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型 机器学习中ALS、凸优化梯度下降等都需要基于数据集或者数据集的衍生数据反复查询、反复操作。MR模式不太合适，即使多MR串行处理，性能和时间也是一个问题，而且数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的Scala语言恰恰擅长函数的处理 Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集(Resilient Distributed Datasets)，它提供了比MapReduce更丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法 Spark和Hadoop的根本差异是多个作业之间的数据通信问题：Spark多个作业之间的数据通信是基于内存的，而Hadoop是基于磁盘的 Spark Task的启动时间快。Spark采用fork线程的方式，而Hadoop采用创建新的进程的方式 Spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都要依赖于磁盘交互 Spark的缓存机制比HDFS的缓存机制高效 综上所述，在绝大多数的数据计算场景中，Spark确实会比MapReduce更有优势。但Spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够而导致Job执行失败，此时MapReduce是一个更好的选择，所以Spark并不能完全替代MR Spark核心模块 Spark Core：Spark Core中提供了Spark最基础与最核心的功能。Spark其他的功能如Spark SQL、Spark Streaming、GraphX以及MLlib都是在Spark Core的基础上进行扩展的 Spark SQL：Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据 Spark Streaming：Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API Spark MLlib：MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语 Spark GraphX：GraphX是Spark面向图计算提供的框架与算法库 Spark快速上手 在IDEA上初体验Spark API 创建Maven项目(IDEA中最简单的maven项目,不采用任何模版项目) IDEA安装Scala插件 pom添加依赖关系 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- 该插件用于将Scala代码编译成class文件 --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;!-- 声明绑定到maven的compile阶段 --&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; WordCount案例 配置log4j日志输出(过滤Spark框架的执行日志)——在resource目录下创建log4j.properties文件 12345678910111213141516171819log4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n# Set the default spark-shell log level to ERROR. When running the spark-shell, the# log level for this class is used to overwrite the root logger's log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=ERROR# Settings to quiet third party logs that are too verboselog4j.logger.org.spark_project.jetty=ERRORlog4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERRORlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERRORlog4j.logger.org.apache.parquet=ERRORlog4j.logger.parquet=ERROR# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive supportlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATALlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR 案例代码： 12345678910111213141516171819202122232425262728293031323334353637383940object Spark02_WordCount &#123; def main(args: Array[String]): Unit = &#123; // Spark是一个计算框架 // 开发人员使用Spark框架的Api实现计算 // 1、准备Spark环境 // setMaster：设定Spark环境的位置 val sparkConfig = new SparkConf() .setMaster(\"local\") .setAppName(\"wordCount\") // 2、建立和Spark的连接 // jdbc：connection val sparkContext = new SparkContext(sparkConfig) // 3、实现业务操作 // 3.1、读取指定目录下的数据文件(多个) // 参数path可以指向单一的文件/文件目录 // RDD: 更适合并行计算的数据模型 val fileRDD: RDD[String] = sparkContext.textFile(path = \"./src/main/resources/input\") // 3.2、将读取的内容进行扁平化操作,切分单词 val wordRDD: RDD[String] = fileRDD.flatMap(_.split(\" \")) // 3.3、将分词后的数据进行结构的转换 // word -&gt; (word,1) val mapRDD: RDD[(String, Int)] = wordRDD.map(word =&gt; (word, 1)) // 3.4、将转换结构后的数据根据单词进行分组聚合 // reduceByKey: 根据数据key进行分组,然后对value进行统计聚合 val wordSumRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_ + _) // 打印 val wordCountArray: Array[(String, Int)] = wordSumRDD.collect() println(wordCountArray.mkString(\",\")) // 4、释放连接 sparkContext.stop() &#125;&#125; Spark运行环境 基本介绍：Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行，在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来 Local本地模式 介绍：所谓的Local模式就是不需要其他任何节点资源就可以在本地执行Spark代码的环境，一般用于教学、调试、演示等。而之前在IDEA中运行代码的环境我们称之为开发环境 环境准备： 解压缩spark文件 引入hadoop等Jar包 启动Local环境 进入解压缩的目录，执行：bin/spark-shell –master local[*] 启动后，可以使用当前主机的4040端口进行Web UI监控 命令行工具 准备：在spark根目录下的data目录中，添加word.txt文件，准备一些英文单词 在Local环境中输入 1sc.textFile(\"data/word.txt\").flatMap(_.split(\"\")).map((_,1)).reduceByKey(_+_).collect 回车后会实时输出结果，sc是Spark Context的简写，该变量由命令行工具提供 退出：:quit(Scala)或者Ctrl + C 提交应用 12345bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master local[2] \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 提交应用参数解释 –class：表示要执行程序的主类 –master local[2]：部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量 spark-examples_2.12-2.4.5.jar：运行的应用类所在的jar包 10：表示程序的入口参数，用于设定当前应用的任务数量 Standalone模式 介绍：local本地模式只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行。Spark自身节点运行的集群模式叫做独立部署(Standalone)模式。Spark的Standalone模式体现了经典的master-slave模式 集群规划： hadoop101 hadoop102 hadoop103 Worker Master Worker Worker 环境准备 解压缩spark文件 引入hadoop等Jar包 修改配置(conf目录) 修改slaves.template文件名改为slaves 修改slaves文件，添加work节点：hadoop101、hadoop102、hadoop103(用回车分割,不能其他空格空行) 修改spark-env.sh.template文件名为spark-env.sh 修改spark-env.sh文件： 123export JAVA_HOME=/opt/module/jdk1.8.0_251SPARK_MASTER_HOST=hadoop101SPARK_MASTER_PORT=7077 分发配置文件：xsync spark-env.sh 启动集群 执行脚本命令：sbin/start-all.sh 查看Master资源监控Web UI界面：http://hadoop101:8080 提交应用(–master spark://hadoop101:7077：独立部署模式,连接到Spark集群) 12345bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master spark://hadoop101:7077 \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 提交参数说明 123456bin/spark-submit \\--class &lt;main-class&gt;--master &lt;master-url&gt; \\... # other options&lt;application-jar&gt; \\[application-arguments] 参数 解释 可选值举例 –class Spark程序中包含主函数的类 / –master Spark程序运行的模式 local模式(local[*])、standalone模式(spark://hadoop101:7077)、Yarn模式(Yarn) –executor-memory 1G 指定每个executor可用内存为1G 符合集群内存配置即可，具体情况具体分析 –total-executor-cores 2 指定所有executor使用的cpu核数为2个 同上 –executor-cores 指定每个executor使用的cpu核数 同上 application-jar 打包好的应用jar(包含依赖)。该URL在集群中全局可见。比如hdfs://共享存储系统；如果是file://path，那么所有的节点的path都要包含同样的jar 同上 application-arguments 传给main()方法的参数 同上 配置历史服务器 介绍：由于spark-shell停止或spark任务结束后，集群监控的4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况 具体配置步骤： 修改spark-defaults.conf.template文件名为spark-defaults.conf 修改spark-default.conf文件： 1234# 设置开启日志记录spark.eventLog.enabled true# 设置日志存储路径spark.eventLog.dir hdfs://hadoop101:9000/spark_log 启动hadoop集群，hdfs上的spark_log目录需要提前存在 12sbin/start-dfs.shhadoop dfs -mkdir /spark_log 修改spark-env.sh文件，添加日志配置： 12345678# 设置历史日志选项# 参数1：Web UI访问端口号# 参数2：指定历史服务器日志存储路径# 参数3: 指定保存Application历史记录的个数,如果超过这个值,旧的应用程序信息将被删除(是内存中的应用数,而不是页面上显示的应用数)export SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=18080-Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/spark_log-Dspark.history.retainedApplications=30\" 分发配置文件：xsync conf 重启集群和历史服务 12sbin/start-all.shsbin/start-history-server.sh 重新执行任务 查看历史服务情况：http://hadoop101:18080 配置高可用(HA) 介绍：所谓的高可用是因为当前集群中的Master节点只有一个，因此会存在单点故障问题。为了解决单点故障问题，需要在集群中配置多个Master节点，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置 集群规划： hadoop101 hadoop102 hadoop103 Master、Zookeeper、Worker Master、Zookeeper、Worker Zookeeper、Worker 停止集群：sbin/stop-all.sh 启动Zookeeper：bin/zkServer.sh start 修改spark-env.sh配置： 12345678910# 注释master的host和port,不能把master固定# SPARK_MASTER_HOST=hadoop101# SPARK_MASTER_PORT=7077SPARK_MASTER_WEBUI_PORT=8989# 设置zookeeper配置export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER-Dspark.deploy.zookeeper.url=hadoop101,hadoop102,hadoop103-Dspark.deploy.zookeeper.dir=/spark\" 分发配置文件：xsync spark-env.sh 重启集群：sbin/start-all.sh 启动hadoop102的单独master节点(使hadoop102节点的master状态处于备用状态)：sbin/start-master.sh 提交应用到高可用集群： 123456bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master spark://hadoop101:7077,hadoop102:7077 \\--deploy-mode cluster \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 停止hadoop101的master进程：kill -9 xxx(Master的进程号) 查看hadoop102的Master资源监控Web UI(8989端口)，经过一段时间，hadoop102节点master状态提升为活动状态 Yarn模式 基本介绍：独立部署(Standalone)模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是Spark主要是计算框架，而不是资源调度框架，所以资源调度并不是它的强项，因此还是和其他专业的资源调度框架集成会更靠谱一些。其中，在国内工作中，Yarn使用的非常多 环境准备 解压缩spark文件 引入hadoop等Jar包 修改配置文件 hadoop的配置文件yarn-site.xml，并分发 1234567891011121314151617&lt;!-- 是否启动一个线程检查每个任务正使用的物理内存量,如果任务超出分配值, 则直接将其杀掉,默认是true--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;!-- 是否启动一个线程检查每个任务正使用的虚拟内存量, 如果任务超出分配值,则直接将其杀掉,默认是true--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; spark的配置文件spark-env.sh，并分发 123export JAVA_HOME=/opt/module/jdk1.8.0_251# 设置yarn配置目录YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop 启动HDFS和YARN集群 提交应用 12345bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\./examples/jars/spark-examples_2.12-2.4.5.jar \\10 之后便可以在hadoop102节点的8088的Web UI上查看到跑的Spark应用 配置历史服务器 参照Standalone模式的spark-env.sh配置 修改spark-defaults.conf配置 12spark.yarn.historyServer.address=hadoop101:18080spark.history.ui.port=18080 k8s以及Mesos模式 Mesos介绍：Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核，在Twitter得到广泛使用，管理着Twitter超过300000台服务器上的应用部署。但国内依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但原理其实都差不多 k8s模式：容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes(k8s)，Spark也在最近的版本中支持了k8s部署模式。具体介绍网址如下：https://spark.apache.org/docs/latest/running-on-kubernetes.html 部署模式对比 模式 机器数 需启动的进程 应用场景 Local 1 无 Spark Standalone 3 Master及Worker 单独部署 Yarn 1 Yarn以及HDFS 混合部署 端口号总结 Spark查看当前Spark-shell运行任务情况端口号：4040(计算) Spark Master内部通信服务端口号：7077 Standalone模式下，Spark Master Web端口号：8080(资源) Spark历史服务器端口号：18080 Hadoop YARN任务运行情况查看端口号：8088 Spark核心编程 基本介绍：Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是： RDD : 弹性分布式数据集 累加器：分布式共享只写变量 广播变量：分布式共享只读变量 RDD 基本介绍：RDD(Resilient Distributed Dataset)弹性分布式数据集，是Spark中最基本的数据处理模型。Scala代码中是一个抽象类，它代表一个弹性的、不可变、可分区并且其中元素可并行计算的集合 重点： 弹性： 存储的弹性：内存与磁盘的自动切换 容错的弹性：数据丢失可以自动恢复 计算的弹性：计算出错重试机制 分片的弹性：可根据需要重新分片 分布式：数据存储在大数据集群(hadoop的HDFS集群)不同节点上 数据集：RDD封装了计算逻辑，并不保存数据 数据抽象：RDD是一个抽象类，需要子类具体实现 不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑 可分区、并行计算 核心属性 分区列表：RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性 1protected def getPartitions: Array[Partition] 分区计算函数：Spark在计算时，是使用分区函数对每一个分区进行计算 12@DeveloperApidef compute(split: Partition, context: TaskContext): Iterator[T] RDD之间的依赖关系：RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系 1protected def getDependencies: Seq[Dependency[_]] = deps 分区器(可选)：当数据为KV类型数据时，可以通过设定分区器自定义数据的分区 1@transient val partitioner: Option[Partitioner] = None 首选位置(可选)：计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算 1protected def getPreferredLocations(split: Partition): Seq[String] = Nil 执行原理从计算的角度来讲，数据处理过程中需要计算资源(内存 &amp; CPU)和计算模型(逻辑)。执行时，需要将计算资源和计算模型进行协调和整合Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上，按照指定的计算模型进行数据计算。最后得到计算结果RDD是Spark框架中用于数据处理的核心模型在Yarn环境中，RDD的工作原理： 启动Yarn集群环境 Spark通过申请资源创建调度节点和计算节点 Spark框架根据需求将计算逻辑根据分区划分成不同的任务 调度节点将任务根据计算节点状态发送到对应的计算节点进行计算RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算 基础编程 RDD创建 从集合(内存)中创建RDD：两个方法parallelize()和makeRDD()，其中后者只是包装了前者 1234567891011val sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从内存中创建RDD// 1、parallelize:并行val list = List(1, 2, 3, 4)val rdd: RDD[Int] = sparkContext.parallelize(list)println(rdd.collect().mkString(\",\"))// makeRDD底层代码就是调用了parallelize,只是为了方便理解val rdd1: RDD[Int] = sparkContext.makeRDD(list)println(rdd1.collect().mkString(\",\"))sparkContext.stop() 从外部存储(文件)创建RDD：包括本地文件系统、所有Hadoop支持的数据集(HDFS、HBase等) 1234567891011121314151617181920212223val sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从磁盘File中创建RDD// path：读取文件(目录)的路径// 相对路径,如果是IDEA,那么是从项目根开始查找// path路径根据环境的不同自动发生改变// Spark读取文件时,默认采用Hadoop读取文件的规则——一行一行读取// 指向文件目录,目录的文本文件都会被读取// 读取目录// val fileRDD: RDD[String] = sparkContext.textFile(path = \"input\")// 读取指定文件// val fileRDD: RDD[String] = sparkContext.textFile(path = \"input/w.txt\")// 读取通配符文件val fileRDD: RDD[String] = sparkContext.textFile(path = \"input/word*.txt\")// 文件路径还可以指向第三方存储系统：HDFS// val fileRDD: RDD[String] = sparkContext.textFile(path = \"hdfs://input/word*.txt\")println(fileRDD.collect().mkString(\",\"))sparkContext.stop() 从其他RDD创建：通过一个RDD运算完后，再产生新的RDD 直接创建RDD(new)：使用new的方式直接构造RDD，一般由Spark框架自身使用 RDD并行度与分区： 基本介绍：默认情况下，Spark可以将一个作业切分成多个任务(Task)后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。该数量可以在构建RDD时指定。这里的并行执行的任务数量并不是指的切分任务的数量，不要混淆了 案例： 内存分区案例1： 12345678910111213141516171819202122val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从内存中创建RDD// makeRDD// 参数1：seq: Seq[T] 数据源// 参数2：numSlices: Int = defaultParallelism(默认并行度——分区的数量)// 简单总结：RDD中分区的数量就是并行度,设定并行度就是在设定分区数量// scheduler.conf.getInt(\"spark.default.parallelism\", totalCores)// 并行度默认会从Spark配置信息中获取spark.default.parallelism的值// 如果获取不到指定参数,会采用默认值totalCores——机器的总核数// 机器总核数= 当前环境中可用核数// local -&gt; 单核(单线程) -&gt; 1// local[4] -&gt; 4核(4个线程) -&gt; 4// local[*] -&gt; 当前最大核数 -&gt; 8val rdd = sparkContext.makeRDD(List(1, 2, 3, 4))// println(rdd.collect().mkString(\",\"))// 将RDD的处理后的数据保存到分区文件中rdd.saveAsTextFile(\"output\")sparkContext.stop() 内存分区案例2： 12345678910111213141516171819202122val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 12,34// 内存中的集合按照平均分的方式进行分区处理// val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)// rdd.saveAsTextFile(\"output1\")// 1234// 1,2,34// 12345// 1,23,45// saveAsTextFile方法如果文件已存在,会发生错误// 内存中数据的分区基本上就是平均分,如果不能整除,会采用一个基本的算法实现分配val rdd1 = sparkContext.makeRDD(List(1, 2, 3, 4,5), 3)rdd1.saveAsTextFile(\"output2\")// 1,2,3,4// val rdd2 = sparkContext.makeRDD(List(1, 2, 3, 4), 4)// rdd2.saveAsTextFile(\"output3\")sparkContext.stop() 文件分区案例1： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// textFile// 参数1 path：读取文件的路径// 参数2 minPartitions：最小分区数量// minPartitions默认值为math.min(defaultParallelism, 2)// 其中defaultParallelism是totalCores// val fileRDD1 = sparkContext.textFile(\"input/w.txt\")// fileRDD1.saveAsTextFile(\"output\")// val fileRDD2 = sparkContext.textFile(\"input/w.txt\", 1)// fileRDD2.saveAsTextFile(\"output2\")// 1、Spark读取文件采用的是Hadoop的读取规则// 文件切片规则：以字节方式来切片// 数据读取规则：以行为单位来读取// 2、问题：// 文件到底切成几片(分区的数量)// 文件字节数,预计切片数量(2)// 所谓的最小分区数,取决于总的字节数是否能整除分区数并且剩余的字节小于一定比率(10%,hadoop方式)// 实际产生的分区数量可能大于最小分区数val fileRDD1 = sparkContext.textFile(\"input/w.txt\", 2)fileRDD1.saveAsTextFile(\"output3\")// 分区的数据如何存储?// 分区数据是以行为单位读取的,不是以字节// 数据是以行的方式读取,但是会考虑偏移量(数据的offset)的设置// 1@@ =&gt; 012// 2@@ =&gt; 345// 3@@ =&gt; 678// 4 =&gt; 9// 10 byte / 4 = 2 .... 2 =&gt; 5// 以行为单位...// 以下左右都是闭区间(取得到)// 0 =&gt; (0, 2) =&gt; 1// 1 =&gt; (2, 4) =&gt; 2// 2 =&gt; (4, 6) =&gt; 3// 3 =&gt; (6, 8) =&gt;// 4 =&gt; (8,10) =&gt; 4// val fileRDD3 = sparkContext.textFile(\"input/w.txt\", 4)// fileRDD3.saveAsTextFile(\"output3\")//// val fileRDD4 = sparkContext.textFile(\"input/w.txt\", 3)// fileRDD4.saveAsTextFile(\"output4\")sparkContext.stop 文件分区案例2： 12345678910111213141516171819val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 6 / 2 = 3// (0 , 0 + 3)// (3 , 3 + 3)// 1@@ =&gt; 012// 234 =&gt; 345// hadoop分区是以文件为单位进行划分的// 读取数据不能跨越文件// 10 / 3 = 3 ... 1 =&gt; 4// (0,3) (3,6)val fileRDD1 = sparkContext.textFile(\"input\", 3)fileRDD1.saveAsTextFile(\"output\")sparkContext.stop 分区原理 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下： 1234567def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = &#123; (0 until numSlices).iterator.map &#123; i =&gt; val start = ((i * length) / numSlices).toInt val end = (((i + 1) * length) / numSlices).toInt (start, end) &#125;&#125; 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下： 123456789101112131415161718192021222324252627282930public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException &#123; long totalSize = 0; // compute total size for (FileStatus file: files) &#123; // check we have valid files if (file.isDirectory()) &#123; throw new IOException(\"Not a file: \"+ file.getPath()); &#125; totalSize += file.getLen(); &#125; long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits); long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input. FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize); // ... for (FileStatus file: files) &#123; // ... if (isSplitable(fs, path)) &#123; long blockSize = file.getBlockSize(); long splitSize = computeSplitSize(goalSize, minSize, blockSize); // ... &#125; &#125;&#125;protected long computeSplitSize(long goalSize, long minSize, long blockSize) &#123; return Math.max(minSize, Math.min(goalSize, blockSize));&#125; RDD转换算子：RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型 Value类型 map 函数签名： 1def map[U: ClassTag](f: T =&gt; U): RDD[U] 函数说明：将处理的数据逐条进行映射转换(可以是类型的转换,也可以是值的转换) 案例1： 123456789101112131415161718192021val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// Spark - RDD - 算子(方法)val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)// 旧RDD -&gt; 转换算子 -&gt; 新RDD// 转换算子能将旧的RDD通过方法转换为新的RDD,但是不会触发作业的执行// 分区问题// RDD中有分区列表// 默认分区数量不变,数据会转换后输出val rdd1 = rdd.map(_ * 2)// 读取数据// collect方法不会转换RDD,会触发作业的执行// 所以将collect这样的方法称之为行动(action)算子// println(rdd1.collect.mkString(\",\"))// rdd1.saveAsTextFile(\"output\")println(rdd1.collect.mkString(\",\"))sparkContext.stop 案例2： 123456789101112131415161718192021val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)val rdd1 = rdd.map(x =&gt; &#123; println(s\"Map 1st : $x\") x&#125;)val rdd2 = rdd1.map(x =&gt; &#123; println(s\"Map 2nd : $x\") x&#125;)// (1, 2) 1(1) 1(2) 2(1) 2(2)// (3, 4) 3(1) 3(2) 4(1) 4(2)// 分区内数据按照顺序依次执行,每一条数据的所有逻辑全部执行完毕后才会执行下一条数据// 分区间数据执行没有顺序,而且无需等待println(rdd2.collect.mkString(\",\"))sparkContext.stop 小功能：从服务器日志数据apache.log中获取用户请求URL资源路径 12345678910111213val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 从服务器日志数据apache.log中获取用户请求URL资源路径val fileRDD = sparkContext.textFile(\"input/apache.log\")val urlRDD = fileRDD.map( line =&gt; &#123; val datas = line.split(\" \") datas(6) &#125;)urlRDD.collect.foreach(println)sparkContext.stop mapPartitions 函数签名： 123def mapPartitions[U: ClassTag]( f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] 函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据) 案例： 1234567891011121314151617181920val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// mapPartitions// 以分区为单位进行计算,和map算子很相似// 区别就在于map算子是一个一个执行,mapPartitions是一个分区一个分区执行// 类似于批处理// map方法是全量数据操作,不能丢失数据// mapPartitions一次性获取分区的所有数据,那么可以执行迭代器集合的所有操作(filter、max、sum)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 2)// val rdd = dataRDD.mapPartitions(iter =&gt; &#123;// iter.map(_ * 2)// &#125;)// println(rdd.collect.mkString(\",\"))val rdd = dataRDD.mapPartitions(iter =&gt; &#123; iter.filter(_ % 2 == 0)&#125;)println(rdd.collect.mkString(\",\"))sparkContext.stop 小功能：获取每个数据分区的最大值 12345678val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 4, 3, 2, 5, 6), 3)// 获取每个数据分区的最大值val rdd = dataRDD.mapPartitions(iter =&gt; List(iter.max).iterator)println(rdd.collect.mkString(\",\"))sparkContext.stop mapPartitionsWithIndex 函数签名： 123def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] 函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据)，在处理时同时可以获取当前分区索引 案例： 123456789101112val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 获取每个分区最大值以及分区号val dataRDD = sparkContext.makeRDD(List(1, 3, 9, 6, 5, 4), 3)val rdd = dataRDD.mapPartitionsWithIndex( (index, iter) =&gt; &#123; List((index, iter.max)).iterator &#125;)println(rdd.collect.mkString(\",\"))sparkContext.stop 小功能：获取第二个数据分区的数据 123456789101112131415val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 获取第二个数据分区的数据val dataRDD = sparkContext.makeRDD(List(1, 3, 9, 6, 5, 4), 3)// 获取的分区索引是从0开始的val rdd = dataRDD.mapPartitionsWithIndex( (index, iter) =&gt; &#123; if (index == 1) iter else Nil.iterator &#125;)println(rdd.collect.mkString(\",\"))sparkContext.stop flatMap 函数签名： 1def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] 函数说明：将处理的数据进行扁平化后再进行映射处理，也称之为扁平映射 案例： 123456789101112val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD( List( List(1, 2), List(3, 4) ))val rdd = dataRDD.flatMap(list =&gt; list)println(rdd.collect.mkString(\",\"))sparkContext.stop 小功能：将List(List(1, 2), 3, List(4, 5))进行扁平化操作 123456789101112131415val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD( List( List(1, 2), 3, List(4, 5) ))val rdd = dataRDD.flatMap &#123; case list: List[_] =&gt; list case d =&gt; List(d)&#125;println(rdd.collect.mkString(\",\"))sparkContext.stop glom 函数签名： 1def glom(): RDD[Array[T]] 函数说明：将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变 案例： 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// glom =&gt; 将每个分组的数据在转换为数组val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 2)val valueRDD = dataRDD.glomvalueRDD.foreach(array =&gt; println(array.mkString(\",\")))sparkContext.stop 小功能：计算所有分区最大值求和(分区内取最大值,分区间最大值求和) 123456789101112131415161718val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// glom// 计算所有分区最大值求和(分区内取最大值,分区间最大值求和)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6), 2)// 将每个分区的数据转换为数组val glomRDD = dataRDD.glom// 将数组中的最大值取出// Array -&gt; maxval maxRDD = glomRDD.map(array =&gt; array.max)// 将取出的最大值求和val sum = maxRDD.collect.sumprintln(sum)sparkContext.stop groupBy 函数签名： 1def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] 函数说明：将数据根据指定的规则进行分组，分区默认不变，但数据会被打乱重新组合(将这样的操作称之为shuffle)。极限情况下，数据可能被分在同一个分区中。一个组的数据在一个分区中，但并不是说一个分区中只有一个组 案例： 12345678910111213141516171819202122232425val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6), 3)// 分组// groupBy方法可以根据指定的规则进行分组,指定的规则的返回值就是分组的key// groupBy方法的返回值为元组// 元组的第一个元素：表示分组的key// 元组的第二个元素：表示相同key的数据形成的可迭代的集合// groupBy方法执行完毕后,会将数据进行分组操作,但是分区不会改变// 不同组的数据会打乱分散到不同的分区中// 如果将上游的分区数据打乱重新组合到下游的分区中,那么这个操作称之为shuffle// 如果数据被打乱重新组合,那么数据就可能出现不均匀的情况,可以改变下游RDD的数据分区// groupBy方法可能会导致数据不均匀,如果想改变分区,可以传递参数val rdd = dataRDD.groupBy(_ % 2, 2)rdd.saveAsTextFile(\"output\")// println(s\"Group Num = $&#123;rdd.glom.collect.length&#125;\")//// rdd.collect.foreach &#123;// case (key, list) =&gt; println(s\"Key = $key , list = &#123; $&#123;list.mkString(\",\")&#125; &#125;\")// &#125;sparkContext.stop 小功能：将List(“Hello”, “hive”, “hbase”, “Hadoop”)根据单词首写字母进行分组 1234567891011val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 根据单词首写字母进行分组val dataRDD = sparkContext.makeRDD(List(\"Hello\", \"hive\", \"hbase\", \"Hadoop\"), 2)// StringOps =&gt; String(0),隐式转换,取首个字符串Charval valueRDD = dataRDD.groupBy(word =&gt; word(0))valueRDD.foreach(println)sparkContext.stop 小功能：从服务器日志数据apache.log中获取每个时间段访问量 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val fileRDD = sparkContext.textFile(\"input/apache.log\")val timeRDD = fileRDD.map(log =&gt; log.split(\" \")(3))val hourRDD = timeRDD.groupBy(time =&gt; time.substring(11, 13))hourRDD.map(it =&gt; (it._1, it._2.size)).foreach(println)sparkContext.stop filter 函数签名： 1def filter(f: T =&gt; Boolean): RDD[T] 函数说明：将数据根据指定的规则进行过滤筛选，符合规则的数据保留，不符合规则的数据丢弃。当数据进行筛选过滤后分区不变，但分区内的数据可能不均衡。生产环境下，可能会出现数据倾斜 案例： 12345678910val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6), 3)// filter过滤// 根据指定的规则对数据进行筛选过滤,满足条件的数据保留,不满足的数据丢弃val rdd = dataRDD.filter(_ % 2 == 0)rdd.collect.foreach(println)sparkContext.stop sample 函数签名： 1234def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] 函数说明：根据指定的规则从数据集中抽取数据 案例： 1234567891011121314151617val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 5, 6))// sample用于从数据集中抽取数据// 参数1：withReplacement(Boolean)表示数据抽取后是否放回,可以重复抽取// 参数2：fraction(Double)表示数据抽取的几率(每个数据被抽取的几率,不是数据总量的比率)——不放回的场合可重复抽取// 参数3：seed(Long,默认Utils.random.nextLong)表示随机数种子,可以确定数据的抽取// 随机数不随机,所谓的随机数依靠随机算法实现val valueDRR = dataRDD.sample(withReplacement = false, 0.5, 1)println(valueDRR.collect.mkString(\",\"))val valueDRR2 = dataRDD.sample(withReplacement = true, 0.5)println(valueDRR2.collect.mkString(\",\"))sparkContext.stop distinct 函数签名： 12def distinct()(implicit ord: Ordering[T] = null): RDD[T]def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] 函数说明：将数据集中重复数据去重 案例： 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 1, 2, 4))// distinct 去重// distinct 可以改变分区的数量dataRDD.distinct.foreach(println)sparkContext.stop coalesce 函数签名： 123def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] 函数说明：根据数据量缩减分区，用于大数据集过滤后提高小数据集的执行效率。当spark程序中存在过多小任务时，可以通过coalesce()方法收缩合并分区、减少分区的个数、减小任务调度成本 案例： 123456789101112131415161718192021222324val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 1, 1, 2, 2, 2), 2)// val filterRDD = dataRDD.filter(num =&gt; num % 2 == 0)// filterRDD.saveAsTextFile(\"output\")// 多 -&gt; 少// 当数据过滤后,发现数据不够均匀,那么可以缩减分区// filterRDD.coalesce(1).saveAsTextFile(\"output\")// 如果发现数据分区不合理,也可以缩减分区// coalesce主要目的是缩减分区,扩大分区时没有效果// 为什么不能扩大分区? 因为在分区缩减时,数据不会打乱重新组合,没有shuffle的过程// dataRDD.coalesce(2).saveAsTextFile(\"output\")// 如果非得要将数据扩大分区,那么必须打乱数据后重新组合,必须使用shuffle// coalesce()// 参数1：numPartitions: Int——表示缩减分区后的分区数量// 参数2：shuffle: Boolean = false——表示分区改变时是否会打乱重新组合数据dataRDD.coalesce(6, shuffle = true).saveAsTextFile(\"output\")sparkContext.stop repartition 函数签名： 1def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] 函数说明：该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程 案例： 123456789val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 1, 1, 2, 2, 2), 3)// 缩减分区 -&gt; coalesce()// 扩大分区 -&gt; repartition()——底层就是coalesce(..., true)dataRDD.repartition(5).saveAsTextFile(\"output\")sparkContext.stop sortBy 函数签名： 12345def sortBy[K]( f: (T) =&gt; K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] 函数说明：该操作用于排序数据。在排序之前可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为正序排列。排序后新产生的RDD的分区数与原RDD的分区数一致 案例： 123456789101112val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List(1, 4, -2, 2))// sortBy// 默认排序规则为升序// sortBy可以通过传递第二个参数改变排序的方式(false逆序)// sortBy可以设定第三个参数,用于改变分区dataRDD.sortBy(num =&gt; num, ascending = false).foreach(println)sparkContext.stop pipe 函数签名： 1def pipe(command: String): RDD[String] 函数说明：管道——针对每个分区，都调用一次shell脚本，返回输出的RDD(注意：shell脚本需要放在计算节点可以访问到的位置) 案例： 编写一个脚本，并增加执行权限 1234567# vim pipe.sh#!/bin/shecho \"Start\"while read LINE; do echo \"&gt;&gt;&gt;\"$&#123;LINE&#125;done# chmod 777 pipe.sh 命令行工具中创建一个只有一个分区的RDD：val rdd = sc.makeRDD(List(“hi”,”Hello”,”how”,”are”,”you”), 1) 将脚本作用该RDD并打印：rdd.pipe(“/opt/data/pipe.sh”).collect 双Value类型 intersection 函数签名： 1def intersection(other: RDD[T]): RDD[T] 函数说明：对源RDD和参数RDD求交集后返回一个新的RDD union 函数签名： 1def union(other: RDD[T]): RDD[T] 函数说明：对源RDD和参数RDD求并集后返回一个新的RDD subtract 函数签名： 1def subtract(other: RDD[T]): RDD[T] 函数说明：以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集 zip 函数签名： 1def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] 函数说明：将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的元素 案例： 12345678910111213141516171819202122232425262728val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD1 = sparkContext.makeRDD(List(1, 2, 3, 4), 2)val dataRDD2 = sparkContext.makeRDD(List(4, 5, 6, 7), 4)// 并集：数据合并,分区也会合并// println(dataRDD1.union(dataRDD2).collect.mkString(\",\"))dataRDD1.union(dataRDD2).saveAsTextFile(\"output11\")// 交集：分区数不变,数据被打乱重组(shuffle),保留最大分区数量// println(dataRDD1.intersection(dataRDD2).collect.mkString(\",\"))dataRDD1.intersection(dataRDD2).saveAsTextFile(\"output12\")// 差集：数据被打乱重组(shuffle)// 当调用rdd的subtract()方法时,以当前rdd的分区为主,所以分区数量等于当前rdd的分区数量// println(dataRDD1.subtract(dataRDD2).collect.mkString(\",\"))dataRDD1.subtract(dataRDD2).saveAsTextFile(\"output13\")// 拉链：分区数不变// 两个RDD的分区一致,但是数据量不相同的场合：// =&gt; Exception：Can only zip RDDs with the same number of elements in each partition// 两个RDD的分区不一致,数据量也不相同,但是每个分区数据量一致/分区数不一致// =&gt; Exception：Can't zip RDDs with unequal numbers of partitions// println(dataRDD1.zip(dataRDD2).collect.mkString(\",\"))dataRDD1.zip(dataRDD2).saveAsTextFile(\"output14\")sparkContext.stop KV类型 partitionBy 函数签名： 1def partitionBy(partitioner: Partitioner): RDD[(K, V)] 函数说明：将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner 案例： 1234567891011121314151617181920212223val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// K-V类型的数据操作val dataRDD = sparkContext.makeRDD( List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 4)), 1)// Spark中很多方法是基于Key进行操作,所以数据格式应该为键值对(对偶元组)// 如果数据类型为K-V类型,那么Spark会自动给RDD补充很多新的方法(扩展)// 隐式转换(A =&gt; B)// partitionBy()方法来自于PairRDDFunctions.class// RDD的伴生对象中提供了隐式函数可以将RDD[K,V]转换为PairRDDFunctions// partitionBy：根据指定的规则对数据进行分区// 其他影响分区的方法：groupBy、filter -&gt; coalesce、repartition(shuffle)// partitionBy(Partitioner)方法,参数为partitioner——分区器对象// Partitioner为抽象类,其实现类有：HashPartitioner、RangePartitioner// HashPartitioner分区规则：将当前数据key的hashCode进行取余操作// HashPartitioner是spark默认的分区器// sortBy()使用了RangePartitionerdataRDD.partitionBy(new HashPartitioner(2)).saveAsTextFile(\"output\")sparkContext.stop 小功能：自定义分区器 123456789101112131415161718192021222324252627282930313233def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\") val sparkContext = new SparkContext(sparkConf) // 自定义分区器——自己决定数据放置在哪个分区做处理 val dataRDD = sparkContext.makeRDD(List( (\"cba\", \"Message 1\"), (\"cba\", \"Message 2\"), (\"cba\", \"Message 3\"), (\"nba\", \"Message 1\"), (\"nba\", \"Message 2\"), (\"wnba\", \"Message 1\") ), 1) val rdd = dataRDD.partitionBy(new MyPartitioner(3)) rdd.mapPartitionsWithIndex((index, datas) =&gt; &#123; datas.map(data =&gt; (index, data)) &#125;).collect.foreach(println) sparkContext.stop&#125;// 自定义分区器// 1、继承Partitionerclass MyPartitioner(num: Int) extends Partitioner &#123; // 获取分区的数量 override def numPartitions: Int = num // 根据数据的Key来决定数据在哪个分区中进行处理 // 方法的返回值表示分区编号(索引) override def getPartition(key: Any): Int = &#123; key match &#123; case \"nba\" =&gt; 0 case _ =&gt; 1 &#125; &#125;&#125; reduceByKey 函数签名： 12def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)] 函数说明：可以将数据按照相同的Key对Value进行聚合 案例： 12345678910val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// reduceByKey：根据数据的Key进行分组,然后对value进行聚合// 参数1——func: (V, V) =&gt; V表示相同key的value的聚合方式// 参数2——numPartitions: Int表示聚合后的分区数量println(sparkContext.makeRDD(List( (\"hello\", 1), (\"scala\", 1), (\"Hello\", 2), (\"scala\", 2))).reduceByKey(_ + _).collect.mkString(\",\"))sparkContext.stop groupByKey 函数签名： 123def groupByKey(): RDD[(K, Iterable[V])]def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] 函数说明：将分区的数据直接转换为相同类型的内存数组进行后续处理 案例： 12345678910111213141516val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// groupByKey：根据固定的规则(数据的Key)进行分组// groupBy：根据指定的规则对数据进行分组// groupByKey()方法返回的数据类型为元组// 元组的元素1——表示为用于分组的Key// 元素的元素2——表示为分组后的相同Key的value的集合sparkContext.makeRDD(List( (\"hello\", 1), (\"scala\", 1), (\"Hello\", 2), (\"scala\", 2))).groupByKey.map &#123; case (word, iter) =&gt; (word, iter.sum)&#125;.collect.foreach(println)sparkContext.stop aggregateByKey 函数签名： 12def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U, combOp: (U, U) =&gt; U): RDD[(K, U)] 函数说明：将数据根据不同的规则进行分区内计算和分区间计算 案例： 123456789101112131415161718192021222324val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 将分区内相同的Key取最大值,分区间相同的Key求和// 分区内和分区间的计算规则不同// reduceByKey：分区内和分区间计算规则相同// 0 -&gt; [(a, 2), (c, 3)]// 1 -&gt; [(b, 4), (c, 6)]// aggregateByKey：根据Key进行数据聚合// Scala语法：函数柯里化// 参数1——zeroValue: U表示计算的初始值,用于在分区内进行计算时当作初始值使用(首次遇到Key)// 参数2——seqOp: (U, V) =&gt; U表示分区内的计算规则,相同key的value计算// 参数3——combOp: (U, U) =&gt; U)表示分区间的计算规则,相同key的value的计算println(sparkContext.makeRDD(List( (\"a\", 1), (\"a\", 2), (\"c\", 3), (\"b\", 4), (\"c\", 5), (\"c\", 6)), 2).aggregateByKey(zeroValue = 0)( (x, y) =&gt; math.max(x, y), (x, y) =&gt; x + y).collect.mkString(\",\"))sparkContext.stop foldByKey 函数签名： 1def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)] 函数说明：当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey 案例： 12345678910111213141516171819val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD = sparkContext.makeRDD(List( (\"a\", 1), (\"a\", 2), (\"c\", 3), (\"b\", 4), (\"c\", 5), (\"c\", 6)), 2)// 计算规则相同,都是求和,可以实现wordCount// dataRDD.aggregateByKey(0)(// (x, y) =&gt; x + y,// (x, y) =&gt; x + y// )println(dataRDD.aggregateByKey(0)(_ + _, _ + _).collect.mkString(\",\"))// 如果分区内计算规则和分区间计算规则相同,可以将aggregateByKey简化为foldByKeyprintln(dataRDD.foldByKey(0)(_ + _).collect.mkString(\",\"))sparkContext.stop combineByKey 函数签名： 1234def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)] 函数说明：最通用的对kv型rdd进行聚集操作的聚集函数(aggregation function)。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致 案例： 1234567891011121314151617181920212223val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// combineByKey// 每个Key的平均值：相同key的数据的总和 / 相同key的数量val dataRDD = sparkContext.makeRDD(List( (\"a\", 88), (\"b\", 95), (\"a\", 91), (\"b\", 93), (\"a\", 95), (\"b\", 98)), 2)// 计算时需要将value的格式发生改变,只需要第一个value改变结构即可// 如果计算时发现相同key的value不符合计算规则的格式时,那么选择combineByKey// combineByKey()// 参数1——createCombiner: V =&gt; C表示将计算的第一个值结构进行转换// mergeValue: (C, V) =&gt; C表示分区内的计算规则// mergeCombiners: (C, C) =&gt; C表示分区间的计算规则dataRDD.combineByKey( value =&gt; (value, 1), (tuple: (Int, Int), value) =&gt; (tuple._1 + value, tuple._2), (tuple1: (Int, Int), tuple2: (Int, Int)) =&gt; (tuple1._1 + tuple2._1, tuple1._2 + tuple2._2)).map &#123; case (key, (total, cnt)) =&gt; (key, total * 1.0 / cnt) &#125; .collect.foreach(println)sparkContext.stop sortByKey 函数签名： 1def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)] 函数说明：在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的 案例： 1234567891011121314151617def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\") val sparkContext = new SparkContext(sparkConf) val dataRDD = sparkContext.makeRDD(List( (new User, 1), (new User, 2), (new User, 3) )) println(dataRDD.sortByKey(ascending = true).collect.mkString(\",\")) sparkContext.stop&#125;// 自定义Key进行排序需要将Key混入特质Orderedcase class User() extends Ordered[User] with Serializable &#123; override def compare(that: User): Int = 1&#125; join 函数签名： 1def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] 函数说明：在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD 案例： 12345678910111213141516val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"rddMemory\")val sparkContext = new SparkContext(sparkConf)val dataRDD1 = sparkContext.makeRDD(List( (\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 5), (\"a\", 10)))val dataRDD2 = sparkContext.makeRDD(List( (\"b\", 2), (\"a\", 5), (\"c\", 9), (\"c\", 2)))// join可以将两个rdd中相同的key的value连接在一起// join性能不太高,会形成笛卡尔积,不建议用println(dataRDD1.join(dataRDD2).collect.mkString(\",\"))sparkContext.stop leftOuterJoin 函数签名： 1def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))] 函数说明：类似于SQL语句的左外连接 cogroup 函数签名： 1def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] 函数说明：在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))类型的RDD 案例： 1234567891011121314151617val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)val dataRDD1 = sparkContext.makeRDD(List( (\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 5), (\"a\", 10)))val dataRDD2 = sparkContext.makeRDD(List( (\"b\", 2), (\"a\", 5), (\"c\", 9), (\"c\", 2), (\"e\", 5)))// dataRDD1.leftOuterJoin(dataRDD2).collect.foreach(println)// println(\"-----------\")// dataRDD1.rightOuterJoin(dataRDD2).collect.foreach(println)dataRDD1.cogroup(dataRDD2).collect.foreach(println)sparkContext.stop 案例实操 数据准备：agent.log(时间戳，省份，城市，用户，广告，中间字段使用空格分隔) 需求描述：统计出每一个省份每个广告被点击数量排行的Top3 代码实现： 123456789101112131415161718192021222324252627282930313233343536val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark-sobxiong\")val sparkContext = new SparkContext(sparkConf)// 统计出每一个省份每个广告被点击数量排行的Top3// 1、获取原始数据val dataRDD = sparkContext.textFile(\"input/agent.log\")// 2、将原始数据进行结构转换,方便统计：(省份-广告, 1)val mapRDD = dataRDD.map(line =&gt; &#123; val datas = line.split(\" \") (s\"$&#123;datas(1)&#125;-$&#123;datas(4)&#125;\", 1)&#125;)// 3、将相同Key的数据进行分组聚合：(省份-广告, sum)val reduceRDD = mapRDD.reduceByKey(_ + _)// 4、将聚合后的结果进行结构的转换：(省份, (广告, sum))val mapRDD2 = reduceRDD.map &#123; case (key, sum) =&gt; val keys = key.split(\"-\") (keys(0), (keys(1), sum))&#125;// 5、将相同的身份的数据分在一个组中：(省份, Iterator[(广告1, sum1)、...])val groupRDD = mapRDD2.groupByKey// 6、将分组后的数据进行排序(降序),取Top3val sortRDD = groupRDD.mapValues(iter =&gt; &#123; iter.toList.sortWith((left, right) =&gt; left._2 &gt; right._2).take(3)&#125;)// 7、采集打印sortRDD.collect.foreach(println)sparkContext.stop RDD行动算子","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Scala","slug":"ProgrammingLanguage/Scala/Scala","date":"2020-07-24T07:02:46.000Z","updated":"2020-09-26T04:01:30.386Z","comments":true,"path":"2020/07/24/ProgrammingLanguage/Scala/Scala/","link":"","permalink":"https://sobxiong.github.io/2020/07/24/ProgrammingLanguage/Scala/Scala/","excerpt":"内容 Scala概述 变量 运算符 程序流程控制 函数式编程基础 面向对象编程-基础 面向对象编程-中级","text":"内容 Scala概述 变量 运算符 程序流程控制 函数式编程基础 面向对象编程-基础 面向对象编程-中级 Scala概述 学习Scala的原因 Spark是新一代内存级大数据计算框架，是大数据的重要内容 Spark就是使用Scala编写的。因此为了更好的学习Spark, 需要掌握Scala这门语言 Scala是Scalable Language的简写，是一门多范式(范式/编程方式[面向对象/函数式编程])的编程语言 联邦理工学院洛桑(EPFL)的Martin Odersky于2001年开始设计Scala(2003年推出) Spark的兴起，带动Scala语言的发展 Scala由来创始人马丁·奥德斯基(Martin Odersky)是编译器及编程的狂热爱好者，长时间的编程之后，希望发明一种语言，能够让写程序这样的基础工作变得高效，简单。所以当接触到JAVA语言后，对JAVA这门便携式，运行在网络，且存在垃圾回收的语言产生了极大的兴趣，所以决定将函数式编程语言的特点融合到JAVA中，由此发明了两种语言(Pizza &amp; Scala)Pizza和Scala极大地推动了Java编程语言的发展(jdk5.0的泛型，for循环增强, 自动类型转换等，都是从Pizza引入的新特性;jdk8.0的类型推断，Lambda表达式就是从scala引入的特性)现在主流JVM的javac编译(jdk5.0、8.0)就是马丁·奥德斯基编写出来的 Scala和Java以及JVM的关系分析图 Scala语言的特点Scala是一门以java虚拟机(JVM)为运行环境并将面向对象和函数式编程的最佳特性结合在一起的静态类型编程语言 Scala是一门多范式(multi-paradigm)的编程语言，Scala支持面向对象和函数式编程 Scala源代码(.scala)会被编译成Java字节码(.class)，然后运行于JVM之上，并可以调用现有的Java类库，实现两种语言的无缝对接 Scala简洁高效 Scala设计时参考了Java的设计思想，源于Java Mac上搭建Scala开发环境(Window/Linux类似) Scala需要Java运行时库，首先先安装JDK环境 在http://www.scala-lang.org/下载mac版本tar.gz包 解压tar.gz包(不配置环境变量) 在命令行下cd进入解压包的bin目录下 输入scala进入Scala Cli，会打印版本信息 搭建IDEA的Scala开发环境 在Plugin面板中安装(如果下载太慢,去官网下载对应版本的插件到本地,在安装) 新建空的maven项目 当前默认不支持scala的框架，需要引入scala框架，右键\b项目点击add framework support 选中scala，在use library中设定解压的目录 右键main目录创建一个diretory，名为scala，右键scala目录，mark directory，选择source root Scala执行流程 .scala源文件通过scalac编译成.class字节码，再通过scala运行 .scala源文件直接通过scala运行(运行慢) Scala程序特点 以.scala为扩展名 执行入口为main()函数 严格区分大小写 方法由一条条语句构成，每个语句后不需要添加分号 如果在一行有多条语句，除了最后一行语句不要分号，其他语句都需要分号 Scala输出的三种方式 字符串通过’+’方式(类似Java) printf方式进行格式化(%,类似C) 字符串通过$引用(类似Kotlin) Scala在IDEA下进行源码关联 在官网下载source源码包 解压到本地 在IDEA中打开一个源码文件，在右上角上点击Attach Sources 选中解压后的本地目录 Scala注释 单行/多行注释(同Java) 文档注释：scaladoc -d 源码.scala 变量 Scala变量声明 基础语法：var | val 变量名 [:变量类型] = 变量值 声明变量时，类型可以省略(编译器自动推导,即类型推导) 类型确定后，就不能修改，说明Scala是强数据类型语言 在声明/定义一个变量时，可以使用var或者val来修饰，var修饰的变量可改变，val修饰的变量不可改(同Kotlin) val修饰的变量在编译后，等同于加上final(同Kotlin) var修饰的对象引用可以改变，val修饰的则不可改变，但对象的状态(值,属性)却是可以改变的(比如自定义对象、数组、集合等等) 变量声明时，需要初始值 数据类型 Scala与Java有着相同的数据类型，在Scala中数据类型都是对象(包装了基础数据类型)，也就是说scala没有java中的原生类型(同Kotlin) Scala数据类型分为两大类AnyVal(值类型)和AnyRef(引用类型)——注意：不管是AnyVal还是AnyRef都是对象 在Scala中有一个根类型Any，他是所有类的父类 scala中一切皆为对象，分为两类AnyVal和AnyRef，它们都是Any子类 Null类型是scala的特殊类型，只有一个值null，是bottom class，也是所有AnyRef类型的子类 Nothing类型也是bottom class，是所有类的子类，开发中通常可以将Nothing类型的值返回给任意变量或函数(抛异常使用很多) 数据类型列表 数据类型 描述 Byte[1] 8位有符号补码整数。数值区间为-128～127 Short[2] 16位有符号补码整数。数值区间为-32768～32767 Int[3] 32位有符号补码整数。数值区间为-2^31～2^31-1 Long[4] 64位有符号补码整数。数值区间为-2^63～2^63-1 Float[4] 32位，IEEE754标准的单精度浮点数 Double[8] 64位，IEEE754标准的双精度浮点数 Char[2] 16位无符号Unicode字符, 区间值为U+0000～U+FFFF String 字符序列 Boolean[1] true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成() Null null Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 整数类型使用细节 Scala各整数类型有固定的表数范围和字段长度，不受具体OS的影响，以保证Scala程序的可移植性 Scala的整型常量/字面量默认为Int型，声明Long型常量/字面量需后加’l’或’L’ 表示特大整数：BigInt类 浮点类型使用细节 与整数类型类似，Scala浮点类型也有固定的表数范围和字段长度，不受具体OS的影响 Scala的浮点型常量默认为Double型，声明Float型常量，须后加’f’或’F’ 两种表示方式 十进制数形式：如5.12、512.0f、.512(必须有小数点) 科学计数法形式：如5.12e2 通常情况下应使用Double类型，比Float类型更精准(小数点后大致7位) 表示更为精确的小数：BigDecimal 字符类型使用细节 字符常量是用单引号’’括起来的单个字符 Scala也允许使用转义字符’&#39;来将其后的字符转变为特殊字符型常量(同Java) Char相当于一个整数，可以进行运算 字符类型存取本质 存储：字符 -&gt; 码值 -&gt; 二进制 -&gt; 存储 读取：二进制 -&gt; 码值 -&gt; 字符 -&gt; 读取 Unit、Null和Nothing类型使用细节 Null类只有一个实例对象null，类似于Java中的null引用。null可以赋值给任意引用类型(AnyRef)，但是不能赋值给值类型 Unit类型用来标识过程，也就是没有明确返回值的函数，类似于Java里的void。Unit只有一个实例() Nothing可以作为没有正常返回值的方法的返回类型，非常直观的告诉你这个方法不会正常返回，而且由于Nothing是其他任意类型的子类，它还能跟要求返回值的方法兼容 值类型转换 隐式转换：当Scala程序在进行赋值或者运算时，精度小的类型自动转换为精度大的数据类型细节说明： 有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算 当我们把精度(容量)大的数据类型赋值给精度(容量)小的数据类型时会报错 byte、short和char之间不会相互自动转换，三者计算时首先都转换为int类型 自动提升原则：表达式结果的类型自动提升为操作数中(容量、精度)最大的类型 强制类型转换：自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转函数，但可能造成精度降低或溢出 1234// javaint num = (int)2.5// scalavar num : Int = 2.7.toInt 细节说明： 强转符号只针对于最近的操作数有效，往往会使用小括号提升优先级 Char类型可以保存Int的常量值，但不能保存Int的变量值，需要强转 值类型和String类型的转换 基本类型转String类型：将基本类型的值 + “”(同Java) String类型转基本数据类型：调用String.toXxx方法 小数字符串转Int会抛出异常，不会进行截取 标识符命名规则 基本和Java一致 首字符为字母，后续字符任意字母和数字，美元符号，可后接下划线_ 数字不可以开头 首字符为操作符(比如+ - * /)，后续字符也需跟至少一个操作符(反编译后scala将其转译) 操作符(比如+-*/)不能在标识符中间和最后 用反引号``包括的任意字符串，即使是关键字也可以 预定义标识符可以用，如Int，但不推荐 Scala的39个关键字 package, import, class, object, trait, extends, with, type, forSome private, protected, abstract, sealed, final, implicit, lazy, override try, catch, finally, throw if, else, match, case, do, while, for, return, yield def, val, var this, super new true, false, null 运算符 运算符分类 算术运算符 %的运算原则：a % b = a - a / b * b(同Java) 在scala中没有++和–，使用+=1和-=1代替 /的整数除和小数除是有区别的；整数除只保留整数部分而舍弃小数部分 关系运算符(==、!=、&gt;、&lt;、&lt;=、&gt;=) 关系运算的结果都是Boolean类型(true/false) 如果两个浮点数进行比较，应当保证数据类型一致 逻辑运算符(&amp;&amp;、||、!) 赋值运算符(=、+=、-=、*=、/=、%=、&lt;&lt;=、&gt;&gt;=、&amp;=、^=、|=) 运算顺序从右往左 赋值运算符的左边只能是变量，右边可以是变量、表达式、常量值/字面量 位运算符(&amp;、|、^、~、&lt;&lt;、&gt;&gt;、&gt;&gt;&gt;) Scala不支持三目运算符(x ? x : x)，使用if-else代替(类似kotlin) 运算符优先级(同Java) ()[] 单目运算 算术运算 移位运算 比较运算 位运算 关系运算 赋值运算 , 键盘输入语句 输入String：StdIn.readLine() 输入Int：StdIn.readInt … 程序流程控制 三大流程控制 顺序控制 分支控制 Scala中任意表达式都是有返回值的，也就意味着if else表达式其实是有返回结果的，具体返回结果的值取决于满足条件的代码体的最后一行内容 Scala中没有switch，使用模式匹配(match-case)来处理 循环控制 for循环 基本语法： for(i &lt;- start to end)：其中i表示循环的变量，i将会从start～end循环，前后闭合 for(item &lt;- list)：集合遍历 for(i &lt;- start until end)：与to不同的是前闭后开 for(i &lt;- start to end if i % 2 != 0)：循环守卫，即循环保护式(也称条件判断式,守卫)。保护式为true则进入循环体内部，为false则跳过，类似于continue for(i &lt;- start to end ; j = f(i))：引入变量，’;’不可少；其次i和j均为val不可变类型变量 for(i &lt;- start1 to end1 ; j &lt;- start2 to end2)：嵌套循环，’;’不可少；上面代码不常用，基本用单层for的嵌套 val res = for(i &lt;- start to end) yield i：循环返回值，将遍历过程中的每个结果i返回到一个新的Vector集合中；yield后可以是一个代码块，在最后一行返回 for(i &lt;- Range(start,end,step))：控制for循环的步长 补充 {}和()对于for表达式都可以 有一个不成文的约定：当for推导式仅包含单一表达式时使用圆括号，当其包含多个表达式时使用大括号 当使用{}来换行写表达式时，分号就不用写了 while/do-while循环(同Java) 与if语句不同，while语句本身没有返回值，即结果是Unit类型 while没有返回值，所以用while语句来计算并返回结果时，不可避免地使用声明在while外部的变量，那么就等同于循环的内部对外部的变量造成了影响，所以不推荐使用，而是推荐使用for循环 while循环的中断 说明：Scala内置控制结构特地去掉了break和continue，是为了更好的适应函数化编程，推荐使用函数式的风格解决break和contine的功能，而不是一个关键字 if-else和循环守卫也可以实现continue效果 举例说明(break的使用)： 12345678910111213141516171819202122232425262728// 导入break相关函数import util.control.Breaks._/*breakable()函数 1、是一个高阶函数：可以接受函数的函数 2、源码实现： def breakable(op: =&gt; Unit) &#123; try &#123; op &#125; catch &#123; case ex: BreakControl =&gt; if (ex ne breakException) throw ex &#125; &#125; op：=&gt; Unit表示接受的参数是一个没有参数和返回值的函数，可简单理解为一段代码块 3、breakable对break()抛出的异常做了处理，代码就继续执行 4、传入代码块时，一般将()改为&#123;&#125;(类似Kotlin)*/breakable&#123; while(n &lt;= 20)&#123; n+=1 if(n == 18)&#123; // 在scala中使用函数break()中断循环 // def break(): Nothing = &#123; throw breakException &#125; break() &#125; &#125;&#125; 函数式编程基础 函数式编程介绍 概念说明 函数式编程 是一种”编程范式” 属于”结构化编程”的一种，主要思想是把运算过程尽量写成一系列嵌套的函数调用 函数式编程中，将函数也当做数据类型，因此可以接受函数当作输入(参数)和输出(返回值) 在scala中，方法和函数几乎可以等同(比如他们的定义、使用、运行机制都一样的)，只是函数的使用方式更加的灵活多样 函数式编程是从编程方式(范式)的角度来谈的，可以这样理解：函数式编程把函数当做一等公民，充分利用函数、支持的函数的多种使用方式。比如：在Scala当中，函数是一等公民，像变量一样，既可以作为函数的参数使用，也可以将函数赋值给一个变量。函数的创建不用依赖于类或者对象；而在Java当中，函数的创建则要依赖于类、抽象类或者接口 Scala中函数式编程和面向对象编程(是以对象为基础的编程方式)融合在一起 函数式编程、面向对象编程关系分析 函数的定义(为完成某一功能的程序指令(语句)的集合) 基本语法 1234def 函数名 ([参数名: 参数类型], ...) [[: 返回值类型] =] &#123; 语句... return 返回值&#125; 语法介绍 函数声明关键字为def(definition) [参数名: 参数类型], …：表示函数的输入(参数列表)，可以没有。如果有，多个参数使用逗号间隔 函数返回值 (: 返回值类型 =) 明确返回值的类型 (=) 表示返回值类型不确定，使用类型推导完成 (空)，表示没有返回值，return不生效 如果没有return，默认以执行到最后一行的结果作为返回值 函数注意事项 递归调用的规则 程序执行一个函数时，就创建一个新的受保护的独立空间(新函数栈) 函数的局部变量是独立的，不会相互影响 递归调用必须有递归出口，否则就是无限递归 函数的形参列表可以是多个，如果函数没有形参，调用时可以不带() 形参列表和返回值列表的数据类型可以是值类型和引用类型 Scala中的函数可以根据函数体最后一行代码自行推断函数返回值类型。那么在这种情况下，return关键字可以省略 因为Scala可以自行推断，所以在省略return关键字的场合，返回值类型也可以省略 如果函数明确使用return关键字，那么函数返回就不能使用自行推断了，这时要明确写成(: 返回类型 = )，当然如果你什么都不写，即使有return，返回值也为() 如果函数明确声明无返回值(声明Unit)，那么函数体中即使使用return关键字也不会有返回值 如果明确函数无返回值或不确定返回值类型，那么返回值类型可以省略(或声明为Any) Scala语法中任何的语法结构都可以嵌套其他语法结构(灵活)，即：函数中可以再声明/定义函数，类中可以再声明类，方法中可以再声明/定义方法 Scala函数的形参，在声明参数时，直接赋初始值(默认值)，这时调用函数时，如果没有指定实参，则会使用默认值。如果指定了实参，则实参会覆盖默认值 如果函数存在多个参数，每一个参数都可以设定默认值，那么这个时候，传递的参数到底是覆盖默认值，还是赋值给没有默认值的参数，就不确定了(默认按照声明顺序[从左到右])。在这种情况下，可以采用带名参数(类似Kotlin) Scala函数的形参默认是val的，因此不能在函数中进行修改 递归函数未执行之前是无法推断出来结果类型，在使用时必须有明确的返回值类型 Scala函数支持可变参数，可变参数必须放在最后，如args :Int* 过程 基本介绍：将函数的返回类型为Unit的函数称之为过程(procedure)，如果明确函数没有返回值，那么等号可以省略 注意区分：如果函数声明时没有返回值类型，但是有等号，可以进行类型推断(最后一行代码)；这时这个函数实际是有返回值的，该函数并不是过程 惰性函数 一种应用场景惰性计算(尽可能延迟表达式求值)是许多函数式编程语言的特性。惰性集合在需要时提供其元素，无需预先计算它们，这带来了一些好处：首先，可以将耗时的计算推迟到绝对需要的时候；其次，可以创造无限个集合，只要它们继续收到请求，就会继续提供元素。函数的惰性使用能够得到更高效的代码。Java并没有为惰性提供原生支持，Scala提供了 Java实现懒加载(单例模式——懒汉式) 12345678910111213public class LazyDemo &#123; private LazyDemo instance = null; private LazyDemo()&#123;&#125; public LazyDemo getInstance() &#123; // 如果没有初始化过,那么进行初始化 if (instance == null) &#123; instance = new LazyDemo(); &#125; return instance; &#125;&#125; 介绍当函数返回值被声明为lazy时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称之为惰性函数，在Java的某些框架代码中称之为懒加载(延迟加载) 案例 12345678910111213141516def main(args: Array[String]): Unit = &#123; lazy val res = sum(10, 20) println(\"-----------------\") println(\"res = \" + res)&#125;def sum(n1 : Int, n2 : Int): Int = &#123; println(\"sum() ~~~\") n1 + n2&#125;/*结果：-----------------sum() ~~~res = 30*/ 注意事项和细节 lazy不能修饰var类型的变量 不但在调用函数时，加了lazy会导致函数的执行被推迟；在声明一个变量时，如果声明了lazy，那么变量值得分配也会推迟。比如lazy val i = 10 异常 介绍 Scala提供try和catch块来处理异常：try块用于包含可能出错的代码；catch块用于处理try块中发生的异常。可以根据需要在程序中有任意数量的try…catch块 语法处理上和Java类似，但是又不尽相同(许多异常包装了Java中的Exception——类似Kotlin) Java异常回顾 示例： 1234567891011try &#123; int i = 0; int b = 10; // ArithmeticException,除0异常 int c = b / i;&#125; catch(Exception e) &#123; e.printStackTrace();&#125; finally &#123; // 最终要执行的代码 System.out.println(\"java finally\");&#125; Java异常处理的注意点： java语言按照try—catch-catch-…—finally的方式来处理异常 不管有没有异常捕获，都会执行finally，因此通常可以在finally代码块中释放资源 可以有多个catch，分别捕获对应的异常，这时需要把范围小的异常类写在前面，把范围大的异常类写在后面，否则编译错误。会提示”Exception ‘java.lang.xxxxxx’ has already been caught” Scala异常处理 示例： 123456789try &#123; val r = 10 / 0&#125; catch &#123; case ex: ArithmeticException=&gt; println(\"ArithmeticException!\") case ex: Exception =&gt; println(\"Normal Exception!\")&#125; finally &#123; // 最终要执行的代码 println(\"scala finally\")&#125; Scala异常处理的注意点： 在scala异常处理中只有一个catch；在catch中有多个case，每个case可以匹配一种异常；”=&gt;”是一个关键符号，表示后面是对该异常的处理代码块 我们将可疑代码封装在try块中。在try块之后使用了一个catch处理程序来捕获异常。如果发生任何异常，catch处理程序将处理它，程序将不会异常终止 Scala的异常的工作机制和Java一样，但是Scala没有”checked(编译期)”异常，即Scala没有编译异常这个概念，异常都是在运行的时候捕获处理 可使用throw关键字抛出一个异常对象。所有异常都是Throwable的子类型。throw表达式是有类型的，就是Nothing(因为Nothing是所有类型的子类型，所以throw表达式可以用在任何需要类型的地方) 1234567def main(args: Array[String]): Unit = &#123; val res = test() println(res)&#125;def test(): Nothing = &#123; throw new Exception(\"My Exception!\")&#125; 在Scala里，借用了模式匹配的思想来做异常的匹配。因此可以在catch的代码里，使用一系列case子句来匹配异常。”=&gt;”可以接着多条语句(换行)，类似java的switch-case语句 异常捕捉的机制与其他语言中一样，如果有异常发生，catch子句是按次序捕捉的。因此在catch子句中，越具体的异常越要靠前，越普遍的异常越靠后。如果把越普遍的异常写在前，把具体的异常写在后，scala不会报错，但这样是非常不好的编程风格 finally子句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作，这点和Java一样 Scala提供了throws关键字来声明异常。可以使用方法定义声明异常。它向调用者函数提供了此方法可能引发此异常的信息。它有助于调用函数处理并将该代码包含在try-catch块中，以避免程序异常终止。在scala中，可以使用throws注释来声明异常 123456789def main(args: Array[String]): Unit = &#123; f11()&#125;// 等同于NumberFormatException.class@throws(classOf[NumberFormatException])def f11() = &#123; \"abc\".toInt&#125; 面向对象编程-基础 Scala是面向对象的 Java是面向对象的编程语言。但由于历史原因，Java中还存在着非面向对象的内容：基本类型、null、静态方法等 Scala来源于Java，所以天生就是面向对象的语言，而且Scala是纯粹的面向对象的语言(即在Scala中，一切皆为对象) Scala定义类 基本语法(基本与Java一致)： 123[修饰符] class 类名 &#123; 类体&#125; 注意事项 在scala语法中类并不声明为public，所有这些类都具有公有可见性(即默认就是public) 一个Scala源文件可以包含多个类(同Java)，而且默认都是public 属性/成员变量注意事项 属性的定义语法同变量：[访问修饰符] var/val 属性名称 [：类型] = 属性值 属性的定义类型可以为任意类型，包含值类型或引用类型 Scala中声明一个属性必须显式初始化，Scala可根据初始化数据的类型自动推断，此时属性类型可以省略(这与Java不同) 如果赋值为null，则一定要加类型。因为不加类型，那么该属性的类型就是Null类型 如果在定义属性时暂时不赋值，也可以使用符号”_”，让系统分配默认值 类型 _对应的值 Byte/Short/Int/Long 0 Float/Double 0.0 Boolean false String和其他引用类型 null 同一类型不同对象的属性相互独立，互不影响 创建对象 基本语法：val | var 对象名 [: 类型] = new 类型() 说明 如果我们不希望改变对象的引用(即内存地址)，应该声明为val，否则声明为var。scala设计者推荐使用val：因为一般来说，在程序中，我们只是改变对象的属性的值，而不是改变对象的引用 scala在声明对象变量时，可以根据创建对象的类型自动推断，所以类型声明可以省略(java不可省略,类似Kotlin)。但当类型和后面new的对象类型有继承关系即多态时，就必须写 访问属性 基本语法：对象名.属性名 原理： 示例： 123456789101112def main(args: Array[String]): Unit = &#123; val test = new Person test.age = 5 test.name = \"xiong\" println(s\"age = $&#123;test.age&#125; , name = $&#123;test.name&#125; , tag = $&#123;test.tag&#125;\")&#125;class Person &#123; var name: String = _ var age: Int = _ val tag = \"SOBXiong\"&#125; 反编译的.class文件 12345678910111213141516171819202122232425262728293031323334// test$.classpublic final class test$ &#123; public static test$ MODULE$; public void main(String[] args) &#123; test.Person test = new test.Person(); // 对象名.属性名修改属性是通过底层编译器翻译包装了java方法实现的 test.age_$eq(5); test.name_$eq(\"xiong\"); // 对象名.属性名获取属性也是通过底层编译器翻译包装了java方法实现的 Predef$.MODULE$.println((new StringBuilder(25)).append(\"age = \").append(test.age()).append(\" , name = \").append(test.name()).append(\" , tag = \").append(test.tag()).toString()); &#125; private test$() &#123; MODULE$ = this;&#125;&#125;public class Person &#123; // var变量翻译后自动生成getter/setter方法 private String name; private int age; public String name() &#123; return this.name; &#125; public void name_$eq(String x$1) &#123; this.name = x$1; &#125; public int age() &#123; return this.age; &#125; public void age_$eq(int x$1) &#123; this.age = x$1; &#125; // val变量翻译为final变量(不可变),且只提供getter方法,不提供setter修改的方法 private final String tag = \"SOBXiong\"; public String tag() &#123; return this.tag; &#125;&#125; 方法 基本说明：Scala中的方法其实就是函数 基本语法： 123def 方法名(参数列表) [：返回值类型] = &#123; 方法体&#125; 方法调用机制原理 当scala程序开始执行时，先在栈区开辟一个main栈。main栈最后被销毁(scala程序终止) 当scala程序执行到一个方法时，总会开一个新的栈 每个栈是独立的空间，变量(基本数据类型)是独立的，相互不影响(引用类型除外) 当方法执行完毕后，该方法开辟的栈就会被JVM机回收 构造器 基本介绍：构造器(constructor)又叫构造方法，是类的一种特殊的方法，主要作用是完成对新对象的初始化 Java构造器回顾 基本语法 123[修饰符] 类名(参数列表) &#123; 构造方法体&#125; 特点 在Java中一个类可以定义多个不同的构造方法(构造方法重载) 如果没有定义构造方法，系统会自动生成一个默认无参构造方法(也叫默认构造器)，比如Person(){} 一旦定义了自己的构造方法，默认的构造方法就被覆盖了，就不能再使用默认的无参构造方法，除非显式地定义一下 Scala构造器 介绍：和Java一样，Scala创建新对象也需要调用构造方法，并且可以有任意多个构造方法(即scala中构造器也支持重载)。Scala类的构造器包括：主构造器和辅助构造器 基本语法 12345class 类名(形参列表) &#123;// 主构造器 // 类体 def this(形参列表) &#123;...&#125;// 辅助构造器1 def this(形参列表) &#123;...&#125;// 辅助构造器2、3...&#125; 构造器参数 Scala类的主构造器的形参若未用任何修饰符修饰，那么这个参数是局部变量 如果参数使用val关键字声明，那么Scala会将参数作为类的私有的只读属性使用 如果参数使用var关键字声明，那么那么Scala会将参数作为类的成员属性使用，并会提供属性对应的xxx()[类似getter]以及xxx_$eq()[类似setter]方法(这时的成员属性是私有的，但是可读写) Bean属性 介绍JavaBeans规范定义了Java的属性是像getXxx()和setXxx()的方法。许多Java框架都依赖这个命名习惯。为了与Java的互操作性，产生了@BeanProperty注解。在Scala字段前加@BeanProperty时会自动生成规范的setXxx()以及getXxx()方法。这时可以使用对象.setXxx()和对象.getXxx()来修改和获取属性值 注意给某个属性加入@BeanPropetry注解后，会生成getXXX和setXXX的方法，并且对原来底层自动生成类似xxx(),xxx_$eq()方法，没有冲突，二者可以共存 注意事项和细节 Scala构造器作用是完成对新对象的初始化，构造器没有返回值 主构造器的声明直接放置于类名之后 主构造器会执行类定义中的所有语句，这可以体会到Scala把函数式编程和面向对象编程融合在一起(构造器也是方法/函数) 如果主构造器无参数，小括号可省略，构建对象时调用的构造方法的小括号也可以省略 辅助构造器名称为this(和Java、Kotlin不一样)，多个辅助构造器通过不同参数列表进行区分(底层就是构造器重载) 如果想让主构造器变成私有的，可以在()之前加上private，这样只能通过辅助构造器来构造对象 辅助构造器的声明不能和主构造器的声明一致，否则会发生错误(构造器名重复) 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def main(args: Array[String]): Unit = &#123; val p1 = new Person(\"sobxiong\") p1.showInfo()&#125;class Person() &#123; println(\"类定义语句~~~\") @BeanProperty var name: String = _ @BeanProperty var age: Int = _ def this(name: String) &#123; /* 辅助构造器无论是直接或间接,最终都一定要调用主构造器,执行主构造器的逻辑 而且需要放在辅助构造器的第一行(这点和Java一样,Java中一个构造器要调用同类的其它构造器也需要放在第一行) */ // 直接调用主构造器 this() this.name = name &#125; def this(name: String, age: Int) &#123; // 直接调用主构造器 this() this.name = name this.age = age &#125; def this(age: Int) &#123; // 间接调用主构造器 this(\"匿名\") this.age = age &#125; def showInfo(): Unit = &#123; println(\"person信息如下:\") println(\"name = \" + this.name) println(\"age = \" + this.age) &#125;&#125;/*结果如下：类定义语句~~~person信息如下:name = sobxiongage = 0*/ 反编译的.class文件 12345678910111213141516171819202122232425262728293031323334353637383940public class Person &#123; private String name; private int age; public String name() &#123; return this.name; &#125; public void name_$eq(String x$1) &#123; this.name = x$1; &#125; public String getName() &#123; return name(); &#125; public void setName(String x$1) &#123; name_$eq(x$1); &#125; public int age() &#123; return this.age; &#125; public void age_$eq(int x$1) &#123; this.age = x$1; &#125; public int getAge() &#123; return age(); &#125; public void setAge(int x$1) &#123; age_$eq(x$1); &#125; public Person() &#123; Predef$.MODULE$.println(\"类定义语句~~~\"); &#125; public Person(int age) &#123; this(\"\"); age_$eq(age); &#125; public Person(String name) &#123; this(); name_$eq(name); &#125; public Person(String name, int age) &#123; this(); name_$eq(name); age_$eq(age); &#125; public void showInfo() &#123; Predef$.MODULE$.println(\"person信息如下:\"); Predef$.MODULE$.println((new StringBuilder(7)).append(\"name = \").append(name()).toString()); Predef$.MODULE$.println((new StringBuilder(6)).append(\"age = \").append(age()).toString()); &#125;&#125; 面向对象编程-中级 包 回顾Java包 Java包的三大作用 区分相同名字的类 当类很多时，可以很好的管理类 控制访问范围 Java包的声明：package xxx; Java包的本质：创建不同的文件夹保存类文件 Java包的要求： 包名和源码所在的系统文件目录结构要一致 编译后的字节码文件路径也和包名保持一致 Scala包 基本语法：package xxx Scala的作用： 区分相同名字的类 当类很多时，可以很好的管理类 控制访问范围 可以对类的功能进行扩展 Scala包名和源码所在的系统文件目录可以不一致，但编译后的.class字节码文件路径和包名会保持一致(该工作由编译器完成) 包的命名规则：只能包含数字、字母、下划线、小圆点(.)，但不能用数字开头，也不要使用关键字 包的命名规范：com.公司名.项目名.业务模块名 Scala自动引用的常用包：java.lang.*、scala、Predef Scala包注意事项和使用细节： scala多种等价的包形式 123456789101112131415161718192021222324252627282930// 传统的包形式package com.atguigu.scalaclass Person&#123; val name = \"Nick\" def play(message: String): Unit = &#123; println(this.name + \" \" + message) &#125;&#125;// 等价的第二种包形式package com.atguigupackage scalaclass Person&#123; val name = \"Nick\" def play(message: String): Unit = &#123; println(this.name + \" \" + message) &#125;&#125;// 嵌套包形式package com.atguigu&#123; package scala&#123; class Person&#123; val name = \"Nick\" def play(message: String): Unit = &#123; println(this.name + \" \" + message) &#125; &#125; &#125;&#125; 嵌套包的好处：可以灵活地在同一个文件中将类(class, object)、特质(trait)创建在不同的包中 作用域原则：可以直接向上访问。即：Scala可在子包中直接访问父包中的内容，大括号体现作用域。(提示: Java中子包使用父包的类,需要import)。在子包和父包类重名时，默认采用就近原则，如果希望指定使用某个类，需要指定包名 父包要访问子包的内容时，需要import对应的类 可以在同一个.scala文件中，声明多个并列的package(建议嵌套的pakage不要超过3层) 包名可以相对也可以绝对。在一般情况下，我们使用相对路径来引入包，只有当包名冲突时，使用绝对路径来处理 包对象 基本介绍：包可以包含类(class, object)和特质(trait)，但不能包含函数/方法或变量的定义。这是Java虚拟机的局限。为了弥补这一点不足，scala提供了包对象的概念来解决这个问题 示例： 示例代码： 123456789101112131415161718192021222324252627282930313233package com.xiong &#123; // 每个包都可以有一个包对象 // 需要在父包中定义它,且名称与子包一样。 package object scala &#123; var name = \"jack\" def sayOk(): Unit = &#123; println(\"package object sayOk!\") &#125; &#125; package scala &#123; class Test &#123; def test(): Unit = &#123; // 包对象scala中声明的name变量 println(name) // 调用包对象scala中声明的sayOk方法 sayOk() &#125; &#125; object TestObj &#123; def main(args: Array[String]): Unit = &#123; val t = new Test() t.test() // 因为TestObj和scala这个包对象在同一包,因此也可以使用name属性 println(\"name =\" + name) &#125; &#125; &#125;&#125; 机制分析： 12345678910111213141516171819202122232425262728293031323334353637383940// 当创建包对象后,在该包下生成final修饰的package和package$类// package$.classpublic final class package$ &#123; public static package$ MODULE$; private String name; public String name() &#123; return this.name; &#125; public void name_$eq(String x$1) &#123; this.name = x$1; &#125; public void sayOk() &#123; scala.Predef$.MODULE$.println(\"package object sayOk!\"); &#125; private package$() &#123; MODULE$ = this; this.name = \"jack\"; &#125;&#125;// Test.classpublic class Test &#123; public void test() &#123; Predef$.MODULE$.println(package$.MODULE$.name()); package$.MODULE$.sayOk(); &#125;&#125;// TestObj$.classpublic final class TestObj$ &#123; public static TestObj$ MODULE$; public void main(String[] args) &#123; Test t = new Test(); t.test(); scala.Predef$.MODULE$.println((new StringBuilder(6)).append(\"name =\").append(package$.MODULE$.name()).toString()); &#125; private TestObj$() &#123; MODULE$ = this; &#125;&#125; 注意事项 每个包都可以有一个包对象，需要在父包中定义它 包对象名称需要和包名一致，一般用来对包的功能进行补充 包的可见性 回顾Java 访问修饰符介绍(控制方法和变量的访问权限、范围) 公开级别：用public修饰，对外公开 受保护级别：用protected修饰，对子类和同一个包中的类公开 默认级别：没有修饰符号，向同一个包的类公开 私有级别：用private修饰，只有类本身可以访问，不对外公开 访问级别 访问控制修饰符 同类 同包 子类 不同包 公开 public √ √ √ √ 受保护 protected √ √ √ × 默认 / √ √ × × 私有 private √ × × × 修饰符注意事项 修饰符可以用来修饰类中的属性，成员方法以及类 只有默认和public才能修饰类，并且遵循上述访问权限的特点 Scala的包的可见性(四种修饰符与Java一样) 当属性访问权限为默认时，从底层看属性是private的，但是因为提供了xxx_$eq()[类似setter]/xxx()[类似getter]方法，因此从使用效果看任何地方都可以访问 当方法访问权限为默认时，默认为public访问权限 private为私有权限，只在类的内部和伴生对象中可用 protected为受保护权限，scala中受保护权限比Java中更严格，只能子类访问，同包无法访问(编译器) 在scala中没有public关键字，即不能用public显式地修饰属性和方法 包访问权限(表示属性有了限制，同时包也有了限制)，这点和Java不一样，体现出Scala包使用的灵活性 1234567891011package com.xiong.scalaclass Person &#123; /* 增加包访问权限后 1、private同时起作用,不仅同类可以使用 2、同时com.xiong.scala中包下其他类也可以使用 3、修饰符也可以设置为public、protected等 4、包可见性可以延展到上曾,如改为xiong */ private[scala] val pname = \"hello\"&#125; 包的引入 基本介绍：Scala引入包也是使用import，基本的原理和机制和Java一样，但Scala中的import功能更加强大，也更灵活 使用细节和注意事项： 在Scala中，import语句可以出现在任何地方，并不仅限于文件顶部，import语句的作用一直延伸到包含该语句的块末尾。这种语法的好处是：在需要时在引入包，缩小import包的作用范围，提高效率 Java中如果想要导入包中所有的类，可以通过通配符*，Scala中采用下_ 如果不想要某个包中全部的类，而是其中的几个类，可以采用选取器(大括号) 如果引入的多个包中含有相同的类，那么可以将不需要的类进行重命名进行区分，这个就是重命名 如果某个冲突的类根本就不会用到，那么这个类可以直接隐藏掉 12345// 将HashMap重命名为JavaHashMapimport java.util.&#123; HashMap=&gt;JavaHashMap, List&#125;// 引入java.util包的所有类,但是忽略HashMapimport java.util.&#123; HashMap=&gt;_, _&#125; 继承 Java继承回顾 语法：class 子类名 extends 父类名 { 类体 } 子类继承父类的属性和方法 单继承 Scala继承 语法同Java 单继承同Java","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"HBase","slug":"BigData/HBase","date":"2020-07-17T03:08:53.000Z","updated":"2020-09-20T14:26:05.713Z","comments":true,"path":"2020/07/17/BigData/HBase/","link":"","permalink":"https://sobxiong.github.io/2020/07/17/BigData/HBase/","excerpt":"内容 HBase简介 HBase快速入门 HBase进阶 HBase-API HBase优化","text":"内容 HBase简介 HBase快速入门 HBase进阶 HBase-API HBase优化 HBase简介 HBase定义：HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库 HBase数据模型：逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构(K-V)来看，HBase更像是一个multi-dimensional map(多维度Map) 逻辑结构 物理存储结构 数据模型 Name Space命名空间，类似于关系型数据库的DatabBase概念，每个命名空间下有多个表。HBase有两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间 Region类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列簇即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景 RowHBase表中的每行数据都由一个RowKey和多个Column(列)组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要 ColumnHBase中的每个列都由Column Family(列簇)和Column Qualifier(列限定符)进行限定，例如info：name，info：age。建表时，只需指明列簇，而列限定符无需预先定义 Time Stamp用于标识数据的不同版本(version)，每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间 Cell由{rowkey,column Family:column Qualifier,time Stamp}唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮 HBase基本架构架构角色： Region ServerRegion Server为Region的管理者，其实现类为HRegionServer，主要作用如下:对于数据的操作：get,put,delete对于Region的操作：splitRegion、compactRegion MasterMaster是所有Region Server的管理者，其实现类为HMaster，主要作用如下：对于表的操作：create,delete,alter对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移 ZookeeperHBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作 HDFSHDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持 HBase快速入门 HBase安装部署 Zookeeper正常部署 Hadoop(主要是HDFS)正常部署 HBase正常部署 解压HBase：tar -zxvf hbase-2.2.5-bin.tar.gz -C /opt/module 修改HBase的配置文件 修改hbase-env.sh的内容： 123export JAVA_HOME=/opt/module/jdk1.8.0_251# 自行管理zookeeperexport HBASE_MANAGES_ZK=false 修改hbase-site.xml的内容： 1234567891011121314151617181920&lt;!-- 设置为分布式部署 --&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置HBase的默认存储文件的路径 --&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000/HBase&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置zookeeper的集群地址 --&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt;&lt;/property&gt;&lt;!-- 设置zookeeper的data目录 --&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.6.1/zkData&lt;/value&gt;&lt;/property&gt; 修改regionservers文件(配置集群节点)： 123hadoop1hadoop2hadoop3 软链接hadoop配置文件到HBase： 12ln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-2.2.5/conf/core-site.xmlln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-2.2.5/conf/hdfs-site.xml 将HBase远程发送到集群其他节点：xsync hbase-2.2.5/ 启动HBase服务 单独启动方式(提示:如果集群之间的节点时间不同步，会导致 regionserver 无法启动，抛出ClockOutOfSyncException 异常) 12bin/hbase-daemon.sh start masterbin/hbase-daemon.sh start regionserver 群起集群启动方式：bin/start-hbase.sh(停止:bin/stop-hbase.sh) 查看HBase页面：http://hadoop1:16010 HBase Shell操作 基本操作 进入HBase客户端命令行：bin/hbase shell 查看帮助命令：help 查看当前数据库的所有表：list 表的操作 创建表 1create 'student','info' 插入(更新)数据 1put 'student','1001','info:sex','male' 扫描查看表数据 12345scan 'student'# 左闭右开scan 'student',&#123;STARTROW =&gt; '1001', STOPROW =&gt; '1003'&#125;# 查看多版本的表数(无需在表上设置存放版本)scan 'student',&#123;RAW =&gt; true, VERSIONS =&gt; 10&#125; 查看表结构 1describe 'student' 查看指定行或指定行的指定列的数据 1234get 'student','1001'get 'student','1001','info:name'# 查看name列的三个版本的数据(需要在表上设置存放版本)get 'student','1001',&#123;COLUMN =&gt; 'info:name',VERSIONS =&gt; 3&#125; 统计表数据行数 1count 'student' 删除数据 1234# 删除某rowkey的全部数据deleteall 'student','1001'# 删除某rowkey的某一列数据delete 'student','1002','info:sex' 清空表数据 12# 提示: 清空表的操作顺序为先disable,然后再truncatetruncate 'student' 删除表 1234# 首先要将表设置为disable状态disable 'student'# 删除表drop 'student' 变更表信息 12# 设置info列簇的数据存放3个版本alter 'student',&#123;NAME =&gt; 'info',VERSIONS =&gt; 3&#125; HBase进阶 架构原理 StoreFile保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile(HFile)，数据在每个StoreFile中都是有序的 MemStore写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的HFile WAL由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件(简称WAL)中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建 写流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问 与目标Region Server进行通讯 将数据顺序写入(追加)到WAL 将数据写入对应的MemStore，数据会在MemStore进行排序 向客户端发送ack 等达到MemStore的刷写时机后，将数据刷写到HFile MemStore FlushMemStore刷写时机： 当某个memstore的大小达到了hbase.hregion.memstore.flush.size(默认值128M)，其所在 region的所有memstore都会刷写。当memstore的大小达到了hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier(默认值4)时，会阻止继续往该memstore写数据 当region server中memstore的总大小达到java_heapsize * hbase.regionserver.global.memstore.size * hbase.regionserver.global.memstore.size.lower.limit(默认值0.95)，region会按照其所有memstore的大小顺序(由大到小)依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下(当前还可以写数据)。当region server中memstore的总大小达到java_heapsize * hbase.regionserver.global.memstore.size时，会阻止继续往所有的memstore写数据 到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置：hbase.regionserver.optionalcacheflushinterval(默认1小时) 当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log 以下(该属性名已经废弃——无法手动设置，默认值为32;但是可以设置每个log文件的blockSize,相当于增加单个log文件的存储量,默认为hdfs的块大小128M) 读流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server 访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问 与目标Region Server进行通讯 分别在Block Cache(读缓存——内存)，MemStore(内存)和Store File(HFile磁盘文件)中查询目标数据(同时查内存和磁盘,因为不知道哪个timestamp先)，并将查到的所有数据进行合并(Merge)。此处所有数据是指同一条数据的不同版本(timestamp)或者不同的类型(Put/Delete) 将从文件中查询到的数据块(Block,HFile数据存储单元,默认大小为64KB)缓存到Block Cache 将合并后的最终结果返回给客户端 StoreFile Compaction由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本(timestamp)和不同类型(Put/Delete)有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile CompactionCompaction分为两种，分别是Minor Compaction和Major Compaction。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据 Region Split默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个 Region转移给其他的Region Server。Region Split时机： 当1个region中的某个Store下所有StoreFile的总大小超过Min(R ^ 2 * hbase.hregion.memstore.flush.size , hbase.hregion.max.filesize)，该Region就会进行拆分，其中 R为当前Region Server中属于该Table的个数 HBase-API 环境准备新建maven项目后在pom.xml中添加依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt; HBaseAPI 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205public class TestApi &#123; private static Connection connection; private static Admin admin; static &#123; try &#123; // 1、获取配置文件信息 Configuration configuration = HBaseConfiguration.create(); configuration.set(\"hbase.zookeeper.quorum\", \"hadoop1,hadoop2,hadoop3\"); // 2、创建连接对象 connection = ConnectionFactory.createConnection(configuration); // 3、创建admin对象 admin = connection.getAdmin(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 关闭资源 public static void close() &#123; if (admin != null) &#123; try &#123; admin.close(); admin = null; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (connection != null) &#123; try &#123; connection.close(); connection = null; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 1、测试表是否存在 public static boolean isTableExist(String tableName) throws IOException &#123; return admin.tableExists(TableName.valueOf(tableName)); &#125; // 2、创建表 public static void createTable(String tableName, String... columnFamilies) throws IOException &#123; // 1、判断是否存在列簇信息 if (columnFamilies.length == 0) &#123; System.out.println(\"请设置列簇信息!\"); return; &#125; // 2、判断表是否存在 if (isTableExist(tableName)) &#123; System.out.println(tableName + \"表已存在!\"); return; &#125; List&lt;ColumnFamilyDescriptor&gt; columnFamilyList = new ArrayList&lt;ColumnFamilyDescriptor&gt;(); // 3、循环添加列簇信息 for (String columnFamily : columnFamilies) &#123; // 添加列簇信息 columnFamilyList.add(ColumnFamilyDescriptorBuilder.newBuilder(columnFamily.getBytes()).build()); &#125; // 4、创建表描述器 TableDescriptor tableDescriptor = TableDescriptorBuilder .newBuilder(TableName.valueOf(tableName)) .setColumnFamilies(columnFamilyList) .build(); // 5、创建表 admin.createTable(tableDescriptor); &#125; // 3、删除表 public static void dropTable(String tableName) throws IOException &#123; // 1、判断表是否存在 if (!isTableExist(tableName)) &#123; System.out.println(tableName + \"表不存在!\"); &#125; // 2、下线表 admin.disableTable(TableName.valueOf(tableName)); // 3、删除表 admin.deleteTable(TableName.valueOf(tableName)); &#125; // 4、创建命名空间 public static void createNameSpace(String nameSpaceName) &#123; // 1、创建命名空间描述器 NamespaceDescriptor namespaceDescriptor = NamespaceDescriptor.create(nameSpaceName) .build(); // 2、创建命名空间 try &#123; admin.createNamespace(namespaceDescriptor); &#125; catch (NamespaceExistException e) &#123; System.out.println(nameSpaceName + \"命名空间已存在!\"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; // 5、向表中插入数据 public static void putData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、创建put对象 Put put = new Put(Bytes.toBytes(rowKey)); // 3、给put对象赋值 put.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); // 4、插入数据 table.put(put); // 5、关闭表 table.close(); &#125; // 6、获取数据(get) public static void getData(String tableName, String rowKey, String columnFamily, String column) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、创建get对象 Get get = new Get(Bytes.toBytes(rowKey)); // 指定获取的列簇和列 if (column != null &amp;&amp; !column.isEmpty() &amp;&amp; columnFamily != null &amp;&amp; !columnFamily.isEmpty()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column)); &#125; else if (columnFamily != null &amp;&amp; !columnFamily.isEmpty()) &#123; get.addFamily(Bytes.toBytes(columnFamily)); &#125; // 设置获取数据的版本数 get.readVersions(2); // 3、获取数据 Result result = table.get(get); // 4、解析result for (Cell cell : result.rawCells()) &#123; // 5、打印数据 System.out.println(\"columnFamily = \" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\"column = \" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\"value = \" + Bytes.toString(CellUtil.cloneValue(cell))); &#125; // 6、关闭表连接 table.close(); &#125; // 7、获取数据(scan) public static void scanTable(String tableName) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、构建scan对象 Scan scan = new Scan(); // 构建过滤器 // RowFilter rowFilter = new RowFilter(CompareOperator.EQUAL, new SubstringComparator(uid + '_')); // scan.setFilter(rowFilter); // 3、扫描表 ResultScanner resultScanner = table.getScanner(scan); // 4、解析resultScanner for (Result result : resultScanner) &#123; // 5、解析result for (Cell cell : result.rawCells()) &#123; // 6、打印数据 System.out.println(\"rowKey = \" + Bytes.toString(CellUtil.cloneRow(cell))); System.out.println(\"columnFamily = \" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(\"column = \" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(\"value = \" + Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125; // 5、关闭table对象 table.close(); &#125; // 8、删除数据,delete是一种特殊的put操作,打标记 public static void deleteData(String tableName, String rowKey, String columnFamily, String column) throws IOException &#123; // 1、获取表对象 Table table = connection.getTable(TableName.valueOf(tableName)); // 2、构建删除对象,deleteFamily标记,rowKey下所有columnFamily的所有version Delete delete = new Delete(Bytes.toBytes(rowKey)); // 设置删除的列簇 deleteFamily标记对应HBase Cli的deleteall命令 // 删除列对应部分delete命令,删除指定columnFamily的所有version // delete.addFamily(Bytes.toBytes(columnFamily)); // 设置删除的列 deleteColumn标记 // delete.addColumns(Bytes.toBytes(columnFamily), Bytes.toBytes(column)); /* 设置删除的列 delete标记 普通情况：上一个老值冒出来了,新值删去 另一种情况：接连着push两条name,flush之后再使用该方式删除,没数据了 无法确定flush时机,删除最好使用addColumns(防止出错) */ // delete.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column)); // 设置删除的列 deleteColumn标记,删除在小于等于timestamp的所有版本 // delete.addColumns(Bytes.toBytes(columnFamily), Bytes.toBytes(column), 1595307747400L); // 设置删除的列 delete标记,删除等于timestamp的那一个版本 delete.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column), 1595307747419L); // 3、执行删除操作 table.delete(delete); // 4、关闭连接 table.close(); &#125; public static void main(String[] args) throws IOException &#123; // System.out.println(\"isTableExist = \" + isTableExist(\"stu4\")); // createTable(\"stu6\", \"info1\", \"info2\"); // System.out.println(\"isTableExist = \" + isTableExist(\"stu5\")); // dropTable(\"stu6\"); // createNameSpace(\"test\"); // createTable(\"test:xixi\", \"info\"); // putData(\"stu4\", \"1006\", \"info\", \"name\", \"haha\"); // scanTable(\"fruit\"); deleteData(\"stu\", \"1008\", \"info1\", \"name\"); close(); &#125;&#125; MapReduce通过HBase的相关Java API，我们可以实现伴随HBase操作的MapReduce过程，比如使用MapReduce将数据从本地文件系统导入到HBase的表中，比如我们从HBase中读取一些原始数据后使用MapReduce做数据分析 官方HBase-MapReduce案例 查看HBase的MapReduce任务的执行需要的依赖包：bin/hbase mapredcp以及bin/hbase classpath 环境变量的导入(/etc/profile中配置;在生产环境中最好使用临时操作,以下命令在命令行下输入,只在当前次登录生效) 12345# HBASE_HOMEexport HBASE_HOME=/opt/module/hbase-2.2.5export HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase classpath`export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:`$&#123;HBASE_HOME&#125;/bin/hbase mapredcp`# 保存后执行source /etc/profile 执行官方的MapReduce任务(输出都在控制台上) 案例一：统计stu表中有多少行数据 1hadoop jar lib/hbase-mapreduce-2.2.5.jar rowcounter stu 案例二：使用MapReduce将本地数据导入到HBase 在本地创建一个tsv格式的文件：fruit.tsv(csv格式以’,’隔开,而tsv格式以’\\t’隔开) 1231001 Apple Red1002 Pear Yellow1003 Pineapple Yellow 创建HBase表(不存在会报错)： 1create 'fruit','info' 将fruit.tsv文件上传到HDFS上(根目录上)：hdfs dfs -put fruit.tsv / 执行MapReduce任务 123hadoop jar lib/hbase-mapreduce-2.2.5.jar \\importtsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\hdfs://hadoop1:9000/fruit.tsv 使用scan命令查看导入后的结果 1scan 'fruit' 自定义HBase-MapReduce 导入相应的依赖包 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-mapreduce&lt;/artifactId&gt; &lt;version&gt;2.2.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-auth&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-common&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 示例一：将HDFS上数据表fruit.tsv导入到HBase的fruit1表中(打包扔到集群上运行) FruitMapper类，用于读取HDFS上数据 123456public class FruitMapper extends Mapper&lt;LongWritable, Text, LongWritable, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; FruitReducer类，用于将数据写入到HBase中 12345678910111213141516public class FruitReducer extends TableReducer&lt;LongWritable, Text, NullWritable&gt; &#123;@Overrideprotected void reduce(LongWritable key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、遍历values: 1001 Apple red for (Text value : values) &#123; // 2、获取每一行数据 String[] fields = value.toString().split(\"\\t\"); // 3、构建Put对象 Put put = new Put(Bytes.toBytes(fields[0])); // 4、给Put对象复制 put.addColumn(Bytes.toBytes(\"info\"), Bytes.toBytes(\"name\"), Bytes.toBytes(fields[1])); put.addColumn(Bytes.toBytes(\"info\"), Bytes.toBytes(\"color\"), Bytes.toBytes(fields[2])); // 5、写出 context.write(NullWritable.get(), put); &#125;&#125; FruitDriver类 12345678910111213141516171819202122232425262728293031323334353637383940public class FruitDriver implements Tool &#123; // 定义一个Configuration private Configuration configuration = null; public int run(String[] args) throws Exception &#123; // 1、获取job对象 Job job = Job.getInstance(configuration); // 2、设置驱动类路径 job.setJarByClass(FruitDriver.class); // 3、设置Mapper和Mapper输出的KV类型 job.setMapperClass(FruitMapper.class); job.setMapOutputKeyClass(LongWritable.class); job.setMapOutputValueClass(Text.class); // 4、设置Reducer类 TableMapReduceUtil.initTableReducerJob( args[1], FruitReducer.class, job ); // 5、设置输入参数 FileInputFormat.setInputPaths(job, new Path(args[0])); // 6、提交任务 boolean result = job.waitForCompletion(true); return result ? 0 : 1; &#125; public void setConf(Configuration configuration) &#123; this.configuration = configuration; &#125; public Configuration getConf() &#123; return configuration; &#125; public static void main(String[] args) &#123; try &#123; Configuration configuration = new Configuration(); int run = ToolRunner.run(configuration, new FruitDriver(), args); System.exit(run); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 使用maven进行package打包操作，将jar包上传到集群上 在命令行中操作 1hadoop jar HBaseTest-1.0-SNAPSHOT.jar com.xiong.mr.FruitDriver /fruit.tsv fruit1 在HBase Cli上查看fruit1表的数据 1scan 'fruit1' 示例二：读取fruit表并过滤数据，将结果输出到fruit2表(远端连接运行) Fruit2Mapper类，用于读取和过滤HBase fruit表中的数据 1234567891011121314151617public class Fruit2Mapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; // 构建Put对象 Put put = new Put(key.get()); // 1、获取数据 for (Cell cell : value.rawCells()) &#123; // 2、判断当前的cell是否为name列 if (\"name\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) &#123; // 3、给Put对象赋值 put.add(cell); &#125; &#125; // 4、写出 context.write(key, put); &#125;&#125; Fruit2Reducer类，用于将处理后的数据输出到fruit2表中 123456789public class Fruit2Reducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; // 遍历写出 for (Put value : values) &#123; context.write(NullWritable.get(), value); &#125; &#125;&#125; Fruit2Driver类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class Fruit2Driver implements Tool &#123; // 定义配置信息 private Configuration configuration = null; public int run(String[] args) throws Exception &#123; // 1、获取Job对象 Job job = Job.getInstance(configuration); // 2、设置主类路径 job.setJarByClass(Fruit2Driver.class); // 3、设置Mapper及输出KV类型 TableMapReduceUtil.initTableMapperJob( \"fruit\", new Scan(), Fruit2Mapper.class, ImmutableBytesWritable.class, Put.class, job ); // 4、设置Reducer及输出表 TableMapReduceUtil.initTableReducerJob( \"fruit2\", Fruit2Reducer.class, job ); // 5、提交任务 boolean result = job.waitForCompletion(true); return result ? 0 : 1; &#125; public void setConf(Configuration configuration) &#123; this.configuration = configuration; &#125; public Configuration getConf() &#123; return configuration; &#125; public static void main(String[] args) &#123; try &#123; Configuration configuration = HBaseConfiguration.create(); int run = ToolRunner.run(configuration, new Fruit2Driver(), args); System.exit(run); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 将集群上HBase的hbase-site.xml内容复制到当前工程下的resource/hbase-site.xml上，文件名不得修改 12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- HBase默认存储文件的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;!-- zk的集群地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt; &lt;/property&gt; &lt;!-- zk的data目录 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.6.1/zkData&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;./tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 运行查看结果(在HBase Cli中scan fruit2表) 与Hive的集成 HBase和Hive的对比 Hive 数据仓库：Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询 用于数据分析、清洗：Hive适用于离线的数据分析和清洗，延迟较高 基于HDFS、MapReduce：Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行 HBase 数据库：是一种面向列簇存储的非关系型数据库 用于存储结构化和非结构化的数据：适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作 基于HDFS：数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理 延迟较低，接入在线业务使用：面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度 HBase和Hive集成使用(可能会有版本兼容问题,生产环境会采用CDH方式或者运维人员帮忙处理) 环境准备后续可能会在操作Hive的同时会对HBase产生影响，所以Hive需要持有操作HBase的Jar，那么需要拷贝Hive所依赖的Jar(或者使用软连接的形式) 123456789101112131415161718# 可以临时设置,也可在/etc/profile中永久设置export HBASE_HOME=/opt/module/hbase-2.2.5export HIVE_HOME=/opt/module/hive-3.1.2# 设置软链接,'\\'为shell命令的分隔符,多行时最好采用ln -s $HBASE_HOME/lib/hbase-common-2.2.5.jar \\$HIVE_HOME/lib/hbase-common-2.2.5.jarln -s $HBASE_HOME/lib/hbase-server-2.2.5.jar \\$HIVE_HOME/lib/hbase-server-2.2.5.jarln -s $HBASE_HOME/lib/hbase-client-2.2.5.jar \\$HIVE_HOME/lib/hbase-client-2.2.5.jarln -s $HBASE_HOME/lib/hbase-protocol-2.2.5.jar \\$HIVE_HOME/lib/hbase-protocol-2.2.5.jarln -s $HBASE_HOME/lib/hbase-it-2.2.5.jar \\$HIVE_HOME/lib/hbase-it-2.2.5.jarln -s $HBASE_HOME/lib/hbase-hadoop2-compat-2.2.5.jar \\$HIVE_HOME/lib/hbase-hadoop2-compat-2.2.5.jarln -s $HBASE_HOME/lib/hbase-hadoop-compat-2.2.5.jar \\$HIVE_HOME/lib/hbase-hadoop-compat-2.2.5.jar 同时需要在hive-site.xml中修改zookeeper的属性(连接hbase需要与zk交互) 123456789101112&lt;!-- 设置zk节点 --&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;&lt;!-- 设置zk client通信端口 --&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt; 实操 案例一：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表 在Hive中创建表同时关联HBase(完成后可在Hive和HBase的Cli中查看) 123456789101112CREATE TABLE hive_hbase_emp_table(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\")TBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\"); 在Hive中创建中间临时表，用于装载文件中的数据(因为hbase的文件格式不是txt,所有不能直接由txt导入,需要中间表) 12345678910CREATE TABLE emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)row format delimited fields terminated by '\\t'; 向Hive中间表中装载数据 1load data local inpath '/opt/module/data/hive/emp.txt' into table emp; 通过insert命令将中间表的数据导入Hive与HBase关联的那张表中：insert into table hive_hbase_emp_table select * from emp; 查看Hive和HBase各自的表中是否已同步地插入了数据 1234# Hiveselect * from hive_hbase_emp_table;# HBasescan 'hbase_emp_table' 案例二：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据 在Hive中创建外部表 123456789101112CREATE EXTERNAL TABLE relevance_hbase_emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\")TBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\"); 查看关联后Hive中外部表：select * from relevance_hbase_emp; 之后便可使用Hive进行一些数据分析 HBase优化 高可用在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果 HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置 关闭启动的HBase集群：bin/stop-hbase.sh 在conf目录下创建backup-masters文件(文件名不得更改)：touch conf/backup-masters 在backup-masters中加入备用HMaster节点(当前HMaster配置是hadoop1,文件不得多出空格和换行)： 12hadoop2hadoop3 分发conf目录到其他hbase集群节点：xsync backup-masters 在hadoop1上启动HBase集群：bin/start-hbase.sh 打开页面查看：http://hadoop1:16010 使用jps查看hadoop1的HMaster进程，并使用kill -9 进程号杀死(杀两次,第二次杀失败,假活) 打开页面查看：http://hadoop2:16010或http://hadoop3:16010 预分区每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致规划好，以提高HBase性能 手动设定预分区 1234# 具体分区信息可以在web页面的table中查看# 四个键分五个区# 逐个字符比较create 'staff1','info','partition1',SPLITS =&gt; ['1000','2000','3000','4000'] 生成16禁止序列预分区(基本不用) 1create 'staff2','info','partition2',&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'&#125; 按照文件中设置的规则预分区创建splits.txt文件内容如下： 1234aabbddcc 采用文件设置预分区形式 12# 默认会对分区文件做排序,不然左开右闭会出问题create 'staff3','partition3',SPLITS_FILE =&gt; '/opt/module/data/hbase/splits.txt' 使用JavaAPI创建预分区 1234// 自定义算法，产生一系列hash散列值存储在二维数组中byte[][] splitKeys = ...// 还有四个参数的方法...admin.createTable(tableDescriptor,splitKeys); RowKey设计一条数据的唯一标识就是RowKey，那么这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内，设计RowKey的主要目的，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈RowKey常用的设计方案 生成随机数、hash、散列值：比如SHA1 字符反转 字符串拼接 示例讲解电信要求统计用户的时间段内的通话流水，根据什么分区，假如要分近300个区？电话号码11位，为了将对应到300个分区，而且要求一个号码需要对应到一个分区。那么如果有些号码电话打的很多，那么一个分区可能还是有问题，所以根据号码和时间进行分区。如果分到1年，时间颗粒度太大；最好采用1月。最终采用取余方式：hash(15988814888(电话号码) + 2020(年) + 07(月)) % 300——此处的’+’只做一个示意作用，具体需要实践和经验。那么假如我需要获取15988814888用户在2020年6月份的通话流水，怎么根据startRowKey和endRowKey获取数据呢？startRowKey(xxx为分区号,是根据公式计算得出的;最后是rowKey比较规则:有比没有大)：xxx_15988814888_2020_06endRowKey(‘|’的ascii码很大,或者也可以为2020_03;这里不会出现第一位大于遮蔽第二位的问题)：xxx_15988814888_2020_06| 内存优化HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死 基础优化 允许的HDFS的文件中追加内容(hdfs-site.xml、hbase-site.xml) 12属性：dfs.support.append解释：开启HDFS追加同步，可以优秀地配合HBase的数据同步和持久化。默认值为true 优化 DataNode 允许的最大文件打开数(hdfs-site.xml) 12属性：dfs.datanode.max.transfer.threads解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096 优化延迟高的数据操作的等待时间(hdfs-site.xml) 12属性：dfs.image.transfer.timeout解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值(默认60000毫秒)，以确保socket不会被timeout掉 优化数据的写入效率(mapred-site.xml) 123属性：mapreduce.map.output.compress mapreduce.map.output.compress.codec解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式 设置RPC监听数量(hbase-site.xml) 12属性：hbase.regionserver.handler.count解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值 优化HStore文件大小(hbase-site.xml) 12属性：hbase.hregion.max.filesize解释：默认值10737418240(10GB)，如果需要运行HBase的MR任务，可以减小此值，因为一个 region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile 优化HBase客户端缓存(hbase-site.xml) 12属性：hbase.client.write.buffer解释：用于指定Hbase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的 指定scan.next扫描HBase所获取的行数(hbase-site.xml) 12属性：hbase.client.scanner.caching解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大 flush、compact、split机制当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二涉及属性：hbase.hregion.memstore.flush.size = 134217728(即128M就是Memstore的默认阈值)这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM 12hbase.regionserver.global.memstore.upperLimit &#x3D; 0.4hbase.regionserver.global.memstore.lowerLimit &#x3D; 0.38 当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Hive","slug":"BigData/Hive","date":"2020-07-06T12:57:06.000Z","updated":"2020-09-23T07:53:25.191Z","comments":true,"path":"2020/07/06/BigData/Hive/","link":"","permalink":"https://sobxiong.github.io/2020/07/06/BigData/Hive/","excerpt":"内容 Hive基本概念 Hive安装 Hive数据类型 DDL数据定义 DML数据操作 查询 例题实战(蚂蚁金服) 函数 压缩和存储 企业级调优 谷粒影音Hive实战 常见错误及解决方案","text":"内容 Hive基本概念 Hive安装 Hive数据类型 DDL数据定义 DML数据操作 查询 例题实战(蚂蚁金服) 函数 压缩和存储 企业级调优 谷粒影音Hive实战 常见错误及解决方案 Hive基本概念 什么是Hive 由Facebook开源用于解决海量结构化日志的数据统计 基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能 Hive处理的数据存储在HDFS Hive分析数据底层的实现是MapReduce 执行程序运行在Yarn上 本质是将HQL(Hive Query Language)转化成MapReduce程序 Hive的优缺点 优点： 操作接口采用类SQL语法，提供快速开发的能力(简单、容易上手) 避免了去写MapReduce，减少开发人员的学习成本 Hive的执行延迟比较高，因此Hive常用于数据分析和对实时性要求不高的场合 Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数 缺点： Hive的HQL表达能力有限： 迭代式算法无法表达 数据挖掘方面不擅长 Hive的效率比较低： Hive自动生成的MapReduce作业，通常情况下不够智能化 Hive调优比较困难，粒度较粗 Hive架构原理 用户接口：ClientCLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive) 元数据：Metastore元数据包括：表名、表所属的数据库(默认是default)、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在目录等； 默认存储在自带的derby数据库中(存在bug)，推荐使用MySQL存储Metastore Hadoop：使用HDFS进行存储，使用MapReduce进行计算 驱动器：Driver 解析器(SQL Parser)：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误 编译器(Physical Plan)：将AST编译生成逻辑执行计划 优化器(Query Optimizer)：对逻辑执行计划进行优化 执行器(Execution)：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/SparkHive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将 执行返回的结果输出到用户交互接口 Hive和数据库比较由于Hive采用了类似SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。下面将从多个方面来阐述Hive和数据库的差异。数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性 查询语言由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言 HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发 数据存储位置Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中 数据更新由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的 索引Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于MapReduce的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive不适合在线数据查询 执行Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的。而数据库通常有自己的执行引擎 执行延迟Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小。当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势 可扩展性由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的(世界上最大的Hadoop集群在Yahoo，2009年的规模在4000台节点左右)。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右 数据规模由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小 Hive安装 安装地址 Hive官网地址：http://hive.apache.org 文档查看地址：https://cwiki.apache.org/confluence/display/Hive/GettingStarted 下载地址：http://archive.apache.org/dist/hive github地址：https://github.com/apache/hive Hive安装部署 Hive安装及配置 上传：把apache-hive-3.1.2-bin.tar.gz上传到/opt/software目录下 解压：tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/ 修改目录名：mv apache-hive-3.1.2-bin hive-3.1.2 修改配置文件(conf目录下)： 备份一份配置文件：cp hive-env.sh.template hive-env.sh.template.copy 修改配置文件后缀：mv hive-env.sh.template hive-env.sh 配置hive-env.sh文件(底部加入)： 12export HADOOP_HOME=/opt/module/hadoop-3.1.3export HIVE_CONF_DIR=/opt/module/hive-3.1.2/conf Hadoop集群启动(hadoop1启动hdfs——sbin/start-dfs.sh,hadoop2启动yarn——sbin/start-yarn.sh) Hive基本操作 初始化默认的derby数据库：bin/schematool -dbType derby -initSchema(初始化后在hive根目录会产生derby.log和metastore目录) 启动hive：bin/hive 启动时会发生Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V错误 这是因为hive内依赖的guava和hadoop内的版本不一致 分别查看hive(lib目录下)和hadoop(share/hadoop/common/lib目录下)的guava依赖版本：guava-19.0.jar和guava-27.0-jre.jar 删除hive的低版本guava-19.0.jar，将hadoop的高版本guava-27.0-jre.jar复制到hive的lib目录下 重新启动hive 查看数据库：show databases; 打开默认数据库：use default; 显示default数据库中的表：show tables; 创建一张表(数据类型为java中类型)：create table student(id int,name string); 查看表的结构：desc student; 向表中插入数据：insert into student values(1,”SOBXiong”); 查询表中数据：select * from student; 退出hive：quit; 本地文件导入Hive需求：将本地/opt/module/data/hive/student.txt的数据导入到hive的student表中 数据准备(tab键隔开) 1231 xixi2 haha3 hehe Hive操作 导入student.txt的数据到之前创建的student表中：load data local inpath ‘/opt/module/data/hive/student.txt’ into table student; 查询结果(发现都是NULL NULL,因为格式不对)：select * from student; 删除已创建的student表：drop table student; 创建新的student表(声明文件分隔符’\\t’)：create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’; 重新导入数据并重新查询结果 查看http://hadoop1:9870中的HDFS文件，发现/user/hive/warehouse/student下就有数据 第二种插入数据的方式：直接将文件上传至HDFS服务器 上传本地文件(相当于cp复制)：hadoop fs -put stu1.txt /user/hive/warehouse/student 上传HDFS文件(相当于mv移动)：hadoop fs -put /stu2.txt /user/hive/warehouse/student derby存储元数据的问题(推荐使用mysql)： 只能开启一个hive客户端 在不同的目录开启hive客户端会在当前目录下创建derby.log和metastore文件，相当于数据不共享 MySql安装 安装包准备： 查看yum中历史的mysql或者mariadb的依赖：rpm -qa | grep mysql/mariadb 如有历史依赖，删除：yum remove mysql-libs/mariadb-libs 下载mysql的rpm包：前往https://dev.mysql.com/downloads/mysql/下载5.7.30的Red Hat Enterprise Linux7版本(CentOS7)的RPM Bundle包 安装MySql 解压tar包：tar -xvf mysql-5.7.30-1.el7.x86_64.rpm-bundle.tar 使用rpm命令安装MySql组件 12345# 依赖关系为common→libs→client→serverrpm -ivh commonrpm -ivh libsrpm -ivh clientrpm -ivh server 启动MySql：systemctl start mysqld.service 查看MySql状态：systemctl status mysqld.service 查看初始化的随机密码：grep ‘temporary password’ /var/log/mysqld.log 登录MySql：mysql -u root -p 修改密码校验策略(不然设置新密码会提示密码错误)：set global validate_password_policy=0; 修改密码：alter user root@localhost identified by ‘your password’; 授权root用户远程访问权限 12grant all privileges on *.* to 'root' @'%' identified by 'your password';flush privileges; 设置MySql完毕，退出：quit; Hive元数据配置到MySql 拷贝mysql-connector JDBC驱动文件 前往https://dev.mysql.com/downloads/connector/j/下载驱动文件5.1.49版本 解压文件mysql-connector-java-5.1.49.tar.gz，拷贝mysql-connector-java-5.1.49-bin.jar到hive的lib目录下 配置metastore到MySql 在conf目录下创建hive-site.xml配置文件：touch hive-site.xml 修改配置文件： 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- xml下的&amp;需要转义为&amp;amp; --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop1:3306/metastore?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;your password&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 初始化Hive的MySql元数据数据库：bin/schematool -dbType mysql -initSchema 启动hive，MySql中新增了metastore数据库(表DBS和TBS比较重要) HiveJDBC访问 停止hadoop： 1234# hadoop1stop-dfs.sh# hadoop2stop-yarn.sh 修改hadoop配置： 12345678910111213141516171819202122&lt;!-- hdfs-site.xml 启用webhdfs --&gt;&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- core-site.xml 设置hadoop的代理用户 hadoop.proxyuser.xxx.hosts xxx是操作的用户 org.apache.hadoop.security.authorize.AuthorizationException: User: sobxiong is not allowed to impersonate root(state=08S01,code=0) User:xxx即为下面该填入的用户--&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.sobxiong.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.sobxiong.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 修改hive配置： 1234567891011&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;description&gt;Bind host on which to run the HiveServer2 Thrift service.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;11000&lt;/value&gt; &lt;description&gt;Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.&lt;/description&gt;&lt;/property&gt; 启动hiveserver2服务：bin/hiveserver2 启动beeline：bin/beeline 连接hiveserver2： 1234beeline&gt; !connect jdbc:hive2://hadoop1:11000Enter username for jdbc:hive2://hadoop102:10000: sobxiongEnter password for jdbc:hive2://hadoop102:10000: your password(数据库的密码)# 接下来的操作就跟Hive Cli使用类似SQL Hive常用交互命令 -e &lt;quoted-query-string&gt;：不进入hive的交互窗口执行sql语句，例如：bin/hive -e “select * from student;” -f &lt;filename&gt;：执行脚本中sql语句 结果打印在terminal上：bin/hive -f /opt/module/data/hive/hive.hql 结果打印在指定文件中：bin/hive -f /opt/module/data/hive/hive.hql &gt; ./hive_result.txt Hive其他命令操作 在Hive Cli命令窗口中查看hdfs文件系统：dfs -ls / 在Hive Cli命令窗口中查看本地文件系统：! ls / 查看在hive中输入的所有历史命令：cat ~/.hivehistory Hive常见属性配置 Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse 在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹 修改default数据仓库原始位置(hive-default.xml.template -&gt; hive-site.xml) 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 查询后信息显示配置 在hive-site.xml加入如下配置： 12345678910&lt;!-- 表列名显示 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 当前使用数据库显示 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 重启Hive Cli Hive运行日志信息配置 Hive的日志信息默认存放在/tmp/{current_user}目录下 修改Hive的日志信息存放在hive安装目录的logs文件夹下修改conf/hive-log4j.properties配置文件(hive-log4j.properties.template -&gt; hive-log4j.properties)：hive.log.dir=/opt/module/hive-3.1.2/logs 参数配置方式 查看当前所有的配置信息(Hive Cli命令窗口下)：set; 参数配置的三种方式 配置文件方式默认配置文件：hive-default.xml用户自定义配置文件：hive-site.xml注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效 命令行参数方式启动Hive时，可以在命令行添加-hiveconf param=value来设定参数例如：bin/hive -hiveconf mapred.reduce.tasks=10;(注意：仅对本次hive启动有效)查看参数设置：set mapred.reduce.tasks; 参数声明方式在HQL中使用SET关键字设定参数(注意：仅对本次hive启动有效)例如：set mapred.reduce.tasks=100;上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了 Hive数据类型 基本数据类型(Hive数据类型大小写不敏感) Hive数据类型 Java数据类型 长度 TINYINT byte 1byte有符号整数 SMALLINT short 2byte有符号整数 INT int 4byte有符号整数 BIGINT long 8byte有符号整数 BOOLEAN boolean 布尔类型，true或false FLOAT float 单精度浮点数 DOUBLE double 双精度浮点数 STRING string 字符系列，可以指定字符集，可以使用单引号或者双引号 TIMESTAMP - 时间类型 BINARY - 字节数组 Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过他不能声明其最多能存储多少个字符，理论上它可以存储2GB的字符数 集合数据类型 数据类型 描述 语法示例 STRUCT 和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用 struct() MAP MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取键last对应的值数据 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用 Array() Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套 集合数据类型案例实操 假设JSON为原始数据，具体如下： 123456789101112&#123; \"name\": \"songsong\", \"friends\": [\"bingbing\" , \"lili\"], \"children\": &#123; \"xiao song\": 18 , \"xiaoxiao song\": 19 &#125;, \"address\":&#123; \"street\": \"hui long guan\" , \"city\": \"beijing\" &#125;&#125; 基于上述数据结构，建立本地测试文件test.txt，具体格式如下： 12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 注意：MAP、STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用’_’ Hive上创建测试表test 1234567891011121314create table test(name string,friends array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)/* 设置列分隔符为',' */row format delimited fields terminated by ','/* 设置map、struct和array的分隔符(数据分割符号)为'_' */collection items terminated by '_'/* 设置map中的key/value的分隔符为',' */map keys terminated by ':'/* 设置行分隔符为'\\n'(也是默认值) */lines terminated by '\\n'; 导入文本数据到测试表中 1load data local inpath '/opt/module/data/hive/test.txt' into table test; 访问三种集合列里的数据 1select friends[1],children['xiao song'],address.city from test; 类型转换Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化。例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作 隐式类型转换规则 任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT 所有整数类型、FLOAT和STRING(符合数字)类型都可以隐式地转换成DOUBLE TINYINT、SMALLINT、INT都可以转换为FLOAT BOOLEAN类型不可以转换为任何其它的类型 使用CAST操作显示进行数据类型转换例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值NULL DDL数据定义 创建数据库 创建一个数据库，默认在HDFS上的存储路径式/user/hive/warehouse/*.db：create database if not exists db_hive;(if not exists避免要创建的数据库已存在) 创建一个数据库，指定在HDFS上存放的路径 1create database db_hive2 location '/db_hive2.db' 查询数据库 显示数据库 显示数据库：show databases; 过滤查询显示的数据库 1show databases like 'db_hive'; 查看数据库 显示数据库信息：desc database db_hive; 显示数据库详细信息(extended)：desc database extended db_hive; 切换当前数据库：use db_hive; 修改数据库用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置修改数据库属性值： 1alter database db_hive set dbproperties('createtime'='20200708') 查看修改结果：desc database extended db_hive; 删除数据库 删除空数据库：drop database if exists db_hive;(if exists避免要删除的数据库不存在) 如果数据库中表不为空，可以采用cascade命令集联强制删除：drop database db_hive cascade; 创建表 建表语法 123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)][CLUSTERED BY (col_name, col_name, ...)[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS][ROW FORMAT row_format][STORED AS file_format][LOCATION hdfs_path] 字段解释说明 CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径(LOCATION)，Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据 COMMENT：为表和列添加注释 PARTITIONED BY创建分区表 CLUSTERED BY创建分桶表 SORTED BY不常用 ROW FORMATDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIE (property_name=property_value, property_name=property_value, …)]用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化 STORED AS指定存储文件类型常用的存储文件类型：SEQUENCEFILE(二进制序列文件)、TEXTFILE(文本)、RCFILE(列式存储格式文件)如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE LOCATION：指定表在HDFS上的存储位置 LIKE允许用户复制现有的表结构，但是不复制数据 管理表 介绍默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会(或多或少地)控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据 实际操作 创建普通表 123456create table if not exists student(id int, name string)row format delimited fields terminated by '\\t'stored as textfilelocation '/user/hive/warehouse/student2'; 根据查询结果创建表(查询的结果会添加到新创建的表中)：create table if not exists student2 as select id, name from student; 根据已存在的表结构创建表：create table if not exists student3 like student; 查询表的类型：desc formatted student; 外部表 介绍：因为表是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉 实际操作(创建表,其余操作与管理表类似) 123456create external table if not exists default.dept(deptno int,dname string,loc int)row format delimited fields terminated by '\\t'; 管理表与外部表 相互转换 12345-- 修改内部表为外部表：alter table student2 set tblproperties('EXTERNAL'='TRUE');-- 修改外部表为内部表：alter table student2 set tblproperties('EXTERNAL'='FALSE');-- 注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写 使用场景每天将收集到的网站日志定期流入HDFS文本文件。在外部表(原始日志表)的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表 分区表分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多 分区表基本操作 创建分区表语法 12345create table dept_partition(deptno int, dname string, loc string)partitioned by (month string)row format delimited fields terminated by '\\t'; 加载数据到分区表中 123hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201709');hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201708');hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201707’); 查询分区表中数据 单分区查询 1select * from dept_partition where month='201709'; 多分区联合查询 12345678-- 查询多个分区select * from dept_partition where month='201709'unionselect * from dept_partition where month='201708'unionselect * from dept_partition where month='201707';-- 查询全部select * from dept_partition; 增加分区 创建单个分区 1alter table dept_partition add partition(month='201706'); 同时创建多个分区 1alter table dept_partition add partition(month='201705') partition(month='201704'); 删除分区： 删除单个分区 1alter table dept_partition drop partition (month='201704'); 同时删除多个分区 1alter table dept_partition drop partition (month='201705'), partition (month='201706'); 查看分区表有多少分区：show partitions dept_partition; 查看分区表结构：desc formatted dept_partition; 分区表扩展用法 创建二级分区表 12345create table dept_partition2(deptno int, dname string, loc string)partitioned by (month string, day string)row format delimited fields terminated by '\\t'; 加载二级分区数据 1load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709', day='13'); 查询二级分区数据 1select * from dept_partition2 where month='201709' and day='13'; 将数据上传到分区目录后，让分区表和数据产生关联的方式 上传数据后修复(适用于数据较多的情况) 上传数据： 123# Hive Cli命令环境下dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;dfs -put /opt/module/data/hive/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; 查询数据(查询不到) 1select * from dept_partition2 where month='201709' and day='12'; 执行修复命令：msck repair table dept_partition2; 上传数据后添加分区 上传数据(同上) 执行添加分区 1alter table dept_partition2 add partition(month='201709',day='11'); 创建文件夹后load数据到分区 创建目录：dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; 上传数据 1load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709',day='10'); 修改表 重命名表 语法：ALTER TABLE table_name RENAME TO new_table_name 实例：alter table dept_partition2 rename to dept_partition3; 添加、修改和删除表分区(同上) 增加、修改、替换列信息 语法： 更新列：ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 添加和替换列：ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …) 注意：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段 实操 查询表结构(用于查看修改结果)：desc dept_partition; 添加列：alter table dept_partition add columns(deptdesc string); 更新列：alter table dept_partition change column deptdesc desc int;(貌似需要符合隐式转换规则) 替换列：alter table dept_partition replace columns(deptno string, dname string, loc string); 删除表：drop table dept_partition; DML数据操作 数据导入 向表中装载数据(Load) 语法 1load data [local] inpath 'path_name' [overwrite] into table table_name [partition (partcol1=val1,...)]; 参数解释 load data：表示加载数据 local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表 inpath：表示加载数据的路径 overwrite：表示覆盖表中已有数据，否则表示追加 into table：表示加载到哪张表 table_name：表示具体的表 partition：表示上传到指定分区 实际操作 加载本地文件到Hive： 1load data local inpath '/opt/module/data/hive/student.txt' into table default.student; 加载(覆盖)HDFS文件数据到Hive中 123# Hive Cli命令环境下dfs -put /opt/module/data/hive/student.txt /user/sobxiong/hive;load data inpath '/user/sobxiong/hive/student.txt' (overwrite)into table default.student; 通过查询语句向表中插入数据(Insert) 基本插入数据 1insert into table student partition(month='201709') values(1,'wangwu'); 根据单表查询结果插入数据 12insert overwrite table student partition(month='201708')select id, name from student where month='201709'; 根据多表查询结果插入数据 12345from studentinsert overwrite table student partition(month='201707')select id, name where month='201709'insert overwrite table student partition(month='201706')select id, name where month='201709'; 查询语句中创建表并加载数据(As Select,查询的结果会添加到新创建的表中)：create table if not exists student3 as select id, name from student; 创建表时通过Location指定加载数据路径 指定在HDFS上的位置创建表 12345create table if not exists student5(id int, name string)row format delimited fields terminated by '\\t'location '/user/hive/warehouse/student5'; 上传数据到HDFS上 查询数据 Import数据到指定Hive表中(注意：先用export导出后,才能导入) 1import table student2 partition(month='201709') from '/user/hive/warehouse/export/student'; 数据导出 Insert导出 将查询的结果导出到本地 1insert overwrite local directory '/opt/module/data/hive/export/student' select * from student; 将查询的结果格式化导出到本地 1insert overwrite local directory '/opt/module/data/hive/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' select * from student; 将查询结果导出到HDFS上(取消local) Hadoop命令导出到本地(Hive Cli命令环境下)：dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/data/hive/export/student3.txt; Hive Shell命令导出：基本语法(hive -f/-e 执行语句或脚本 &gt; file_name) 1bin/hive -e 'select * from default.student;' &gt; /opt/module/data/hive/export/student4.txt; Export导出到HDFS上(导出数据包括表数据和元数据) 1export table default.student to '/user/hive/warehouse/export/student'; Sqoop导出(敬请期待) 清除表中数据(注意：只能删除管理表,不能删除外部表中数据)：truncate table student; 查询 查询语句语法官方wiki文档：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select 12345678910[WITH CommonTableExpression (, CommonTableExpression)*]SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [ORDER BY col_list] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] ] [LIMIT number] 基本查询 全表和特定列查询注意： SQL大小写不敏感 SQL可以写在一行/多行 关键字不能被缩写也不能分行 各子句一般要分行写 使用缩进提高语句的可读性 列别名 重命名一个列 便于计算 紧跟列名(或在列名和别名之间加入关键字AS) 算术运算符 +、-、*、/、%、&amp;、|、^、- 常用函数 求总行数：count() 求最大值/最小值：max()/min() 求总和：sum() 求平均值：avg() limit语句：典型的查询会返回多行数据。LIMIT子句用于限制返回的行数 Where语句：使用Where子句可以过滤掉不满足条件的行，需要紧跟From子句 比较运算符(同样可以用于Join… on和Having语句)以下只介绍除=、&gt;和&lt;等简单的运算符 操作符 支持的数据类型 描述 A&lt;=&gt;B 基本数据类型 如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL A&lt;&gt;B, A!=B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE A [NOT] BETWEEN B AND C 基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果 A IS [NOT] NULL 所有数据类型 如果A(不)等于NULL，则返回TRUE(FALSE)，反之返回FALSE(TRUE) [NOT] IN(数值1, 数值2) 所有数据类型 (不)使用IN运算显示列表中的值 A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果 A RLIKE B, A REGEXP B STRING 类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配 Like和RLike 使用LIKE运算选择类似的值 选择条件可以包含字符或数字： % 代表零个或多个字符(任意个字符) _ 代表一个字符 RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件 案例 查找以2开头薪水的员工信息 1select * from emp where sal LIKE '2%'; 查找第二个数值为2的薪水的员工信息 1select * from emp where sal LIKE '_2%'; 查找薪水中含有2的员工信息 1select * from emp where String(sal) RLIKE '[2]'; 逻辑运算符(And/Or/Not) 分组 Group By语句GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作 Having语句与where不同点： where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据 where后面不能写分组函数，而having后面可以使用分组函数 having只用于group by分组统计语句 Join语句 等值Join 12select e.empno, e.ename, d.deptno, d.dname from emp e join dept don e.deptno = d.deptno; 表的别名好处：(1)简化查询；(2)有限地提高执行效率 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来 12select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno= d.deptno; 左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回 12select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno= d.deptno; 右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回 12select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno= d.deptno; 满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代 12select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno= d.deptno; 多表连接：连接n个表，一般至少需要n-1个连接条件 123456SELECT e.ename, d.deptno, l. loc_nameFROM emp eJOIN dept dON d.deptno = e.deptnoJOIN location lON d.loc = l.loc; 多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的 笛卡尔积一般会在下面情况下出现： 省略连接条件 连接条件无效 所有表中的所有行互相连接一般会设置禁止出现笛卡尔积，如果有特殊情况，需要在单独在命令执行前设置一次性环境 连接谓词在新版中支持or 123-- 无实际意义(ename = dname),只做可行性试验select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno= d.deptno or e.ename=d.dname; 排序 全局排序(Order By:一个Reducer) 排序方式：ASC(ascend升序,默认)、DESC(descend降序) Order By子句在Select语句的结尾 多个列排序(同MySQL) 内部排序(Sort By:每个Reduce内部进行排序,对全局结果集来说不是排序) 注意：如果使用sort by不使用distribute by(即没有指定分区字段)，那么就采用一种产生随机数的函数分配分区(避免数据倾斜) 设置reduce数：set mapreduce.job.reduce=3; 按照部门编号降序排序： 123-- 三个结果文件insert overwrite local directory '/opt/module/data/hive/sortby-result'select * from emp sort by deptno desc; 分区排序(Distribute By:类似MR中partition,进行分区,结合sort by使用) DISTRIBUTE BY语句要写在SORT BY语句之前。distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果 设置reduce数：set mapreduce.job.reduces=3; 先按照部门编号分区，再按照员工编号降序排序 1insert overwrite local directory '/opt/module/data/hive/distribute-result' select * from emp distribute by deptno sort by empno desc; Clubster By 当distribute by和sorts by字段相同时，可以使用cluster by方式。 cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC 具体实操 123-- 以下两种写法等价select * from emp cluster by deptno;select * from emp distribute by deptno sort by deptno; 注意：按照部门编号分区，不一定就是固定死的数值(要看具体数据表中的字段不同的数目以及reduce设置的数目)，可以是20号和30号部门分到一个分区里面去 分桶及抽样查询 分桶表数据存储 介绍：分区针对的是数据的存储路径；分桶针对的是数据文件。分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。分桶是将数据集分解成更容易管理的若干部分的另一个技术 通过导入数据文件方式创建分桶表 创建表 1234create table stu_buck(id int, name string)clustered by(id)into 4 bucketsrow format delimited fields terminated by '\\t'; 查看表结构：desc formatted stu_back;(Num Buckets: 4) 导入数据到分桶表 1load data local inpath '/opt/module/data/hive/student.txt' into table stu_buck; 在浏览器上查看创建的分桶表是否分成4个桶(4个文件)：在新版本中分成四个桶 通过子查询导入数据方式创建分桶表 先创建普通的stu表 1create table stu(id int, name string) row format delimited fields terminated by '\\t'; 向普通stu表中导入数据 1load data local inpath '/opt/module/data/hive/student.txt' into table stu; 清空stu_buck表中数据：truncate table stu_buck; 子查询方式导入数据到分桶表：insert into table stu_buck select id, name from stu; 浏览器查看：新版本中有4个分桶(文件) 需要设置Hive的属性(老版本) 1234567# 设置启用分桶set hive.enforce.bucketing=true;# 设置reduce数目为-1,会自动使用分桶数作为reduce的数目set mapreduce.job.reduces=-1;# 清空分桶表,重新导入数据,再去查看浏览器中的分桶truncate table stu_back;insert into table stu_buck select id, name from stu; 分桶抽样查询 介绍：对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求 实操：select * from stu_buck tablesample(bucket 1 out of 4 on id); 语法：tablesample是抽样语句，语法：tablesample(bucket x out of y) 参数解释 y：y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2)2个bucket的数据，当y=8时，抽取(4/8)1/2个bucket的数据 x：x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取(4/2)2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据 注意：x的值必须小于等于y的值 其他常用查询函数 空字段赋值 函数说明：NVL：给值为NULL的数据赋值，它的格式是NVL(string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL，则返回NULL(replace_with可以是常量也可以是同表的另一个列) 查询(常量)：select nvl(comm,-1) from emp; 查询(另一列)：select nvl(comm,mgr) from emp; 时间类 date_format(格式化时间,第一个变量时间串只能是以’-‘分割) 123select date_format('2020-07-11','yyyy:MM:dd');-- 时间不以'-'分割,可以通过regex正则表达式替换/自定义函数select regexp_replace('2020/07/11','/','-'); date_add/sub(时间跟天数相加/相减) 1select date_add('2020-07-11',-5/5); datediff(时间相差的间隔,前者-后者) 1select datediff('2020-07-11','2020-07-08'); CASE WHEN 数据准备 123456悟空 A 男大海 A 男宋宋 B 男凤姐 A 女婷姐 B 女婷婷 B 女 需求：求出不同部门的男女各多少人 创建emp_set.txt，复制数据 创建Hive表并导入数据 12345678-- 建表create table emp_sex(name string,dept_id string,sex string)row format delimited fields terminated by \"\\t\";-- 导入本地数据load data local inpath '/opt/module/data/hive/emp_sex.txt' into table emp_sex; 查询数据 12345678select dept_id, sum(case sex when '男' then 1 else 0 end) male_count, sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id; 行转列 相关函数说明CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段 数据准备 12345孙悟空 白羊座 A大海 射手座 A宋宋 白羊座 B猪八戒 白羊座 A凤姐 射手座 A 需求：把星座和血型一样的人归类到一起。结果如下 person_info.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table person_info(name string,constellation string,blood_type string)row format delimited fields terminated by \"\\t\";-- 导入数据load data local inpath '/opt/module/data/hive/person_info.txt' into table person_info; 查询数据 123456789101112131415161718192021222324252627-- 总查询语句select t1.constellation_blood_type, concat_ws('|', collect_set(t1.name)) namefrom (select name, concat(constellation, \",\", blood_type) constellation_blood_type from person_info ) t1group by t1.constellation_blood_type;-- 第一步,查询出'射手座,A' '大海'select concat(constellation, \",\", blood_type) constellation_blood_type, namefrom person_info;-- 第二步,连接相同星座和血型的nameselect constellation_blood_type, concat_ws('|', collect_set(t1.name)) namefrom t1group by t1.constellation_blood_type;-- 最后一步,替换from后的t1为第一步中的临时表 列转行 函数说明EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行LateRal View： 用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias 解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合 数据准备 123《疑犯追踪》 悬疑,动作,科幻,剧情《Lie to me》 悬疑,警匪,动作,心理,剧情《战狼2》 战争,动作,灾难 需求：将电影分类中的数组数据展开，结果如下所示 123456789101112《疑犯追踪》 悬疑《疑犯追踪》 动作《疑犯追踪》 科幻《疑犯追踪》 剧情《Lie to me》 悬疑《Lie to me》 警匪《Lie to me》 动作《Lie to me》 心理《Lie to me》 剧情《战狼2》 战争《战狼2》 动作《战狼2》 灾难 创建本地movie.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table movie_info(movie string,category array&lt;string&gt;)row format delimited fields terminated by \"\\t\"collection items terminated by \",\";-- 导入数据load data local inpath \"/opt/module/data/hive/movie.txt\" into table movie_info; 按需查询数据 12345select movie, category_namefrom movie_info lateral view explode(category) table_tmp as category_name; 窗口函数 函数说明OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化 参数说明 Over()内 CURRENT ROW：当前行 n PRECEDING：往前n行数据 n FOLLOWING：往后n行数据 UNBOUNDED：起点，UNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点 Over()外 LAG(col,n)：往前第n行数据 LEAD(col,n)：往后第n行数据 NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型 数据准备 123456789101112131415&#x2F;&#x2F; name,orderdate,costjack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94 需求 查询在2017年4月份购买过的顾客及总人数 查询顾客的购买明细及月购买总额 上述的场景，要将cost按照日期进行累加 查询顾客上次的购买时间 查询前20%时间的订单信息 创建本地business.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table business(name string,orderdate string,cost int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';-- 导入数据load data local inpath \"/opt/module/data/hive/business.txt\" into table business; 按需查询数据 查询在2017年4月份购买过的顾客及总人数 1234select name,count(*) over()from businesswhere substring(orderdate,1,7) = '2017-04'group by name; 查询顾客的购买明细及月购买总额 12select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) frombusiness; 上述的场景,要将cost按照日期进行累加 12345678910-- partition by ... order by和distribute by ... sort by效果相同,可替换select name,orderdate,cost, sum(cost) over() as sample1,--所有行相加 sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行from business; 查看顾客上次的购买时间 1select name,orderdate,cost,lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate) as last_time from business; 查询前20%时间的订单信息 12345select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) ntile_id from business) bwhere ntile_id = 1; Rank排名函数 函数说明： Rank()：排序相同时会重复，总数不会变 DENSE_RANK()：排序相同时会重复，总数会减少 ROW_NUMBER()：会根据顺序计算 数据准备 12345678910111213&#x2F;&#x2F; name subject score孙悟空 语文 87孙悟空 数学 95孙悟空 英语 68大海 语文 94大海 数学 56大海 英语 84宋宋 语文 64宋宋 数学 86宋宋 英语 84婷婷 语文 65婷婷 数学 85婷婷 英语 78 需求：计算各学科成绩排名 创建本地score.txt文件，复制数据 创建Hive表并导入数据 12345678-- 建表create table score(name string,subject string,score int)row format delimited fields terminated by \"\\t\";-- 导入数据load data local inpath '/opt/module/data/hive/score.txt' into table score; 按需查询 12345678select name, subject, score, rank() over(partition by subject order by score desc) rank, dense_rank() over(partition by subject order by score desc) dense_rank, row_number() over(partition by subject order by score desc) row_numberfrom score; 例题实战(蚂蚁金服) 背景说明用户每天的蚂蚁森林低碳生活领取的记录流水表(user_low_carbon表) 字段名 注释 user_id 用户编号 data_dt 日期 low_carbon 减少碳排放(g:克) 用于记录申领环保植物所需要减少的碳排放量的蚂蚁森林植物换购表(plant_carbon) 字段名 注释 plant_id 植物编号 plant_name 植物名 low_carbon 换购植物所需要的碳 题目 蚂蚁森林蚂蚁森林植物申领统计问题：假设2017年1月1日开始记录低碳数据(user_low_carbon)，假设2017年10月1日之前满足申领条件的用户都申领了一颗p004-胡杨，剩余的能量全部用来领取p002-沙柳统计在10月1日累计申领p002-沙柳排名前10的用户信息、以及他比后一名多领了几颗沙柳得到的统计结果如下表样式： user_id plant_count less_cout(比后一名多领的棵树) u_101 1000 100 u_088 900 400 u_103 500 … 蚂蚁森林低碳用户排名分析问题：查询user_low_carbon表中每日流水记录，条件为：用户在2017年，连续三天(或以上)的天数里，每天减少碳排放(low_carbon)都超过100g的用户低碳流水。需要查询返回满足以上条件的user_low_carbon表中的记录流水。例如用户u_002符合条件的记录如下，因为2017/1/2~2017/1/5连续四天的碳排放量之和都大于等于100g： seq(序号,不涉及当前列) user_id data_dt low_carbon xxxxx10 u_002 2017/1/2 150 xxxxx11 u_002 2017/1/2 70 xxxxx12 u_002 2017/1/3 30 xxxxx13 u_002 2017/1/3 80 xxxxx14 u_002 2017/1/4 150 xxxxx14 u_002 2017/1/5 101 备注：统计方法不限于sql、procedure、python,java等 解决 前期准备 创建表 12create table user_low_carbon(user_id String,data_dt String,low_carbon int) row format delimited fields terminated by '\\t';create table plant_carbon(plant_id string,plant_name String,low_carbon int) row format delimited fields terminated by '\\t'; 加载数据 12load data local inpath \"/opt/module/data/hive/user_low_carbon.txt\" into table user_low_carbon;load data local inpath \"/opt/module/data/hive/plant_carbon.txt\" into table plant_carbon; 设置本地模式(加快运行速度)：set hive.exec.mode.local.auto=true; 求解问题一 统计在10月1日前每个用户减少碳排放量的总和(取前11名：为了求与后一名的差值,并在第一阶段过滤数据集,加快运行速度) 1234567-- t1表select user_id,sum(low_carbon) sum_carbonfrom user_low_carbonwhere datediff(regexp_replace(data_dt,\"/\",\"-\"),\"2017-10-1\")&lt;0group by user_idorder by sum_carbon desclimit 11; 取出申领胡杨的碳量 12-- t2表select low_carbon from plant_carbon where plant_id=\"p004\"; 取出申领沙柳的碳量 12-- t3表select low_carbon from plant_carbon where plant_id=\"p002\"; 求出能申领沙柳的棵树 1234567891011121314151617181920212223242526-- t4表(floor下取整)select user_id, floor((t1.sum_carbon-t2.low_carbon) / t3.low_carbon) treeCountfrom t1,t2,t3;-- 替换t1,t2,t3select user_id, floor((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCountfrom ( select user_id,sum(low_carbon) sum_carbon from user_low_carbon where datediff(regexp_replace(data_dt,\"/\",\"-\"),\"2017-10-1\")&lt;0 group by user_id order by sum_carbon desc limit 11 )t1, ( select low_carbon from plant_carbon where plant_id=\"p004\" )t2, ( select low_carbon from plant_carbon where plant_id=\"p002\" )t3 求出前一名比后一名多几棵 12345678910111213141516171819202122232425262728293031323334select user_id, treeCount, treeCount - (lead(treeCount,1) over(order by treeCount desc))from t4limit 10;-- 替换t4表select user_id, treeCount, treeCount-(lead(treeCount,1) over(order by treeCount desc)) less_countfrom ( select user_id, floor((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCount from ( select user_id,sum(low_carbon) sum_carbon from user_low_carbon where datediff(regexp_replace(data_dt,\"/\",\"-\"),\"2017-10-1\")&lt;0 group by user_id order by sum_carbon desc limit 11 )t1, ( select low_carbon from plant_carbon where plant_id=\"p004\" )t2, ( select low_carbon from plant_carbon where plant_id=\"p002\" )t3 )t4limit 10; 求解问题二 方式一(Hive Sql简单版) 过滤出2017年且单日低碳量超过100g 123456789101112-- t1表select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dtfrom user_low_carbonwhere substring(data_dt,1,4) = '2017'group by user_id,data_dthaving sum(low_carbon) &gt;= 100; 将前两行数据以及后两行数据的日期放至当前行 123456789101112131415161718192021222324252627282930313233-- t2表select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2from t1;-- 替t1select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1; 计算当前日期跟前后两行时间的差值 12345678910111213141516171819202122232425262728293031323334353637383940414243-- t3表select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_difffrom t2;-- 替换t2select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_difffrom ( select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2 from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2; 过滤出连续3天超过100g的用户 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657-- t4表select user_id, data_dtfrom t3where (lag2_diff = 2 and lag1_diff = 1) or (lag1_diff = 1 and lead1_diff = -1) or (lead1_diff = -1 and lead2_diff = -2);-- 替换t3select user_id, data_dtfrom ( select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_diff from ( select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2 from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2 )t3where (lag2_diff = 2 and lag1_diff = 1) or (lag1_diff = 1 and lead1_diff = -1) or (lead1_diff = -1 and lead2_diff = -2); 关联原表，获取流水信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970select ulc.user_id, ulc.data_dt, ulc.low_carbonfrom t4join user_low_carbon ulcon t4.user_id = ulc.user_id and t4.data_dt = date_format(regexp_replace(ulc.data_dt,'/','-'),'yyyy-MM-dd');-- 替换t4select ulc.user_id, ulc.data_dt, ulc.low_carbonfrom ( select user_id, data_dt from ( select user_id, data_dt, datediff(data_dt,lag2) lag2_diff, datediff(data_dt,lag1) lag1_diff, datediff(data_dt,lead1) lead1_diff, datediff(data_dt,lead2) lead2_diff from ( select user_id, data_dt, lag(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lag2, lag(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lag1, lead(data_dt,1,'1970-01-01') over(partition by user_id order by data_dt) lead1, lead(data_dt,2,'1970-01-01') over(partition by user_id order by data_dt) lead2 from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2 )t3 where (lag2_diff = 2 and lag1_diff = 1) or (lag1_diff = 1 and lead1_diff = -1) or (lead1_diff = -1 and lead2_diff = -2) )t4join user_low_carbon ulcon t4.user_id = ulc.user_id and t4.data_dt = date_format(regexp_replace(ulc.data_dt,'/','-'),'yyyy-MM-dd'); 方式二(Hive Sql困难版,使用等差数列) 过滤出2017年且单日低碳量超过100g(同方式一第一步) 123456789101112-- t1表select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dtfrom user_low_carbonwhere substring(data_dt,1,4) = '2017'group by user_id,data_dthaving sum(low_carbon) &gt;= 100; 按照日期进行排序,并给每一条数据一个标记 123456789101112131415161718192021222324252627-- t2表select user_id, data_dt, rank() over(partition by user_id order by data_dt) rkfrom t1;-- 替换t1表select user_id, data_dt, rank() over(partition by user_id order by data_dt) rkfrom ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1; 将日期减去当前的rank值 1234567891011121314151617181920212223242526272829303132333435363738394041424344/* 如果是连续的话,data_dt - rk结果一致 user_id , data_dt , rk ,data_sub_rk u_001 , 2017-01-02 , 1 , 2017-01-01 u_001 , 2017-01-06 , 2 , 2017-01-04 u_002 , 2017-01-02 , 1 , 2017-01-01 u_002 , 2017-01-03 , 2 , 2017-01-01 u_002 , 2017-01-04 , 3 , 2017-01-01 u_002 , 2017-01-05 , 4 , 2017-01-01*/-- t3表select user_id, data_dt, date_sub(data_dt,rk) data_sub_rkfrom t2;-- 替换t2select user_id, data_dt, date_sub(data_dt,rk) data_sub_rkfrom ( select user_id, data_dt, rank() over(partition by user_id order by data_dt) rk from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2; 过滤出连续3天超过100g的用户 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-- 当前适用于连续n天,只需要改3为n,具有通式性-- 当前只能过滤出用户,流水不行select user_idfrom t3group by user_id,data_sub_rkhaving count(*) &gt;= 3;-- 替换t3select user_idfrom ( select user_id, data_dt, date_sub(data_dt,rk) data_sub_rk from ( select user_id, data_dt, rank() over(partition by user_id order by data_dt) rk from ( select user_id, date_format(regexp_replace(data_dt,'/','-'),'yyyy-MM-dd') data_dt from user_low_carbon where substring(data_dt,1,4) = '2017' group by user_id,data_dt having sum(low_carbon) &gt;= 100 )t1 )t2 )t3group by user_id,data_sub_rkhaving count(*) &gt;= 3; 方式三(MapReduce) 123456789101112131415161718192021222324252627282930313233mapper(key:user_id + date,value:一行)grouping:user_idreduce()values:&#123; date = 1970-01-01 list = new ArrayList(); values.for( // 首次 if(date == 1970-01-01)&#123; list.add(value); date = value.date; &#125;else&#123; // 不是首次 if(value.date - date == 1)&#123; list.add(value); date = value.date; &#125;else&#123; if(list.size() &gt;= 3)&#123; context.write(list); &#125; list.clear(); list.add(value); date = value.date; &#125; &#125; ) // 防止漏了最后一行 if(list.size() &gt;= 3)&#123; context.write(list); &#125;&#125; 函数 系统内置函数 查看系统自带的函数：show functions; 显示自带的函数的用法：desc function split; 详细显示自带的函数的用法：desc function extened split; 自定义函数 Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展 当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF：user-defined function) 根据用户自定义函数类别分为以下三种： UDF(User-Defined-Function)：一进一出 UDAF(User-Defined Aggregation Function)：聚集函数，多进一出；类似于：count/max/min UDTF(User-Defined Table-Generating Functions)：一进多出，如lateral view explore() 官方文档地址：https://cwiki.apache.org/confluence/display/Hive/HivePlugins 编程步骤： 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF(org.apache.hadoop.hive.ql.UDF已被废弃) 重写三个方法 在Hive的命令行窗口创建函数 添加jar资源：add jar ‘jar_path’; 创建function：create [temporary] function [dbname.]function_name AS class_name;(temporary只在当前次使用Hive Cli有效,退出重进无效;dbname.标识限定使用函数的数据库,不写默认为default数据库) 在Hive的命令行窗口删除函数：Drop [temporary] function [if exists] [dbname.]function_name; 自定义UDF/UDTF函数 在IDE中创建一个Maven工程 导入依赖 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建自定义UDF函数 创建类继承GenericUDF 12345678910111213141516171819202122232425262728293031323334// UDF被废弃public class MyUDF extends GenericUDF &#123; // 输入类型int private transient IntObjectInspector arg0; // 返回值类型int private IntWritable res; // 这个方法只调用一次,并且在evaluate()方法之前调用 // 该方法接受的参数是一个ObjectInspectors数组 // 该方法检查接受正确的参数类型和参数个数 public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123; // 输入类型 this.arg0 = (IntObjectInspector) arguments[0]; // 返回值类型 this.res = new IntWritable(); // 确定返回值类型 return PrimitiveObjectInspectorFactory.writableIntObjectInspector; &#125; // 这个方法类似UDF的evaluate()方法。它处理真实的参数，并返回最终结果 public Object evaluate(DeferredObject[] arguments) throws HiveException &#123; Object arg0 = arguments[0].get(); int inputNum = this.arg0.get(arg0); res.set(inputNum + 5); return res; &#125; // 这个方法用于当实现的GenericUDF出错的时候，打印出提示信息。而提示信息就是你实现该方法最后返回的字符串 public String getDisplayString(String[] children) &#123; assert (children.length == 1); return \"param: \" + children[0]; &#125;&#125; 打成Jar包上传到服务器 将jar包添加到Hive的classpath：add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar; 创建临时函数与开发好的java class关联 12# className需要使用全类名create temporary function addFive as 'com.xiong.hive.MyUDF'; 在Hql中使用自定义的函数：select addFive(id) from cc; 创建自定义UDTF函数 创建类继承GenericUDTF 1234567891011121314151617181920212223242526272829303132333435public class MyUDTF extends GenericUDTF &#123; private List&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); // 定义输出数据的列名和数据类型 @Override public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException &#123; // 定义输出数据的列名 List&lt;String&gt; fieldNames = new ArrayList&lt;String&gt;(); fieldNames.add(\"word\"); // 定义输出数据的类型 List&lt;ObjectInspector&gt; fieldOIs = new ArrayList&lt;ObjectInspector&gt;(); fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); &#125; public void process(Object[] args) throws HiveException &#123; // 1、获取数据 String inputStr = args[0].toString(); // 2、获取分隔符 final String splitKey = args[1].toString(); // 3、切粉数据 final String[] words = inputStr.split(splitKey); // 4、遍历写出 for (String word : words) &#123; // 5、将数据放至集合 dataList.clear(); dataList.add(word); // 6、写出数据 forward(dataList); &#125; &#125; public void close() throws HiveException &#123; &#125;&#125; 与自定义UDF类似 123add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar;create temporary function udtf_split as 'com.xiong.hive.MyUDTF';select udtf_split('hello,sobxiong,nice boy!',','); 压缩和存储 Hadoop源码编译支持Snappy压缩 资源准备 虚拟机准备：连接外网、采用root角色编译，减少文件夹权限问题 软件包准备 软件包安装 JDK安装 Maven安装 编译源码 Hadoop压缩配置 MR支持的压缩编码 压缩格式 是否hadoop自带 算法 文件扩展名 是否可切分 换成压缩格式后,原来程序是否需要修改 DEFLATE 是,直接使用 DEFLATE .deflate 否 和文本处理一样,不需要修改 Gzip 是,直接使用 DEFLATE .gz 否 和文本处理一样,不需要修改 bzip2 是,直接使用 bzip2 .bz2 是 和文本处理一样,不需要修改 LZO 否,需要安装 LZO .lzo 是 需要建索引,还需要指定输入格式 Snappy 否,需要安装 Snappy .snappy 否 和文本处理一样,不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec Gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s Snappy 8.3GB 较大 最快 最快 压缩参数配置要在Hadoop中启用压缩，可以配置如下参数(mapred-site.xml文件中) 参数 默认值 阶段 建议 io.compression.codecs(在core-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress(在mapred-site.xml中配置) false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置) false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置) RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 开启Map输出阶段压缩开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量具体操作： 开启Hive中间传输数据压缩功能：set hive.exec.compress.intermediate=true; 开启mapreduce中map输出压缩功能：set mapreduce.map.output.compress=true; 设置mapreduce中map输出数据的压缩方式：set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; 执行查询语句 开启Reduce输出阶段压缩当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能具体操作： 开启Hive最终输出数据压缩功能：set hive.exec.compress.output=true; 开启mapreduce最终输出数据压缩：set mapreduce.output.fileoutputformat.compress=true; 设置mapreduce最终数据输出压缩方式：set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec; 设置mapreduce最终数据输出压缩为块压缩：set mapreduce.output.fileoutputformat.compress.type=BLOCK; 测试输出结果是否是压缩文件：insert overwrite local directory ‘/opt/module/data/hive/distribute-result’ select * from emp distribute by deptno sort by empno desc; 文件存储格式Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET 列式存储和行式存储 行存储的特点：查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快 列存储的特点：因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法 主要存储格式对应的存储方式 TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的 ORC和PARQUET是基于列式存储的 TextFile格式默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作 Orc格式Orc(Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer具体解释： Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储 Stripe Footer：存的是各个Stream的类型，长度等信息每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读 Parquet格式Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度一个Parquet文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页 主流文件存储格式对比实验从存储文件的压缩比和查询速度两个角度对比 存储文件的压缩比测试 TextFile 创建表(存储数据格式为TEXTFILE,默认) 1234567891011create table log_text (track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as textfile; 向表中加载数据 1load data local inpath '/opt/module/data/hive/log.data' into table log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_text; ORC 创建表(存储格式为ORC) 1234567891011create table log_orc(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orc; 向表中加载数据(不能直接导入数据)：insert into table log_orc select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_orc; Parquet 创建表(存储格式为parquet) 1234567891011create table log_parquet(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as parquet; 向表中加载数据(不能直接导入数据)：insert into table log_parquet select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_parquet; 存储文件的压缩比总结：ORC &gt; Parquet &gt; textFile 存储文件的查询速度总结(都运行select * from log_sortType)：查询速度相近 存储和压缩结合 修改Hadoop集群具有Snappy压缩方式 查看hadoop本地库支持情况：hadoop checknative 将编译好的支持Snappy压缩的hadoop源码包解压，将lib/native里面的内容复制到原本的hadoop下的lib/native下替换 分发集群：xsync native/ 再次查看hadoop本地库支持情况：hadoop checknative 重新启动hadoop集群和Hive 测试存储和压缩 创建一个非压缩的ORC存储方式 建表语句 1234567891011create table log_orc_none(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orc tblproperties (\"orc.compress\"=\"NONE\"); 插入数据：insert into table log_orc_none select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_none; 创建一个SNAPPY压缩的ORC存储方式 建表语句 1234567891011create table log_orc_snappy(track_time string,url string,session_id string,referer string,ip string,end_user_id string,city_id string)row format delimited fields terminated by '\\t'stored as orc tblproperties (\"orc.compress\"=\"SNAPPY\"); 插入数据：insert into table log_orc_snappy select * from log_text; 查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_snappy; 上一节中默认的ORC存储方式，采用默认的ZLIB压缩 介绍ORC存储相关信息：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORCORC存储方式的压缩 Key Default Notes orc.compress ZLIB high level compression(one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries(must be &gt;= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns “” comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter(must &gt; 0.0 and &lt; 1.0) 存储方式和压缩总结 在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo存储方式和压缩方式不是同一个东西，文件后缀名只是人为加上的(压缩后会带有压缩格式的后缀) 企业级调优 Fetch抓取Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees；在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce 12345678910111213&lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;more&lt;/value&gt; &lt;description&gt; Expects one of [none, minimal, more]. Some select queries can be converted to single FETCH task minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins. 0. none : disable hive.fetch.task.conversion 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns) &lt;/description&gt;&lt;/property&gt; 本地模式大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化 123456# 开启本地mrset hive.exec.mode.local.auto=true;# 设置local mr的最大输入数据量,当输入数据量小于这个值时采用local mr的方式,默认为134217728,即128Mset hive.exec.mode.local.auto.inputbytes.max=50000000;# 设置local mr的最大输入文件个数,当输入文件个数小于这个值时采用local mr的方式,默认为4set hive.exec.mode.local.auto.input.files.max=10; 表的优化 小表、大表Join将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表(1000条以下的记录条数)先进内存。在map端完成reduce 实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别 大表Join大表 空Key过滤有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下： 空Key转换有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如 MapJoin如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理 开启MapJoin参数设置 设置自动选择MapJoin：set hive.auto.convert.join = true;(默认true) 大表小表的阈值设置：set hive.mapjoin.smalltable.filesize=25000000;(默认25MB) MapJoin工作机制 Group By默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果 开启Map端聚合参数设置 设置是否在Map端进行聚合：set hive.map.aggr = true;(默认为true) 设置在Map端进行聚合操作的条目数据：set hive.groupby.mapaggr.checkinterval = 100000; 设置有数据倾斜的时候是否进行负载均衡：set hive.groupby.skewindata = true;(默认为false) 当选项设定为true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中(这个过程可以保证相同的Group By Key被分布到同一个Reduce中)，最后完成最终的聚合操作 Count(Distinct)去重统计数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换(虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的)实际操作： 创建一张大表 123create table bigtable(id bigint, time bigint, uid string, keywordstring, url_rank int, click_num int, click_url string) row formatdelimited fields terminated by '\\t'; 加载数据 12load data local inpath '/opt/module/data/hive/bigtable' into tablebigtable; 设置reduce个数为5：set mapreduce.job.reduces = 5; 执行去重id查询 1select count(distinct id) from bigtable; 采用Group by去重id 1select count(id) from (select id from bigtable group by id) a; 笛卡尔积尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积 行列过滤列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤行处理实际操作： 测试先关联两张表，再用where条件过滤 123select o.id from bigtable bjoin ori o on o.id = b.idwhere o.id &lt;= 10; 通过子查询后，在关联表 12select b.id from bigtable bjoin (select id from ori where id &lt;= 10 ) o on b.id = o.id; 动态分区调整关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置 开启动态分区参数设置 开启动态分区功能(默认开启,为true)：hive.exec.dynamic.partition 设置为非严格模式(动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区)：hive.exec.dynamic.partition.mode 在所有执行MR的节点上最大一共可以创建动态分区的个数(默认为1000)：hive.exec.max.dynamic.partitions 在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错：hive.exec.max.dynamic.partitions.pernode 整个MR Job中，最大可以创建多少个HDFS文件(默认为100000)：hive.exec.max.created.files 当有空分区生成时，是否抛出异常。一般不需要设置(默认为false)：hive.error.on.empty.partition 案例实操 分桶：详见之前介绍 分区：详见之前介绍 数据倾斜 合理设置Map数 通常情况下，作业会通过input的目录产生一个或者多个map任务主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小 是不是map数越多越好？答案是否定的。如果一个任务有很多小文件(远远小于块大小128m)，则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的 是不是保证每个map处理接近128m的文件块，就高枕无忧了？答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数 小文件进行合并在map执行前合并小文件，减少map数；CombineHiveInputFormat具有对小文件进行合并的功能(系统默认的格式)。HiveInputFormat没有对小文件合并功能：hive.input.format 复杂文件增加Map数当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize))) = blocksize = 128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数实际操作 执行查询：select count(*) from emp; 设置最大切片值为100个字节：set mapreduce.input.fileinputformat.split.maxsize=100; 再次查询 合理设置Reduce数 调整reduce个数的方法一 每个Reduce处理的数据量默认是256MB：hive.exec.reducers.bytes.per.reducer 每个任务最大的reduce数，默认为1009：hive.exec.reducers.max=1009 计算reducer数的公式：N=min(参数2，总输入数据量/参数1) 调整reduce个数的方法二 在hadoop的mapred-default.xml文件中修改设置每个job的Reduce个数 在Hive Cli中设置：set mapreduce.job.reduces = 15; reduce个数并不是越多越好 过多的启动和初始化reduce也会消耗时间和资源 另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题 在设置reduce个数的时候也需要考虑这两个原则： 处理大数据量利用合适的reduce数 使单个reduce任务处理数据量大小要合适 并行执行Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。如果有更多的阶段可以并行执行，那么job可能就越快完成通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来参数设置： 1234# 打开任务并行执行set hive.exec.parallel=true;# 同一个sql允许最大并行度,默认为8set hive.exec.parallel.thread.number=16; 严格模式Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询 对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表 对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间 限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况 12345678910111213&lt;property&gt; &lt;name&gt;hive.mapred.mode&lt;/name&gt; &lt;value&gt;strict&lt;/value&gt; &lt;description&gt; The mode in which the Hive operations are being performed. In strict mode, some risky queries are not allowed to run. They include: Cartesian Product. No partition being picked up for a query. Comparing bigints and strings. Comparing bigints and doubles. Orderby without limit. &lt;/description&gt;&lt;/property&gt; JVM重用JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出 1234567&lt;property&gt; &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;description&gt;How many tasks to run per jvm. If set to -1, there is no limit. &lt;/description&gt;&lt;/property&gt; 这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放 推测执行在分布式集群环境下，因为程序Bug(包括Hadoop本身的bug)，负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务(比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕)，则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行(Speculative Execution)机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果 设置开启推测执行参数，Hadoop的mapred-site.xml文件中进行配置： 12345678910111213&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; hive本身也提供了配置项来控制reduce-side的推测执行： 12345&lt;property&gt; &lt;name&gt;hive.mapred.reduce.tasks.speculative.execution&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Whether speculative execution for reducers should be turned on. &lt;/description&gt;&lt;/property&gt; 关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大 压缩：详见之前介绍 执行计划(Explain) 基本语法：EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query 案例实操 查看执行计划：explain select deptno, avg(sal) avg_sal from emp group by deptno; 查看详细执行计划：explain extended select deptno, avg(sal) avg_sal from emp group by deptno; 谷粒影音Hive实战 需求描述统计硅谷影音视频网站的常规指标，各种TopN指标： 统计视频观看数Top10 统计视频类别热度Top10 统计出视频观看数Top20所属类别以及类别包含Top20视频的个数 统计视频观看数Top50所关联视频的所属类别Rank 统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 项目 数据结构 视频表 字段 备注 详细描述 videoId 视频唯一id 11位字符串 uploader 视频上传者 上传视频的用户名String age 视频年龄 视频在平台上的整数天 category 视频类别 上传视频指定的视频分类 length 视频长度 整形数字标识的视频长度 views 观看次数 视频被浏览的次数 rate 视频评分 满分5分 ratings 流量 视频的流量,整型数字 conments 评论数 一个视频的整数评论数 relatedIds 相关视频id 相关视频的id,最多20个 用户表 字段 备注 字符类型 uploader 上传者用户名 string videos 上传视频数 int friends 朋友数量 int ETL(Extraction-Transformation-Loading,数据抽取、转换和加载)原始数据通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用’\\t’进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用’&amp;’分割，同时去掉两边空格，多个相关视频id也使用’&amp;’进行分割 ETLUtil清洗数据 1234567891011121314151617181920212223242526/*** @param oriStr 原始数据* @return 过滤后的数据*/public static String etlStr(String oriStr) &#123; StringBuffer sb = new StringBuffer(); // 1、切割字符串 String[] fields = oriStr.split(\"\\t\"); // 2、过滤字段长度 if (fields.length &lt; 9) return null; // 3、去掉类别字段中的空格 fields[3] = fields[3].replaceAll(\" \", \"\"); // 4、修改相关视频ID字段的分隔符,把'\\t'替换为'&amp;' for (int i = 0; i &lt; fields.length; i++) &#123; // 非相关id if (i &lt; 9) &#123; if (i == fields.length - 1) sb.append(fields[i]); else sb.append(fields[i]).append('\\t'); &#125; else &#123; // 相关id if (i == fields.length - 1) sb.append(fields[i]); else sb.append(fields[i]).append('&amp;'); &#125; &#125; // 5、返回结果 return sb.toString();&#125; ETL之Mapper 123456789101112131415161718// 输出写NullWritable,不需要排序,节省资源public class ETLMapper extends Mapper&lt;LongWritable, Text, NullWritable, Text&gt; &#123; // 定义全局value private Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取数据 String oriStr = value.toString(); // 2、过滤数据 String eltStr = ETLUtil.etlStr(oriStr); // 3、写出 if (eltStr == null) &#123; return; &#125; v.set(eltStr); context.write(NullWritable.get(), v); &#125;&#125; ETL之Driver 123456789101112131415161718192021222324252627282930313233343536373839404142// 官方推荐采用继承Tool方式// 在ToolRunner中帮做了GenericOptionsParserpublic class ETLDriver implements Tool &#123; private Configuration conf; @Override public int run(String[] args) throws Exception &#123; // 1、获取job对象 Job job = Job.getInstance(conf); // 2、设置jar包路径 job.setJarByClass(ETLDriver.class); // 3、设置Mapper类和输出KV类型 job.setMapperClass(ETLMapper.class); job.setMapOutputKeyClass(NullWritable.class); job.setMapOutputValueClass(Text.class); // 4、设置最终输出的KV类型 job.setOutputKeyClass(NullWritable.class); job.setOutputValueClass(Text.class); // 5、设置输入输出的路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、提交任务 boolean result = job.waitForCompletion(true); return result ? 0 : 1; &#125; @Override public void setConf(Configuration conf) &#123; this.conf = conf; &#125; @Override public Configuration getConf() &#123; return conf; &#125; public static void main(String[] args) &#123; // 构建配置信息 Configuration conf = new Configuration(); try &#123; int result = ToolRunner.run(conf, new ETLDriver(), args); System.out.println(\"result = \" + result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 执行ETL jar包(经过maven的package,然后扔到集群上)：bin/hadoop jar /opt/module/data/hive/guli-vedio-1.0-SNAPSHOT.jar com.xiong.mr.ETLDriver /gulivideo/video/2008/0222 /guliOutput 准备工作 创建表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051-- gulivideo_oricreate table gulivideo_ori( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;)row format delimitedfields terminated by \"\\t\"collection items terminated by \"&amp;\"stored as textfile;-- gulivideo_user_oricreate table gulivideo_user_ori( uploader string, videos int, friends int)row format delimitedfields terminated by \"\\t\"stored as textfile;-- gulivideo_orccreate table gulivideo_orc( videoId string, uploader string, age int, category array&lt;string&gt;, length int, views int, rate float, ratings int, comments int, relatedId array&lt;string&gt;)clustered by (uploader) into 8 bucketsrow format delimited fields terminated by \"\\t\"collection items terminated by \"&amp;\"stored as orc;-- gulivideo_user_orccreate table gulivideo_user_orc( uploader string, videos int, friends int)row format delimitedfields terminated by \"\\t\"stored as orc; 导入ETL后的数据 12345678# gulivideo_oriload data inpath \"/guliOutput\" into table gulivideo_ori;# gulivideo_user_oriload data inpath \"/gulivideo/user/2008/0903\" into table gulivideo_user_ori;# gulivideo_orcinsert into table gulivideo_orc select * from gulivideo_ori;# gulivideo_user_orcinsert into table gulivideo_user_orc select * from gulivideo_user_ori; 业务分析 统计视频观看数Top10 12345678select videoId, viewsfrom gulivideo_orcorder by views desclimit 10; 统计视频类别热度Top10(某类视频的个数作为视频类别热度) 1234567891011121314151617181920212223242526272829303132333435363738-- 1、使用UDTF函数将类别炸裂select videoId, category_namefrom gulivideo_orclateral view explode(category) tmp_category as category_name;t1-- 2、按照category_name进行分组,统计每种类别视频的总数,同时按照该总数进行倒序排名,取前10select category_name, count(*) category_countfrom t1group by category_nameorder by category_count desclimit 10;-- 最终SQLselect category_name, count(*) category_countfrom ( select videoId, category_name from gulivideo_orc lateral view explode(category) tmp_category as category_name )t1group by category_nameorder by category_count desclimit 10; 统计出视频观看数Top20所属类别以及类别包含Top20视频的个数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758-- 1、统计视频观看数Top20select videoId, views, categoryfrom gulivideo_orcorder by views desclimit 20;t1-- 2、对t1表中的category进行炸裂select videoId, category_namefrom t1lateral view explode(category) tmp_category as category_name;t2-- 3、对t2表进行分组(category_name)求和(总数)select category_name, count(*) category_countfrom t2group by category_nameorder by category_count desc;-- 最终SQLselect category_name, count(*) category_countfrom ( select videoId, category_name from ( select videoId, views, category from gulivideo_orc order by views desc limit 20 )t1 lateral view explode(category) tmp_category as category_name )t2group by category_nameorder by category_count desc; 统计视频观看数Top50所关联视频的所属类别Rank 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687-- 1、统计视频观看数Top50select relatedId, viewsfrom gulivideo_orcorder by views desclimit 50;t1-- 2、对t1表中的relatedId进行炸裂并去重select related_idfrom t1lateral view explode(relatedId) tmp_related as related_idgroup by related_id;t2-- 3、取出观看数前50视频关联ID视频的类别select categoryfrom t2join gulivideo_orc orcon t2.related_id = orc.videoId;t3-- 4、对t3表中的category进行炸裂select explode(category) category_namefrom t3;t4-- 5、分组(类别)求和(总数)select category_name, count(*) category_countfrom t4group by category_nameorder by category_count desc;-- 最终SQLselect category_name, count(*) category_countfrom ( select explode(category) category_name from ( select category from ( select related_id from ( select relatedId, views from gulivideo_orc order by views desc limit 50 )t1 lateral view explode(relatedId) tmp_related as related_id group by related_id )t2 join gulivideo_orc orc on t2.related_id = orc.videoId )t3 )t4group by category_nameorder by category_count desc; 统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10 1234567891011121314151617181920212223242526-- 1、给每一种类别根据视频观看数添加rank值(倒序)select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rkfrom gulivideo_category;-- 2、过滤前十select categoryId, videoId, viewsfrom ( select categoryId, videoId, views, rank() over(partition by categoryId order by views desc) rk from gulivideo_category )t1where rk &lt;= 10; 统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-- 1、统计上传视频最多的用户Top10select uploader, videosfrom gulivideo_user_orcorder by videos desclimit 10;t1-- 2、取出这10个人上传的所有视频,按照观看次数进行排名,取前20select video.videoId, video.viewsfrom t1join gulivideo_orc videoon t1.uploader = video.uploaderorder by views desclimit 20;-- 最终SQLselect video.videoId, video.viewsfrom ( select uploader, videos from gulivideo_user_orc order by videos desc limit 10 )t1join gulivideo_orc videoon t1.uploader = video.uploaderorder by views desclimit 20; 常见错误及解决方案 启动MR任务报错：virtual memory used. Killing container(虚拟内存不足)修改hadoop的配置，修改检查虚拟内存的属性为false 12345&lt;!-- yarn-site.xml --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Zookeeper","slug":"BigData/Zookeeper","date":"2020-06-26T09:26:19.000Z","updated":"2020-09-20T14:27:02.531Z","comments":true,"path":"2020/06/26/BigData/Zookeeper/","link":"","permalink":"https://sobxiong.github.io/2020/06/26/BigData/Zookeeper/","excerpt":"内容 Zookeeper入门 Zookeeper安装 Zookeeper实战 Zookeeper内部原理 面试真题","text":"内容 Zookeeper入门 Zookeeper安装 Zookeeper实战 Zookeeper内部原理 面试真题 Zookeeper入门 概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目 特点 数据结构 应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等 统一命名服务 统一配置管理 统一集群管理 服务器节点动态上下线 软负载均衡 下载地址：官网地址——https://zookeeper.apache.org Zookeeper安装 本地模式安装部署 安装前准备 安装jdk 拷贝Zookeeper安装包到Linux系统下 解压到指定目录：tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/ 配置修改 修改配置文件(conf目录下)：mv zoo_sample.cfg zoo.cfg 打开zoo.cfg文件，修改dataDir路径：dataDir=/opt/module/zookeeper-3.6.1/zkData 新建zkData目录(不同于Hadoop目录不能存在)：mkdir zkData 操作Zookeeper 启动Zookeeper Server(服务端)：bin/zkServer.sh start 查看进程是否启动：jps(正常会有一个QuorumPeerMain) 查看状态：bin/zkServer.sh status 启动Zookeeper Client(客户端)：bin/zkCli.sh 查看文件列表：ls /(一开始只有[zookeeper]) 退出Zookeeper Client：quit 停止Zookeeper Server：bin/zkServer.sh stop 配置参数解读Zookeeper中的配置文件zoo.cfg中参数含义解读如下： tickTime = 2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) initLimit = 10：LF初始通信时限集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数(tickTime的数量)，用它来限定集群中的Zookeeper服务器连接到Leader的时限 syncLimit = 5：LF同步通信时限集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer dataDir：数据文件目录 + 数据持久化路径主要用于保存Zookeeper中的数据 clientPort = 2181：客户端连接端口监听客户端连接的端口 Zookeeper实战 分布式安装部署 集群规划：在hadoop1、hadoop2、hadoop3三个节点上部署Zookeeper形成集群 安装：分发zookeeper到hadoop2、hadoop3：xsync zookeeper-3.6.1/ 配置服务器编号： 在zookeeper-3.6.1目录下创建zkData目录：mkdir zkData 在zkData目录下创建myid文件：touch myid 编辑myid文件(设置当前server编号)：1 同步zkData到hadoop2、hadoop3上：xsync zkData/ 在hadoop2、hadoop3上修改myid中的内容为2、3 配置zoo.cfg文件 新增集群节点配置 1234# clusterserver.1=hadoop1:2888:3888server.2=hadoop2:2888:3888server.3=hadoop3:2888:3888 同步zoo.cfg文件：xsync zoo.cfg 配置参数解读：server.A=B:C:D A是一个数，表示这是第几号服务器。集群模式下配置文件myid中的数字就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server B是这个服务器的地址 C是这个服务器Follower与集群中的Leader服务器交换信息的端口 D是用来执行选举时服务器相互通信的端口：万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader 集群操作 分别启动Zookeeper(启动前需要关闭Linux防火墙,使得各节点能够相互通信)：bin/zkServer.sh start 查看状态：bin/zkServer.sh status 客户端命令行操作(启动命令行：bin/zkCli.sh) 命令基本语法 功能描述 help(打错也是一个效果,当前无help命令) 显示所有操作命令 ls [-s] [-w] [-R] path 使用ls命令来查看当前path下znode中所包含的内容(-s：查看更新次数等详细数据,替代ls2;-w：设置watcher监听器,只有效一次;-R：递归查看节点) create [-s] [-e] path [data] 创建节点(-s：含有序列;-e：临时(重启或者超时消失);data：写入path的内容,如果没有data创建不出节点) get [-s] [-w] path 获得节点的值(-s：获取更加详细的节点数据;-w：设置watcher监听器,只有效一次) set path data 设置节点的具体值 stat path 查看节点状态 delete path 删除节点 deleteall path 递归删除节点 API应用 IDEA环境搭建 创建空maven项目 添加pom文件： 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.13.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在resources目录下新建一个日志配置文件log4j.properties 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 创建ZooKeeper客户端 123456789101112131415// 访问的ipprivate String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\";// 会话超时时间private int sessionTimeout = 2000;// zookeeper客户端private ZooKeeper zkClient;@Beforepublic void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; &#125; &#125;);&#125; 创建子节点 1234567// 1、创建节点@Testpublic void createNode() throws IOException, KeeperException, InterruptedException &#123; String path = zkClient.create(\"/sobxiong\", \"sobxiong,xixixihahaha\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(\"path = \" + path);&#125; 获取子节点并监听节点变化 1234567891011121314151617181920212223242526272829// 2、获取子节点,并监控节点的变化@Testpublic void getDataAndWatch() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(\"child = \" + child); &#125; System.out.println(\"------------\"); Thread.sleep(Long.MAX_VALUE);&#125;// 设置watcher中的process方法,使其继续调用自身继续监听@Beforepublic void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(\"child = \" + child); &#125; System.out.println(\"------------\"); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;);&#125; 判断Znode是否存在 123456// 3、判断节点是否存在@Testpublic void judgeNodeExist() throws KeeperException, InterruptedException &#123; Stat stat = zkClient.exists(\"/sobxiong\", false); System.out.println(\"stat = \" + stat);&#125; 监听服务器节点动态上下线案例 需求：某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线 案例分析： 具体实现 先在集群上创建/servers节点：create /servers “servers” 服务端向Zookeeper注册： 12345678910111213141516171819202122232425262728293031323334public class DistributeServer &#123; // 访问的ip private String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\"; // 会话超时时间 private int sessionTimeout = 2000; // zookeeper客户端 private ZooKeeper zkClient; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeServer server = new DistributeServer(); // 1、连接zookeeper集群 server.getConnect(); // 2、注册节点 server.register(args[0]); // 3、业务逻辑 server.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void register(String hostName) throws KeeperException, InterruptedException &#123; String path = zkClient.create(\"/servers/server\", hostName.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostName + \" is online...\"); &#125; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123;&#125; &#125;); &#125;&#125; 客户端注册监听： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class DistributeClient &#123; // 访问的ip private String connectString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\"; // 会话超时时间 private int sessionTimeout = 2000; // zookeeper客户端 private ZooKeeper zkClient; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributeClient client = new DistributeClient(); // 1、获取zookeeper集群连接 client.getConnect(); // 2、注册监听 client.getChildren(); // 3、业务逻辑处理 client.business(); &#125; private void business() throws InterruptedException &#123; Thread.sleep(Long.MAX_VALUE); &#125; private void getChildren() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/servers\", true); // 存储服务器节点主机名称集合 List&lt;String&gt; hostNames = new ArrayList(); for (String child : children) &#123; byte[] data = zkClient.getData(\"/servers/\" + child, false, null); hostNames.add(new String(data)); &#125; // 将所有在线主机名称打印 System.out.println(\"hostNames = \" + hostNames); &#125; private void getConnect() throws IOException &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; getChildren(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125;&#125; Zookeeper内部原理 节点类型 Stat结构体 cZxid：创建节点的事务zxid——每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生 ctime：znode被创建的毫秒数(从1970年开始) mzxid：znode最后更新的事务zxid mtime：znode最后修改的毫秒数(从1970年开始) pZxid：znode最后更新的子节点zxid cversion：znode子节点变化号，znode子节点修改次数 dataversion：znode数据变化号 aclVersion：znode访问控制列表的变化号 ephemeralOwner：如果是临时节点，这个是znode拥有者的session id；如果不是临时节点则是0 dataLength：znode的数据长度 numChildren：znode子节点数量 监听器原理 选举机制 半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器 Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的 选举过程的举例(假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的——没有历史数据，在存放数据量这一点上，都是一样的)： 服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上(3票)，选举无法完成，服务器1状态保持为LOOKING 服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的(服务器1)大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1、2状态保持LOOKING 服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING 服务器4启动，发起一次选举。此时服务器1、2、3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING 服务器5启动，同4一样当小弟 写数据流程 面试真题 请简述ZooKeeper的选举机制？参见4.4 ZooKeeper的监听原理是什么？参见4.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？(1)部署方式：单机模式、集群模式；(2)角色：Leader和Follower；(3)集群最少需要机器数：3 ZooKeeper的常用命令有哪些？ls create get delete set","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]},{"title":"Hadoop","slug":"BigData/Hadoop","date":"2020-06-03T13:48:10.000Z","updated":"2020-09-20T14:30:07.349Z","comments":true,"path":"2020/06/03/BigData/Hadoop/","link":"","permalink":"https://sobxiong.github.io/2020/06/03/BigData/Hadoop/","excerpt":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode HDFS2.X新特性 MapReduce概述 Hadoop序列化 MapReduce框架原理 Hadoop数据压缩 Yarn资源调度器 Hadoop企业优化 MapReduce扩展案例 常见错误及解决方案","text":"内容 概论 Hadoop介绍 环境搭建 Hadoop运行模式 Hadoop编译源码 HDFS概述 HDFS的Shell操作 HDFS客户端操作 HDFS的数据流 NameNode和SecondaryNameNode DataNode HDFS2.X新特性 MapReduce概述 Hadoop序列化 MapReduce框架原理 Hadoop数据压缩 Yarn资源调度器 Hadoop企业优化 MapReduce扩展案例 常见错误及解决方案 概论 概念：大数据指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。需要解决的问题：海量数据的存储和海量数据的分析计算问题。 大数据特点(4V)： Volume(大量) Velocity(高速) Variety(多样)：结构化/非结构化数据，结构化数据以数据库/文本为主，非结构化数据包括网络日志、音频、视频、图片和地理位置信息等。 Value(低价值密度)：价值密度的高度与数据总量的大小成反比，如何快速对有价值数据“提纯”称为目前大数据背景下待解决的难题。 大数据应用场景：物流仓储、零售、旅游、商品广告推荐、保险、金融、房产、人工智能 大数据部门业务流程：产品人员提需求(统计总用户数、日活跃用户数、回流用户数等) =&gt; 数据部门搭建数据平台、分析数据指标 =&gt; 数据可视化(报表展示、邮件发送、大屏幕展示等) 大数据部门组织结构： Hadoop介绍 Hadoop是什么： 是一个由Apache基金会开发的分布式系统基础架构。 主要解决海量数据的存储和海量数据的分析计算问题。 广义上来说，Hadoop通常指更广泛的概念——Hadoop生态圈。 Hadoop发展历史 Lucene框架是Doug Cutting开创的开源软件，用Java编写，实现与Goole类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询和索引引擎。 2001年年底Lucene称为Apache基金会的一个子项目。 对于海量数据的场景，Lucene面对与Google同样的困难，存储数据困难，检索速度慢。 学习和模仿Google解决这些问题的办法：微型版Nutch。 Google是Hadoop的思想之源(其在大数据方面的三篇论文)GFS -&gt; HDFS Map-Reduce -&gt; MR BigTable -&gt; HBase 2003年~04年，Google公开部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。 2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。 2006年3月份，Map-Reduce和Nutch Distributed File System(NDFS)分别被纳入到Hadoop项目中，Hadoop就正式诞生，标志着大数据十代来临。 Hadoop名字来源于Doug Cutting儿子的玩具大象 Hadoop三大发行版本 Apache：最原始(基础)的版本，对于入门学习最好 Cloudera：在大型互联网企业中用的较多，产品主要为CDH，Cloudera Manager，Cloudera Support： CDH是Cloudera的Hadoop发行版，完全开源，比Apache版本在兼容性、安全性、稳定性上有所增强。 Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。 Cloudera Support即是对Hadoop的技术支持。 Hortonworks：文档较好 Hortonworks的主打产品是Hortonworks Data Platform(HDP)，也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari——一款开源的安装和管理系统。 HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 Hadoop的优势(4高) 高可靠性：Hadoop底层维护多个数据副本，即使某个计算元素或存储出现故障，也不会导致数据的丢失。 高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点。 高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。 高容错性：能够自动将失败的任务重新分配。 Hadoop组成 1.x：Common(辅助工具)、HDFS(数据存储)、MapReduce(计算+资源调度) 2.x：Common(辅助工具)、HDFS(数据存储)、Yarn(资源调度)、MapReduce(计算) HDFS架构概述： HDFS全名——Hadoop Distributed File System 组成： NameNode(nn)：存储文件的元数据，如文件名、文件目录结构、文件属性(生成时间、副本数、文件权限)以及每个文件的块列表和块所在的DataNode等——类似书的目录(索引) DataNode(dn)：在本地文件系统存储文件块数据以及块数据的校验和——具体的书章节内容 Secondary NameNode(2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照——辅助NameNode工作 Yarn架构概述 MapReduce架构概述 将计算分为两个阶段：Map和Reduce Map阶段并行处理输入数据 Reduce阶段对Map结果进行汇总 大数据技术生态体系 环境搭建 配置Java环境变量 123456789&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##JAVA_HOMEexport JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_251export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Java版本java -version 配置Hadoop环境变量 12345678910&#x2F;&#x2F; 修改环境变量sudo vim &#x2F;etc&#x2F;profile##HADOOP_HOMEexport HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;binexport PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin&#x2F;&#x2F; 让环境变量修改生效source &#x2F;etc&#x2F;profile&#x2F;&#x2F; 查看Hadoop版本hadoop version Hadoop目录说明 bin目录：存放对Hadoop相关服务(HDFS,YARN)进行操作的脚本 etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 lib目录：存放Hadoop的本地库(对数据进行压缩解压缩功能) sbin目录：存放启动或停止Hadoop相关服务的脚本 share目录：存放Hadoop的依赖jar包、文档、和官方案例 Hadoop运行模式 本地模式 官方WordCount案例(统计单词数目)： 1234567891011&#x2F;&#x2F; 创建wcinput文件夹mkdir wcinput&#x2F;&#x2F; 创建wc.input文件cd wcinputtouch wc.input&#x2F;&#x2F; 编辑wc.input随意输入字符vim wc.input&#x2F;&#x2F; 回到Hadoop目录执行程序hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput&#x2F; wcouput&#x2F;&#x2F; 查看结果cat wcoutput&#x2F;part-r-00000 伪分布式模式 配置集群 设置hadoop-env.sh：在文件中设置JAVA_HOME为本地JDK地址 设置core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; 设置hdfs-site.xml： 12345&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 启动集群 格式化NameNode：bin/hdfs namenode -format 启动NameNode和DataNode：sbin/start-dfs.sh(关闭stop) 查看集群 查看是否启动成功：jps(JDK中的命令,不是Linux命令,类似ps) web端查看HDFS文件系统：http://192.168.232.100:9870(需要CentOS主机上设置关闭防火墙,在3.x版本端口号默认为9870) 查看产生的log日志：cd /hadoop/logs 注意：不能一直格式化NameNode，格式化NameNode会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。(最好关闭NameNode和DataNode) 操作集群(所有命令类似于在Linux Terminal的命令行操作,需要加上固定前缀bin/hdfs dfs -) 在HDFS文件系统上创建一个input文件夹：bin/hdfs dfs -mkdir -p /user/sobxiong/input 将测试文件内容上传到文件系统上：bin/hdfs dfs -put wcinput/wc.input /user/sobxiong/input/ 查看上传的文件是否正确： 12bin/hdfs dfs -ls /user/sobxiong/input/bin/hdfs dfs -cat /user/sobxiong/input/wc.input 运行MapReduce程序：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/sobxiong/input/ /user/sobxiong/output 查看输出结果：bin/hdfs dfs -cat /user/sobxiong/output/* 也可以在浏览器的文件系统中查看 将测试文件内容下载到本地：bin/hdfs dfs -get /user/sobxiong/output/part-r-00000 ./wcoutput/ 删除输出结果：bin/hdfs dfs -rm -r /user/sobxiong/output 启动Yarn并运行MapReduce程序 配置集群 配置yarn-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置yarn-site.xml 1234567891011121314&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置Yarn应用的classPath --&gt;&lt;!-- 不配置出现：错误: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster --&gt;&lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt;命令行下输入hadoop classpath的一长串环境&lt;/value&gt;&lt;/property&gt;&lt;!-- 不需要设置yarn.resourcemanager.hostname，可以直接通过ip+端口号的方式访问 --&gt; 配置mapred-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251 配置mapred-site.xml： 123456&lt;!-- 指定MR运行在YARN上 --&gt;&lt;!-- 默认是local，本地文件 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 启动集群 启动前必须保证NameNode和DataNode已启动 启动ResourceManager和NodeManager：sbin/start-yarn.sh(关闭stop) 集群操作 yarn浏览器页面查看：8088端口 删除文件系统上的output文件：bin/hdfs dfs -rm -r /user/sobxiong/output 执行MapReduce程序：同上hadoop操作 查看结果：同上cat操作，也可以在浏览器端查看 配置历史服务器 配置mapred-site.xml： 1234567891011&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;172.16.85.130:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;172.16.85.130:19888&lt;/value&gt;&lt;/property&gt; 启动历史服务器：bin/mapred –daemon start historyserver(stop关闭) 查看历史服务器是否启动：jps 查看JobHistory：http://172.16.85.130:19888/jobhistory 配置日志的聚集： 概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上 好处：可以方便的查看到程序运行详情，方便开发调试 注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryManager 配置yarn-site.xml： 1234567891011&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 配置文件说明 默认配置文件： core-defalut.xml - hadoop-common-3.1.3.jar/core-default.xml hdfs-default.xml - hadoop-hdfs-3.1.3.jar/hdfs-default.xml yarn-default.xml - hadoop-yarn-common-3.1.3.jar/yarn-default.xml mapred-default.xml - hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml 自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置(优先级高) 完全分布式运行模式 虚拟机准备(3台，完全复制) 编写集群分发脚本xsync scp(secure copy)安全拷贝 定义：scp可以实现服务器与服务器之间的数据拷贝 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 scp -r(递归) $pdir/$fname $user@$host:$pdir/$fname rsync远程同步工具 作用：主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点 rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去 基本语法： 命令 参数 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称 rsync -r(递归)v(显示复制过程)l(拷贝符号连接) $pdir/$fname $user@$host:$pdir/$fname xsync集群分发脚本 需求：循环复制文件到所有节点的相同目录下 需求分析： rsync命令原始：rsync -rvl /opt/module root@hadoop2:/opt/ 期望脚本：xsync 需同步的文件名 说明：在home/sobxiong/bin这个目录下存放的脚本，sobxiong用户在系统任何地方都可以直接执行 脚本实现 在/home/sobxiong目录下创建bin目录，并在bin目录下创建xsync文件 在xsync中键入如下代码： 12345678910111213141516171819202122232425#!/bin/bash#1 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi#2 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname#3 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir#4 获取当前用户名称user=`whoami`#5 循环for((host=2; host&lt;4; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 修改脚本xsync具有执行权限：chmod 777 xsync 调用脚本形式：xsync 文件名 集群配置 集群部署规划： 类型 hadoop1 hadoop2 hadoop3 HDFS NameNode、DataNode DataNode SecondaryNameNode、DataNode YARN NodeManager ResourceManager、NodeManager NodeManager 配置集群 核心配置文件core-site.xml： 1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp&lt;/value&gt;&lt;/property&gt; HDFS配置文件hdfs-site.xml 1234567891011&lt;!-- 副本数目 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop3:50090&lt;/value&gt;&lt;/property&gt; YARN配置文件yarn-site.xml 1234567891011&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop2&lt;/value&gt;&lt;/property&gt; MapReduce配置文件mapred-site.xml 12345&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 在集群上分发配置好的hadoop配置文件：xsync /opt/module/hadoop-3.1.3/etc 查看文件分发情况 集群单点启动 集群第一次启动，需要格式化NameNode：hadoop namenode -format(把log和data文件删除) 在hadoop1上启动NameNode：hadoop-daemon.sh start namenode 在hadoop1、2、3上分别启动DataNode：hadoop-daemon.sh start datanode SSH免密登陆配置 配置ssh 基本语法：ssh ip 无密钥配置 免密登录原理： 生成公钥和私钥：ssh-keygen -t rsa(生成文件id_rsa-私钥,id_rsa.pub-公钥) 将公钥拷贝到要免密登录的目标机器上：ssh-copy-id hadoop2(只是当前用户,root还需要另外配置) .ssh文件下(~/.ssh)的文件功能 known_hosts：记录ssh访问过的计算机的公钥 id_rsa：生成的私钥 id_rsa.pub：生成的公钥 authorized_keys：存放授权过的无密登录服务器公钥 群起集群 配置workers：vim etc/hadoop/workers；加入hadoop1、hadoop2、hadoop3(不能有空行和空格)；同步所有节点配置文件 - xsync etc/hadoop/workers 启动集群 集群第一次启动，需要格式化NameNode(格式化前关闭启动的所有namenode和datanode进程,然后再删除data和log数据)：bin/hdfs namenode -format 启动HDFS：sbin/start-dfs.sh(在hadoop1上启动,这样hadoo1、2、3均会启动对应的进程) 启动YARN：sbin/start-yarn.sh(在hadoop2上启动,在ResourceManager所在机器hadoop2上启动Yarn) 查看NameNode：hadoop1:9870 集群基本测试 上传文件到集群：bin/hdfs dfs -put xx xx 查看上传文件存储位置 查看HDFS文件存储路径：/opt/module/hadoop-3.1.3/data/tmp/dfs/data/current/BP-1002151198-172.16.85.130-1591848799222/current/finalized/subdir0/subdir0 查看HDFS在磁盘存储文件的内容：cat blk_xxx(文本文件) 拼接大文件：cat blk_xxx &gt;&gt; temp，最后temp就是初始的文件 集群启动/停止方式总结 各个服务组件逐一启动/停止 分别启动/停止HDFS组件：hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode 启动/停止YARN：yarn-daemon.sh start/stop resourcemanager/nodemanager 各个模块分开启动/停止(配置ssh是前提)常用 整体启动/停止HDFS：start-dfs.sh/stop-dfs.sh 整体启动/停止YARN：start-yarn.sh/stop-yarn.sh 集群时间同步 crontab定时任务： 基本语法：crontab[选项] 选项说明 -e：编辑crontab定时任务 -l：查询crontab任务 -r：删除当前用户所有的crontab任务 参数说明：***** [任务] *的含义： 第一个：一小时当中的第几分钟(0~59) 第二个：一天当中的第几个小时(0~23) 第三个：一个月当中的第几天(1~31) 第四个：一年当中的第几月(1~12) 第五个：一周当中的星期几(0~7,0和7均代表星期日) 特殊符号： ：代表任何时间。比如第一个“”代表一小时中每分钟都执行一次 ,：代表不连续的时间。如“0 8,12,16 * * *”命令，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 -：代表连续的时间范围。比如“0 5 * * 1-6”命令，代表在周一到周六的凌晨5点0分执行命令 /n：代表每隔多久执行一次。比如“/10 * * * *”命令，代表每隔10分钟就执行一遍命令 ntp方式进行同步 具体思路：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 具体实操 时间服务器配置： 检查ntp是否安装：rpm -qa&#124;grep ntp，有ntp、fontpackages-filesystem以及ntpdate 修改ntp配置文件 123456789101112# 授权1172.16.85.0-172.16.85.255网段上的所有机器可以从这台机器上查询和同步时间restrict 172.16.85.0 mask 172.16.85.130.0 nomodify notrap# 修改集群在局域网中,不使用其他互联网上的时间#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst# 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步server 127.127.1.0fudge 127.127.1.0 stratum 10 修改/etc/sysconfig/ntpd文件：SYNC_HWCLOCK=yes(让硬件时间与系统时间一起同步) 重新启动ntpd服务： 12service ntpd statusservice ntpd start 设置ntpd服务开机自启动：chkconfig ntpd on 其他机器配置(root用户)： 配置10分钟与时间服务器同步一次： 12crontab -e*/10 * * * * /usr/sbin/ntpdate hadoop1 修改任意机器时间：date -s “2020-11-11 11:11:11” 十分钟后查看机器是否与时间服务器同步：date Hadoop编译源码 前期准备 jar包准备(hadoop源码、JDK8、Maven、Ant、Protobuf)：Protobuf在Google的github中的Release页面(3.1.3Hadoop对应2.5.0版本) jar包安装 安装JDK 12345678tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module/# JAVA_HOME(/etc/profile)export JAVA_HOME=/opt/module/jdk1.8.0_251export PATH=$PATH:$JAVA_HOME/binsource /etc/profilejava -version 安装Maven 12345678910111213141516tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/# MAVEN_HOME(/etc/profile)export MAVEN_HOME=/opt/module/apache-maven-3.6.3export PATH=$PATH:$MAVEN_HOME/binsource /etc/profilemvn -version# 修改maven仓库镜像&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 安装Ant 12345678tar -zxvf apache-ant-1.10.8-bin.tar.gz -C /opt/module/# ANT_HOME(/etc/profile)export ANT_HOME=/opt/module/apache-ant-1.10.8export PATH=$PATH:$ANT_HOME/binsource /etc/profileant -version 安装glibc-headers和g++：yum install glibc-headers、yum install gcc-c++ 安装make和cmake：yum install make 安装cmake(要装3.x版本,低版本编译不通过) 1234567891011tar -zxvf cmake-3.17.3.tar.gz -C /opt/modulecd /opt/module/cmake-3.17.3./configuremakemake install# CMAKE_HOME(/etc/profile)export CMAKE_HOME=/opt/module/cmake-3.17.3export PATH=$PATH:$CMAKE_HOME/binsource /etc/profilecmake --version 安装protobuf： 12345678910111213tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/cd /opt/module/protobuf-2.5.0/./configuremakemake checkmake installldconfig# LD_LIBRARY_PATH(/etc/profile)export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0export PATH=$PATH:$LD_LIBRARY_PATHprotoc --version 安装openssl库：yum install openssl-devel 安装ncurses-devel库：yum install ncurses-devel 编译源码 解压源码到/opt目录 进入hadoop源码主目录 通过maven执行编译命令：mvn package -Pdist,native -DskipTests -Dtar HDFS概述 HDFS产出背景及定义 产生背景：随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种 定义：HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色 使用背景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用 HDFS优缺点 优点： 高容错性 数据自动保存多个副本。它通过增加副本的形式，提高容错性 某一个副本丢失以后，它可以自动恢复(通过将副本复制到另一个可用的节点) 适合处理大数据 数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据 文件规模：能够处理百万规模以上的文件数量，数量相当之大 可构建在廉价机器上，通过多副本机制，提高可靠性 缺点： 不适合低延时数据访问，比如毫秒级的存储数据，是做不到的 无法高效的对大量小文件进行存储： 存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的 小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标 不支持并发写入、文件随机修改： 一个文件只能有一个写，不允许多个线程同时写 仅支持数据appen(追加)，不支持文件的随机修改 HDFS组成架构 NameNode(nn)：Master，一个主管、管理者 管理HDFS的名称空间 配置副本策略 管理数据块(Block)映射信息 处理客户端读写请求 DataNode：Slave。NameNode下达命令，DataNode执行实际的操作 存储实际的数据块 执行数据块的读/写操作 Client：客户端 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传 与NameNode交互，获取文件的位置信息 与DataNode交互，读取或者写入数据 Client提供一些命令来管理HDFS，比如NameNode格式化 Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作 Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务 辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode 在紧急情况下，可辅助恢复NameNode HDFS文件块大小HDFS中的文件在物理上是分块存储(Block)，块的大小可以通过配置参数(dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M为什么文件块的大小不能设置太小，也不能设置太大？ HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢 总结：HDFS块的大小设置主要取决于磁盘传输速率 HDFS的Shell操作 基本语法bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令其中dfs是fs的实现类 命令大全：bin/hadoop fs 使用命令： -help：输出命令的帮助(hadoop fs -help rm) -ls：显示目录信息(hadoop fs -ls /) -mkdir：在HDFS上创建目录[-p递归](hdoop fs -mkdir -p /sobxiong/test) -moveFromLocal：从本地剪切粘贴到HDFS[前路径为本地,后路径为HDFS](hadoop fs -moveFromLocal ./test.txt /sobxiong/test/) -appendToFile：追加一个文件到已经存在的文件末尾[前路径为本地,后路径为HDFS](hadoop fs -appendToFile ./test.txt /sobxiong/test/test.txt) -cat：显示文件内容(hadoop fs -cat /sobxiong/test/test.txt) -chgrp、-chmod、-chown：修改文件所属的权限，同Linux文件系统中的用法 -copyFromLocal：从本地文件系统拷贝文件到HDFS中，同-moveFromLocal -copyToLocal：从HDFS拷贝文件到本地[前路径为HDFS,后路径为本地](hadoop fs -copyToLocal /sobxiong/test/test.txt ./) -cp：把文件从HDFS的一个路径拷贝到HDFS的另一个路径 -mv：把文件从HDFS的一个路径移动到HDFS的另一个路径 -get：等同于copyToLocal(用法同copyToLocal)，从HDFS下载文件到本地 -getmerge：合并下载多个文件(hadoop fs -getmerge /sobxiong/test/* ./all.txt) -put：等同于copyFromLocal(用法同copyFromLocal) -tail：显示一个文件的末尾(hadoop fs -tail /sobxiong/test/test.txt) -rm：删除文件或文件夹[-r递归删除目录] -rmdir：删除空目录 -du：统计文件夹的大小信息[-h显示单位,-s总和](hadoop fs -du -h -s /) -setrep：设置HDFS中文件的副本数目[这里设置的副本数只是记录在NameNode的元数据中,是否真的会有这么多副本还得看DataNode的数量.因为目前只有3台设备,最多也就3个副本,只有节点数的增加到10台时,副本数才能达到10;只要加入一台设备,就会把副本复制到设备上,直到加到10台](hadoop fs -setrep 10 /sobxiong/test/test.txt) HDFS客户端操作 客户端环境准备 将Hadoop安装到mac上，并设置环境变量 123456# HADOOP_HOME(~/.bash_profile)export HADOOP_HOME=\"/Users/sobxiong/module/hadoop-3.1.3\"export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinsource ~/.bash_profilehadoop version 创建Maven工程测试：idea创建quickstart项目 导入依赖： 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt; 创建测试类 12345678910111213141516public class HDFSClient &#123; public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException &#123; Configuration configuration = new Configuration(); // configuration.set(\"fs.defaultFS\", \"hdfs://hadoop1:9000\"); // 1、获取hdfs客户端对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、在hdfs上创建路径 fileSystem.mkdirs(new Path(\"/sobxiong2/test\")); // 3、关闭资源 fileSystem.close(); System.out.println(\"finish\"); &#125;&#125; HDFS的API操作 文件上传 12345678910111213141516171819202122232425262728293031/** * 参数优先级： * 1、客户端代码中设置的值 * 2、ClassPath(resources)下的用户自定义配置文件(hdfs-site.xml) * 3、服务器的默认配置 * @throws Exception */// 1、文件上传@Testpublic void testCopyFromLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); configuration.set(\"dfs.replication\", \"2\"); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行上传API fileSystem.copyFromLocalFile(new Path(\"/Users/sobxiong/Documents/文件块大小大致计算.png\"), new Path(\"/sobxiong/test2.png\")); // 3、关闭资源 fileSystem.close();&#125;// 将hdfs-site.xml拷贝至项目的根目录resources资源文件夹下&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 文件下载 123456789101112131415// 2、文件下载@Testpublic void testCopyToLocalFile() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行下载操作 // fileSystem.copyToLocalFile(new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test.png\")); // 本地模式,true,不会产生crc文件 fileSystem.copyToLocalFile(false, new Path(\"/sobxiong/test2.png\"), new Path(\"/Users/sobxiong/Documents/test1.png\"), true); // 3、关闭资源 fileSystem.close();&#125; 文件删除 12345678910111213// 3、文件删除@Testpublic void testDelete() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、文件删除(第二个参数,是否递归删除,文件夹时有效) fileSystem.delete(new Path(\"/sobxiong/test2.png\"), false); // 3、关闭资源 fileSystem.close();&#125; 文件更名 12345678910111213// 4、文件更名@Testpublic void testRename() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、执行更名操作 fileSystem.rename(new Path(\"/sobxiong/test1.png\"), new Path(\"/sobxiong/1tset.png\")); // 3、关闭资源 fileSystem.close();&#125; 文件详情查看 1234567891011121314151617181920212223242526272829303132// 5、文件详情查看@Testpublic void testListFiles() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、查看文件详情 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(new Path(\"/\"), true); while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); // 查看文件名称、权限、长度 System.out.println(\"name: \" + fileStatus.getPath().getName()); System.out.println(\"permission: \" + fileStatus.getPermission()); System.out.println(\"length: \" + fileStatus.getLen()); // 查看块信息 BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; String[] hosts = blockLocation.getHosts(); for (String host : hosts) &#123; System.out.println(\"host = \" + host); &#125; System.out.println(\"----------------\"); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; 判断是文件还是文件夹 1234567891011121314151617181920// 6、判断是文件还是文件夹@Testpublic void testListStatus() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、判断操作 FileStatus[] fileStatuses = fileSystem.listStatus(new Path(\"/\")); for (FileStatus fileStatus : fileStatuses) &#123; if (fileStatus.isFile()) &#123; System.out.println(\"file = \" + fileStatus.getPath().getName()); &#125; else &#123; System.out.println(\"dir = \" + fileStatus.getPath().getName()); &#125; &#125; // 3、关闭资源 fileSystem.close();&#125; HDFS的I/O流操作 HDFS文件上传 1234567891011121314151617// 把本地文件上传到HDFS根目录@Testpublic void upload() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FileInputStream fileInputStream = new FileInputStream(new File(\"/Users/sobxiong/Downloads/课件.rar\")); // 3、获取输出流 FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path(\"/test.rar\")); // 4、流的对拷 IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fsDataOutputStream); IOUtils.closeStream(fileInputStream); fileSystem.close();&#125; HDFS文件下载 1234567891011121314151617// 从HDFS下载文件到本地磁盘@Testpublic void download() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/test.rar\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/test1.rar\")); // 4、流的对拷 IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125; 定位文件获取 12345678910111213141516171819202122232425262728293031323334353637383940414243// 下载第一块@Testpublic void readFileSeek1() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part1\")); // 4、流的对拷(只拷贝第一个块128MB) byte[] buf = new byte[1024]; for (int i = 0; i &lt; 1024 * 128; i++) &#123; fsDataInputStream.read(buf); fileOutputStream.write(buf); &#125; // 5、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载第二块@Testpublic void readFileSeek2() throws Exception &#123; Configuration configuration = new Configuration(); // 1、获取fs对象 FileSystem fileSystem = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), configuration, \"sobxiong\"); // 2、获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/hadoop-3.1.3.tar.gz\")); // 3、设置指定读取的起点 fsDataInputStream.seek(1024 * 1024 * 128); // 4、获取输出流 FileOutputStream fileOutputStream = new FileOutputStream(new File(\"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part2\")); // 5、流的对拷(拷贝剩下的两个Block块) IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration); // 6、关闭资源 IOUtils.closeStream(fileOutputStream); IOUtils.closeStream(fsDataInputStream); fileSystem.close();&#125;// 下载完第一块和剩余的部分后,可通过cat hadoop-3.1.3.tar.gz.part2 &gt;&gt; hadoop-3.1.3.tar.gz.part1将剩余部分追加到第一块上,修改文件名(删去.part1),就得到完整的文件 HDFS的数据流 HDFS写数据流程 剖析文件写入： 客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在 NameNode返回是否可以上传 客户端请求第一个Block上传到哪几个DataNode服务器上 NameNode返回3个DataNode节点，分别为dn1、dn2、dn3 客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成 dn1、dn2、dn3逐级应答客户端 客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器(此后重复执行3-7步) 网络拓扑-节点距离计算在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。节点距离：两个节点到达最近的共同祖先的距离总和 机架感知(2.7.2版本副本节点选择,性能和安全的综合考量) 第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个 第二个副本和第一个副本位于相同机架，随机节点 第三个副本位于不同机架，随机节点 HDFS读数据流程 客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址 挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据 DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验) 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件 NameNode和SecondaryNameNode NN和2NN工作机制思考：NameNode中的元数据是存储在哪里的？首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并 第一阶段：NameNode启动 第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存 客户端对元数据进行增删改的请求 NameNode记录操作日志，更新滚动日志(先记日志,类似数据库) NameNode在内存中对数据进行增删改 第二阶段：Secondary NameNode工作 Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果 Secondary NameNode请求执行CheckPoint NameNode滚动正在写的Edits日志 将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode Secondary NameNode加载编辑日志和镜像文件到内存，并合并 生成新的镜像文件fsimage.chkpoint 拷贝fsimage.chkpoint到NameNode NameNode将fsimage.chkpoint重新命名成fsimage 补充：Fsimage：NameNode内存中元数据序列化后形成的文件。Edits：记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)。NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息)，如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并(所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage)。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。SecondaryNameNode首先会询问NameNode是否需要CheckPoint(触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了)。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中 Fsimage和Edits解析 概念 NameNode被格式化之后，将在/data/tmp/dfs/name/current目录中产生如下文件fsimage_0000000000000000000fsimage_0000000000000000000.md5seen_txidVERSION Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息 Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中 seen_txid文件保存的是一个数字，就是最后一个edits_的数字 每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并 查看Fsimage文件：hdfs oiv -p 文件类型(XML) -i 镜像文件 -o 转换后文件输出路径例：hdfs oiv -p XML -i fsimage_0000000000000000025 o fsimage.xmlFsimage中没有记录块所对应的DataNode，为什么？在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报 查看Edits文件：hdfs oev -p 文件类型(XML) -i 编辑日志 -o 转换后文件输出路径例：hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o edits.xmlNameNode如何确定下次开机启动的时候合并那些Edits？通过seen_txid查看 CheckPoint时间设置 通常情况下，SecondaryNameNode每隔一小时执行一次 一分钟检查一次操作次数 当操作次数达到1百万时，SecondaryNameNode执行一次 123456789101112131415&lt;!-- hdfs-default.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt;1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; NameNode故障处理 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录 kiil -9 NameNode进程编号(用jps查看NameNode的进程编号) 删除NameNode存储的数据(data/tmp/dfs/name)：rm -rf /data/tmp/dfs/name/* 拷贝SecondaryNameNode(hadoop2)中数据到原NameNode(hadoop1)存储数据目录：scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary/* ./name/ 重新启动NameNode(hadoop1)：sbin/hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中 修改hdfs-site.xml(加入下述内容)： 123456789&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; kill -9 NameNode进程 删除NameNode存储的数据(同方法一) 如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 123scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary /data/tmp/dfs/cd /data/tmp/dfs/namesecondaryrm -rf in_use.lock 导入检查点数据(等待一会ctrl+c结束掉) 启动NameNode：sbin/hadoop-daemon.sh start namenode 集群安全模式 概述 NameNode启动NameNode启动时，首先将镜像文件(Fsimage)载入内存，并执行编辑日志(Edits)中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的 DataNode启动 系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统 安全模式退出判断如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值：dfs.replication.min=1)。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式 基本语法集群处于安全模式，不能执行重要操作(写操作)。集群启动完成后，自动退出安全模式 查看安全模式状态：bin/hdfs dfsadmin -safemode get 进入安全模式状态：bin/hdfs dfsadmin -safemode enter 离开安全模式状态：bin/hdfs dfsadmin -safemode leave 等待安全模式状态：bin/hdfs dfsadmin -safemode wait 案例模拟等待安全模式 查看当前模式：bin/hdfs dfsadmin -safemode get 先进入安全模式：bin/hdfs dfsadmin -safemode enter 创建并执行下面的脚本 12345678910touch safemode.shvim safemode.sh# safemode.sh#!/bin/bashhdfs dfsadmin -safemode waithdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /chmod 777 safemode.sh./safemode.sh 再打开一个窗口，执行：hdfs dfsadmin -safemode leave 安全模式退出，HDFS集群上已经有上传的数据了 NameNode多目录配置 NameNode的本地目录可以配置成多个，但每个目录存放内容相同(相当于备份)，增加了可靠性 具体配置如下 在hdfs-site.xml文件中增加如下内容 1234&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 停止集群，删除data和logs中所有数据 12345hadoop2：sbin/stop-yarn.shhadoop2：rm -rf data/ logs/hadoop1：sbin/stop-dfs.shhadoop2：rm -rf data/ logs/shadoop3：rm -rf data/ logs/s 格式化集群并启动 123hadoop1：bin/hdfs namenode -formathadoop1：sbin/start-dfs.shhadoop2：sbin/start-yarn.sh 查看结果：dfs目录下出现两个目录name1和name2 DataNode DataNode工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳 DataNode启动后向NameNode注册，通过后，周期性(1小时)的向NameNode上报所有的块信息 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用 集群运行中可以安全加入和退出一些机器 数据完整性DataNode节点保证数据完整性的方法： 当DataNode读取Block的时候，它会计算CheckSum(类似crc校验位) 如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏 Client读取其他DataNode上的Block DataNode在其文件创建后周期验证CheckSum 掉线时限参数设置\bhdfs-default.xml： 12345678910&lt;!-- heartbeat.recheck.interval单位为毫秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;!-- dfs.heartbeat.interval单位为秒 --&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 服役新数据节点(hadoop4未服役) 需求：随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点 环境准备 利用hadoop3主机再克隆一台hadoop4主机 修改hadoop4主机IP地址和主机名称 在hadoop1主机上将/etc/hosts下添加hadoop4的ip地址映射条目，并分发到hadoop2-4 hadoop4主机删除原来HDFS文件系统留存的文件(data和log目录)——不然会发生3和4轮换出现的问题,因为3和4有着一样的data和log reboot重启加载配置 服役新节点具体步骤 hadoop1-3按之前步骤已启动 在hadoop4主机上单独启动： 12hdfs --daemon start datanodeyarn --daemon start nodemanager 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 刷新http://hadoop1:9870web页面，等待 在hadoop4上上传文件 如果数据不均衡，可以使用命令实现集群的在平衡：sbin/start-balancer.sh 结束后在workers文件中加入hadoop4，之后直接start-dfs.sh和start-yarn.sh即可启动 退役旧数据节点(hadoop4未退役) 添加白名单(hadoop4未退役)添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出 在NameNode的hadoop-3.1.3/etc/hadoop目录下创建dfs.hosts文件 1234567touch dfs.hostsvim dfs.hosts# dfs.hosts(不添加hadoop4,不允许有空行和空格)hadoop1hadoop2hadoop3 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 配置文件分发：xsync hdfs-site.xml;xsync dfs.hosts 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 在web页面刷新等待 黑名单退役(hadoop4未退役)在黑名单上面的主机都会被强制退出。注意：不允许白名单和黑名单中同时出现同一个主机名称 在hadoop-3.1.3/etc/hadoop下创建dfs.hosts.exclude文件，并加入要退役节点 12345touch dfs.hosts.excludevim dfs.hosts.exclude# dfs.hosts.excludehadoop4 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 刷新NameNode和ResourceManager： 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 刷新web页面等待 DataNode多目录配置 DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本 需要在hdfs-site.xml上修改配置 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; 关闭当前运行的hadoop节点 删除各节点的data和log目录 格式化dataNode 重启部署hadoop节点 HDFS2.X新特性 集群间数据拷贝 scp实现两个远程主机之间的文件复制 123456# 从当前主机向目的主机 推 pushscp -r hello.txt root@hadoop2:/user/sobxiong/hello.txt# 从目的主机向当前主机 拉 pullscp -r root@hadoop2:/user/sobxiong/hello.txt hello.txt# 通过本主机中转实现两个远程主机的文件复制scp -r root@hadoop2:/user/sobxiong/hello.txt root@hadoop3:/user/sobxiong 采用distcp命令实现两个Hadoop集群之间的递归数据复制 1hadoop distcp hdfs://haoop1:9000/user/sobxiong/hello.txt hdfs://hadoop2:9000/user/sobxiong/hello.txt 小文件存档 HDFS存储小文件弊端每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB 解决存储小文件办法之一HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存 实际操作 启动YARN进程：start-yarn.sh(hadoop2) 归档文件(归档后的路径不得实现存在)把/sobxiong目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/sobxiongOutput路径下：hadoop archive -archiveName input.har -p /sobxiong /sobxiongOutput 查看归档： 123456789101112# 普通查看文件命令hadoop fs -ls R /sobxiongOutput/input.har# -rw-r--r-- 3 sobxiong supergroup 0 2020-06-24 16:06 /sobxiongOutput/input.har/_SUCCESS# -rw-r--r-- 3 sobxiong supergroup 305 2020-06-24 16:06 /sobxiongOutput/input.har/_index# -rw-r--r-- 3 sobxiong supergroup 23 2020-06-24 16:06 /sobxiongOutput/input.har/_masterindex# -rw-r--r-- 3 sobxiong supergroup 317029 2020-06-24 16:06 /sobxiongOutput/input.har/part-0# 采用har格式查看文件(可以像以往一样操作内部文件,也需要har格式)hadoop fs -ls R har:///sobxiongOutput/input.har# -rw-r--r-- 3 sobxiong supergroup 84942 2020-06-21 11:38 har:///sobxiongOutput/input.har/1tset.png# -rw-r--r-- 3 sobxiong supergroup 84942 2020-06-21 11:33 har:///sobxiongOutput/input.har/test.png# -rw-r--r-- 3 sobxiong supergroup 147145 2020-06-23 13:52 har:///sobxiongOutput/input.har/test6.txt 解归档文件：hadoop fs -cp har:///sobxiongOutput/input.har/* / 回收站开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用 开启回收站功能参数说明： 默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间(分钟为单位) 默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。该值设置和fs.trash.interval的参数值相同(要求fs.trash.checkpoint.interval &lt;= fs.trash.interval) 回收站工作机制 启动回收站：修改core-site.xml，配置垃圾回收时间为1分钟 1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 查看回收站：回收站在集群中的路径：/user/sobxiong/.Trash/ 修改访问回收站用户名称：进入垃圾回收站用户名称，默认是dr.who，修改为sobxiong(同样是core-site.xml) 1234&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;atguigu&lt;/value&gt;&lt;/property&gt; 恢复回收站数据：hadoop fs -mv /user/sobxiong/.Trash/Current/user/sobxiong/input / 清空回收站：hadoop fs -expunge 快照管理 命令介绍 实际操作 开启/禁用指定目录的快照功能：hdfs dfsadmin -allowSnapshot(-disallowSnapshot) /user/sobxiong/input 对目录创建快照 1234567hdfs dfs -createSnapshot /user/sobxiong/input# 快照和源文件使用相同数据hdfs dfs -ls R /user/sobxiong/input/.snapshot/# 指定名称创建快照hdfs dfs -createSnapshot /user/sobxiong/input test 重命名快照：hdfs dfs -renameSnapshot /user/sobxiong/input test test01 列出当前用户所有可快照目录：hdfs lsSnapshottableDir 比较两个快照目录的不同之处(可以是源文件和快照,使用’.’,此时”.snapshot/name”用于指定具体快照)：hdfs snapshotDiff /user/sobxiong/input . .snapshot/test01 恢复快照：hdfs dfs -cp /user/sobxiong/input/.snapshot/s20200624-134303.027 / MapReduce概述 MapReduce定义 MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上 MapReduce优缺点 优点： MapReduce易于编程它简单地实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行 良好的扩展性(hadoop)当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力 高容错性MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的 适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力 缺点： 不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果 擅长流式计算流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的 不擅长DAG(有向图)计算多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下 核心思想 MapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调 MapTask：负责Map阶段的整个数据处理流程 ReuceTask：负责Reduce阶段的整个数据处理流程 常用数据序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable MapReduce编程规范用户编写的程序分成三个部分：Mapper、Reducer和Driver Mapper 用户自定义的Mapper要继承自己的父类 Mapper的输入数据是KV对的形式(KV的类型可自定义) Mapper中的业务逻辑写在map()方法中 Mapper的输出数据是KV对的形式(KV的类型可自定义) map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次 Reducer 用户自定义的Reducer要继承自己的父类 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 Driver：相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象 WordCount案例实操 需求：在给定的文本文件中统计输出每一个单词出现的总次数 需求分析 环境准备 创建maven空项目 改pom 123456789101112131415161718192021222324252627282930&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.13.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt; 在resources资源文件夹下新建log4j.properties文件 12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 编写MapReduce程序 编写mapper类 12345678910111213141516171819202122232425262728/*** map阶段* KEYIN：输入数据的key类型(默认写LongWritable:偏移量)* VALUEIN：输入数据的value类型* KEYOUT：输出数据的key类型* VALUEOUT：输出数据的value类型* &lt;sobxiong,1&gt;输出数据* 输出作为reduce阶段的输入*/public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private Text k = new Text(); private IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // sobxiong sobxiong // 1、获取一行 String lineStr = value.toString(); // 2、切割单词 String[] words = lineStr.split(\" \"); // 3、循环写出 for (String word : words) &#123; // &lt;sobxiong,1&gt; k.set(word); context.write(k, v); &#125; &#125;&#125; 编写Reducer类 1234567891011121314151617181920/*** reduce阶段* KEYIN,VALUEIN：map阶段输出的kv* KEYOUT,VALUEOUT：reduce输出的kv*/public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; // 1、累加求和 for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 &lt;sobxiong,2&gt; v.set(sum); context.write(key, v); &#125;&#125; 编写Driver驱动类 123456789101112131415161718192021222324252627public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设置jar存储位置 job.setJarByClass(WordCountDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job // job.submit(); // true：打印一些信息 boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1); &#125;&#125; 本地测试：启动旁边的下箭头Edit Configuration，新增Application，选择Main Class并在Program arguments中加入两个参数中间用空格隔开，前者是input文件所在文件夹，后者是输出文件夹，要求不能存在(不然会出错)。例如：/Users/sobxiong/Downloads/input /Users/sobxiong/Downloads/ouputTest 在集群上测试 用maven打jar包，添加打包插件依赖 12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.xiong.hadoop.WordCountDriver&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 将程序打成jar包，maven install即可 将获取的两个jar包(一个不带依赖,另一带)，将不带依赖的jar上传到hadoop1主机上 启动Hadoop集群 执行WordCount程序：hadoop jar WordCount.jar com.xiong.hadoop.WordCountDriver /sobxiong /outputTest(第四个参数为启动的主类名,第五个为输入文件所在文件夹,第六个为输出文件夹——不能事先存在) Hadoop序列化 序列化概述 什么是序列化：序列化就是把内存中的对象，转换成字节序列(或其他数据传输协议)以便于存储到磁盘(持久化)和网络传输；反序列化就是将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象 为什么要序列化：一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机 为什么不用Java的序列化：Java的序列化是一个重量级序列化框架(Serializable)，一个对象被序列化后，会附带很多额外的信息(各种校验信息，Header，继承体系等)，不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制(Writable) Hadoop序列化特点： 紧凑：高效使用存储空间 快速：读写数据的额外开销小 可扩展：随着通信协议的升级而可升级 互操作：支持多语言的交互 自定义bean对象实现序列化接口(Writable)在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。具体实现bean对象序列化步骤如下7步： 实现Writable接口 反序列化时，需要反射调用空参构造函数，所以必须有空参构造 重写序列化方法 重写反序列化方法(注意反序列化的顺序和序列化的顺序完全一致) 要想把结果显示在文件中，需要重写toString()，可用”\\t”分开，方便后续用 如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框架中的Shuffle过程要求对key必须能排序 序列化案例实操 需求：统计每一个手机号耗费的总上行流量、下行流量、总流量 案例分析 编写MapReduce程序 编写流量统计的Bean对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/*测试数据1;13736230513;192.196.100.1;www.atguigu.com;2481;24681;2002;13846544121;192.196.100.2;264;0;2003;13956435636;192.196.100.3;132;1512;2004;13966251146;192.168.100.1;240;0;4045;18271575951;192.168.100.2;www.atguigu.com;1527;2106;2006;84188413;192.168.100.3;www.atguigu.com;4116;1432;2007;13590439668;192.168.100.4;1116;954;2008;15910133277;192.168.100.5;wwww.haol23.com;3156;2936;2009;13729199489;192.168.100.6;240;0;20010;13630577991;192.168.100.7;www.shouhu.com;6960;690;20011;15043685818;192.168.100.8;www.baidu.com;3659;3538;20012;15959002129;192.168.100.9;www.atguigu.com;1938;180;50013;13560439638;192.168.100.10;918;4938;20014;13470253144;192.168.100.11;180;180;20015;13682846555;192.168.100.12;wwww.qq.com;1938;2910;20016;13992314666;192.168.100.13;www.gaga.com;3008;3720;20017;13509468723;192.168.100.14;www.qinghua.com;7335;110349;40418;18390173782;192.168.100.15;www.sogou.com;9531;2412;20019;13975057813;192.168.100.16;www.baidu.com;11058;48243;20020;13768778790;192.168.100.17;120;120;20021;13568436656;192.168.100.18;www.alibaba.com;2481;24681;20022;13568436656;192.168.100.19;1116;954;200*/public class FlowBean implements Writable &#123; // 上行流量 private long upFlow; // 下行流量 private long downFlow; // 总流量 private long sumFlow; // 空参构造,为了后续反射 public FlowBean() &#123; &#125; // 序列化方法 @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeLong(upFlow); dataOutput.writeLong(downFlow); dataOutput.writeLong(sumFlow); &#125; // 反序列化方法 @Override public void readFields(DataInput dataInput) throws IOException &#123; // 必须要求和序列化方法顺序一致 upFlow = dataInput.readLong(); downFlow = dataInput.readLong(); sumFlow = dataInput.readLong(); &#125; @Override public String toString() &#123; return upFlow + \"\\t\" + downFlow + \"\\t\" + sumFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125;&#125; 编写mapper类 123456789101112131415161718public class FlowMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; private Text k = new Text(); private FlowBean flowBean = new FlowBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割\\t String[] fields = lineStr.split(\"\\t\"); // 3、封装对象 k.set(fields[1]); long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); flowBean.set(upFlow, downFlow); // 4、写出 context.write(k, flowBean); &#125;&#125; 编写Reducer类 12345678910111213141516public class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; private FlowBean v = new FlowBean(); @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 long sumUpFlow = 0; long sumDownFlow = 0; for (FlowBean value : values) &#123; sumUpFlow += value.getUpFlow(); sumDownFlow += value.getDownFlow(); &#125; // 2、写出 v.set(sumUpFlow, sumDownFlow); context.write(key, v); &#125;&#125; 编写Driver驱动类 12345678910111213141516171819202122public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设计jar路径 job.setJarByClass(FlowDriver.class); // 3、关联mapper和reducer job.setMapperClass(FlowMapper.class); job.setReducerClass(FlowReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce框架原理 InputFormat数据输入 切片与MapTask并行度决定机制 问题引出：MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ MapTask并行度决定机制 数据块：Block是HDFS物理上把数据分成一块一块数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储 Job提交流程源码和切片源码详解 Job提交流程源码详解 1234567891011121314151617181920212223242526waitForCompletion();submit(); // 1、建立连接 connect(); // 1)创建提交Job的代理 new Cluster(getConfiguration()); // 判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2、提交job submitter.submitJobInternal(Job.this, cluster); // 1)创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2)获取jobid,并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3)拷贝jar包到集群 copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // 4)计算切片,生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job); // 5)向Stag路径写XML配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 6)提交Job,返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); FileInputFormat切片源码解析(input.getSplits(job)) 程序先找到你数据存储的目录 开始遍历处理(规划切片)目录下的每一个文件 遍历第一个文件ss.txt 获取文件大小fs.sizeOf(ss.txt) 计算切片大小：computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M(YARN集群默认128M——2.x,62M-1.x;本地运行默认32M) 默认情况下，切片大小=blocksize 开始切，形成第1个切片：ss.txt—0:128M、第2个切片ss.txt—128:256M、第3个切片ss.txt—256M:300M(每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片) 将切片信息写到一个切片规划文件中 整个切片的核心过程在getSplit()方法中完成 InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等 提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数 FileInputFormat切片机制 切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于Block大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 案例分析 输入数据有两个文件：file1.txt - 320M;file2.txt - 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： 文件 切片区间 file1.txt.split1 0~128 file1.txt.split2 129~256 file1.txt.split3 257~320 file2.txt.split1 0~10 切片大小参数配置 源码中计算切片大小的公式：Math.max(minSize, Math.min(maxSize, blockSize));mapreduce.input.fileinputformat.split.minsize=1 默认值为1mapreduce.input.fileinputformat.split.maxsize=Long.MAXValue 默认值Long.MAXValue 默认情况下，切片大小=blocksize 切片大小设置maxsize(切片最大值)：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值minsize(切片最小值)：参数调的比blockSize大，则可以让切片变得比blockSize还大 获取切片信息API 1234// 获取切片的文件名称String name = inputSplit.getPath().getName();// 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) context.getInputSplit(); CombineTextInputFormat切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下 应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理 虚拟存储切片最大值设置：CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);(注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值) 切片机制切片机制：生成切片过程包括虚拟存储过程和切片过程二部分 虚拟存储过程将输入目录下所有文件的大小依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块(防止出现太小切片)例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成(2.01M和2.01M)两个文件 切片过程 判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片 如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片 测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M、(2.55M、2.55M)、3.4M、(3.4M、3.4M)。最终会形成3个切片，大小分别为：(1.7M + 2.55M)，(2.55M + 3.4M)，(3.4M + 3.4M) CombineTextInputFormat案例实操 需求：将输入的大量小文件合并成一个切片统一处理 输入数据：准备4个小文件 期望：期望一个切片处理4个文件 实现过程 不做任何处理，运行之前的WordCount案例程序，观察切片个数为4——(number of splits:4) 在WordcountDriver中增加如下代码(设置切片最大值为4m)，运行程序，观察运行的切片个数为3 1234// 如果不设置InputFormat，它默认用的是TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304); 在WordcountDriver中增加如下代码(设置切片最大值为20m)，运行程序，观察运行的切片个数为1(代码同上,值改为20971520) FileInputFormat实现类思考：在运行MapReduce程序时，输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢?FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等 TextInputFormatTextInputFormat是默认的FileInputFormat实现类。按行读取每条记录，键是存储该行在整个文件中的起始字节偏移量——LongWritable类型；值是这行的内容，不包括任何行终止符(换行符和回车符)——Text类型 1234567891011&#x2F;&#x2F; 示例：Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对：(0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为&lt;key,value&gt;对。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “\\t”)来设定分隔符——默认分隔符是tab(\\t) 1234567891011&#x2F;&#x2F; 示例(其中——&gt;表示一个水平方向的制表符)：line1 ——&gt;Rich learning formline2 ——&gt;Intelligent learning engineline3 ——&gt;Learning more convenientline4 ——&gt;From the real demand for more close to the enterprise&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对(键是每行排在制表符之前的Text序列)：(line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) NLineInputFormat如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按Block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数 / N = 切片数，如果不整除，切片数 = 商 + 1 123456789101112&#x2F;&#x2F; 示例：Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise&#x2F;&#x2F; 如果N是2,则每个输入分片包含两行。开启2个MapTask(键和值与TextInputFormat生成的一样)：(0,Rich learning form)(19,Intelligent learning engine)&#x2F;&#x2F; 另一个mapper则收到后两行：(47,Learning more convenient)(72,From the real demand for more close to the enterprise) KeyValueTextInputFormat使用案例 需求：统计输入文件中每一行的第一个单词相同的行数 案例分析 代码实现 Mapper类 123456789public class KVTextMapper extends Mapper&lt;Text, Text, Text, IntWritable&gt; &#123; private IntWritable intWritable = new IntWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、封装对象 // 2、写出 context.write(key, intWritable); &#125;&#125; Reducer类 1234567891011121314public class KVTextReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable intWritable = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 intWritable.set(sum); context.write(key, intWritable); &#125;&#125; Driver类 12345678910111213141516171819202122232425public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration configuration = new Configuration(); configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, \" \"); Job job = Job.getInstance(configuration); // 2、设置jar存储位置 job.setJarByClass(KVTextDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setInputFormatClass(KeyValueTextInputFormat.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; NLineInputFormat使用案例 需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中 案例分析 代码实现 Mapper和Reducer类参照WordCount Driver类 123456789101112131415161718192021222324252627public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 设置切片InputSplit中划分三条记录 NLineInputFormat.setNumLinesPerSplit(job, 3); // 使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class); // 2、设置jar存储位置 job.setJarByClass(NLineDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(NLineMapper.class); job.setReducerClass(NLineReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; 观察控制台打印的number of splits 自定义InputFormat在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题自定义InputFormat步骤如下： 自定义一个类继承FileInputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件 自定义InputFormat案例实操 需求：将多个小文件合并成一个SequenceFile文件(SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式)，SequenceFile里面存储着多个文件，存储的形式为key——文件路径 + 名称，value——文件内容 案例分析 代码实现 自定义InputFormat 12345678public class WholeFileInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt; &#123; @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; 自定义RecordReader类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt; &#123; private FileSplit split; private Configuration configuration; private Text k = new Text(); private BytesWritable v = new BytesWritable(); boolean isProgress = true; @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; // 初始化 this.split = (FileSplit) split; configuration = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; // 核心业务逻辑 // 每个文件创建一次reader if (isProgress) &#123; // 1、获取fs对象 Path path = split.getPath(); FileSystem fileSystem = path.getFileSystem(configuration); // 2、获取输入流 FSDataInputStream fis = fileSystem.open(path); // 3、拷贝 byte[] buffer = new byte[(int) split.getLength()]; IOUtils.readFully(fis, buffer, 0, buffer.length); // 4、封装v v.set(buffer, 0, buffer.length); // 5、封装key k.set(path.toString()); // 6、关闭资源 IOUtils.closeStream(fis); isProgress = false; return true; &#125; return false; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return k; &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return v; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123;&#125;&#125; Mapper类 123456public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; Reducer类 123456789public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 循环写出 for (BytesWritable value : values) &#123; context.write(key, value); &#125; &#125;&#125; Driver类 123456789101112131415161718192021222324public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setInputFormatClass(WholeFileInputFormat.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); // 2、设计jar路径 job.setJarByClass(SequenceFileDriver.class); // 3、关联mapper和reducer job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce工作流程 流程示意图 流程详解上面的流程是整个MapReduce的全部工作流程，Shuffle过程是从第7步开始到第16步结束，具体Shuffle过程详解，如下： MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序 ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据 ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并(归并排序) 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程——从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法 注意Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M Shuffle Shuffle机制：Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle Partition分区 问题引出：要求将统计结果按照条件输出到不同文件中(分区)。比如：将统计结果按照手机归属地不同省份输出到不同文件中(分区) 默认Partitionr分区：默认分区是根据key的hashCode对ReduceTasks个数取模得到的(如果分区数大于1)。用户没法控制哪个key存储到哪个分区 1234567public class HashPartitioner&lt;K2, V2&gt; implements Partitioner&lt;K2, V2&gt; &#123; public void configure(JobConf job) &#123;&#125; /** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 自定义Partitioner步骤 自定义类继承Partitioner，重写getPartition()方法 在Job驱动中，设置自定义Partitioner：job.setPartitionerClass(CustomPartitioner.class); 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask：job.setNumReduceTasks(5); 分区总结 如果ReduceTask的数量 &gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx 如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，会抛出异常 如果ReduceTask的数量 = 1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000 分区号必须从零开始，逐一累加 案例分析：假设自定义分区数为5，则 job.setNumReduceTasks(1)：会正常运行，只不过会产生一个输出文件 job.setNumReduceTasks(2)：会报错 job.setNumReduceTasks(6)：大于5，程序会正常运行，会产生空文件part-r-00005 Partition分区案例实操 需求：将统计结果按照手机归属地不同省份输出到不同文件中(分区) 案例分析 实操 在FlowBean案例基础上，增加一个分区类 12345678910111213141516171819public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text text, FlowBean flowBean, int numPartitions) &#123; // key是手机号,value是流量信息 // 获取手机号前三位 String prePhoneNum = text.toString().substring(0, 3); int partition = 4; if (\"136\".equals(prePhoneNum)) &#123; partition = 0; &#125; else if (\"137\".equals(prePhoneNum)) &#123; partition = 1; &#125; else if (\"138\".equals(prePhoneNum)) &#123; partition = 2; &#125; else if (\"139\".equals(prePhoneNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在Driver驱动主函数中添加自定义数据分区设置和ReduceTask设置 1234// 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 指定ReduceTask的数目job.setNumReduceTasks(6); WritableComparable排序 排序概述：排序是MapReduce框架中最重要的操作之一。MapTask和ReduceTask均会对数据按照key进行排序，该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 排序的分类 部分排序：MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序 全排序：最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构 辅助排序(GroupingComparator分组)：在Reduce端对key进行分组。应用——在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序 二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序 自定义排序WritableComparable 原理分析：bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序 案例实操(全排序) 需求：对FlowBean案例产生的结果再次对总流量进行排序 案例分析 代码实现 FlowBean对象在之前案例基础上实现WritableComparable接口，实现compareTo()方法 12345@Overridepublic int compareTo(FlowBean bean) &#123; // 按总流量倒序 return this.sumFlow &gt; bean.sumFlow ? -1 : 1;&#125; 编写Mapper类 12345678910111213141516171819public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; private Text v = new Text(); private FlowBean flowBean = new FlowBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、封装对象 v.set(fields[1]); long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); flowBean.set(upFlow, downFlow); // 4、写出 context.write(flowBean, v); &#125;&#125; 编写Reducer类 12345678public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; context.write(value, key); &#125; &#125;&#125; 编写Driver类 1234567891011121314151617181920212223public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、设计jar路径 job.setJarByClass(FlowCountSortDriver.class); // 3、关联mapper和reducer job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); // 4、设置mapper输出的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 5、设置最终输出的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Combiner合并 Combiner介绍 自定义Combiner实现步骤 自定义一个Combiner继承Reducer，重写reduce方法 在Driver驱动类中设置：job.setCombinerClass(xxCombiner.class); 自定义Combiner实操 需求：统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能 案例分析 代码实现 方案一 12345678910111213141516171819// 新建WordCountCombiner类public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1、累加求和 int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; // 2、写出 v.set(sum); context.write(key, v); &#125;&#125;// 在WordCountDriver驱动类中指定Combinerjob.setCombinerClass(WordcountCombiner.class); 方案二：将WordCountReducer作为Combiner 结果查看：控制台打印中的Map-Reduce Framework中的Combine记录 GroupingComparator分组(辅助排序) 简要介绍：对Reduce阶段的数据根据某一个或几个字段进行分组 分组排序步骤 自定义类继承WritableComparator 重写compare()方法 创建一个构造将比较对象的类传给父类 GroupingComparator分组实操 需求：求出每一个订单中最贵的商品 案例分析 利用“订单id和成交金额price”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序。Reduce将从Map中获取排序好的数据 在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品 代码实现 编写订单信息OrderBean类 12345678910111213141516171819202122232425262728// 忽略了空参/全参构造器、getter/setter和toString方法public class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int orderId; private double price; @Override public int compareTo(OrderBean bean) &#123; int result; if (orderId &gt; bean.orderId) &#123; result = 1; &#125; else if (orderId &lt; bean.orderId) &#123; result = -1; &#125; else &#123; result = price &gt; bean.price ? -1 : 1; &#125; return result; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(orderId); out.writeDouble(price); &#125; @Override public void readFields(DataInput in) throws IOException &#123; orderId = in.readInt(); price = in.readDouble(); &#125;&#125; 编写OrderGroupingComparator类 123456789101112131415public class OrderGroupingComparator extends WritableComparator &#123; protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; // 要求只要id相同,就认为是相同的key OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; if (aBean.getOrderId() == bBean.getOrderId()) &#123; return 0; &#125; return aBean.getOrderId() &gt; bBean.getOrderId() ? 1 : -1; &#125;&#125; 编写OrderMapper类 12345678910111213141516public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; private OrderBean orderBean = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、封装对象 orderBean.setOrderId(Integer.parseInt(fields[0])); orderBean.setPrice(Double.parseDouble(fields[2])); // 4、写出 context.write(orderBean, NullWritable.get()); &#125;&#125; 编写OrderReducer类 1234567891011public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 输出第一个 context.write(key, NullWritable.get()); // 循环几次输出前几 //for (NullWritable value : values) &#123; // context.write(key, NullWritable.get()); //&#125; &#125;&#125; 编写OrderDriver类 123456789101112131415161718192021222324public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取Job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、设置jar存储位置 job.setJarByClass(OrderDriver.class); // 3、关联Map和Reduce类 job.setMapperClass(OrderMapper.class); job.setReducerClass(OrderReducer.class); // 4、设置map阶段输出数据的kv类型 job.setMapOutputKeyClass(OrderBean.class); job.setMapOutputValueClass(NullWritable.class); // 5、设置最终数据输出的kv类型 job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); job.setGroupingComparatorClass(OrderGroupingComparator.class); // 6、设置程序运行的输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、提交job boolean result = job.waitForCompletion(true); // 额外 System.exit(result ? 0 : 1);&#125; MapTask工作机制 Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区(调用Partitioner)，并写入一个环形内存缓冲区中 Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作 溢写步骤1：利用快速排序算法对缓存区内的数据进行排序。排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序 溢写步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out(N表示当前溢写次数)中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作 溢写步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中 Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor(默认10)个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销 ReduceTask ReduceTask工作机制 Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中 Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多 Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可 Reduce阶段：reduce()函数将计算结果写到HDFS上 设置ReduceTask并行度(个数)ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置(默认值为1)：job.setNumReduceTasks(4); 一个测试ReduceTask数目的实验 实验环境：1个Master节点，16个Slave节点；CPU：8GHZ，内存: 2G 实验结论(数据量为1G) ReduceTask数目 总时间 1 892 5 146 10 110 15 92 16 88 20 100 25 128 30 101 45 145 60 104 注意事项 ReduceTask = 0，表示没有Reduce阶段，输出文件个数和Map个数一致 ReduceTask默认值就是1，所以输出文件个数为一个 如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜(一些节点很忙,其余空闲) ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask 具体多少个ReduceTask，需要根据集群性能而定 如果分区数不是1，但是ReduceTask为1，是否执行分区过程？答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行 OutputFormat数据输出 OutputFormat接口实现类OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。以下是几种常见的OutputFormat实现类 文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串 SequenceFileOutputFormat将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩 自定义OutputFormat：根据用户需求，自定义实现输出 使用场景为了实现控制最终文件的输出路径和输出格式，可以自定义OutputFormat例如：要在一个MapReduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义OutputFormat来实现 自定义OutputFormat步骤 自定义一个类继承FileOutputFormat 改写RecordWriter，具体改写输出数据的方法write() 自定义OutputFormat案例实操 需求：过滤输入的log日志，指定包含某特定字段的记录输出到一个文件，其他则输出到另一个文件 案例分析 代码编写 Mapper类 1234567public class FilterMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // http://www.baidu.com context.write(value, NullWritable.get()); &#125;&#125; Reducer类 12345678910111213public class FilterReducer extends Reducer&lt;Text, NullWritable, Text, NullWritable&gt; &#123; private Text k = new Text(); @Override protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; String line = key.toString(); line += \"\\r\\n\"; k.set(line); for (NullWritable value : values) &#123; context.write(k, NullWritable.get()); &#125; &#125;&#125; 自定义OutputFormat、RecordWriter类 12345678910111213141516171819202122232425262728293031323334353637383940public class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; return new FRecordWriter(job); &#125;&#125;public class FRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; private FSDataOutputStream fosSobxiong; private FSDataOutputStream fosOther; public FRecordWriter(TaskAttemptContext job) &#123; try &#123; // 1、获取文件系统 FileSystem fs = FileSystem.get(job.getConfiguration()); // 2、创建输出到sobxiong.log的输出流 fosSobxiong = fs.create(new Path(\"/Users/sobxiong/Downloads/sobxiong.log\")); // 3、创建输出到other.log的输出流 fosOther = fs.create(new Path(\"/Users/sobxiong/Downloads/other.log\")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断key中是否有sobxiong,如果有写出到sobxiong,否则输出到other if (key.toString().contains(\"sobxiong\")) &#123; fosSobxiong.write(key.toString().getBytes()); &#125; else &#123; fosOther.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; IOUtils.closeStream(fosOther); IOUtils.closeStream(fosSobxiong); &#125;&#125; Driver类 1234567891011121314151617181920212223242526public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FilterDriver.class); job.setMapperClass(FilterMapper.class); job.setReducerClass(FilterReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要将自定义的输出格式组件设置到job中 job.setOutputFormatClass(FilterOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 虽然我们自定义了outputFormat,但是因为我们的outputFormat继承自fileOutputFormat // 而fileOutputFormat要输出一个_SUCCESS文件,所以,在这还得指定一个输出目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Join多种应用 Reduce Join 工作原理Map端的主要工作：为来自不同表或文件的key/value对，打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在Map阶段已经打标志)分开，最后进行合并 案例实操 需求：将商品信息表中数据根据商品pid合并到订单数据表中 案例分析：通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联 代码编写 合并后的Bean类 123456789101112131415161718192021222324252627282930313233343536// 略去空参/全参构造器、getter/setter方法public class TableBean implements Writable &#123; // 订单id private String id; // 产品id private String pid; // 数量 private int amount; // 产品名称 private String pName; // 标记: 产品/订单 private String flag; @Override public String toString() &#123; return id + '\\t' + amount + '\\t' + pName; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(id); out.writeUTF(pid); out.writeInt(amount); out.writeUTF(pName); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; id = in.readUTF(); pid = in.readUTF(); amount = in.readInt(); pName = in.readUTF(); flag = in.readUTF(); &#125;&#125; Mapper类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt; &#123; private String fileName; private final TableBean tableBean = new TableBean(); private final Text key = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取文件的名称 FileSplit inputSplit = (FileSplit) context.getInputSplit(); fileName = inputSplit.getPath().getName(); &#125; // id pid amount // 1001 01 1 // pid pname // 01 小米 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); String[] fields = lineStr.split(\";\"); if (fileName.startsWith(\"order\")) &#123; // 订单表 // 封装kv tableBean.setId(fields[0]); tableBean.setPid(fields[1]); tableBean.setAmount(Integer.parseInt(fields[2])); // 属性不能为空,不然会序列化会出错 tableBean.setpName(\"\"); tableBean.setFlag(\"order\"); this.key.set(fields[1]); &#125; else &#123; // 产品表 // 封装kv tableBean.setId(\"\"); tableBean.setPid(fields[0]); tableBean.setAmount(0); tableBean.setpName(fields[1]); tableBean.setFlag(\"pd\"); this.key.set(fields[0]); &#125; // 写出 context.write(this.key, tableBean); &#125;&#125; Reducer类 1234567891011121314151617181920212223242526272829303132public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 存储所有订单集合 List&lt;TableBean&gt; beans = new ArrayList&lt;&gt;(); // 存储产品信息 TableBean pdBean = new TableBean(); for (TableBean value : values) &#123; if (\"order\".equals(value.getFlag())) &#123; TableBean tmpBean = new TableBean(); try &#123; // value是引用,tmpBean是实实在在的对象 BeanUtils.copyProperties(tmpBean, value); beans.add(tmpBean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; else &#123; try &#123; BeanUtils.copyProperties(pdBean, value); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 拼接表,设置商品名称 for (TableBean bean : beans) &#123; bean.setpName(pdBean.getpName()); context.write(bean, NullWritable.get()); &#125; &#125;&#125; Driver类 12345678910111213141516171819202122public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取配置信息,创建job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定本程序的jar包所在的本地路径 job.setJarByClass(TableDriver.class); // 3、指定本业务job要使用的Mapper/Reducer业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 4、指定Mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 5、指定最终输出的数据的kv类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 6、指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 测试结果： pid pname amount 1001 小米 1 1001 小米 1 1002 华为 2 1002 华为 2 1003 格力 3 1003 格力 3 总结 缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜 解决方案：Map端实现数据合并 Map Join 使用场景：适用于一张表十分小、一张表很大的场景 优点思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜 具体方法：采用DistributedCache 在Mapper的setup阶段，将文件读取到缓存集合中 在驱动函数中加载缓存(缓存普通文件到Task运行节点)：job.addCacheFile(new URI(“file:///Users/sobxiong/Downloads/testInput3/pd.txt”)) 案例实操 需求同Reduce Join 案例分析 代码编写 Driver类 1234567891011121314151617181920212223public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException &#123; // 1、获取job信息 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、设置加载jar包路径 job.setJarByClass(DistributedCacheDriver.class); // 3、关联map job.setMapperClass(DistributedCacheMapper.class); // 没有reduce阶段,map阶段输出即为最终输出 // 4、设置最终输出数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 5、设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、加载缓存数据 job.addCacheFile(new URI(\"file:///Users/sobxiong/Downloads/testInput3/pd.txt\")); // 7、Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0 job.setNumReduceTasks(0); // 8、提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; Mapper类 1234567891011121314151617181920212223242526272829303132333435363738394041public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; private Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(5); private Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 缓存小表 String cachePath = context.getCacheFiles()[0].getPath(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(new FileInputStream(cachePath), StandardCharsets.UTF_8)); String lineStr; while (StringUtils.isNotEmpty(lineStr = bufferedReader.readLine())) &#123; // 1、切割 // pid pname // 01 小米 String[] fields = lineStr.split(\";\"); // 2、封装到集合去 pdMap.put(fields[0], fields[1]); &#125; // 2、关闭资源 IOUtils.closeStream(bufferedReader); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // id pid amount // 1001 01 1 // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\";\"); // 3、获取pid String pid = fields[1]; // 4、取出pname String pName = pdMap.get(pid); // 5、拼接 lineStr = lineStr.replace(';', '\\t') + '\\t' + pName; k.set(lineStr); // 6、写出 context.write(k, NullWritable.get()); &#125;&#125; 计数器应用Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量 计数器API 采用枚举的方式统计计数 123enum MyCounter&#123;MALFORORMED,NORMAL&#125;//对枚举定义的自定义计数器加1context.getCounter(MyCounter.MALFORORMED).increment(1); 采用计数器组、计数器名称的方式统计 12// 组名和计数器名称随便起,但最好有意义context.getCounter(\"counterGroup\", \"counter\").increment(1); 计数结果在程序运行后的控制台上查看 数据清洗(ETL)在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序 案例实操(简单解析版——运用计数器)——复杂版(字段多,过滤的需求多,思路与下面无差) 需求：去除日志中字段长度小于等于11的日志 代码编写 Mapper类 123456789101112131415161718192021222324public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String line = value.toString(); // 2、解析数据 boolean isDirty = parseLog(line, context); if (!isDirty) &#123; // 3、解析通过,写出 context.write(value, NullWritable.get()); &#125; &#125; private boolean parseLog(String line, Context context) &#123; String[] fields = line.split(\" \"); if (fields.length &gt; 11) &#123; context.getCounter(\"map\", \"clean\").increment(1); return false; &#125; else &#123; context.getCounter(\"map\", \"dirty\").increment(1); return true; &#125; &#125;&#125; Driver类 1234567891011121314151617181920public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 1、获取job信息 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 2、加载jar包 job.setJarByClass(LogDriver.class); // 3、关联map job.setMapperClass(LogMapper.class); // 4、设置最终输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 设置reduceTask个数为0 job.setNumReduceTasks(0); // 5、设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 6、提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; MapReduce开发总结编写MapReduce程序时，需要考虑如下方面 输入数据接口：InputFormat 默认使用的实现类是：TextInputFormat TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回 KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key/value。默认分隔符是tab(\\t) NlineInputFormat按照指定的行数N来划分切片 CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率 自定义InputFormat 逻辑处理接口：Mapper用户根据业务需求实现其中三个方法：map()、setup()、cleanup() Partitioner分区 默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号(分区数大于1时) 1key.hashCode() &amp; Integer.MAXVALUE % numReduces 如果业务上有特别的需求，可以自定义分区 Comparable排序 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法 部分排序：对最终输出的每一个文件进行内部排序 全排序：对所有数据进行排序，通常只有一个Reduce 二次排序：排序的条件有两个 Combiner合并Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果 Reduce端分组：GroupingComparator在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序 逻辑处理接口：Reduce用户根据业务需求实现其中三个方法：reduce()、setup()、cleanup() 输出数据接口：OutputFormat 默认实现类是TextOutputFormat，功能逻辑是：将每一个kv对向目标文本文件输出一行 将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩 自定义OutputFormat Hadoop数据压缩 概述压缩技术能够有效减少底层存储系统(HDFS)读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价 压缩策略和原则压缩是提高Hadoop运行效率的一种优化策略通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度 注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能压缩基本原则 运算密集型的job，少用压缩 IO密集型的job，多用压缩 MR支持的压缩编码 压缩格式 是否hadoop自带 算法 文件扩展名 是否可切分 换成压缩格式后,原来程序是否需要修改 DEFLATE 是,直接使用 DEFLATE .deflate 否 和文本处理一样,不需要修改 Gzip 是,直接使用 DEFLATE .gz 否 和文本处理一样,不需要修改 bzip2 是,直接使用 bzip2 .bz2 是 和文本处理一样,不需要修改 LZO 否,需要安装 LZO .lzo 是 需要建索引,还需要指定输入格式 Snappy 否,需要安装 Snappy .snappy 否 和文本处理一样,不需要修改 为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec Gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 压缩性能的比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s Snappy 8.3GB 较大 最快 最快 压缩方式选择 Gzip压缩 优点压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便 缺点：不支持Split(切片) 应用场景 当每个文件压缩之后在130M以内的(1个块大小内)，都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件 Bzip2压缩 优点：支持Split(切片)；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便 缺点：压缩/解压速度慢 应用场景适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split(切片)，而且兼容之前的应用程序的情况 Lzo压缩 优点：压缩/解压速度也比较快，合理的压缩率；支持Split(切片)，是Hadoop中最流行的压缩格式之一；可以在Linux系统下安装lzop命令，使用方便 缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建索引，还需要指定InputFormat为Lzo格式) 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显 Snappy压缩 优点：高速压缩速度和合理的压缩率 缺点：不支持Split(切片)；压缩率比Gzip要低；Hadoop本身不支持，需要安装 应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入 压缩位置选择压缩可以在MapReduce作用的任意阶段启用 压缩参数配置要在Hadoop中启用压缩，可配置如下参数： 参数 默认值 阶段 建议 io.compression.codecs(在core-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec 输入压缩 Hadoop使用文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress(在mapred-site.xml中配置) false mapper输出 这个参数设为true启用压缩 mapreduce.map.output.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec mapper输出 企业多使用LZO或Snappy编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置) false reducer输出 这个参数设为true启用压缩 mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置) org.apache.hadoop.io.compress.DefaultCodec reducer输出 使用标准工具或者编解码器，如gzip和bzip2 mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置) RECORD reducer输出 SequenceFile输出使用的压缩类型：NONE和BLOCK 压缩实操 数据流的压缩和解压缩CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据： 要想对正在被写入一个输出流的数据进行压缩，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流 相反，要想对从输入流读取而来的数据进行解压缩，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据测试如下的压缩方式： 压缩格式 编解码类 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec 12345678910111213141516171819202122232425262728293031323334353637383940414243public static void main(String[] args) throws Exception &#123; compress(\"/Users/sobxiong/Downloads/test.txt\", \"org.apache.hadoop.io.compress.BZip2Codec\"); decompress(\"/Users/sobxiong/Downloads/test.txt.bz2\");&#125;// 解压缩private static void decompress(String filePath) throws IOException &#123; // 1、压缩方式检查 CompressionCodecFactory factory = new CompressionCodecFactory(new Configuration()); CompressionCodec codec = factory.getCodec(new Path(filePath)); if (codec == null) &#123; System.out.println(\"Can't process!\"); return; &#125; // 2、获取输入流 FileInputStream fis = new FileInputStream(new File(filePath)); CompressionInputStream cis = codec.createInputStream(fis); // 3、获取输出流 FileOutputStream fos = new FileOutputStream(new File(filePath + \".decode\")); // 4、流的对拷 IOUtils.copyBytes(cis, fos, 1024 * 1024 * 10, false); // 5、关闭资源 IOUtils.closeStream(fos); IOUtils.closeStream(cis); IOUtils.closeStream(fis);&#125;// 压缩private static void compress(String filePath, String compressTypeClassName) throws IOException, ClassNotFoundException &#123; // 1、获取输入流 FileInputStream fis = new FileInputStream(new File(filePath)); // 2、获取输出流 Class&lt;?&gt; classCodec = Class.forName(compressTypeClassName); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(classCodec, new Configuration()); FileOutputStream fos = new FileOutputStream(new File(filePath + codec.getDefaultExtension())); CompressionOutputStream cos = codec.createOutputStream(fos); // 3、流的对拷 IOUtils.copyBytes(fis, cos, 1024 * 1024 * 10, false); // 4、关闭资源 IOUtils.closeStream(cos); IOUtils.closeStream(fos); IOUtils.closeStream(fis);&#125; Map输出端采用压缩(以万能的WordCount案例为例)即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可具体实现(只修改Driver部分代码,Mapper和Reducer不变)： 123456789// 1、获取Job对象Configuration conf = new Configuration();// 开启map端输出压缩conf.setBoolean(\"mapreduce.map.output.compress\", true);// 设置map端输出压缩方式conf.setClass(\"mapreduce.map.output.compress.codec\", BZip2Codec.class, CompressionCodec.class);Job job = Job.getInstance(conf);// 之后代码保持原样 Reduce输出端采用压缩(以万能的WordCount案例为例)具体实现(只修改Driver部分代码,Mapper和Reducer不变)： 1234567891011// 之前代码保持不变// 设置reduce端输出压缩开启FileOutputFormat.setCompressOutput(job, true);// 设置压缩的方式FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);// 6、设置程序运行的输入和输出路径FileInputFormat.setInputPaths(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1]));// 之后代码保持不变 Yarn资源调度器Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序 Yarn基本架构YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成 Yarn工作机制 Yarn工作机制图解 工作机制详解 MR程序提交到客户端所在的节点 YarnRunner向ResourceManager申请一个Application RM将该应用程序的资源路径返回给YarnRunner 该程序将运行所需资源提交到HDFS上 程序资源提交完毕后，申请运行MrAppMaster RM将用户的请求初始化成一个Task 其中一个NodeManager领取到Task任务 该NodeManager创建容器Container，并产生MrAppmaster Container从HDFS上拷贝资源到本地 MrAppmaster向RM申请运行MapTask的资源 RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器 MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序 MrAppMaster等待所有MapTask运行完毕后，向RM申请容器运行ReduceTask ReduceTask向MapTask获取相应分区的数据 程序运行完毕后，MR会向RM申请注销自己 作业提交全过程 作业提交过程之Yarn图解 作业提交过程之MapReduce图解 作业提交过程详解 作业提交 Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业 Client向RM申请一个作业id RM给Client返回该job资源的提交路径和作业id Client提交jar包、切片信息和配置文件到指定的资源提交路径 Client提交完资源后，向RM申请运行MrAppMaster 作业初始化 当RM收到Client的请求后，将该job添加到容量调度器中 某一个空闲的NM领取到该Job 该NM创建Container，并产生MrAppmaster 下载Client提交的资源到本地 任务分配 MrAppMaster向RM申请运行多个MapTask任务资源 RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器 任务运行 MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序 MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask ReduceTask向MapTask获取相应分区的数据 程序运行完毕后，MR会向RM申请注销自己 进度和状态更新YARN中的任务将其进度和状态(包括counter)返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新，展示给用户 作业完成除了向应用管理器请求作业进度外，客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后，应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查 资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。 先进先出调度器(FIFO) 容量调度器(Capacity Scheduler) 公平调度器(Fair Scheduler) Hadoop3.1.3默认的资源调度器是Capacity Scheduler。具体设置详见yarn-default.xml文件 12345&lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; 任务的推测执行 作业完成时间取决于最慢的任务完成时间一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？ 推测执行机制发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果 执行推测任务的前提条件 每个Task只能有一个备份任务 当前Job已完成的Task必须不小于0.05(5%) 开启推测执行参数设置。mapred-site.xml文件中默认是打开的 1234567891011&lt;property&gt; &lt;name&gt;mapreduce.map.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some map tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;If true, then multiple instances of some reduce tasks may be executed in parallel.&lt;/description&gt;&lt;/property&gt; 不能启用推测执行机制情况 任务间存在严重的负载倾斜 特殊任务，比如任务向数据库中写数据 算法原理 Hadoop企业优化 MapReduce跑的慢的原因MapReduce程序效率的瓶颈在于两点： 计算机性能：CPU、内存、磁盘健康、网络 I/O操作优化 I/O操作优化 Map和Reduce数设置不合理 Map运行时间太长，导致Reduce等待过久 小文件过多 大量的不可分块的超大文件 Spill次数过多 Merge次数过多等 MapReduce优化方法MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数 数据输入 合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢 采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景 Map阶段 减少溢写(Spill)次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO 减少合并(Merge)次数：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间 在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少I/O Reduce阶段 合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误 设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间 规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗 合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整 I/O传输 采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZO压缩编码器 使用SequenceFile二进制文件 数据倾斜问题 数据倾斜现象 数据频率倾斜：某一个区域的数据量要远远大于其他区域 数据大小倾斜：部分记录的大小远远大于平均值 减少数据倾斜的方法 抽样和范围分区：可以通过对原始数据进行抽样得到的结果集来预设分区边界值 自定义分区：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例 Combine：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据 采用Map Join，尽量避免Reduce Join 常用的调优参数 资源相关参数 以下参数是在用户自己的MR应用程序中配置就可以生效(mapred-default.xml) 配置参数 参数说明 mapreduce.map.memory.mb 一个MapTask可使用的资源上限(单位:MB)，默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死 mapreduce.reduce.memory.mb 一个ReduceTask可使用的资源上限(单位:MB)，默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死 mapreduce.map.cpu.vcores 每个MapTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.cpu.vcores 每个ReduceTask可使用的最多cpu core数目，默认值: 1 mapreduce.reduce.shuffle.parallelcopies 每个Reduce去Map中取数据的并行数。默认值是5 mapreduce.reduce.shuffle.merge.percent Buffer中的数据达到多少比例开始写入磁盘。默认值0.66 mapreduce.reduce.shuffle.input.buffer.percent Buffer大小占Reduce可用内存的比例。默认值0.7 mapreduce.reduce.input.buffer.percent 指定多少比例的内存用来存放Buffer中的数据，默认值是0.0 应该在YARN启动之前就配置在服务器的配置文件中才能生效(yarn-default.xml) 配置参数 参数说明 yarn.scheduler.minimum-allocation-mb 给应用程序Container分配的最小内存，默认值：1024 yarn.scheduler.maximum-allocation-mb 给应用程序Container分配的最大内存，默认值：8192 yarn.scheduler.minimum-allocation-vcores 每个Container申请的最小CPU核数，默认值：1 yarn.scheduler.maximum-allocation-vcores 每个Container申请的最大CPU核数，默认值：32 yarn.nodemanager.resource.memory-mb 给Containers分配的最大物理内存，默认值：8192 Shuffle性能优化的关键参数，应在YARN启动之前就配置好(mapred-default.xml) 配置参数 参数说明 mapreduce.task.io.sort.mb Shuffle的环形缓冲区大小，默认100m mapreduce.map.sort.spill.percent 环形缓冲区溢出的阈值，默认80% 容错相关参数(MapReduce性能优化) 配置参数 参数说明 mapreduce.map.maxattempts 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4 mapreduce.reduce.maxattempts 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4 mapreduce.task.timeout Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间(单位毫秒)，默认是600000。如果你的程序对每条输入数据的处理时间过长(比如会访问数据库，通过网络拉取数据等)，建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.” HDFS小文件优化方法 HDFS小文件弊端HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件。一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢 HDFS小文件解决方案 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS 在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并 在MapReduce处理时，可采用CombineTextInputFormat提高效率 具体方案 Hadoop Archive是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了NameNode的内存使用 Sequence FileSequence File由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件 CombineFileInputFormatCombineFileInputFormat是一种新的InputFormat，用于将多个文件合并成一个单独的Split，另外，它会考虑数据的存储位置 开启JVM重用对于大量小文件Job，可以开启JVM重用会减少45%运行时间。JVM重用原理：一个Map运行在一个JVM上，开启重用的话，该Map在JVM上运行完毕后，JVM继续运行其他Map。具体设置：mapreduce.job.jvm.numtasks值在10-20之间 MapReduce扩展案例 倒排索引案例(多job串联) 需求：有大量的文本(文档、网页)，需要建立搜索索引 案例分析 第一次处理 OneIndexMapper 123456789101112131415161718192021222324public class OneIndexMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private String fileName; private Text k = new Text(); private IntWritable v = new IntWritable(1); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; FileSplit fileSplit = (FileSplit) context.getInputSplit(); fileName = fileSplit.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\" \"); // 3、写出 for (String field : fields) &#123; k.set(field + \"--\" + fileName); context.write(k, v); &#125; &#125;&#125; OneIndexReducer 123456789101112131415public class OneIndexReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable v = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; // 1、累加求和 for (IntWritable value : values) &#123; sum += value.get(); &#125; v.set(sum); // 2、写出 context.write(key, v); &#125;&#125; OneIndexDriver 1234567891011121314public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(OneIndexDriver.class); job.setMapperClass(OneIndexMapper.class); job.setReducerClass(OneIndexReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true);&#125; 第二次处理 TwoIndexMapper 123456789101112131415161718public class TwoIndexMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; private Text k = new Text(); private Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // haha--a.txt 2 // 1、获取一行 String lineStr = value.toString(); // 2、切割 String[] fields = lineStr.split(\"--\"); // 3、封装 k.set(fields[0]); v.set(fields[1]); // 4、写出 context.write(k, v); &#125;&#125; TwoIndexReducer 1234567891011121314public class TwoIndexReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123; private Text v = new Text(); @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text value : values) &#123; sb.append(value.toString().replace(\"\\t\", \"--&gt;\")) .append('\\t'); &#125; v.set(sb.toString()); context.write(key, v); &#125;&#125; TwoIndexDriver 123456789101112131415public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration config = new Configuration(); Job job = Job.getInstance(config); job.setJarByClass(TwoIndexDriver.class); job.setMapperClass(TwoIndexMapper.class); job.setReducerClass(TwoIndexReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result?0:1);&#125; 先运行OneIndexDriver，将得到的输入作为TwoIndexDriver的输入，在运行TwoIndexDriver得到最终结果 TopN案例 需求：输出流量使用量在前10的用户信息 案例分析 代码实现 TopNMapper 12345678910111213141516171819202122232425262728293031323334353637383940public class TopNMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123; // 定义一个TreeMap作为存储数据的容器(天然按key排序) private TreeMap&lt;FlowBean, Text&gt; flowMap = new TreeMap&lt;FlowBean, Text&gt;(); private FlowBean kBean; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; kBean = new FlowBean(); Text v = new Text(); // 1、获取一行 String line = value.toString(); // 2、切割 String[] fields = line.split(\"\\t\"); // 3、封装数据 String phoneNum = fields[0]; long upFlow = Long.parseLong(fields[1]); long downFlow = Long.parseLong(fields[2]); long sumFlow = Long.parseLong(fields[3]); kBean.setDownFlow(downFlow); kBean.setUpFlow(upFlow); kBean.setSumFlow(sumFlow); v.set(phoneNum); // 4、向TreeMap中添加数据 flowMap.put(kBean, v); // 5、限制TreeMap的数据量,超过10条就删除掉流量最小的一条数据 if (flowMap.size() &gt; 10) &#123; flowMap.remove(flowMap.lastKey()); &#125; &#125; @Override protected void cleanup(Context context) throws IOException, InterruptedException &#123; // 6、遍历treeMap集合,输出数据 Iterator&lt;FlowBean&gt; bean = flowMap.keySet().iterator(); while (bean.hasNext()) &#123; FlowBean k = bean.next(); context.write(k, flowMap.get(k)); &#125; &#125;&#125; TopNReducer 12345678910111213141516171819202122232425262728public class TopNReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; // 定义一个TreeMap作为存储数据的容器（天然按key排序） TreeMap&lt;FlowBean, Text&gt; flowMap = new TreeMap&lt;FlowBean, Text&gt;(); @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context)throws IOException, InterruptedException &#123; for (Text value : values) &#123; FlowBean bean = new FlowBean(); bean.set(key.getDownFlow(), key.getUpFlow()); // 1、向treeMap集合中添加数据 flowMap.put(bean, new Text(value)); // 2、限制TreeMap数据量,超过10条就删除掉流量最小的一条数据 if (flowMap.size() &gt; 10) &#123; flowMap.remove(flowMap.lastKey()); &#125; &#125; &#125; @Override protected void cleanup(Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context) throws IOException, InterruptedException &#123; // 3、遍历集合,输出数据 Iterator&lt;FlowBean&gt; it = flowMap.keySet().iterator(); while (it.hasNext()) &#123; FlowBean v = it.next(); context.write(new Text(flowMap.get(v)), v); &#125; &#125;&#125; TopNDriver 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; // 1、获取配置信息,或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定本程序的jar包所在的本地路径 job.setJarByClass(TopNDriver.class); // 3、指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(TopNMapper.class); job.setReducerClass(TopNReducer.class); // 4、指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 5、指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6、指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 找博客共同好友案例 需求以下是博客的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友(数据中的好友关系是单向的)。求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？ 示例数据 1234567891011121314A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J 案例分析 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&#x2F;&#x2F; 第一次先求出A、B、C...等是谁的好友A I,K,C,B,G,F,H,O,DB A,F,J,EC A,E,B,H,F,G,KD G,C,K,A,L,F,E,HE G,M,L,H,A,F,B,DF L,M,D,C,G,AG MH OI O,CJ OK BL D,EM E,FO A,H,I,J,F&#x2F;&#x2F; 第二次找出共同好友A-B E CA-C D FA-D E FA-E D B CA-F O B C D EA-G F E C DA-H E C D OA-I OA-J O BA-K D CA-L F E DA-M E FB-C AB-D A EB-E CB-F E A CB-G C E AB-H A E CB-I AB-K C AB-L EB-M EB-O AC-D A FC-E DC-F D AC-G D F AC-H D AC-I AC-K A DC-L D FC-M FC-O I AD-E LD-F A ED-G E A FD-H A ED-I AD-K AD-L E FD-M F ED-O AE-F D M C BE-G C DE-H C DE-J BE-K C DE-L DF-G D C A EF-H A D O E CF-I O AF-J B OF-K D C AF-L E DF-M EF-O AG-H D C E AG-I AG-K D A CG-L D F EG-M E FG-O AH-I O AH-J OH-K A C DH-L D EH-M EH-O AI-J OI-K AI-O AK-L DK-O AL-M E F 代码实现 第一次Mapper 123456789101112131415161718public class OneShareFriendsMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; // 1、获取一行 A:B,C,D,F,E,O String line = value.toString(); // 2、切割 String[] fields = line.split(\":\"); // 3、获取person和好友 String person = fields[0]; String[] friends = fields[1].split(\",\"); // 4、写出去 for(String friend: friends)&#123; // 输出 &lt;好友，人&gt; context.write(new Text(friend), new Text(person)); &#125; &#125;&#125; 第一次Reducer 1234567891011121314public class OneShareFriendsReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); // 1、拼接 for(Text person: values)&#123; sb.append(person).append(\",\"); &#125; // 取出多余的',' sb.deleteCharAt(sb.length() - 1); // 2、写出 context.write(key, new Text(sb.toString())); &#125;&#125; 第一次Driver 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; // 1、获取job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定jar包运行的路径 job.setJarByClass(OneShareFriendsDriver.class); // 3、指定map/reduce使用的类 job.setMapperClass(OneShareFriendsMapper.class); job.setReducerClass(OneShareFriendsReducer.class); // 4、指定map输出的数据类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); // 5、指定最终输出的数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); // 6、指定job的输入原始所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 第二次Mapper 12345678910111213141516171819public class TwoShareFriendsMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // A I,K,C,B,G,F,H,O,D // 友 人,人,人 String line = value.toString(); String[] friend_persons = line.split(\"\\t\"); String friend = friend_persons[0]; String[] persons = friend_persons[1].split(\",\"); Arrays.sort(persons); for (int i = 0; i &lt; persons.length - 1; i++) &#123; for (int j = i + 1; j &lt; persons.length; j++) &#123; // 发出 &lt;人-人,好友&gt;,这样，相同的“人-人”对的所有好友就会到同1个reduce中去 context.write(new Text(persons[i] + \"-\" + persons[j]), new Text(friend)); &#125; &#125; &#125;&#125; 第二次Reducer 12345678910public class TwoShareFriendsReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context)throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text friend : values) &#123; sb.append(friend).append(\" \"); &#125; context.write(key, new Text(sb.toString())); &#125;&#125; 第二次Driver 12345678910111213141516171819202122public static void main(String[] args) throws Exception &#123; // 1、获取job对象 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2、指定jar包运行的路径 job.setJarByClass(TwoShareFriendsDriver.class); // 3、指定map/reduce使用的类 job.setMapperClass(TwoShareFriendsMapper.class); job.setReducerClass(TwoShareFriendsReducer.class); // 4、指定map输出的数据类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); // 5、指定最终输出的数据类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); // 6、指定job的输入原始所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1);&#125; 常见错误及解决方案 导包错误，尤其是Text和CombineTextInputFormat Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable。报的错误是类型转换异常 java.lang.Exception: java.io.IOException: Illegal partition for 13926435656(4)；说明Partition和ReduceTask个数没对上，调整ReduceTask个数 如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行 报类型转换异常：通常都是在驱动函数中设置Map输出和最终输出时编写错误；Map输出的key如果没有排序，也会报类型转换异常 集群中运行wc.jar时出现了无法获得输入文件。原因：WordCount案例的输入文件不能放用HDFS集群的根目录 自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空","categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"}]}],"categories":[{"name":"编程","slug":"编程","permalink":"https://sobxiong.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"BasicSkill","slug":"BasicSkill","permalink":"https://sobxiong.github.io/tags/BasicSkill/"},{"name":"LeetCode题解","slug":"LeetCode题解","permalink":"https://sobxiong.github.io/tags/LeetCode%E9%A2%98%E8%A7%A3/"},{"name":"Middleware","slug":"Middleware","permalink":"https://sobxiong.github.io/tags/Middleware/"},{"name":"MySQL","slug":"MySQL","permalink":"https://sobxiong.github.io/tags/MySQL/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://sobxiong.github.io/tags/SpringMVC/"},{"name":"Spring","slug":"Spring","permalink":"https://sobxiong.github.io/tags/Spring/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://sobxiong.github.io/tags/SpringCloud/"},{"name":"Java高级","slug":"Java高级","permalink":"https://sobxiong.github.io/tags/Java%E9%AB%98%E7%BA%A7/"},{"name":"BigData","slug":"BigData","permalink":"https://sobxiong.github.io/tags/BigData/"},{"name":"Linux","slug":"Linux","permalink":"https://sobxiong.github.io/tags/Linux/"},{"name":"Java","slug":"Java","permalink":"https://sobxiong.github.io/tags/Java/"}]}