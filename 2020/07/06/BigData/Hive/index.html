<!DOCTYPE html>
<html>
<head hexo-theme='https://volantis.js.org/#2.6.6'>
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
    
  
  <!-- 渲染优化 -->
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- 页面元数据 -->
  
    <title>Hive - SOBXiong的博客</title>
  
    <meta name="keywords" content="大数据,Hive">
  
  
    <meta name="description" content="内容
Hive基本概念
Hive安装
Hive数据类型
DDL数据定义
DML数据操作
查询
例题实战(蚂蚁金服)
函数
压缩和存储
企业级调优
谷粒影音Hive实战
常见错误及解决方案
">
  

  <!-- feed -->
  

  <!-- import meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13/css/all.min.css">
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">

  

  
  <link rel="shortcut icon" type='image/x-icon' href="https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/favicon.png">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css">
  

  

  <!-- import link -->
  

  
  
    
<link rel="stylesheet" href="/css/style.css">

  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
</head>

<body>
  
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>
<header class="l_header shadow blur">
  <div class='container'>
  <div class='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h'>
        <li><a class="s-comment fas fa-comments fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
          <li><a class="s-toc fas fa-list fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
          
            <i class='https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/favicon.png'></i>
          
          
            SOBXiong
          
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h'>
          
          
          
            
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
				</ul>
			</div>

      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fas fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>

			<ul class='switcher nav-list-h'>
				
					<li><a class="s-search fas fa-search fa-fw" target="_self" href='javascript:void(0)'></a></li>
				
				<li>
          <a class="s-menu fas fa-bars fa-fw" target="_self" href='javascript:void(0)'></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
          </ul>
        </li>
			</ul>
		</div>
	</div>
  </div>
</header>

<script>setLoadingBarProgress(40);</script>



  <div class="l_body nocover">
    <div class='body-wrapper'>
      

<div class='l_main'>
  

  
    <article id="post" class="post white-box reveal shadow article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
      
      
      <div class="meta" id="header-meta">
        
          
  <h1 class="title">
    <a href="/2020/07/06/BigData/Hive/">
      Hive
    </a>
  </h1>


        
        <div class='new-meta-box'>
          
            
          
            
              
<div class='new-meta-item author'>
  <a href="http://xiongjc.top" target="_blank" rel="nofollow noopener">
    <img src="https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/hdImg_f757fa7e0fd65077ce52d251fc7a01d815860762755.jpg">
    <p>SOBXiong</p>
  </a>
</div>

            
          
            
              
  
  <div class='new-meta-item category'>
    <a href='/categories/%E7%BC%96%E7%A8%8B/' rel="nofollow">
      <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>
      <p>编程</p>
    </a>
  </div>


            
          
            
              <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：2020年7月6日</p>
  </a>
</div>

            
          
            
              

            
          
        </div>
        
          <hr>
        
      </div>
    
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          
          <h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul>
<li><a href="#Hive基本概念">Hive基本概念</a></li>
<li><a href="#Hive安装">Hive安装</a></li>
<li><a href="#Hive数据类型">Hive数据类型</a></li>
<li><a href="#DDL数据定义">DDL数据定义</a></li>
<li><a href="#DML数据操作">DML数据操作</a></li>
<li><a href="#查询">查询</a></li>
<li><a href="#例题实战(蚂蚁金服)">例题实战(蚂蚁金服)</a></li>
<li><a href="#函数">函数</a></li>
<li><a href="#压缩和存储">压缩和存储</a></li>
<li><a href="#企业级调优">企业级调优</a></li>
<li><a href="#谷粒影音Hive实战">谷粒影音Hive实战</a></li>
<li><a href="#常见错误及解决方案">常见错误及解决方案</a></li>
</ul>
<a id="more"></a>

<h2 id="Hive基本概念"><a href="#Hive基本概念" class="headerlink" title="Hive基本概念"></a>Hive基本概念</h2><ul>
<li><p>什么是Hive</p>
<ul>
<li>由Facebook开源用于解决海量结构化日志的数据统计</li>
<li>基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能<ul>
<li>Hive处理的数据存储在HDFS</li>
<li>Hive分析数据底层的实现是MapReduce</li>
<li>执行程序运行在Yarn上</li>
</ul>
</li>
<li>本质是将HQL(Hive Query Language)转化成MapReduce程序<br><img src="SQL-MapReduce.png" alt="SQL-MapReduce"></li>
</ul>
</li>
<li><p>Hive的优缺点</p>
<ul>
<li>优点：<ul>
<li>操作接口采用类SQL语法，提供快速开发的能力(简单、容易上手)</li>
<li>避免了去写MapReduce，减少开发人员的学习成本</li>
<li>Hive的执行延迟比较高，因此Hive常用于数据分析和对实时性要求不高的场合</li>
<li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li>
<li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li>
</ul>
</li>
<li>缺点：<ul>
<li>Hive的HQL表达能力有限：<ul>
<li>迭代式算法无法表达</li>
<li>数据挖掘方面不擅长</li>
</ul>
</li>
<li>Hive的效率比较低：<ul>
<li>Hive自动生成的MapReduce作业，通常情况下不够智能化</li>
<li>Hive调优比较困难，粒度较粗</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive架构原理<br><img src="Hive%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png" alt="Hive架构原理"></p>
<ul>
<li>用户接口：Client<br>CLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive)</li>
<li>元数据：Metastore<br>元数据包括：表名、表所属的数据库(默认是default)、表的拥有者、列/分区字段、表的类型(是否是外部表)、表的数据所在目录等；</li>
</ul>
<p><strong>默认存储在自带的derby数据库中(存在bug)，推荐使用MySQL存储Metastore</strong></p>
<ul>
<li>Hadoop：使用HDFS进行存储，使用MapReduce进行计算</li>
<li>驱动器：Driver<ul>
<li>解析器(SQL Parser)：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误</li>
<li>编译器(Physical Plan)：将AST编译生成逻辑执行计划</li>
<li>优化器(Query Optimizer)：对逻辑执行计划进行优化</li>
<li>执行器(Execution)：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark<br><img src="Hive%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6.png" alt="Hive运行机制"><br>Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将 执行返回的结果输出到用户交互接口</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive和数据库比较<br>由于Hive采用了类似SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。下面将从多个方面来阐述Hive和数据库的差异。数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性</p>
<ul>
<li>查询语言<br>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言 HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发</li>
<li>数据存储位置<br>Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中</li>
<li>数据更新<br>由于Hive是针对数据仓库应用设计的，而<strong>数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的</strong>。而数据库中的数据通常是需要经常进行修改的</li>
<li>索引<br>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。<strong>Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据</strong>，因此访问延迟较高。由于MapReduce的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive不适合在线数据查询</li>
<li>执行<br>Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的。而数据库通常有自己的执行引擎</li>
<li>执行延迟<br>Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小。当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势</li>
<li>可扩展性<br>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的(世界上最大的Hadoop集群在Yahoo，2009年的规模在4000台节点左右)。而数据库由于 ACID语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右</li>
<li>数据规模<br>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小</li>
</ul>
</li>
</ul>
<h2 id="Hive安装"><a href="#Hive安装" class="headerlink" title="Hive安装"></a>Hive安装</h2><ul>
<li><p>安装地址</p>
<ul>
<li>Hive官网地址：<a href="http://hive.apache.org" target="_blank" rel="noopener">http://hive.apache.org</a></li>
<li>文档查看地址：<a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li>
<li>下载地址：<a href="http://archive.apache.org/dist/hive" target="_blank" rel="noopener">http://archive.apache.org/dist/hive</a></li>
<li>github地址：<a href="https://github.com/apache/hive" target="_blank" rel="noopener">https://github.com/apache/hive</a></li>
</ul>
</li>
<li><p>Hive安装部署</p>
<ul>
<li><p>Hive安装及配置</p>
<ul>
<li><p>上传：把apache-hive-3.1.2-bin.tar.gz上传到/opt/software目录下</p>
</li>
<li><p>解压：tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt/module/</p>
</li>
<li><p>修改目录名：mv apache-hive-3.1.2-bin hive-3.1.2</p>
</li>
<li><p>修改配置文件(conf目录下)：</p>
<ul>
<li>备份一份配置文件：cp hive-env.sh.template hive-env.sh.template.copy</li>
<li>修改配置文件后缀：mv hive-env.sh.template hive-env.sh</li>
<li>配置hive-env.sh文件(底部加入)：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export HIVE_CONF_DIR=/opt/module/hive-3.1.2/conf</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Hadoop集群启动(hadoop1启动hdfs——sbin/start-dfs.sh,hadoop2启动yarn——sbin/start-yarn.sh)</p>
</li>
<li><p>Hive基本操作</p>
<ul>
<li>初始化默认的derby数据库：bin/schematool -dbType derby -initSchema(初始化后在hive根目录会产生derby.log和metastore目录)</li>
<li>启动hive：bin/hive</li>
<li>启动时会发生Exception in thread “main” java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V错误<ul>
<li>这是因为hive内依赖的guava和hadoop内的版本不一致</li>
<li>分别查看hive(lib目录下)和hadoop(share/hadoop/common/lib目录下)的guava依赖版本：guava-19.0.jar和guava-27.0-jre.jar</li>
<li>删除hive的低版本guava-19.0.jar，将hadoop的高版本guava-27.0-jre.jar复制到hive的lib目录下</li>
<li>重启启动hive</li>
</ul>
</li>
<li>查看数据库：show databases;</li>
<li>打开默认数据库：use default;</li>
<li>显示default数据库中的表：show tables;</li>
<li>创建一张表(数据类型为java中类型)：create table student(id int,name string);</li>
<li>查看表的结构：desc student;</li>
<li>向表中插入数据：insert into student values(1,”SOBXiong”);</li>
<li>查询表中数据：select * from student;</li>
<li>退出hive：quit;</li>
</ul>
</li>
</ul>
</li>
<li><p>本地文件导入Hive<br>需求：将本地/opt/module/data/hive/student.txt的数据导入到hive的student表中</p>
<ul>
<li>数据准备(tab键隔开)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 xixi</span><br><span class="line">2 haha</span><br><span class="line">3 hehe</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Hive操作</p>
<ul>
<li>导入student.txt的数据到之前创建的student表中：load data local inpath ‘/opt/module/data/hive/student.txt’ into table student;</li>
<li>查询结果(发现都是NULL NULL,因为格式不对)：select * from student;</li>
<li>删除已创建的student表：drop table student;</li>
<li>创建新的student表(声明文件分隔符’\t’)：create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</li>
<li>重新导入数据并重新查询结果</li>
<li>查看<a href="http://hadoop1:9870" target="_blank" rel="noopener">http://hadoop1:9870</a>中的HDFS文件，发现/user/hive/warehouse/student下就有数据</li>
<li>第二种插入数据的方式：直接将文件上传至HDFS服务器<ul>
<li>上传本地文件(相当于cp复制)：hadoop fs -put stu1.txt /user/hive/warehouse/student</li>
<li>上传HDFS文件(相当于mv移动)：hadoop fs -put /stu2.txt /user/hive/warehouse/student</li>
</ul>
</li>
</ul>
</li>
<li><p>derby存储元数据的问题(推荐使用mysql)：</p>
<ul>
<li>只能开启一个hive客户端</li>
<li>在不同的目录开启hive客户端会在当前目录下创建derby.log和metastore文件，相当于数据不共享</li>
</ul>
</li>
</ul>
</li>
<li><p>MySql安装</p>
<ul>
<li><p>安装包准备：</p>
<ul>
<li>查看yum中历史的mysql或者mariadb的依赖：rpm -qa | grep mysql/mariadb</li>
<li>如有历史依赖，删除：yum remove mysql-libs/mariadb-libs</li>
<li>下载mysql的rpm包：前往<a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/</a>下载5.7.30的Red Hat Enterprise Linux7版本(CentOS7)的RPM Bundle包</li>
</ul>
</li>
<li><p>安装MySql</p>
<ul>
<li>解压tar包：tar -xvf mysql-5.7.30-1.el7.x86_64.rpm-bundle.tar</li>
<li>使用rpm命令安装MySql组件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 依赖关系为common→libs→client→server</span></span><br><span class="line">rpm -ivh common</span><br><span class="line">rpm -ivh libs</span><br><span class="line">rpm -ivh client</span><br><span class="line">rpm -ivh server</span><br></pre></td></tr></table></figure>

<ul>
<li>启动MySql：systemctl start mysqld.service</li>
<li>查看MySql状态：systemctl status mysqld.service</li>
<li>查看初始化的随机密码：grep ‘temporary password’ /var/log/mysqld.log</li>
<li>登录MySql：mysql -u root -p</li>
<li>修改密码校验策略(不然设置新密码会提示密码错误)：set global validate_password_policy=0;</li>
<li>修改密码：alter user root@localhost identified by ‘your password’;</li>
<li>授权root用户远程访问权限</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'root'</span> @<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'your password'</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>设置MySql完毕，退出：quit;</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive元数据配置到MySql</p>
<ul>
<li><p>拷贝mysql-connector JDBC驱动文件</p>
<ul>
<li>前往<a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/</a>下载驱动文件5.1.49版本</li>
<li>解压文件mysql-connector-java-5.1.49.tar.gz，拷贝mysql-connector-java-5.1.49-bin.jar到hive的lib目录下</li>
</ul>
</li>
<li><p>配置metastore到MySql</p>
<ul>
<li>在conf目录下创建hive-site.xml配置文件：touch hive-site.xml</li>
<li>修改配置文件：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- xml下的&amp;需要转义为&amp;amp; --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop1:3306/metastore?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>your password<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化Hive的MySql元数据数据库：bin/schematool -dbType mysql -initSchema</p>
</li>
<li><p>启动hive，MySql中新增了metastore数据库(表DBS和TBS比较重要)</p>
</li>
</ul>
</li>
<li><p>HiveJDBC访问</p>
<ul>
<li>停止hadoop：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop1</span></span><br><span class="line">stop-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop2</span></span><br><span class="line">stop-yarn.sh</span><br></pre></td></tr></table></figure>

<ul>
<li>修改hadoop配置：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml 启用webhdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">  core-site.xml 设置hadoop的代理用户</span></span><br><span class="line"><span class="comment">  hadoop.proxyuser.xxx.hosts</span></span><br><span class="line"><span class="comment">  xxx是操作的用户</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  org.apache.hadoop.security.authorize.AuthorizationException: User: sobxiong is not allowed to impersonate root(state=08S01,code=0)</span></span><br><span class="line"><span class="comment">  User:xxx即为下面该填入的用户</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.sobxiong.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.sobxiong.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>修改hive配置：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Bind host on which to run the HiveServer2 Thrift service.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>11000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>启动hiveserver2服务：bin/hiveserver2</li>
<li>启动beeline：bin/beeline</li>
<li>连接hiveserver2：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://hadoop1:11000</span></span><br><span class="line">Enter username for jdbc:hive2://hadoop102:10000: sobxiong</span><br><span class="line">Enter password for jdbc:hive2://hadoop102:10000: your password(数据库的密码)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 接下来的操作就跟Hive Cli使用类似SQL</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Hive常用交互命令</p>
<ul>
<li>-e &lt;quoted-query-string&gt;：不进入hive的交互窗口执行sql语句，例如：bin/hive -e “select * from student;”</li>
<li>-f &lt;filename&gt;：执行脚本中sql语句<ul>
<li>结果打印在terminal上：bin/hive -f /opt/module/data/hive/hive.hql</li>
<li>结果打印在指定文件中：bin/hive -f /opt/module/data/hive/hive.hql  &gt; ./hive_result.txt</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive其他命令操作</p>
<ul>
<li>在Hive Cli命令窗口中查看hdfs文件系统：dfs -ls /</li>
<li>在Hive Cli命令窗口中查看本地文件系统：! ls /</li>
<li>查看在hive中输入的所有历史命令：cat ~/.hivehistory</li>
</ul>
</li>
<li><p>Hive常见属性配置</p>
<ul>
<li>Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse</li>
<li><strong>在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹</strong></li>
<li>修改default数据仓库原始位置(hive-default.xml.template -&gt; hive-site.xml)</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>查询后信息显示配置</p>
<ul>
<li>在hive-site.xml加入如下配置：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 表列名显示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 当前使用数据库显示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>重启Hive Cli</li>
</ul>
</li>
<li><p>Hive运行日志信息配置</p>
<ul>
<li>Hive的日志信息默认存放在/tmp/{current_user}目录下</li>
<li>修改Hive的日志信息存放在hive安装目录的logs文件夹下<br>修改conf/hive-log4j.properties配置文件(hive-log4j.properties.template -&gt; hive-log4j.properties)：hive.log.dir=/opt/module/hive-3.1.2/logs</li>
</ul>
</li>
<li><p>参数配置方式</p>
<ul>
<li>查看当前所有的配置信息(Hive Cli命令窗口下)：set;</li>
<li>参数配置的三种方式<ul>
<li>配置文件方式<br>默认配置文件：hive-default.xml<br>用户自定义配置文件：hive-site.xml<br>注意：<strong>用户自定义配置会覆盖默认配置</strong>。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效</li>
<li>命令行参数方式<br>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数<br>例如：bin/hive -hiveconf mapred.reduce.tasks=10;(<strong>注意：仅对本次hive启动有效</strong>)<br>查看参数设置：set mapred.reduce.tasks;</li>
<li>参数声明方式<br>在HQL中使用SET关键字设定参数(<strong>注意：仅对本次hive启动有效</strong>)<br>例如：set mapred.reduce.tasks=100;<br>上述三种设定方式的优先级依次递增。即配置文件 &lt; 命令行参数 &lt; 参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><ul>
<li>基本数据类型(Hive数据类型大小写不敏感)</li>
</ul>
<table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
</tr>
<tr>
<td>SMALLINT</td>
<td>short</td>
<td>2byte有符号整数</td>
</tr>
<tr>
<td><strong>INT</strong></td>
<td>int</td>
<td>4byte有符号整数</td>
</tr>
<tr>
<td><strong>BIGINT</strong></td>
<td>long</td>
<td>8byte有符号整数</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或false</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
</tr>
<tr>
<td><strong>DOUBLE</strong></td>
<td>double</td>
<td>双精度浮点数</td>
</tr>
<tr>
<td><strong>STRING</strong></td>
<td>string</td>
<td>字符系列，可以指定字符集，可以使用单引号或者双引号</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>-</td>
<td>时间类型</td>
</tr>
<tr>
<td>BINARY</td>
<td>-</td>
<td>字节数组</td>
</tr>
</tbody></table>
<p>Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过他不能声明其最多能存储多少个字符，理论上它可以存储2GB的字符数</p>
<ul>
<li>集合数据类型</li>
</ul>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用</td>
<td>struct()</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取键last对应的值数据</td>
<td>map()</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用</td>
<td>Array()</td>
</tr>
</tbody></table>
<p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套</p>
<ul>
<li><p>集合数据类型案例实操</p>
<ul>
<li>假设JSON为原始数据，具体如下：</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"songsong"</span>,</span><br><span class="line">  <span class="attr">"friends"</span>: [<span class="string">"bingbing"</span> , <span class="string">"lili"</span>],</span><br><span class="line">  <span class="attr">"children"</span>: &#123;</span><br><span class="line">      <span class="attr">"xiao song"</span>: <span class="number">18</span> ,</span><br><span class="line">      <span class="attr">"xiaoxiao song"</span>: <span class="number">19</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"address"</span>:&#123;</span><br><span class="line">    <span class="attr">"street"</span>: <span class="string">"hui long guan"</span> ,</span><br><span class="line">    <span class="attr">"city"</span>: <span class="string">"beijing"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>基于上述数据结构，建立本地测试文件test.txt，具体格式如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意：MAP、STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用’_’</p>
<ul>
<li>Hive上创建测试表test</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;,</span><br><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line"><span class="comment">/* 设置列分隔符为',' */</span></span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line"><span class="comment">/* 设置map、struct和array的分隔符(数据分割符号)为'_' */</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span><br><span class="line"><span class="comment">/* 设置map中的key/value的分隔符为',' */</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="comment">/* 设置行分隔符为'\n'(也是默认值) */</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>导入文本数据到测试表中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/test.txt' into table test;</span><br></pre></td></tr></table></figure>

<ul>
<li>访问三种集合列里的数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> friends[<span class="number">1</span>],children[<span class="string">'xiao song'</span>],address.city <span class="keyword">from</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>类型转换<br>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化。例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作</p>
<ul>
<li>隐式类型转换规则<ul>
<li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT</li>
<li>所有整数类型、FLOAT和<strong>STRING(符合数字)</strong>类型都可以隐式地转换成DOUBLE</li>
<li>TINYINT、SMALLINT、INT都可以转换为FLOAT</li>
<li>BOOLEAN类型不可以转换为任何其它的类型</li>
</ul>
</li>
<li>使用CAST操作显示进行数据类型转换<br>例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值NULL</li>
</ul>
</li>
</ul>
<h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><ul>
<li><p>创建数据库</p>
<ul>
<li>创建一个数据库，默认在HDFS上的存储路径式/user/hive/warehouse/*.db：create database if not exists db_hive;(if not exists避免要创建的数据库已存在)</li>
<li>创建一个数据库，指定在HDFS上存放的路径</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive2 location <span class="string">'/db_hive2.db'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查询数据库</p>
<ul>
<li><p>显示数据库</p>
<ul>
<li>显示数据库：show databases;</li>
<li>过滤查询显示的数据库</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span> <span class="keyword">like</span> <span class="string">'db_hive'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看数据库</p>
<ul>
<li>显示数据库信息：desc database db_hive;</li>
<li>显示数据库详细信息(extended)：desc database extended db_hive;</li>
</ul>
</li>
<li><p>切换当前数据库：use db_hive;</p>
</li>
</ul>
</li>
<li><p>修改数据库<br>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。<strong>数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置</strong><br>修改数据库属性值：</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20200708'</span>)</span><br></pre></td></tr></table></figure>

<p>查看修改结果：desc database extended db_hive;</p>
<ul>
<li><p>删除数据库</p>
<ul>
<li>删除空数据库：drop database if exists db_hive;(if exists避免要删除的数据库不存在)</li>
<li>如果数据库中表不为空，可以采用cascade命令集联强制删除：drop database db_hive cascade;</li>
</ul>
</li>
<li><p>创建表</p>
<ul>
<li>建表语法</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...)</span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format]</span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure>

<ul>
<li><p>字段解释说明</p>
<ul>
<li>CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常</li>
<li>EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径(LOCATION)，<strong>Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</strong></li>
<li>COMMENT：为表和列添加注释</li>
<li>PARTITIONED BY创建分区表</li>
<li>CLUSTERED BY创建分桶表</li>
<li>SORTED BY不常用</li>
<li>ROW FORMAT<br>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]<br>| SERDE serde_name [WITH SERDEPROPERTIE (property_name=property_value, property_name=property_value, …)]<br>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化</li>
<li>STORED AS指定存储文件类型<br>常用的存储文件类型：SEQUENCEFILE(二进制序列文件)、TEXTFILE(文本)、RCFILE(列式存储格式文件)<br>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE</li>
<li>LOCATION：指定表在HDFS上的存储位置</li>
<li>LIKE允许用户复制现有的表结构，但是不复制数据</li>
</ul>
</li>
<li><p>管理表</p>
<ul>
<li><p>介绍<br>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会(或多或少地)控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。<strong>当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据</strong></p>
</li>
<li><p>实际操作</p>
<ul>
<li>创建普通表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>根据查询结果创建表(查询的结果会添加到新创建的表中)：create table if not exists student2 as select id, name from student;</li>
<li>根据已存在的表结构创建表：create table if not exists student3 like student;</li>
<li>查询表的类型：desc formatted student;</li>
</ul>
</li>
</ul>
</li>
<li><p>外部表</p>
<ul>
<li>介绍：因为表是外部表，所以Hive并非认为其完全拥有这份数据。<strong>删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉</strong></li>
<li>实际操作(创建表,其余操作与管理表类似)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> default.dept(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>管理表与外部表</p>
<ul>
<li>相互转换</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 修改内部表为外部表：</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>);</span><br><span class="line"><span class="comment">-- 修改外部表为内部表：</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>);</span><br><span class="line"><span class="comment">-- 注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写</span></span><br></pre></td></tr></table></figure>

<ul>
<li>使用场景<br>每天将收集到的网站日志定期流入HDFS文本文件。在外部表(原始日志表)的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表</li>
</ul>
</li>
</ul>
</li>
<li><p>分区表<br>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<strong>Hive中的分区就是分目录</strong>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多</p>
<ul>
<li><p>分区表基本操作</p>
<ul>
<li>创建分区表语法</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition(</span><br><span class="line">deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>加载数据到分区表中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201709');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201708');</span><br><span class="line">hive (default)&gt; load data local inpath '/opt/module/data/hive/dept.txt' into table default.dept_partition partition(month='201707’);</span><br></pre></td></tr></table></figure>

<ul>
<li><p>查询分区表中数据</p>
<ul>
<li>单分区查询</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>多分区联合查询</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询多个分区</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">union</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201708'</span></span><br><span class="line"><span class="keyword">union</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201707'</span>;</span><br><span class="line"><span class="comment">-- 查询全部</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition;</span><br></pre></td></tr></table></figure>
</li>
<li><p>增加分区</p>
<ul>
<li>创建单个分区</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>同时创建多个分区</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201705'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除分区：</p>
<ul>
<li>删除单个分区</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>同时删除多个分区</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201705'</span>), <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201706'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看分区表有多少分区：show partitions dept_partition;</p>
</li>
<li><p>查看分区表结构：desc formatted dept_partition;</p>
</li>
</ul>
</li>
<li><p>分区表扩展用法</p>
<ul>
<li>创建二级分区表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>加载二级分区数据</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709', day='13');</span><br></pre></td></tr></table></figure>

<ul>
<li>查询二级分区数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'13'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>将数据上传到分区目录后，让分区表和数据产生关联的方式</p>
<ul>
<li><p>上传数据后修复(适用于数据较多的情况)</p>
<ul>
<li>上传数据：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive Cli命令环境下</span></span><br><span class="line">dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br><span class="line">dfs -put /opt/module/data/hive/dept.txt  /user/hive/warehouse/dept_partition2/month=201709/day=12;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询数据(查询不到)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dept_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'12'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>执行修复命令：msck repair table dept_partition2;</li>
</ul>
</li>
<li><p>上传数据后添加分区</p>
<ul>
<li>上传数据(同上)</li>
<li>执行添加分区</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition2 <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>,<span class="keyword">day</span>=<span class="string">'11'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建文件夹后load数据到分区</p>
<ul>
<li>创建目录：dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10;</li>
<li>上传数据</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/dept.txt' into table dept_partition2 partition(month='201709',day='10');</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>修改表</p>
<ul>
<li>重命名表<ul>
<li>语法：ALTER TABLE table_name RENAME TO new_table_name</li>
<li>实例：alter table dept_partition2 rename to dept_partition3;</li>
</ul>
</li>
<li>添加、修改和删除表分区(同上)</li>
<li>增加、修改、替换列信息<ul>
<li>语法：<ul>
<li>更新列：ALTER TABLE table_name <strong>CHANGE</strong> [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</li>
<li>添加和替换列：ALTER TABLE table_name <strong>ADD|REPLACE</strong> COLUMNS (col_name data_type [COMMENT col_comment], …)</li>
<li><strong>注意：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段</strong></li>
</ul>
</li>
<li>实操<ul>
<li>查询表结构(用于查看修改结果)：desc dept_partition;</li>
<li>添加列：alter table dept_partition add columns(deptdesc string);</li>
<li>更新列：alter table dept_partition change column deptdesc desc int;(貌似需要符合隐式转换规则)</li>
<li>替换列：alter table dept_partition replace columns(deptno string, dname string, loc string);</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>删除表：drop table dept_partition;</p>
</li>
</ul>
<h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><ul>
<li><p>数据导入</p>
<ul>
<li><p>向表中装载数据(Load)</p>
<ul>
<li>语法</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data [local] inpath 'path_name' [overwrite] into table table_name [partition (partcol1=val1,...)];</span><br></pre></td></tr></table></figure>

<ul>
<li><p>参数解释</p>
<ul>
<li>load data：表示加载数据</li>
<li>local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li>
<li>inpath：表示加载数据的路径</li>
<li>overwrite：表示覆盖表中已有数据，否则表示追加</li>
<li>into table：表示加载到哪张表</li>
<li>table_name：表示具体的表</li>
<li>partition：表示上传到指定分区</li>
</ul>
</li>
<li><p>实际操作</p>
<ul>
<li>加载本地文件到Hive：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/student.txt' into table default.student;</span><br></pre></td></tr></table></figure>

<ul>
<li>加载(覆盖)HDFS文件数据到Hive中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive Cli命令环境下</span></span><br><span class="line">dfs -put /opt/module/data/hive/student.txt /user/sobxiong/hive;</span><br><span class="line">load data inpath '/user/sobxiong/hive/student.txt' (overwrite)into table default.student;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>通过查询语句向表中插入数据(Insert)</p>
<ul>
<li>基本插入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span>  student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'wangwu'</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>根据单表查询结果插入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201708'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>根据多表查询结果插入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询语句中创建表并加载数据(As Select,查询的结果会添加到新创建的表中)：create table if not exists student3 as select id, name from student;</p>
</li>
<li><p>创建表时通过Location指定加载数据路径</p>
<ul>
<li>指定在HDFS上的位置创建表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student5(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/student5'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>上传数据到HDFS上</li>
<li>查询数据</li>
</ul>
</li>
<li><p>Import数据到指定Hive表中(注意：先用export导出后,才能导入)</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import table student2 partition(month='201709') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据导出</p>
<ul>
<li><p>Insert导出</p>
<ul>
<li>将查询的结果导出到本地</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/export/student'</span> <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>

<ul>
<li>将查询的结果格式化导出到本地</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/export/student1'</span> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> <span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>

<ul>
<li>将查询结果导出到HDFS上(取消local)</li>
</ul>
</li>
<li><p>Hadoop命令导出到本地(Hive Cli命令环境下)：dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/data/hive/export/student3.txt;</p>
</li>
<li><p>Hive Shell命令导出：基本语法(hive -f/-e 执行语句或脚本 &gt; file_name)</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -e 'select * from default.student;' &gt; /opt/module/data/hive/export/student4.txt;</span><br></pre></td></tr></table></figure>

<ul>
<li>Export导出到HDFS上(导出数据包括表数据和元数据)</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export table default.student to '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure>

<ul>
<li>Sqoop导出(<strong>敬请期待</strong>)</li>
</ul>
</li>
<li><p>清除表中数据(注意：只能删除管理表,不能删除外部表中数据)：truncate table student;</p>
</li>
</ul>
<h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><ul>
<li>查询语句语法<br>官方wiki文档：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select</a></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*]</span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">    | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li><p>基本查询</p>
<ul>
<li><p>全表和特定列查询<br>注意：</p>
<ul>
<li><strong>SQL大小写不敏感</strong></li>
<li>SQL可以写在一行/多行</li>
<li>关键字不能被缩写也不能分行</li>
<li>各子句一般要分行写</li>
<li>使用缩进提高语句的可读性</li>
</ul>
</li>
<li><p>列别名</p>
<ul>
<li>重命名一个列</li>
<li>便于计算</li>
<li>紧跟列名(或在列名和别名之间加入关键字AS)</li>
</ul>
</li>
<li><p>算术运算符</p>
</li>
</ul>
<p>+、-、*、/、%、&amp;、|、^、-</p>
<ul>
<li><p>常用函数</p>
<ul>
<li>求总行数：count()</li>
<li>求最大值/最小值：max()/min()</li>
<li>求总和：sum()</li>
<li>求平均值：avg()</li>
</ul>
</li>
<li><p>limit语句：典型的查询会返回多行数据。LIMIT子句用于限制返回的行数</p>
</li>
</ul>
</li>
<li><p>Where语句：使用Where子句可以过滤掉不满足条件的行，需要紧跟From子句</p>
<ul>
<li>比较运算符(同样可以用于Join… on和Having语句)<br>以下只介绍除=、&gt;和&lt;等简单的运算符</li>
</ul>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A&lt;=&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL</td>
</tr>
<tr>
<td>A&lt;&gt;B, A!=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果</td>
</tr>
<tr>
<td>A IS [NOT] NULL</td>
<td>所有数据类型</td>
<td>如果A(不)等于NULL，则返回TRUE(FALSE)，反之返回FALSE(TRUE)</td>
</tr>
<tr>
<td>[NOT] IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>(不)使用IN运算显示列表中的值</td>
</tr>
<tr>
<td>A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果</td>
</tr>
<tr>
<td>A RLIKE B, A REGEXP B</td>
<td>STRING 类型</td>
<td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配</td>
</tr>
</tbody></table>
<ul>
<li><p>Like和RLike</p>
<ul>
<li><p>使用LIKE运算选择类似的值</p>
</li>
<li><p>选择条件可以包含字符或数字：</p>
<ul>
<li>% 代表零个或多个字符(任意个字符)</li>
<li>_ 代表一个字符</li>
</ul>
</li>
<li><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件</p>
</li>
<li><p>案例</p>
<ul>
<li>查找以2开头薪水的员工信息</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">'2%'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>查找第二个数值为2的薪水的员工信息</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">LIKE</span> <span class="string">'_2%'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>查找薪水中含有2的员工信息</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> <span class="keyword">String</span>(sal) <span class="keyword">RLIKE</span> <span class="string">'[2]'</span>;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>逻辑运算符(And/Or/Not)</p>
</li>
</ul>
</li>
<li><p>分组</p>
<ul>
<li>Group By语句<br>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作</li>
<li>Having语句<br>与where不同点：<ul>
<li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据</li>
<li>where后面不能写分组函数，而having后面可以使用分组函数</li>
<li>having只用于group by分组统计语句</li>
</ul>
</li>
</ul>
</li>
<li><p>Join语句</p>
<ul>
<li>等值Join</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno, d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d</span><br><span class="line"><span class="keyword">on</span> e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<ul>
<li>表的别名<br>好处：(1)简化查询；(2)有限地提高执行效率</li>
<li>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure>

<ul>
<li>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure>

<ul>
<li>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure>

<ul>
<li>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno;</span><br></pre></td></tr></table></figure>

<ul>
<li>多表连接：连接n个表，一般至少需要n-1个连接条件</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.ename, d.deptno, l. loc_name</span><br><span class="line"><span class="keyword">FROM</span>   emp e</span><br><span class="line"><span class="keyword">JOIN</span>   dept d</span><br><span class="line"><span class="keyword">ON</span>     d.deptno = e.deptno</span><br><span class="line"><span class="keyword">JOIN</span>   location l</span><br><span class="line"><span class="keyword">ON</span>     d.loc = l.loc;</span><br></pre></td></tr></table></figure>

<p>多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。<br><strong>注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的</strong></p>
<ul>
<li><p>笛卡尔积<br>一般会在下面情况下出现：</p>
<ul>
<li>省略连接条件</li>
<li>连接条件无效</li>
<li>所有表中的所有行互相连接<br>一般会设置禁止出现笛卡尔积，如果有特殊情况，需要在单独在命令执行前设置一次性环境</li>
</ul>
</li>
<li><p>连接谓词在新版中支持or</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 无实际意义(ename = dname),只做可行性试验</span></span><br><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno</span><br><span class="line">= d.deptno <span class="keyword">or</span> e.ename=d.dname;</span><br></pre></td></tr></table></figure>
</li>
<li><p>排序</p>
<ul>
<li><p>全局排序(Order By:<strong>一个Reducer</strong>)</p>
<ul>
<li>排序方式：ASC(ascend升序,默认)、DESC(descend降序)</li>
<li>Order By子句在Select语句的结尾</li>
</ul>
</li>
<li><p>多个列排序(同MySQL)</p>
</li>
<li><p>内部排序(Sort By:<strong>每个Reduce内部进行排序,对全局结果集来说不是排序</strong>)</p>
<ul>
<li>注意：如果使用sort by不使用distribute by(即没有指定分区字段)，那么就采用一种产生随机数的函数分配分区(避免数据倾斜)</li>
<li>设置reduce数：set mapreduce.job.reduce=3;</li>
<li>按照部门编号降序排序：</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 三个结果文件</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/sortby-result'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>分区排序(Distribute By:<strong>类似MR中partition,进行分区,结合sort by使用</strong>)</p>
</li>
</ul>
<p><strong>DISTRIBUTE BY语句要写在SORT BY语句之前</strong>。distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果</p>
<ul>
<li><p>设置reduce数：set mapreduce.job.reduces=3;</p>
</li>
<li><p>先按照部门编号分区，再按照员工编号降序排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/data/hive/distribute-result'</span> <span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<ul>
<li><p>Clubster By</p>
<ul>
<li>当distribute by和sorts by字段相同时，可以使用cluster by方式。</li>
<li>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC</li>
<li>具体实操</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 以下两种写法等价</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>

<ul>
<li>注意：按照部门编号分区，不一定就是固定死的数值(要看具体数据表中的字段不同的数目以及reduce设置的数目)，可以是20号和30号部门分到一个分区里面去</li>
</ul>
</li>
</ul>
</li>
<li><p>分桶及抽样查询</p>
<ul>
<li><p>分桶表数据存储</p>
<ul>
<li><p>介绍：<strong>分区针对的是数据的存储路径；分桶针对的是数据文件</strong>。分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。分桶是将数据集分解成更容易管理的若干部分的另一个技术</p>
</li>
<li><p>通过导入数据文件方式创建分桶表</p>
<ul>
<li>创建表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>查看表结构：desc formatted stu_back;(Num Buckets: 4)</li>
<li>导入数据到分桶表</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/student.txt' into table stu_buck;</span><br></pre></td></tr></table></figure>

<ul>
<li>在浏览器上查看创建的分桶表是否分成4个桶(4个文件)：在新版本中分成四个桶</li>
</ul>
</li>
<li><p>通过子查询导入数据方式创建分桶表</p>
<ul>
<li>先创建普通的stu表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>向普通stu表中导入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure>

<ul>
<li>清空stu_buck表中数据：truncate table stu_buck;</li>
<li>子查询方式导入数据到分桶表：insert into table stu_buck select id, name from stu;</li>
<li>浏览器查看：新版本中有4个分桶(文件)</li>
<li>需要设置Hive的属性(老版本)</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 设置启用分桶</span></span><br><span class="line">set hive.enforce.bucketing=true;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置reduce数目为-1,会自动使用分桶数作为reduce的数目</span></span><br><span class="line">set mapreduce.job.reduces=-1;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 清空分桶表,重新导入数据,再去查看浏览器中的分桶</span></span><br><span class="line">truncate table stu_back;</span><br><span class="line">insert into table stu_buck select id, name from stu;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>分桶抽样查询</p>
<ul>
<li>介绍：对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求</li>
<li>实操：select * from stu_buck tablesample(bucket 1 out of 4 on id);</li>
<li>语法：tablesample是抽样语句，语法：tablesample(bucket x out of y)</li>
<li>参数解释<ul>
<li>y：y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2)2个bucket的数据，当y=8时，抽取(4/8)1/2个bucket的数据</li>
<li>x：<strong>x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y</strong>。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取(4/2)2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据</li>
<li><strong>注意：x的值必须小于等于y的值</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>其他常用查询函数</p>
<ul>
<li><p>空字段赋值</p>
<ul>
<li>函数说明：NVL：给值为NULL的数据赋值，它的格式是NVL(string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL，则返回NULL(replace_with可以是常量也可以是同表的另一个列)</li>
<li>查询(常量)：select nvl(comm,-1) from emp;</li>
<li>查询(另一列)：select nvl(comm,mgr) from emp;</li>
</ul>
</li>
<li><p>时间类</p>
<ul>
<li>date_format(格式化时间,第一个变量时间串只能是以’-‘分割)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-07-11'</span>,<span class="string">'yyyy:MM:dd'</span>);</span><br><span class="line"><span class="comment">-- 时间不以'-'分割,可以通过regex正则表达式替换/自定义函数</span></span><br><span class="line"><span class="keyword">select</span> regexp_replace(<span class="string">'2020/07/11'</span>,<span class="string">'/'</span>,<span class="string">'-'</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>date_add/sub(时间跟天数相加/相减)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">'2020-07-11'</span>,<span class="number">-5</span>/<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>datediff(时间相差的间隔,前者-后者)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">'2020-07-11'</span>,<span class="string">'2020-07-08'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>CASE WHEN</p>
<ul>
<li>数据准备</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">悟空  A 男</span><br><span class="line">大海  A 男</span><br><span class="line">宋宋  B 男</span><br><span class="line">凤姐  A 女</span><br><span class="line">婷姐  B 女</span><br><span class="line">婷婷  B 女</span><br></pre></td></tr></table></figure>

<ul>
<li>需求：求出不同部门的男女各多少人</li>
<li>创建emp_set.txt，复制数据</li>
<li>创建Hive表并导入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">dept_id <span class="keyword">string</span>,</span><br><span class="line">sex <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入本地数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/emp_sex.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  dept_id,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">'男'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">'女'</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  emp_sex</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  dept_id;</span><br></pre></td></tr></table></figure>
</li>
<li><p>行转列</p>
<ul>
<li>相关函数说明<br>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串<br>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间<br>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段</li>
<li>数据准备</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">孙悟空  白羊座  A</span><br><span class="line">大海  射手座  A</span><br><span class="line">宋宋  白羊座  B</span><br><span class="line">猪八戒  白羊座  A</span><br><span class="line">凤姐  射手座  A</span><br></pre></td></tr></table></figure>

<ul>
<li>需求：把星座和血型一样的人归类到一起。结果如下</li>
<li>person_info.txt文件，复制数据</li>
<li>创建Hive表并导入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">constellation <span class="keyword">string</span>,</span><br><span class="line">blood_type <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/person_info.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 总查询语句</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  t1.constellation_blood_type,</span><br><span class="line">  <span class="keyword">concat_ws</span>(<span class="string">'|'</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (<span class="keyword">select</span></span><br><span class="line">      <span class="keyword">name</span>,</span><br><span class="line">      <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) constellation_blood_type</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      person_info</span><br><span class="line">  ) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  t1.constellation_blood_type;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 第一步,查询出'射手座,A' '大海'</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) constellation_blood_type,</span><br><span class="line">  <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span> person_info;</span><br><span class="line"><span class="comment">-- 第二步,连接相同星座和血型的name</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  constellation_blood_type,</span><br><span class="line">  <span class="keyword">concat_ws</span>(<span class="string">'|'</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span> t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  t1.constellation_blood_type;</span><br><span class="line"><span class="comment">-- 最后一步,替换from后的t1为第一步中的临时表</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>列转行</p>
<ul>
<li>函数说明<br>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行<br>LateRal View：<ul>
<li>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li>
<li>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</li>
</ul>
</li>
<li>数据准备</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》  悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》 悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》 战争,动作,灾难</span><br></pre></td></tr></table></figure>

<ul>
<li>需求：将电影分类中的数组数据展开，结果如下所示</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》      悬疑</span><br><span class="line">《疑犯追踪》      动作</span><br><span class="line">《疑犯追踪》      科幻</span><br><span class="line">《疑犯追踪》      剧情</span><br><span class="line">《Lie to me》   悬疑</span><br><span class="line">《Lie to me》   警匪</span><br><span class="line">《Lie to me》   动作</span><br><span class="line">《Lie to me》   心理</span><br><span class="line">《Lie to me》   剧情</span><br><span class="line">《战狼2》        战争</span><br><span class="line">《战狼2》        动作</span><br><span class="line">《战狼2》        灾难</span><br></pre></td></tr></table></figure>

<ul>
<li>创建本地movie.txt文件，复制数据</li>
<li>创建Hive表并导入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">movie <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/opt/module/data/hive/movie.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure>

<ul>
<li>按需查询数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  movie,</span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure>
</li>
<li><p>窗口函数</p>
<ul>
<li>函数说明<br>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化<ul>
<li>参数说明<ul>
<li>Over()内<ul>
<li>CURRENT ROW：当前行</li>
<li>n PRECEDING：往前n行数据</li>
<li>n FOLLOWING：往后n行数据</li>
<li>UNBOUNDED：起点，UNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点</li>
</ul>
</li>
<li>Over()外<ul>
<li>LAG(col,n)：往前<strong>第</strong>n行数据</li>
<li>LEAD(col,n)：往后<strong>第</strong>n行数据</li>
<li>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>数据准备</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; name,orderdate,cost</span><br><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure>

<ul>
<li>需求<ul>
<li>查询在2017年4月份购买过的顾客及总人数</li>
<li>查询顾客的购买明细及月购买总额</li>
<li>上述的场景，要将cost按照日期进行累加</li>
<li>查询顾客上次的购买时间</li>
<li>查询前20%时间的订单信息</li>
</ul>
</li>
<li>创建本地business.txt文件，复制数据</li>
<li>创建Hive表并导入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">orderdate <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">cost</span> <span class="built_in">int</span></span><br><span class="line">) <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">"/opt/module/data/hive/business.txt"</span> <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>按需查询数据</p>
<ul>
<li>查询在2017年4月份购买过的顾客及总人数</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">count</span>(*) <span class="keyword">over</span>()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) = <span class="string">'2017-04'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询顾客的购买明细及月购买总额</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,<span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate)) <span class="keyword">from</span></span><br><span class="line">business;</span><br></pre></td></tr></table></figure>

<ul>
<li>上述的场景,要将cost按照日期进行累加</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- partition by ... order by和distribute by ... sort by效果相同,可替换</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,</span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行</span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行</span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<ul>
<li>查看顾客上次的购买时间</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,lag(orderdate,<span class="number">1</span>,<span class="string">'1900-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> last_time <span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询前20%时间的订单信息</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) ntile_id</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) b</span><br><span class="line"><span class="keyword">where</span> ntile_id = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Rank排名函数</p>
<ul>
<li>函数说明：<ul>
<li>Rank()：排序相同时会重复，总数不会变</li>
<li>DENSE_RANK()：排序相同时会重复，总数会减少</li>
<li>ROW_NUMBER()：会根据顺序计算</li>
</ul>
</li>
<li>数据准备</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; name subject score</span><br><span class="line">孙悟空  语文  87</span><br><span class="line">孙悟空  数学  95</span><br><span class="line">孙悟空  英语  68</span><br><span class="line">大海  语文  94</span><br><span class="line">大海  数学  56</span><br><span class="line">大海  英语  84</span><br><span class="line">宋宋  语文  64</span><br><span class="line">宋宋  数学  86</span><br><span class="line">宋宋  英语  84</span><br><span class="line">婷婷  语文  65</span><br><span class="line">婷婷  数学  85</span><br><span class="line">婷婷  英语  78</span><br></pre></td></tr></table></figure>

<ul>
<li>需求：计算各学科成绩排名</li>
<li>创建本地score.txt文件，复制数据</li>
<li>创建Hive表并导入数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">subject <span class="keyword">string</span>,</span><br><span class="line">score <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span>;</span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/data/hive/score.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure>

<ul>
<li>按需查询</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">name</span>,</span><br><span class="line">  subject,</span><br><span class="line">  score,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">rank</span>,</span><br><span class="line">  <span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">dense_rank</span>,</span><br><span class="line">  row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) row_number</span><br><span class="line"><span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="例题实战-蚂蚁金服"><a href="#例题实战-蚂蚁金服" class="headerlink" title="例题实战(蚂蚁金服)"></a>例题实战(蚂蚁金服)</h2><ul>
<li>背景说明<br>用户每天的蚂蚁森林低碳生活领取的记录流水表(user_low_carbon表)</li>
</ul>
<table>
<thead>
<tr>
<th>字段名</th>
<th>注释</th>
</tr>
</thead>
<tbody><tr>
<td>user_id</td>
<td>用户编号</td>
</tr>
<tr>
<td>data_dt</td>
<td>日期</td>
</tr>
<tr>
<td>low_carbon</td>
<td>减少碳排放(g:克)</td>
</tr>
</tbody></table>
<p>用于记录申领环保植物所需要减少的碳排放量的蚂蚁森林植物换购表(plant_carbon)</p>
<table>
<thead>
<tr>
<th>字段名</th>
<th>注释</th>
</tr>
</thead>
<tbody><tr>
<td>plant_id</td>
<td>植物编号</td>
</tr>
<tr>
<td>plant_name</td>
<td>植物名</td>
</tr>
<tr>
<td>low_carbon</td>
<td>换购植物所需要的碳</td>
</tr>
</tbody></table>
<ul>
<li><p>题目</p>
<ul>
<li>蚂蚁森林蚂蚁森林植物申领统计<br>问题：假设2017年1月1日开始记录低碳数据(user_low_carbon)，假设2017年10月1日之前满足申领条件的用户都申领了一颗p004-胡杨，剩余的能量全部用来领取p002-沙柳<br>统计在10月1日累计申领p002-沙柳排名前10的用户信息、以及他比后一名多领了几颗沙柳<br>得到的统计结果如下表样式：</li>
</ul>
<table>
<thead>
<tr>
<th>user_id</th>
<th>plant_count</th>
<th>less_cout(比后一名多领的棵树)</th>
</tr>
</thead>
<tbody><tr>
<td>u_101</td>
<td>1000</td>
<td>100</td>
</tr>
<tr>
<td>u_088</td>
<td>900</td>
<td>400</td>
</tr>
<tr>
<td>u_103</td>
<td>500</td>
<td>…</td>
</tr>
</tbody></table>
<ul>
<li>蚂蚁森林低碳用户排名分析<br>问题：查询user_low_carbon表中每日流水记录，条件为：用户在2017年，连续三天(或以上)的天数里，每天减少碳排放(low_carbon)都超过100g的用户低碳流水。需要查询返回满足以上条件的user_low_carbon表中的记录流水。<br>例如用户u_002符合条件的记录如下，因为2017/1/2~2017/1/5连续四天的碳排放量之和都大于等于100g：</li>
</ul>
<table>
<thead>
<tr>
<th>seq(序号,不涉及当前列)</th>
<th>user_id</th>
<th>data_dt</th>
<th>low_carbon</th>
</tr>
</thead>
<tbody><tr>
<td>xxxxx10</td>
<td>u_002</td>
<td>2017/1/2</td>
<td>150</td>
</tr>
<tr>
<td>xxxxx11</td>
<td>u_002</td>
<td>2017/1/2</td>
<td>70</td>
</tr>
<tr>
<td>xxxxx12</td>
<td>u_002</td>
<td>2017/1/3</td>
<td>30</td>
</tr>
<tr>
<td>xxxxx13</td>
<td>u_002</td>
<td>2017/1/3</td>
<td>80</td>
</tr>
<tr>
<td>xxxxx14</td>
<td>u_002</td>
<td>2017/1/4</td>
<td>150</td>
</tr>
<tr>
<td>xxxxx14</td>
<td>u_002</td>
<td>2017/1/5</td>
<td>101</td>
</tr>
</tbody></table>
<p>备注：统计方法不限于sql、procedure、python,java等</p>
</li>
<li><p>解决</p>
<ul>
<li><p>前期准备</p>
<ul>
<li>创建表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_low_carbon(user_id <span class="keyword">String</span>,data_dt <span class="keyword">String</span>,low_carbon <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> plant_carbon(plant_id <span class="keyword">string</span>,plant_name <span class="keyword">String</span>,low_carbon <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>加载数据</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath "/opt/module/data/hive/user_low_carbon.txt" into table user_low_carbon;</span><br><span class="line">load data local inpath "/opt/module/data/hive/plant_carbon.txt" into table plant_carbon;</span><br></pre></td></tr></table></figure>

<ul>
<li>设置本地模式(加快运行速度)：set hive.exec.mode.local.auto=true;</li>
</ul>
</li>
<li><p>求解问题一</p>
<ul>
<li>统计在10月1日前每个用户减少碳排放量的总和(取前11名：为了求与后一名的差值,并在第一阶段过滤数据集,加快运行速度)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t1表</span></span><br><span class="line"><span class="keyword">select</span> user_id,<span class="keyword">sum</span>(low_carbon) sum_carbon</span><br><span class="line"><span class="keyword">from</span> user_low_carbon</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">datediff</span>(regexp_replace(data_dt,<span class="string">"/"</span>,<span class="string">"-"</span>),<span class="string">"2017-10-1"</span>)&lt;<span class="number">0</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> sum_carbon <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">11</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>取出申领胡杨的碳量</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t2表</span></span><br><span class="line"><span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p004"</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>取出申领沙柳的碳量</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t3表</span></span><br><span class="line"><span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p002"</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>求出能申领沙柳的棵树</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t4表(floor下取整)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">floor</span>((t1.sum_carbon-t2.low_carbon) / t3.low_carbon) treeCount</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1,t2,t3;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t1,t2,t3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">floor</span>((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCount</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span> user_id,<span class="keyword">sum</span>(low_carbon) sum_carbon</span><br><span class="line">    <span class="keyword">from</span> user_low_carbon</span><br><span class="line">    <span class="keyword">where</span> <span class="keyword">datediff</span>(regexp_replace(data_dt,<span class="string">"/"</span>,<span class="string">"-"</span>),<span class="string">"2017-10-1"</span>)&lt;<span class="number">0</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span> sum_carbon <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">11</span></span><br><span class="line">  )t1,</span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p004"</span></span><br><span class="line">  )t2,</span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p002"</span></span><br><span class="line">  )t3</span><br></pre></td></tr></table></figure>

<ul>
<li>求出前一名比后一名多几棵</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  treeCount,</span><br><span class="line">  treeCount - (<span class="keyword">lead</span>(treeCount,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> treeCount <span class="keyword">desc</span>))</span><br><span class="line"><span class="keyword">from</span> t4</span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t4表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  treeCount,</span><br><span class="line">  treeCount-(<span class="keyword">lead</span>(treeCount,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> treeCount <span class="keyword">desc</span>)) less_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      <span class="keyword">floor</span>((t1.sum_carbon-t2.low_carbon)/t3.low_carbon) treeCount</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span> user_id,<span class="keyword">sum</span>(low_carbon) sum_carbon</span><br><span class="line">        <span class="keyword">from</span> user_low_carbon</span><br><span class="line">        <span class="keyword">where</span> <span class="keyword">datediff</span>(regexp_replace(data_dt,<span class="string">"/"</span>,<span class="string">"-"</span>),<span class="string">"2017-10-1"</span>)&lt;<span class="number">0</span></span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line">        <span class="keyword">order</span> <span class="keyword">by</span> sum_carbon <span class="keyword">desc</span></span><br><span class="line">        <span class="keyword">limit</span> <span class="number">11</span></span><br><span class="line">      )t1,</span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p004"</span></span><br><span class="line">      )t2,</span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span> low_carbon <span class="keyword">from</span> plant_carbon <span class="keyword">where</span> plant_id=<span class="string">"p002"</span></span><br><span class="line">      )t3</span><br><span class="line">  )t4</span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>求解问题二</p>
<ul>
<li><p>方式一(Hive Sql简单版)</p>
<ul>
<li>过滤出2017年且单日低碳量超过100g</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t1表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  user_low_carbon</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_dt</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>将前两行数据以及后两行数据的日期放至当前行</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t2表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">  lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替t1</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">  lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">  <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      user_low_carbon</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">      <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">      user_id,data_dt</span><br><span class="line">    <span class="keyword">having</span></span><br><span class="line">      <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">  )t1;</span><br></pre></td></tr></table></figure>

<ul>
<li>计算当前日期跟前后两行时间的差值</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t3表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t2</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">  <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">      lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">      <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">      <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          user_low_carbon</span><br><span class="line">        <span class="keyword">where</span></span><br><span class="line">          <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">          user_id,data_dt</span><br><span class="line">        <span class="keyword">having</span></span><br><span class="line">          <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">      )t1</span><br><span class="line">  )t2;</span><br></pre></td></tr></table></figure>

<ul>
<li>过滤出连续3天超过100g的用户</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t4表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t3</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  (lag2_diff = <span class="number">2</span> <span class="keyword">and</span> lag1_diff = <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lag1_diff = <span class="number">1</span> <span class="keyword">and</span> lead1_diff = <span class="number">-1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lead1_diff = <span class="number">-1</span> <span class="keyword">and</span> lead2_diff = <span class="number">-2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">      <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          data_dt,</span><br><span class="line">          lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">          lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">          <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">          <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              user_id,</span><br><span class="line">              <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              user_low_carbon</span><br><span class="line">            <span class="keyword">where</span></span><br><span class="line">              <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">              user_id,data_dt</span><br><span class="line">            <span class="keyword">having</span></span><br><span class="line">              <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">          )t1</span><br><span class="line">      )t2</span><br><span class="line">  )t3</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  (lag2_diff = <span class="number">2</span> <span class="keyword">and</span> lag1_diff = <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lag1_diff = <span class="number">1</span> <span class="keyword">and</span> lead1_diff = <span class="number">-1</span>)</span><br><span class="line">  <span class="keyword">or</span></span><br><span class="line">  (lead1_diff = <span class="number">-1</span> <span class="keyword">and</span> lead2_diff = <span class="number">-2</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>关联原表，获取流水信息</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  ulc.user_id,</span><br><span class="line">  ulc.data_dt,</span><br><span class="line">  ulc.low_carbon</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t4</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  user_low_carbon ulc</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t4.user_id = ulc.user_id</span><br><span class="line">  <span class="keyword">and</span></span><br><span class="line">  t4.data_dt = <span class="keyword">date_format</span>(regexp_replace(ulc.data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t4</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  ulc.user_id,</span><br><span class="line">  ulc.data_dt,</span><br><span class="line">  ulc.low_carbon</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          data_dt,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lag2) lag2_diff,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lag1) lag1_diff,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lead1) lead1_diff,</span><br><span class="line">          <span class="keyword">datediff</span>(data_dt,lead2) lead2_diff</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              user_id,</span><br><span class="line">              data_dt,</span><br><span class="line">              lag(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag2,</span><br><span class="line">              lag(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lag1,</span><br><span class="line">              <span class="keyword">lead</span>(data_dt,<span class="number">1</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead1,</span><br><span class="line">              <span class="keyword">lead</span>(data_dt,<span class="number">2</span>,<span class="string">'1970-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) lead2</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              (</span><br><span class="line">                <span class="keyword">select</span></span><br><span class="line">                  user_id,</span><br><span class="line">                  <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">                <span class="keyword">from</span></span><br><span class="line">                  user_low_carbon</span><br><span class="line">                <span class="keyword">where</span></span><br><span class="line">                  <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">                <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">                  user_id,data_dt</span><br><span class="line">                <span class="keyword">having</span></span><br><span class="line">                  <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">              )t1</span><br><span class="line">          )t2</span><br><span class="line">      )t3</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">      (lag2_diff = <span class="number">2</span> <span class="keyword">and</span> lag1_diff = <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">or</span></span><br><span class="line">      (lag1_diff = <span class="number">1</span> <span class="keyword">and</span> lead1_diff = <span class="number">-1</span>)</span><br><span class="line">      <span class="keyword">or</span></span><br><span class="line">      (lead1_diff = <span class="number">-1</span> <span class="keyword">and</span> lead2_diff = <span class="number">-2</span>)</span><br><span class="line">  )t4</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  user_low_carbon ulc</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t4.user_id = ulc.user_id</span><br><span class="line">  <span class="keyword">and</span></span><br><span class="line">  t4.data_dt = <span class="keyword">date_format</span>(regexp_replace(ulc.data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>方式二(Hive Sql困难版,使用等差数列)</p>
<ul>
<li>过滤出2017年且单日低碳量超过100g(同方式一第一步)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t1表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  user_low_carbon</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_dt</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>按照日期进行排序,并给每一条数据一个标记</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- t2表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t1表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      user_low_carbon</span><br><span class="line">    <span class="keyword">where</span></span><br><span class="line">      <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">      user_id,data_dt</span><br><span class="line">    <span class="keyword">having</span></span><br><span class="line">      <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">  )t1;</span><br></pre></td></tr></table></figure>

<ul>
<li>将日期减去当前的rank值</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  如果是连续的话,data_dt - rk结果一致</span></span><br><span class="line"><span class="comment">  user_id , data_dt , rk ,data_sub_rk</span></span><br><span class="line"><span class="comment">  u_001 , 2017-01-02 , 1 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_001 , 2017-01-06 , 2 , 2017-01-04</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-02 , 1 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-03 , 2 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-04 , 3 , 2017-01-01</span></span><br><span class="line"><span class="comment">  u_002 , 2017-01-05 , 4 , 2017-01-01</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">-- t3表</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">date_sub</span>(data_dt,rk) data_sub_rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t2</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id,</span><br><span class="line">  data_dt,</span><br><span class="line">  <span class="keyword">date_sub</span>(data_dt,rk) data_sub_rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          user_low_carbon</span><br><span class="line">        <span class="keyword">where</span></span><br><span class="line">          <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">          user_id,data_dt</span><br><span class="line">        <span class="keyword">having</span></span><br><span class="line">          <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">      )t1</span><br><span class="line">  )t2;</span><br></pre></td></tr></table></figure>

<ul>
<li>过滤出连续3天超过100g的用户</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 当前适用于连续n天,只需要改3为n,具有通式性</span></span><br><span class="line"><span class="comment">-- 当前只能过滤出用户,流水不行</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t3</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_sub_rk</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">count</span>(*) &gt;= <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换t3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  user_id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      data_dt,</span><br><span class="line">      <span class="keyword">date_sub</span>(data_dt,rk) data_sub_rk</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          user_id,</span><br><span class="line">          data_dt,</span><br><span class="line">          <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> data_dt) rk</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              user_id,</span><br><span class="line">              <span class="keyword">date_format</span>(regexp_replace(data_dt,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM-dd'</span>) data_dt</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              user_low_carbon</span><br><span class="line">            <span class="keyword">where</span></span><br><span class="line">              <span class="keyword">substring</span>(data_dt,<span class="number">1</span>,<span class="number">4</span>) = <span class="string">'2017'</span></span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">              user_id,data_dt</span><br><span class="line">            <span class="keyword">having</span></span><br><span class="line">              <span class="keyword">sum</span>(low_carbon) &gt;= <span class="number">100</span></span><br><span class="line">          )t1</span><br><span class="line">      )t2</span><br><span class="line">  )t3</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  user_id,data_sub_rk</span><br><span class="line"><span class="keyword">having</span></span><br><span class="line">  <span class="keyword">count</span>(*) &gt;= <span class="number">3</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>方式三(MapReduce)</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">mapper(key:user_id + date,value:一行)</span><br><span class="line">grouping:user_id</span><br><span class="line">reduce()</span><br><span class="line"></span><br><span class="line">values:</span><br><span class="line">&#123;</span><br><span class="line">    date = <span class="number">1970</span>-<span class="number">01</span>-<span class="number">01</span></span><br><span class="line">    list = <span class="keyword">new</span> ArrayList();</span><br><span class="line">    values.<span class="keyword">for</span>(</span><br><span class="line">        <span class="comment">// 首次</span></span><br><span class="line">        <span class="keyword">if</span>(date == <span class="number">1970</span>-<span class="number">01</span>-<span class="number">01</span>)&#123;</span><br><span class="line">            list.add(value);</span><br><span class="line">            date = value.date;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123; <span class="comment">// 不是首次</span></span><br><span class="line">            <span class="keyword">if</span>(value.date - date == <span class="number">1</span>)&#123;</span><br><span class="line">                list.add(value);</span><br><span class="line">                date = value.date;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(list.size() &gt;= <span class="number">3</span>)&#123;</span><br><span class="line">                    context.write(list);</span><br><span class="line">                &#125;</span><br><span class="line">                list.clear();</span><br><span class="line">                list.add(value);</span><br><span class="line">                date = value.date;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 防止漏了最后一行</span></span><br><span class="line">    <span class="keyword">if</span>(list.size() &gt;= <span class="number">3</span>)&#123;</span><br><span class="line">        context.write(list);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ul>
<li><p>系统内置函数</p>
<ul>
<li>查看系统自带的函数：show functions;</li>
<li>显示自带的函数的用法：desc function split;</li>
<li>详细显示自带的函数的用法：desc function extened split;</li>
</ul>
</li>
<li><p>自定义函数</p>
<ul>
<li>Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展</li>
<li>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF：user-defined function)</li>
<li>根据用户自定义函数类别分为以下三种：<ul>
<li><strong>UDF(User-Defined-Function)</strong>：一进一出</li>
<li>UDAF(User-Defined Aggregation Function)：聚集函数，多进一出；类似于：count/max/min</li>
<li>UDTF(User-Defined Table-Generating Functions)：一进多出，如lateral view explore()</li>
</ul>
</li>
<li>官方文档地址：<a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></li>
<li>编程步骤：<ul>
<li>继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF(org.apache.hadoop.hive.ql.UDF已被废弃)</li>
<li>重写三个方法</li>
<li>在Hive的命令行窗口创建函数<ul>
<li>添加jar资源：add jar ‘jar_path’;</li>
<li>创建function：create [temporary] function [dbname.]function_name AS class_name;(temporary只在当前次使用Hive Cli有效,退出重进无效;dbname.标识限定使用函数的数据库,不写默认为default数据库)</li>
</ul>
</li>
<li>在Hive的命令行窗口删除函数：Drop [temporary] function [if exists] [dbname.]function_name;</li>
</ul>
</li>
</ul>
</li>
<li><p>自定义UDF/UDTF函数</p>
<ul>
<li>在IDE中创建一个Maven工程</li>
<li>导入依赖</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>创建自定义UDF函数</p>
<ul>
<li>创建类继承GenericUDF</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UDF被废弃</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 输入类型int</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">transient</span> IntObjectInspector arg0;</span><br><span class="line">  <span class="comment">// 返回值类型int</span></span><br><span class="line">  <span class="keyword">private</span> IntWritable res;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个方法只调用一次,并且在evaluate()方法之前调用</span></span><br><span class="line">  <span class="comment">// 该方法接受的参数是一个ObjectInspectors数组</span></span><br><span class="line">  <span class="comment">// 该方法检查接受正确的参数类型和参数个数</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    <span class="comment">// 输入类型</span></span><br><span class="line">    <span class="keyword">this</span>.arg0 = (IntObjectInspector) arguments[<span class="number">0</span>];</span><br><span class="line">    <span class="comment">// 返回值类型</span></span><br><span class="line">    <span class="keyword">this</span>.res = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    <span class="comment">// 确定返回值类型</span></span><br><span class="line">    <span class="keyword">return</span> PrimitiveObjectInspectorFactory.writableIntObjectInspector;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个方法类似UDF的evaluate()方法。它处理真实的参数，并返回最终结果</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    Object arg0 = arguments[<span class="number">0</span>].get();</span><br><span class="line">    <span class="keyword">int</span> inputNum = <span class="keyword">this</span>.arg0.get(arg0);</span><br><span class="line">    res.set(inputNum + <span class="number">5</span>);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个方法用于当实现的GenericUDF出错的时候，打印出提示信息。而提示信息就是你实现该方法最后返回的字符串</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">assert</span> (children.length == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"param: "</span> + children[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>打成Jar包上传到服务器</li>
<li>将jar包添加到Hive的classpath：add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar;</li>
<li>创建临时函数与开发好的java class关联</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> className需要使用全类名</span></span><br><span class="line">create temporary function addFive as 'com.xiong.hive.MyUDF';</span><br></pre></td></tr></table></figure>

<ul>
<li>在Hql中使用自定义的函数：select addFive(id) from cc;</li>
</ul>
</li>
<li><p>创建自定义UDTF函数</p>
<ul>
<li>创建类继承GenericUDTF</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> List&lt;String&gt; dataList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义输出数据的列名和数据类型</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    <span class="comment">// 定义输出数据的列名</span></span><br><span class="line">    List&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">    fieldNames.add(<span class="string">"word"</span>);</span><br><span class="line">    <span class="comment">// 定义输出数据的类型</span></span><br><span class="line">    List&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;ObjectInspector&gt;();</span><br><span class="line">    fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">    <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取数据</span></span><br><span class="line">    String inputStr = args[<span class="number">0</span>].toString();</span><br><span class="line">    <span class="comment">// 2、获取分隔符</span></span><br><span class="line">    <span class="keyword">final</span> String splitKey = args[<span class="number">1</span>].toString();</span><br><span class="line">    <span class="comment">// 3、切粉数据</span></span><br><span class="line">    <span class="keyword">final</span> String[] words = inputStr.split(splitKey);</span><br><span class="line">    <span class="comment">// 4、遍历写出</span></span><br><span class="line">    <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">      <span class="comment">// 5、将数据放至集合</span></span><br><span class="line">      dataList.clear();</span><br><span class="line">      dataList.add(word);</span><br><span class="line">      <span class="comment">// 6、写出数据</span></span><br><span class="line">      forward(dataList);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>与自定义UDF类似</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">add jar /opt/module/data/hive/Hive-1.0-SNAPSHOT.jar;</span><br><span class="line">create temporary function udtf_split as 'com.xiong.hive.MyUDTF';</span><br><span class="line">select udtf_split('hello,sobxiong,nice boy!',',');</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="压缩和存储"><a href="#压缩和存储" class="headerlink" title="压缩和存储"></a>压缩和存储</h2><ul>
<li><p>Hadoop源码编译支持Snappy压缩</p>
<ul>
<li>资源准备<ul>
<li>虚拟机准备：连接外网、<strong>采用root角色编译</strong>，减少文件夹权限问题</li>
<li>软件包准备</li>
</ul>
</li>
<li>软件包安装<ul>
<li>JDK安装</li>
<li>Maven安装</li>
</ul>
</li>
<li>编译源码</li>
</ul>
</li>
<li><p>Hadoop压缩配置</p>
<ul>
<li>MR支持的压缩编码</li>
</ul>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>是否hadoop自带</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后,原来程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是,直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样,不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是,直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样,不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是,直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样,不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否,需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引,还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否,需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样,不需要修改</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>Gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
<tr>
<td>Snappy</td>
<td>8.3GB</td>
<td>较大</td>
<td>最快</td>
<td>最快</td>
</tr>
</tbody></table>
<ul>
<li>压缩参数配置<br>要在Hadoop中启用压缩，可以配置如下参数(mapred-site.xml文件中)<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs(在core-site.xml中配置)</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress(在mapred-site.xml中配置)</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec(在mapred-site.xml中配置)</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置)</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置)</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置)</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>开启Map输出阶段压缩<br>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量<br>具体操作：</p>
<ul>
<li>开启Hive中间传输数据压缩功能：set hive.exec.compress.intermediate=true;</li>
<li>开启mapreduce中map输出压缩功能：set mapreduce.map.output.compress=true;</li>
<li>设置mapreduce中map输出数据的压缩方式：set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</li>
<li>执行查询语句</li>
</ul>
</li>
<li><p>开启Reduce输出阶段压缩<br>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能<br>具体操作：</p>
<ul>
<li>开启Hive最终输出数据压缩功能：set hive.exec.compress.output=true;</li>
<li>开启mapreduce最终输出数据压缩：set mapreduce.output.fileoutputformat.compress=true;</li>
<li>设置mapreduce最终数据输出压缩方式：set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</li>
<li>设置mapreduce最终数据输出压缩为块压缩：set mapreduce.output.fileoutputformat.compress.type=BLOCK;</li>
<li>测试输出结果是否是压缩文件：insert overwrite local directory ‘/opt/module/data/hive/distribute-result’ select * from emp distribute by deptno sort by empno desc;</li>
</ul>
</li>
<li><p>文件存储格式<br>Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET</p>
<ul>
<li><p>列式存储和行式存储<br><img src="%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8.png" alt="列式存储和行式存储"></p>
<ul>
<li>行存储的特点：查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快</li>
<li>列存储的特点：因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法</li>
<li>主要存储格式对应的存储方式<ul>
<li><strong>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的</strong></li>
<li><strong>ORC和PARQUET是基于列式存储的</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>TextFile格式<br>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作</p>
</li>
<li><p>Orc格式<br>Orc(Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式<br>如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer<br>具体解释：</p>
<ul>
<li>Index Data：一个轻量级的index，<strong>默认是每隔1W行做一个索引</strong>。这里做的索引应该只是记录某行的各字段在Row Data中的offset</li>
<li>Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储</li>
<li>Stripe Footer：存的是各个Stream的类型，长度等信息<br><img src="Orc%E6%A0%BC%E5%BC%8F.png" alt="Orc格式"><br>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读</li>
</ul>
</li>
<li><p>Parquet格式<br>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目<br>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的<br>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度<br>一个Parquet文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页</p>
</li>
<li><p>主流文件存储格式对比实验<br>从存储文件的压缩比和查询速度两个角度对比</p>
<ul>
<li><p>存储文件的压缩比测试</p>
<ul>
<li><p>TextFile</p>
<ul>
<li>创建表(存储数据格式为TEXTFILE,默认)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure>

<ul>
<li>向表中加载数据</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/log.data' into table log_text;</span><br></pre></td></tr></table></figure>

<ul>
<li>查看文件大小：dfs -du -h /user/hive/warehouse/log_text;</li>
</ul>
</li>
<li><p>ORC</p>
<ul>
<li>创建表(存储格式为ORC)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure>

<ul>
<li>向表中加载数据(不能直接导入数据)：insert into table log_orc select * from log_text;</li>
<li>查看文件大小：dfs -du -h /user/hive/warehouse/log_orc;</li>
</ul>
</li>
<li><p>Parquet</p>
<ul>
<li>创建表(存储格式为parquet)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure>

<ul>
<li>向表中加载数据(不能直接导入数据)：insert into table log_parquet select * from log_text;</li>
<li>查看文件大小：dfs -du -h /user/hive/warehouse/log_parquet;</li>
</ul>
</li>
</ul>
</li>
<li><p>存储文件的压缩比总结：ORC &gt; Parquet &gt; textFile</p>
</li>
<li><p>存储文件的查询速度总结(都运行select * from log_sortType)：<strong>查询速度相近</strong></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>存储和压缩结合</p>
<ul>
<li><p>修改Hadoop集群具有Snappy压缩方式</p>
<ul>
<li>查看hadoop本地库支持情况：hadoop checknative</li>
<li>将编译好的支持Snappy压缩的hadoop源码包解压，将lib/native里面的内容复制到原本的hadoop下的lib/native下替换</li>
<li>分发集群：xsync native/</li>
<li>再次查看hadoop本地库支持情况：hadoop checknative</li>
<li>重新启动hadoop集群和Hive</li>
</ul>
</li>
<li><p>测试存储和压缩</p>
<ul>
<li><p>创建一个非压缩的ORC存储方式</p>
<ul>
<li>建表语句</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"NONE"</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>插入数据：insert into table log_orc_none select * from log_text;</li>
<li>查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_none;</li>
</ul>
</li>
<li><p>创建一个SNAPPY压缩的ORC存储方式</p>
<ul>
<li>建表语句</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">track_time <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">url</span> <span class="keyword">string</span>,</span><br><span class="line">session_id <span class="keyword">string</span>,</span><br><span class="line">referer <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">end_user_id <span class="keyword">string</span>,</span><br><span class="line">city_id <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">"orc.compress"</span>=<span class="string">"SNAPPY"</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>插入数据：insert into table log_orc_snappy select * from log_text;</li>
<li>查看文件大小：dfs -du -h /user/hive/warehouse/log_orc_snappy;</li>
</ul>
</li>
<li><p>上一节中默认的ORC存储方式，采用默认的ZLIB压缩</p>
</li>
</ul>
</li>
</ul>
<p>介绍ORC存储相关信息：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a><br>ORC存储方式的压缩</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level compression(one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>67,108,864</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries(must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter(must &gt; 0.0 and &lt; 1.0)</td>
</tr>
</tbody></table>
<ul>
<li>存储方式和压缩总结</li>
</ul>
<p><strong>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo</strong><br>存储方式和压缩方式不是同一个东西，文件后缀名只是人为加上的(压缩后会带有压缩格式的后缀)</p>
</li>
</ul>
<h2 id="企业级调优"><a href="#企业级调优" class="headerlink" title="企业级调优"></a>企业级调优</h2><ul>
<li>Fetch抓取<br>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees；在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。<br>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    Expects one of [none, minimal, more].</span><br><span class="line">    Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">    Currently the query should be single sourced not having any subquery and should not have</span><br><span class="line">    any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">    0. none : disable hive.fetch.task.conversion</span><br><span class="line">    1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">    2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>本地模式<br>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短<br>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启本地mr</span></span><br><span class="line">set hive.exec.mode.local.auto=true;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">local</span> mr的最大输入数据量,当输入数据量小于这个值时采用<span class="built_in">local</span> mr的方式,默认为134217728,即128M</span></span><br><span class="line">set hive.exec.mode.local.auto.inputbytes.max=50000000;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置<span class="built_in">local</span> mr的最大输入文件个数,当输入文件个数小于这个值时采用<span class="built_in">local</span> mr的方式,默认为4</span></span><br><span class="line">set hive.exec.mode.local.auto.input.files.max=10;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>表的优化</p>
<ul>
<li>小表、大表Join<br>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表(1000条以下的记录条数)先进内存。在map端完成reduce</li>
</ul>
<p><strong>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别</strong></p>
<ul>
<li><p>大表Join大表</p>
<ul>
<li><p>空Key过滤<br>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：</p>
</li>
<li><p>空Key转换<br>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。例如</p>
</li>
</ul>
</li>
<li><p>MapJoin<br>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理</p>
<ul>
<li>开启MapJoin参数设置<ul>
<li>设置自动选择MapJoin：set hive.auto.convert.join = true;(默认true)</li>
<li>大表小表的阈值设置：set hive.mapjoin.smalltable.filesize=25000000;(默认25MB)</li>
</ul>
</li>
<li>MapJoin工作机制<br><img src="MapJoin%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="MapJoin工作机制"></li>
</ul>
</li>
<li><p>Group By<br>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了<br>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果</p>
<ul>
<li>开启Map端聚合参数设置<ul>
<li>设置是否在Map端进行聚合：set hive.map.aggr = true;(默认为true)</li>
<li>设置在Map端进行聚合操作的条目数据：set hive.groupby.mapaggr.checkinterval = 100000;</li>
<li>设置有数据倾斜的时候是否进行负载均衡：set hive.groupby.skewindata = true;(默认为false)</li>
</ul>
</li>
</ul>
<p><strong>当选项设定为true，生成的查询计划会有两个MR Job</strong>。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，<strong>这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中</strong>，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中(这个过程可以保证相同的Group By Key被分布到同一个Reduce中)，最后完成最终的聚合操作</p>
</li>
<li><p>Count(Distinct)去重统计<br>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换(<strong>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的</strong>)<br>实际操作：</p>
<ul>
<li>创建一张大表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(<span class="keyword">id</span> <span class="built_in">bigint</span>, <span class="built_in">time</span> <span class="built_in">bigint</span>, uid <span class="keyword">string</span>, keyword</span><br><span class="line"><span class="keyword">string</span>, url_rank <span class="built_in">int</span>, click_num <span class="built_in">int</span>, click_url <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span></span><br><span class="line"><span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>加载数据</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/opt/module/data/hive/bigtable' into table</span><br><span class="line">bigtable;</span><br></pre></td></tr></table></figure>

<ul>
<li>设置reduce个数为5：set mapreduce.job.reduces = 5;</li>
<li>执行去重id查询</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">id</span>) <span class="keyword">from</span> bigtable;</span><br></pre></td></tr></table></figure>

<ul>
<li>采用Group by去重id</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) a;</span><br></pre></td></tr></table></figure>
</li>
<li><p>笛卡尔积<br>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积</p>
</li>
<li><p>行列过滤<br>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *<br>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤行处理实际操作：</p>
<ul>
<li>测试先关联两张表，再用where条件过滤</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> ori o <span class="keyword">on</span> o.id = b.id</span><br><span class="line"><span class="keyword">where</span> o.id &lt;= <span class="number">10</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>通过子查询后，在关联表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> ori <span class="keyword">where</span> <span class="keyword">id</span> &lt;= <span class="number">10</span> ) o <span class="keyword">on</span> b.id = o.id;</span><br></pre></td></tr></table></figure>
</li>
<li><p>动态分区调整<br>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置</p>
<ul>
<li>开启动态分区参数设置<ul>
<li>开启动态分区功能(默认开启,为true)：hive.exec.dynamic.partition</li>
<li>设置为非严格模式(动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区)：hive.exec.dynamic.partition.mode</li>
<li>在所有执行MR的节点上最大一共可以创建动态分区的个数(默认为1000)：hive.exec.max.dynamic.partitions</li>
<li>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错：hive.exec.max.dynamic.partitions.pernode</li>
<li>整个MR Job中，最大可以创建多少个HDFS文件(默认为100000)：hive.exec.max.created.files</li>
<li>当有空分区生成时，是否抛出异常。一般不需要设置(默认为false)：hive.error.on.empty.partition</li>
</ul>
</li>
<li>案例实操</li>
</ul>
</li>
<li><p>分桶：详见之前介绍</p>
</li>
<li><p>分区：详见之前介绍</p>
</li>
</ul>
</li>
<li><p>数据倾斜</p>
<ul>
<li>合理设置Map数<ul>
<li>通常情况下，作业会通过input的目录产生一个或者多个map任务<br>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小</li>
<li>是不是map数越多越好？<br>答案是否定的。如果一个任务有很多小文件(远远小于块大小128m)，则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的</li>
<li>是不是保证每个map处理接近128m的文件块，就高枕无忧了？<br>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时<br>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数</li>
</ul>
</li>
<li>小文件进行合并<br>在map执行前合并小文件，减少map数；CombineHiveInputFormat具有对小文件进行合并的功能(系统默认的格式)。HiveInputFormat没有对小文件合并功能：hive.input.format</li>
<li>复杂文件增加Map数<br>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率<br>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize))) = blocksize = 128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数<br>实际操作<ul>
<li>执行查询：select count(*) from emp;</li>
<li>设置最大切片值为100个字节：set mapreduce.input.fileinputformat.split.maxsize=100;</li>
<li>再次查询</li>
</ul>
</li>
<li>合理设置Reduce数<ul>
<li>调整reduce个数的方法一<ul>
<li>每个Reduce处理的数据量默认是256MB：hive.exec.reducers.bytes.per.reducer</li>
<li>每个任务最大的reduce数，默认为1009：hive.exec.reducers.max=1009</li>
<li>计算reducer数的公式：N=min(参数2，总输入数据量/参数1)</li>
</ul>
</li>
<li>调整reduce个数的方法二<ul>
<li>在hadoop的mapred-default.xml文件中修改设置每个job的Reduce个数</li>
<li>在Hive Cli中设置：set mapreduce.job.reduces = 15;</li>
</ul>
</li>
<li>reduce个数并不是越多越好<ul>
<li>过多的启动和初始化reduce也会消耗时间和资源</li>
<li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题</li>
</ul>
</li>
<li>在设置reduce个数的时候也需要考虑这两个原则：<ul>
<li><strong>处理大数据量利用合适的reduce数</strong></li>
<li><strong>使单个reduce任务处理数据量大小要合适</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>并行执行<br>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。如果有更多的阶段可以并行执行，那么job可能就越快完成<br>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来<br>参数设置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 打开任务并行执行</span></span><br><span class="line">set hive.exec.parallel=true;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 同一个sql允许最大并行度,默认为8</span></span><br><span class="line">set hive.exec.parallel.thread.number=16;</span><br></pre></td></tr></table></figure>
</li>
<li><p>严格模式<br>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询<br>通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询</p>
<ul>
<li>对于分区表，<strong>除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行</strong>。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表</li>
<li><strong>对于使用了order by语句的查询，要求必须使用limit语句</strong>。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间</li>
<li><strong>限制笛卡尔积的查询</strong>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The mode in which the Hive operations are being performed. </span><br><span class="line">    In strict mode, some risky queries are not allowed to run. They include:</span><br><span class="line">      Cartesian Product.</span><br><span class="line">      No partition being picked up for a query.</span><br><span class="line">      Comparing bigints and strings.</span><br><span class="line">      Comparing bigints and doubles.</span><br><span class="line">      Orderby without limit.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>JVM重用<br>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短<br>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is</span><br><span class="line">  no limit.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放</p>
</li>
<li><p>推测执行<br>在分布式集群环境下，因为程序Bug(包括Hadoop本身的bug)，负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务(比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕)，则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行(Speculative Execution)机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果</p>
<p>设置开启推测执行参数，Hadoop的mapred-site.xml文件中进行配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks</span><br><span class="line">              may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks</span><br><span class="line">              may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hive本身也提供了配置项来控制reduce-side的推测执行：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.reduce.tasks.speculative.execution<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether speculative execution for reducers should be turned on. <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>关于调优这些推测执行变量，还很难给一个具体的建议。<strong>如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉</strong>。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大</p>
</li>
<li><p>压缩：详见之前介绍</p>
</li>
<li><p>执行计划(Explain)</p>
<ul>
<li>基本语法：EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</li>
<li>案例实操<ul>
<li>查看执行计划：explain select deptno, avg(sal) avg_sal from emp group by deptno;</li>
<li>查看详细执行计划：explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="谷粒影音Hive实战"><a href="#谷粒影音Hive实战" class="headerlink" title="谷粒影音Hive实战"></a>谷粒影音Hive实战</h2><ul>
<li><p>需求描述<br>统计硅谷影音视频网站的常规指标，各种TopN指标：</p>
<ul>
<li>统计视频观看数Top10</li>
<li>统计视频类别热度Top10</li>
<li>统计出视频观看数Top20所属类别以及类别包含Top20视频的个数</li>
<li>统计视频观看数Top50所关联视频的所属类别Rank</li>
<li>统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10</li>
<li>统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</li>
</ul>
</li>
<li><p>项目</p>
<ul>
<li><p>数据结构</p>
<ul>
<li><p>视频表</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td>videoId</td>
<td>视频唯一id</td>
<td>11位字符串</td>
</tr>
<tr>
<td>uploader</td>
<td>视频上传者</td>
<td>上传视频的用户名String</td>
</tr>
<tr>
<td>age</td>
<td>视频年龄</td>
<td>视频在平台上的整数天</td>
</tr>
<tr>
<td>category</td>
<td>视频类别</td>
<td>上传视频指定的视频分类</td>
</tr>
<tr>
<td>length</td>
<td>视频长度</td>
<td>整形数字标识的视频长度</td>
</tr>
<tr>
<td>views</td>
<td>观看次数</td>
<td>视频被浏览的次数</td>
</tr>
<tr>
<td>rate</td>
<td>视频评分</td>
<td>满分5分</td>
</tr>
<tr>
<td>ratings</td>
<td>流量</td>
<td>视频的流量,整型数字</td>
</tr>
<tr>
<td>conments</td>
<td>评论数</td>
<td>一个视频的整数评论数</td>
</tr>
<tr>
<td>relatedIds</td>
<td>相关视频id</td>
<td>相关视频的id,最多20个</td>
</tr>
</tbody></table>
</li>
<li><p>用户表</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>字符类型</th>
</tr>
</thead>
<tbody><tr>
<td>uploader</td>
<td>上传者用户名</td>
<td>string</td>
</tr>
<tr>
<td>videos</td>
<td>上传视频数</td>
<td>int</td>
</tr>
<tr>
<td>friends</td>
<td>朋友数量</td>
<td>int</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>ETL(Extraction-Transformation-Loading,数据抽取、转换和加载)原始数据<br>通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割，且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用’\t’进行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清洗操作。即：将所有的类别用’&amp;’分割，同时去掉两边空格，多个相关视频id也使用’&amp;’进行分割</p>
<ul>
<li><p>ETLUtil清洗数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> oriStr 原始数据</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span> 过滤后的数据</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">etlStr</span><span class="params">(String oriStr)</span> </span>&#123;</span><br><span class="line">  StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">  <span class="comment">// 1、切割字符串</span></span><br><span class="line">  String[] fields = oriStr.split(<span class="string">"\t"</span>);</span><br><span class="line">  <span class="comment">// 2、过滤字段长度</span></span><br><span class="line">  <span class="keyword">if</span> (fields.length &lt; <span class="number">9</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  <span class="comment">// 3、去掉类别字段中的空格</span></span><br><span class="line">  fields[<span class="number">3</span>] = fields[<span class="number">3</span>].replaceAll(<span class="string">" "</span>, <span class="string">""</span>);</span><br><span class="line">  <span class="comment">// 4、修改相关视频ID字段的分隔符,把'\t'替换为'&amp;'</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; fields.length; i++) &#123;</span><br><span class="line">    <span class="comment">// 非相关id</span></span><br><span class="line">    <span class="keyword">if</span> (i &lt; <span class="number">9</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (i == fields.length - <span class="number">1</span>) sb.append(fields[i]);</span><br><span class="line">      <span class="keyword">else</span> sb.append(fields[i]).append(<span class="string">'\t'</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// 相关id</span></span><br><span class="line">      <span class="keyword">if</span> (i == fields.length - <span class="number">1</span>) sb.append(fields[i]);</span><br><span class="line">      <span class="keyword">else</span> sb.append(fields[i]).append(<span class="string">'&amp;'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 5、返回结果</span></span><br><span class="line">  <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>ETL之Mapper</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出写NullWritable,不需要排序,节省资源</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">// 定义全局value</span></span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取数据</span></span><br><span class="line">    String oriStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、过滤数据</span></span><br><span class="line">    String eltStr = ETLUtil.etlStr(oriStr);</span><br><span class="line">    <span class="comment">// 3、写出</span></span><br><span class="line">    <span class="keyword">if</span> (eltStr == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    v.set(eltStr);</span><br><span class="line">    context.write(NullWritable.get(), v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>ETL之Driver</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 官方推荐采用继承Tool方式</span></span><br><span class="line"><span class="comment">// 在ToolRunner中帮做了GenericOptionsParser</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLDriver</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Configuration conf;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取job对象</span></span><br><span class="line">    Job job = Job.getInstance(conf);</span><br><span class="line">    <span class="comment">// 2、设置jar包路径</span></span><br><span class="line">    job.setJarByClass(ETLDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3、设置Mapper类和输出KV类型</span></span><br><span class="line">    job.setMapperClass(ETLMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、设置最终输出的KV类型</span></span><br><span class="line">    job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 5、设置输入输出的路径</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 6、提交任务</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> result ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration conf)</span> </span>&#123; <span class="keyword">this</span>.conf = conf; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> conf; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 构建配置信息</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">int</span> result = ToolRunner.run(conf, <span class="keyword">new</span> ETLDriver(), args);</span><br><span class="line">      System.out.println(<span class="string">"result = "</span> + result);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行ETL jar包(经过maven的package,然后扔到集群上)：bin/hadoop jar /opt/module/data/hive/guli-vedio-1.0-SNAPSHOT.jar com.xiong.mr.ETLDriver /gulivideo/video/2008/0222 /guliOutput</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>准备工作</p>
<ul>
<li><p>创建表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- gulivideo_ori</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_ori(</span><br><span class="line">  videoId <span class="keyword">string</span>,</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  age <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">  <span class="keyword">length</span> <span class="built_in">int</span>,</span><br><span class="line">  views <span class="built_in">int</span>,</span><br><span class="line">  rate <span class="built_in">float</span>,</span><br><span class="line">  ratings <span class="built_in">int</span>,</span><br><span class="line">  comments <span class="built_in">int</span>,</span><br><span class="line">  relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"&amp;"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- gulivideo_user_ori</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_ori(</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  videos <span class="built_in">int</span>,</span><br><span class="line">  friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- gulivideo_orc</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_orc(</span><br><span class="line">  videoId <span class="keyword">string</span>,</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  age <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">category</span> <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">  <span class="keyword">length</span> <span class="built_in">int</span>,</span><br><span class="line">  views <span class="built_in">int</span>,</span><br><span class="line">  rate <span class="built_in">float</span>,</span><br><span class="line">  ratings <span class="built_in">int</span>,</span><br><span class="line">  comments <span class="built_in">int</span>,</span><br><span class="line">  relatedId <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;)</span><br><span class="line">clustered <span class="keyword">by</span> (uploader) <span class="keyword">into</span> <span class="number">8</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"&amp;"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- gulivideo_user_orc</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_orc(</span><br><span class="line">  uploader <span class="keyword">string</span>,</span><br><span class="line">  videos <span class="built_in">int</span>,</span><br><span class="line">  friends <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">"\t"</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入ETL后的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_ori</span></span><br><span class="line">load data inpath "/guliOutput" into table gulivideo_ori;</span><br><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_user_ori</span></span><br><span class="line">load data inpath "/gulivideo/user/2008/0903" into table gulivideo_user_ori;</span><br><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_orc</span></span><br><span class="line">insert into table gulivideo_orc select * from gulivideo_ori;</span><br><span class="line"><span class="meta">#</span><span class="bash"> gulivideo_user_orc</span></span><br><span class="line">insert into table gulivideo_user_orc select * from gulivideo_user_ori;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>业务分析</p>
<ul>
<li><p>统计视频观看数Top10</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>统计视频类别热度Top10(某类视频的个数作为视频类别热度)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、使用UDTF函数将类别炸裂</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、按照category_name进行分组,统计每种类别视频的总数,同时按照该总数进行倒序排名,取前10</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      videoId,</span><br><span class="line">      category_name</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      gulivideo_orc</span><br><span class="line">      <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name</span><br><span class="line">  )t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>统计出视频观看数Top20所属类别以及类别包含Top20视频的个数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、统计视频观看数Top20</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  views,</span><br><span class="line">  <span class="keyword">category</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">20</span>;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、对t1表中的category进行炸裂</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  videoId,</span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name;t2</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3、对t2表进行分组(category_name)求和(总数)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      videoId,</span><br><span class="line">      category_name</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          videoId,</span><br><span class="line">          views,</span><br><span class="line">          <span class="keyword">category</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          gulivideo_orc</span><br><span class="line">        <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">          views <span class="keyword">desc</span></span><br><span class="line">        <span class="keyword">limit</span> <span class="number">20</span></span><br><span class="line">      )t1</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) tmp_category <span class="keyword">as</span> category_name</span><br><span class="line">  )t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>统计视频观看数Top50所关联视频的所属类别Rank</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、统计视频观看数Top50</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  relatedId,</span><br><span class="line">  views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">50</span>;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、对t1表中的relatedId进行炸裂并去重</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  related_id</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(relatedId) tmp_related <span class="keyword">as</span> related_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  related_id;t2</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3、取出观看数前50视频关联ID视频的类别</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">category</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t2</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  gulivideo_orc orc</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t2.related_id = orc.videoId;t3</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4、对t3表中的category进行炸裂</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  <span class="keyword">explode</span>(<span class="keyword">category</span>) category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t3;t4</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 5、分组(类别)求和(总数)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t4</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  category_count <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_name,</span><br><span class="line">  <span class="keyword">count</span>(*) category_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      <span class="keyword">explode</span>(<span class="keyword">category</span>) category_name</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">          <span class="keyword">category</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">              related_id</span><br><span class="line">            <span class="keyword">from</span></span><br><span class="line">              (</span><br><span class="line">                <span class="keyword">select</span></span><br><span class="line">                  relatedId,</span><br><span class="line">                  views</span><br><span class="line">                <span class="keyword">from</span></span><br><span class="line">                  gulivideo_orc</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">                  views <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">limit</span> <span class="number">50</span></span><br><span class="line">              )t1</span><br><span class="line">            <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(relatedId) tmp_related <span class="keyword">as</span> related_id</span><br><span class="line">            <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">              related_id</span><br><span class="line">          )t2</span><br><span class="line">        <span class="keyword">join</span></span><br><span class="line">          gulivideo_orc orc</span><br><span class="line">        <span class="keyword">on</span></span><br><span class="line">          t2.related_id = orc.videoId</span><br><span class="line">      )t3</span><br><span class="line">  )t4</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">    category_count <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>统计每个类别中的视频热度Top10/统计每个类别中视频流量Top10/统计每个类别视频观看数Top10</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、给每一种类别根据视频观看数添加rank值(倒序)</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  categoryId,</span><br><span class="line">  videoId,</span><br><span class="line">  views,</span><br><span class="line">  <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> categoryId <span class="keyword">order</span> <span class="keyword">by</span> views <span class="keyword">desc</span>) rk</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_category;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、过滤前十</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  categoryId,</span><br><span class="line">  videoId,</span><br><span class="line">  views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      categoryId,</span><br><span class="line">      videoId,</span><br><span class="line">      views,</span><br><span class="line">      <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> categoryId <span class="keyword">order</span> <span class="keyword">by</span> views <span class="keyword">desc</span>) rk</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      gulivideo_category</span><br><span class="line">  )t1</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">  rk &lt;= <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>统计上传视频最多的用户Top10以及他们上传的观看次数在前20的视频</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1、统计上传视频最多的用户Top10</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  uploader,</span><br><span class="line">  videos</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  gulivideo_user_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  videos <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;t1</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2、取出这10个人上传的所有视频,按照观看次数进行排名,取前20</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  video.videoId,</span><br><span class="line">  video.views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  t1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  gulivideo_orc video</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t1.uploader = video.uploader</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最终SQL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  video.videoId,</span><br><span class="line">  video.views</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      uploader,</span><br><span class="line">      videos</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">      gulivideo_user_orc</span><br><span class="line">    <span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">      videos <span class="keyword">desc</span></span><br><span class="line">    <span class="keyword">limit</span> <span class="number">10</span></span><br><span class="line">  )t1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">  gulivideo_orc video</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">  t1.uploader = video.uploader</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  views <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">20</span>;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="常见错误及解决方案"><a href="#常见错误及解决方案" class="headerlink" title="常见错误及解决方案"></a>常见错误及解决方案</h2><ul>
<li><p>启动MR任务报错：virtual memory used. Killing container(虚拟内存不足)<br>修改hadoop的配置，修改检查虚拟内存的属性为false</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>

          
            <div class='article_footer'>
              
                
  
    
    



  

  
    
    



  

  
    
    

<section class="widget copyright  desktop mobile">
  <div class='content'>
    
      <blockquote>
        
          
            <p>博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</p>

          
        
          
            <p>本文永久链接是：<a href=https://sobxiong.github.io/2020/07/06/BigData/Hive/>https://sobxiong.github.io/2020/07/06/BigData/Hive/</a></p>
          
        
      </blockquote>
    
  </div>
</section>

  

  
    
    

<section class="widget qrcode  desktop mobile">
  

  <div class='content article-entry'>
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
  </div>
</section>

  


              
            </div>
          
        </div>
        
          


  <section class='meta' id="footer-meta">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2020-07-18T16:37:11+08:00">
  <a class='notlink'>
    <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：2020年7月18日</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>大数据</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/Hive/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>Hive</p></a></div>


        
      
        
          

        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=https://sobxiong.github.io/2020/07/06/BigData/Hive/&title=Hive - SOBXiong的博客&summary=内容
Hive基本概念
Hive安装
Hive数据类型
DDL数据定义
DML数据操作
查询
例题实战(蚂蚁金服)
函数
压缩和存储
企业级调优
谷粒影音Hive实战
常见错误及解决方案
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://sobxiong.github.io/2020/07/06/BigData/Hive/&title=Hive - SOBXiong的博客&summary=内容
Hive基本概念
Hive安装
Hive数据类型
DDL数据定义
DML数据操作
查询
例题实战(蚂蚁金服)
函数
压缩和存储
企业级调优
谷粒影音Hive实战
常见错误及解决方案
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=https://sobxiong.github.io/2020/07/06/BigData/Hive/&title=Hive - SOBXiong的博客&summary=内容
Hive基本概念
Hive安装
Hive数据类型
DDL数据定义
DML数据操作
查询
例题实战(蚂蚁金服)
函数
压缩和存储
企业级调优
谷粒影音Hive实战
常见错误及解决方案
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
          <div class="prev-next">
            
              <a class='prev' href='/2020/07/17/BigData/HBase/'>
                <p class='title'><i class="fas fa-chevron-left" aria-hidden="true"></i>HBase</p>
                <p class='content'>内容
HBase简介
HBase快速入门
HBase进阶
HBase-API
HBase优化



HBase简介
HBase定义：HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数...</p>
              </a>
            
            
              <a class='next' href='/2020/06/26/BigData/Zookeeper/'>
                <p class='title'>Zookeeper入门<i class="fas fa-chevron-right" aria-hidden="true"></i></p>
                <p class='content'>内容
Zookeeper入门
Zookeeper安装
Zookeeper实战
Zookeeper内部原理
面试真题



Zookeeper入门
概述Zookeeper是一个开源的分布式的，为分...</p>
              </a>
            
          </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  <article class="post white-box reveal comments shadow">
    <section class="article typo">
      <p ct><i class='fas fa-comments'></i> 评论</p>
      
      
      
      
      
      
        <section id="comments">
          <div id="valine_container" class="valine_thread">
            <i class="fas fa-cog fa-spin fa-fw fa-2x"></i>
          </div>
        </section>
      
      
    </section>
  </article>


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->



  <script>
    window.subData = {
      title: 'Hive',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
  

  
    
    



  <section class="widget toc-wrapper shadow desktop mobile" id="toc-div" >
    
  <header>
    
      <i class="fas fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#内容"><span class="toc-text">内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive基本概念"><span class="toc-text">Hive基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive安装"><span class="toc-text">Hive安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive数据类型"><span class="toc-text">Hive数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DDL数据定义"><span class="toc-text">DDL数据定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DML数据操作"><span class="toc-text">DML数据操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#查询"><span class="toc-text">查询</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例题实战-蚂蚁金服"><span class="toc-text">例题实战(蚂蚁金服)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#函数"><span class="toc-text">函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#压缩和存储"><span class="toc-text">压缩和存储</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#企业级调优"><span class="toc-text">企业级调优</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#谷粒影音Hive实战"><span class="toc-text">谷粒影音Hive实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#常见错误及解决方案"><span class="toc-text">常见错误及解决方案</span></a></li></ol>
    </div>
  </section>


  


</aside>


  
  <footer class="clearfix">
    <br><br>
    
      
        <br>
        <div class="social-wrapper">
          
            
              <a href="mailto:1942991710@qq.com"
                class="social fas fa-envelope flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
            
              <a href="https://github.com/SOBXiong"
                class="social fab fa-github flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
        </div>
      
    
      
        <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
      
    
      
        <div class='copyright'>
        <p><a href="http://xiongjc.top" target="_blank" rel="noopener">Copyright © 2017-2020 SOBXiong</a></p>

        </div>
      
    
  </footer>

<script>setLoadingBarProgress(80);</script>


      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4/dist/jquery.min.js"></script>


  <script>
    
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/" || "/";
    if (!ROOT.endsWith('/')) ROOT += '/';
  </script>





  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/instant_page.js" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>


  <script src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.6/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      ScrollReveal().reveal('.l_main .reveal', {
        distance: '8px',
        duration: '800',
        interval: '100',
        scale: '1'
      });
    });
  </script>


  
<script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>

  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script defer src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>



  
  
  
    
<script src="https://cdn.jsdelivr.net/npm/jquery-backstretch@2.1.18/jquery.backstretch.min.js"></script>

    <script type="text/javascript">
      $(function(){
        var imgs=["https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg"];
        if ('true' == 'true') {
          function shuffle(arr){
            /*From countercurrent-time*/
            var n = arr.length;
            while(n--) {
              var index = Math.floor(Math.random() * n);
              var temp = arr[index];
              arr[index] = arr[n];
              arr[n] = temp;
            }
          }
          shuffle(imgs);
        }
        if ('.cover') {
          $('.cover').backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        } else {
          $.backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        }
      });
    </script>
  



  
    
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js"></script>

  
    
<script src="https://cdn.jsdelivr.net/npm/meting@2.0/dist/Meting.min.js"></script>

  









  
    
<script src="https://cdn.jsdelivr.net/npm/valine@1.4/dist/Valine.min.js"></script>

  
  <script>
  var GUEST_INFO = ['nick','mail','link'];
  var meta = 'nick,mail,link'.split(',').filter(function(item){
    return GUEST_INFO.indexOf(item) > -1
  });
  var REQUIRED_FIELDS = ['nick','mail','link'];
  var requiredFields = 'nick,mail'.split(',').filter(function(item){
    return REQUIRED_FIELDS.indexOf(item) > -1
  });
  var valine = new Valine();
  function emoji(path, idx, ext) {
      return path + "/" + path + "-" + idx + "." + ext;
  }
  var emojiMaps = {};
  for (var i = 1; i <= 54; i++) {
    emojiMaps['tieba-' + i] = emoji('tieba', i, 'png');
  }
  for (var i = 1; i <= 101; i++) {
    emojiMaps['qq-' + i] = emoji('qq', i, 'gif');
  }
  for (var i = 1; i <= 116; i++) {
    emojiMaps['aru-' + i] = emoji('aru', i, 'gif');
  }
  for (var i = 1; i <= 125; i++) {
    emojiMaps['twemoji-' + i] = emoji('twemoji', i, 'png');
  }
  for (var i = 1; i <= 4; i++) {
    emojiMaps['weibo-' + i] = emoji('weibo', i, 'png');
  }
  valine.init({
    el: '#valine_container',
    meta: meta,
    
    appId: "dogUA2FSKGTo029M1SEwGROT-MdYXbMMI",
    appKey: "u0NdtQ8nvHoMdJPSYqm1LRxE",
    placeholder: "快来评论吧~",
    pageSize:'10',
    avatar:'robohash',
    lang:'zh-cn',
    visitor: 'true',
    highlight: 'true',
    mathJax: 'false',
    enableQQ: 'true',
    requiredFields: requiredFields,
    emojiCDN: 'https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/emoji/valine/',
    emojiMaps: emojiMaps
  })
  </script>





  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2.6.5/js/app.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2.6.5/js/search.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/comment_typing.js"></script>






<!-- 复制 -->

  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="fas fa-copy"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-check-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-check-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-times-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-times-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>




<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  function pjax_fancybox() {
    $(".article-entry").find("img").not('.inline').not('a img').each(function () { //渲染 fancybox
      var element = document.createElement("a"); // a 标签
      $(element).attr("pjax-fancybox", "");  // 过滤 pjax
      $(element).attr("href", $(this).attr("src"));
      if ($(this).attr("data-original")) {
        $(element).attr("href", $(this).attr("data-original"));
      }
      $(element).attr("data-fancybox", "images");
      var caption = "";   // 描述信息
      if ($(this).attr('alt')) {  // 标准 markdown 描述信息
        $(element).attr('data-caption', $(this).attr('alt'));
        caption = $(this).attr('alt');
      }
      var div = document.createElement("div");
      $(div).addClass("fancybox");
      $(this).wrap(div); // 最外层套 div ，其实主要作用还是 class 样式
      var span = document.createElement("span");
      $(span).addClass("image-caption");
      $(span).text(caption); // 加描述
      $(this).after(span);  // 再套一层描述
      $(this).wrap(element);  // 最后套 a 标签
    })
    $(".article-entry").find("img").fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
      closeClick: true,
      helpers: {
        overlay: {closeClick: true}
      },
      buttons: [
        "zoom",
        "close"
      ]
    });
  };
  $(function () {
    pjax_fancybox();
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
