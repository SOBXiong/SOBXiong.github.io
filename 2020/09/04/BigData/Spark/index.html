<!DOCTYPE html>
<html>
<head hexo-theme='https://volantis.js.org/#2.6.6'>
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
    
  
  <!-- 渲染优化 -->
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- 页面元数据 -->
  
    <title>Spark - SOBXiong的博客</title>
  
    <meta name="keywords" content="大数据,Spark">
  
  
    <meta name="description" content="内容
Spark概述
Spark快速上手
Spark运行环境
Spark核心编程
">
  

  <!-- feed -->
  

  <!-- import meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13/css/all.min.css">
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">

  

  
  <link rel="shortcut icon" type='image/x-icon' href="https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/favicon.png">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css">
  

  

  <!-- import link -->
  

  
  
    
<link rel="stylesheet" href="/css/style.css">

  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
</head>

<body>
  
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>
<header class="l_header shadow blur">
  <div class='container'>
  <div class='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h'>
        <li><a class="s-comment fas fa-comments fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
          <li><a class="s-toc fas fa-list fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
          
            <i class='https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/favicon.png'></i>
          
          
            SOBXiong
          
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h'>
          
          
          
            
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
				</ul>
			</div>

      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fas fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>

			<ul class='switcher nav-list-h'>
				
					<li><a class="s-search fas fa-search fa-fw" target="_self" href='javascript:void(0)'></a></li>
				
				<li>
          <a class="s-menu fas fa-bars fa-fw" target="_self" href='javascript:void(0)'></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
          </ul>
        </li>
			</ul>
		</div>
	</div>
  </div>
</header>

<script>setLoadingBarProgress(40);</script>



  <div class="l_body nocover">
    <div class='body-wrapper'>
      

<div class='l_main'>
  

  
    <article id="post" class="post white-box reveal shadow article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
      
      
      <div class="meta" id="header-meta">
        
          
  <h1 class="title">
    <a href="/2020/09/04/BigData/Spark/">
      Spark
    </a>
  </h1>


        
        <div class='new-meta-box'>
          
            
          
            
              
<div class='new-meta-item author'>
  <a href="http://xiongjc.top" target="_blank" rel="nofollow noopener">
    <img src="https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/hdImg_f757fa7e0fd65077ce52d251fc7a01d815860762755.jpg">
    <p>SOBXiong</p>
  </a>
</div>

            
          
            
              

            
          
            
              <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：2020年9月4日</p>
  </a>
</div>

            
          
            
              

            
          
        </div>
        
          <hr>
        
      </div>
    
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          
          <h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul>
<li><a href="#Spark概述">Spark概述</a></li>
<li><a href="#Spark快速上手">Spark快速上手</a></li>
<li><a href="#Spark运行环境">Spark运行环境</a></li>
<li><a href="#Spark核心编程">Spark核心编程</a></li>
</ul>
<a id="more"></a>

<h2 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h2><ul>
<li>Spark是什么：一种基于内存的快速、通用、可扩展的大数据分析计算引擎(unified analytics engine for large-scale data processing)</li>
<li>Spark And Hadoop<ul>
<li>从时间节点上看：<ul>
<li>Hadoop<ul>
<li>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</li>
<li>2008年1月，Hadoop成为Apache顶级项目</li>
<li>2011年1.0正式发布</li>
<li>2012年3月稳定版发布</li>
<li>2013年10月发布2.X(Yarn)版本</li>
</ul>
</li>
<li>Spark<ul>
<li>2009年，Spark诞生于伯克利大学的AMPLab实验室</li>
<li>2010年，伯克利大学正式开源了Spark项目</li>
<li>2013年6月，Spark成为了Apache基金会下的项目</li>
<li>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</li>
<li>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark</li>
</ul>
</li>
</ul>
</li>
<li>从功能上看：<ul>
<li>Hadoop<ul>
<li>Hadoop是由Java编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架</li>
<li>作为Hadoop分布式文件系统，HDFS处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的The Google File System这篇论文，是GFS的开源实现</li>
<li>MapReduce是一种编程模型，Hadoop根据Google的MapReduce论文将其实现。作为Hadoop的分布式计算模型，MapReduce是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易</li>
<li>HBase是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop中非常重要的组件</li>
</ul>
</li>
<li>Spark<ul>
<li>Spark是一种由Scala开发的快速、通用、可扩展的大数据分析引擎</li>
<li>Spark Core中提供了Spark最基础与最核心的功能</li>
<li>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据</li>
<li>Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API</li>
</ul>
</li>
</ul>
</li>
<li>综合以上，Spark出现的时间相对较晚，并且主要功能主要是用于数据计算。因此Spark一直被认为是Hadoop MapReduce框架的升级版</li>
</ul>
</li>
<li>Spark Or Hadoop<ul>
<li>Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景(如：机器学习、图挖掘算法、交互式数据挖掘算法)中存在诸多计算效率等问题。因此Spark应运而生，Spark就是在传统的MapReduce计算框架的基础上，优化其计算过程，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型</li>
<li>机器学习中ALS、凸优化梯度下降等都需要基于数据集或者数据集的衍生数据反复查询、反复操作。MR模式不太合适，即使多MR串行处理，性能和时间也是一个问题，而且数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的Scala语言恰恰擅长函数的处理</li>
<li>Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集(Resilient Distributed Datasets)，它提供了比MapReduce更丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法</li>
<li><strong>Spark和Hadoop的根本差异是多个作业之间的数据通信问题：Spark多个作业之间的数据通信是基于内存的，而Hadoop是基于磁盘的</strong></li>
<li>Spark Task的启动时间快。Spark采用fork线程的方式，而Hadoop采用创建新的进程的方式</li>
<li>Spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都要依赖于磁盘交互</li>
<li>Spark的缓存机制比HDFS的缓存机制高效</li>
<li>综上所述，在绝大多数的数据计算场景中，Spark确实会比MapReduce更有优势。但Spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够而导致Job执行失败，此时MapReduce是一个更好的选择，所以Spark并不能完全替代MR</li>
</ul>
</li>
<li>Spark核心模块<br><img src="Spark%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97.png" alt="Spark核心模块"><ul>
<li>Spark Core：Spark Core中提供了Spark最基础与最核心的功能。Spark其他的功能如Spark SQL、Spark Streaming、GraphX以及MLlib都是在Spark Core的基础上进行扩展的</li>
<li>Spark SQL：Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言(HQL)来查询数据</li>
<li>Spark Streaming：Spark Streaming是Spark平台针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API</li>
<li>Spark MLlib：MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语</li>
<li>Spark GraphX：GraphX是Spark面向图计算提供的框架与算法库</li>
</ul>
</li>
</ul>
<h2 id="Spark快速上手"><a href="#Spark快速上手" class="headerlink" title="Spark快速上手"></a>Spark快速上手</h2><ul>
<li><p>在IDEA上初体验Spark API</p>
<ul>
<li><p>创建Maven项目(IDEA中最简单的maven项目,不采用任何模版项目)</p>
</li>
<li><p>IDEA安装Scala插件</p>
</li>
<li><p>pom添加依赖关系</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 该插件用于将Scala代码编译成class文件 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">          <span class="comment">&lt;!-- 声明绑定到maven的compile阶段 --&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>WordCount案例</p>
<ul>
<li><p>配置log4j日志输出(过滤Spark框架的执行日志)——在resource目录下创建log4j.properties文件</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootCategory</span>=<span class="string">ERROR, console</span></span><br><span class="line"><span class="meta">log4j.appender.console</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.console.target</span>=<span class="string">System.err</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout.ConversionPattern</span>=<span class="string">%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n</span></span><br><span class="line"><span class="comment"># Set the default spark-shell log level to ERROR. When running the spark-shell, the</span></span><br><span class="line"><span class="comment"># log level for this class is used to overwrite the root logger's log level, so that</span></span><br><span class="line"><span class="comment"># the user can have different defaults for the shell and regular Spark apps.</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.Main</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="comment"># Settings to quiet third party logs that are too verbose</span></span><br><span class="line"><span class="meta">log4j.logger.org.spark_project.jetty</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.parquet</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.parquet</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="comment"># SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler</span>=<span class="string">FATAL</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry</span>=<span class="string">ERROR</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>案例代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Spark是一个计算框架</span></span><br><span class="line">    <span class="comment">// 开发人员使用Spark框架的Api实现计算</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1、准备Spark环境</span></span><br><span class="line">    <span class="comment">// setMaster：设定Spark环境的位置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConfig = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">"local"</span>)</span><br><span class="line">      .setAppName(<span class="string">"wordCount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、建立和Spark的连接</span></span><br><span class="line">    <span class="comment">// jdbc：connection</span></span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConfig)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、实现业务操作</span></span><br><span class="line">    <span class="comment">// 3.1、读取指定目录下的数据文件(多个)</span></span><br><span class="line">    <span class="comment">// 参数path可以指向单一的文件/文件目录</span></span><br><span class="line">    <span class="comment">// RDD: 更适合并行计算的数据模型</span></span><br><span class="line">    <span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(path = <span class="string">"./src/main/resources/input"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.2、将读取的内容进行扁平化操作,切分单词</span></span><br><span class="line">    <span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.3、将分词后的数据进行结构的转换</span></span><br><span class="line">    <span class="comment">// word -&gt; (word,1)</span></span><br><span class="line">    <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.4、将转换结构后的数据根据单词进行分组聚合</span></span><br><span class="line">    <span class="comment">// reduceByKey: 根据数据key进行分组,然后对value进行统计聚合</span></span><br><span class="line">    <span class="keyword">val</span> wordSumRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    <span class="keyword">val</span> wordCountArray: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordSumRDD.collect()</span><br><span class="line">    println(wordCountArray.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、释放连接</span></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Spark运行环境"><a href="#Spark运行环境" class="headerlink" title="Spark运行环境"></a>Spark运行环境</h2><ul>
<li><p>基本介绍：Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行，在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来<br><img src="Spark%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.png" alt="Spark运行环境"></p>
</li>
<li><p>Local本地模式</p>
<ul>
<li><p>介绍：所谓的Local模式就是不需要其他任何节点资源就可以在本地执行Spark代码的环境，一般用于教学、调试、演示等。而之前在IDEA中运行代码的环境我们称之为开发环境</p>
</li>
<li><p>环境准备：</p>
<ul>
<li>解压缩spark文件</li>
<li>引入hadoop等Jar包</li>
</ul>
</li>
<li><p>启动Local环境</p>
<ul>
<li><p>进入解压缩的目录，执行：bin/spark-shell –master local[*]</p>
</li>
<li><p>启动后，可以使用当前主机的4040端口进行Web UI监控</p>
</li>
<li><p>命令行工具</p>
<ul>
<li><p>准备：在spark根目录下的data目录中，添加word.txt文件，准备一些英文单词</p>
</li>
<li><p>在Local环境中输入</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"data/word.txt"</span>).flatMap(_.split(<span class="string">""</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure>
</li>
<li><p>回车后会实时输出结果，sc是Spark Context的简写，该变量由命令行工具提供</p>
</li>
<li><p>退出：:quit(Scala)或者Ctrl + C</p>
</li>
</ul>
</li>
<li><p>提交应用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master local[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交应用参数解释</p>
<ul>
<li>–class：表示要执行程序的主类</li>
<li>–master local[2]：部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量</li>
<li>spark-examples_2.12-2.4.5.jar：运行的应用类所在的jar包</li>
<li>10：表示程序的入口参数，用于设定当前应用的任务数量</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Standalone模式</p>
<ul>
<li><p>介绍：local本地模式只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行。Spark自身节点运行的集群模式叫做独立部署(Standalone)模式。Spark的Standalone模式体现了经典的master-slave模式</p>
</li>
<li><p>集群规划：</p>
<table>
<thead>
<tr>
<th>hadoop101</th>
<th>hadoop102</th>
<th>hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td>Worker Master</td>
<td>Worker</td>
<td>Worker</td>
</tr>
</tbody></table>
</li>
<li><p>环境准备</p>
<ul>
<li><p>解压缩spark文件</p>
</li>
<li><p>引入hadoop等Jar包</p>
</li>
<li><p>修改配置(conf目录)</p>
<ul>
<li><p>修改slaves.template文件名改为slaves</p>
</li>
<li><p>修改slaves文件，添加work节点：hadoop101、hadoop102、hadoop103(用回车分割,不能其他空格空行)</p>
</li>
<li><p>修改spark-env.sh.template文件名为spark-env.sh</p>
</li>
<li><p>修改spark-env.sh文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line">SPARK_MASTER_HOST=hadoop101</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>
</li>
<li><p>分发配置文件：xsync spark-env.sh</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>启动集群</p>
<ul>
<li>执行脚本命令：sbin/start-all.sh</li>
<li>查看Master资源监控Web UI界面：<a href="http://hadoop101:8080" target="_blank" rel="noopener">http://hadoop101:8080</a></li>
</ul>
</li>
<li><p>提交应用(–master spark://hadoop101:7077：独立部署模式,连接到Spark集群)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop101:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交参数说明</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \</span><br><span class="line">[application-arguments]</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>解释</th>
<th>可选值举例</th>
</tr>
</thead>
<tbody><tr>
<td>–class</td>
<td>Spark程序中包含主函数的类</td>
<td>/</td>
</tr>
<tr>
<td>–master</td>
<td>Spark程序运行的模式</td>
<td>local模式(local[*])、standalone模式(spark://hadoop101:7077)、Yarn模式(Yarn)</td>
</tr>
<tr>
<td>–executor-memory 1G</td>
<td>指定每个executor可用内存为1G</td>
<td>符合集群内存配置即可，具体情况具体分析</td>
</tr>
<tr>
<td>–total-executor-cores 2</td>
<td>指定所有executor使用的cpu核数为2个</td>
<td>同上</td>
</tr>
<tr>
<td>–executor-cores</td>
<td>指定每个executor使用的cpu核数</td>
<td>同上</td>
</tr>
<tr>
<td>application-jar</td>
<td>打包好的应用jar(包含依赖)。该URL在集群中全局可见。比如hdfs://共享存储系统；如果是file://path，那么所有的节点的path都要包含同样的jar</td>
<td>同上</td>
</tr>
<tr>
<td>application-arguments</td>
<td>传给main()方法的参数</td>
<td>同上</td>
</tr>
</tbody></table>
</li>
<li><p>配置历史服务器</p>
<ul>
<li><p>介绍：由于spark-shell停止或spark任务结束后，集群监控的4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况</p>
</li>
<li><p>具体配置步骤：</p>
<ul>
<li><p>修改spark-defaults.conf.template文件名为spark-defaults.conf</p>
</li>
<li><p>修改spark-default.conf文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 设置开启日志记录</span><br><span class="line">spark.eventLog.enabled  true</span><br><span class="line"># 设置日志存储路径</span><br><span class="line">spark.eventLog.dir  hdfs://hadoop101:9000/spark_log</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动hadoop集群，hdfs上的spark_log目录需要提前存在</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br><span class="line">hadoop dfs -mkdir /spark_log</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改spark-env.sh文件，添加日志配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 设置历史日志选项</span><br><span class="line"># 参数1：Web UI访问端口号</span><br><span class="line"># 参数2：指定历史服务器日志存储路径</span><br><span class="line"># 参数3: 指定保存Application历史记录的个数,如果超过这个值,旧的应用程序信息将被删除(是内存中的应用数,而不是页面上显示的应用数)</span><br><span class="line">export SPARK_HISTORY_OPTS="</span><br><span class="line">-Dspark.history.ui.port=18080</span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/spark_log</span><br><span class="line">-Dspark.history.retainedApplications=30"</span><br></pre></td></tr></table></figure>
</li>
<li><p>分发配置文件：xsync conf</p>
</li>
<li><p>重启集群和历史服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新执行任务</p>
</li>
<li><p>查看历史服务情况：<a href="http://hadoop101:18080" target="_blank" rel="noopener">http://hadoop101:18080</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>配置高可用(HA)</p>
<ul>
<li><p>介绍：所谓的高可用是因为当前集群中的Master节点只有一个，因此会存在单点故障问题。为了解决单点故障问题，需要在集群中配置多个Master节点，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置</p>
</li>
<li><p>集群规划：</p>
<table>
<thead>
<tr>
<th>hadoop101</th>
<th>hadoop102</th>
<th>hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td>Master、Zookeeper、Worker</td>
<td>Master、Zookeeper、Worker</td>
<td>Zookeeper、Worker</td>
</tr>
</tbody></table>
</li>
<li><p>停止集群：sbin/stop-all.sh</p>
</li>
<li><p>启动Zookeeper：bin/zkServer.sh start</p>
</li>
<li><p>修改spark-env.sh配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 注释master的host和port,不能把master固定</span><br><span class="line"># SPARK_MASTER_HOST=hadoop101</span><br><span class="line"># SPARK_MASTER_PORT=7077</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"></span><br><span class="line"># 设置zookeeper配置</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS="</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url=hadoop101,hadoop102,hadoop103</span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark"</span><br></pre></td></tr></table></figure>
</li>
<li><p>分发配置文件：xsync spark-env.sh</p>
</li>
<li><p>重启集群：sbin/start-all.sh</p>
</li>
<li><p>启动hadoop102的单独master节点(使hadoop102节点的master状态处于备用状态)：sbin/start-master.sh</p>
</li>
<li><p>提交应用到高可用集群：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop101:7077,hadoop102:7077 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
</li>
<li><p>停止hadoop101的master进程：kill -9 xxx(Master的进程号)</p>
</li>
<li><p>查看hadoop102的Master资源监控Web UI(8989端口)，经过一段时间，hadoop102节点master状态提升为活动状态</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Yarn模式</p>
<ul>
<li><p>基本介绍：独立部署(Standalone)模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是Spark主要是计算框架，而不是资源调度框架，所以资源调度并不是它的强项，因此还是和其他专业的资源调度框架集成会更靠谱一些。其中，在国内工作中，Yarn使用的非常多</p>
</li>
<li><p>环境准备</p>
<ul>
<li><p>解压缩spark文件</p>
</li>
<li><p>引入hadoop等Jar包</p>
</li>
<li><p>修改配置文件</p>
<ul>
<li><p><strong>hadoop的配置文件yarn-site.xml</strong>，并分发</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  是否启动一个线程检查每个任务正使用的物理内存量,如果任务超出分配值,</span></span><br><span class="line"><span class="comment">  则直接将其杀掉,默认是true</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  是否启动一个线程检查每个任务正使用的虚拟内存量,</span></span><br><span class="line"><span class="comment">  如果任务超出分配值,则直接将其杀掉,默认是true</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>spark的配置文件spark-env.sh，并分发</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line"># 设置yarn配置目录</span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>启动HDFS和YARN集群</p>
</li>
<li><p>提交应用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
</li>
<li><p>之后便可以在hadoop102节点的8088的Web UI上查看到跑的Spark应用</p>
</li>
<li><p>配置历史服务器</p>
<ul>
<li><p>参照Standalone模式的spark-env.sh配置</p>
</li>
<li><p>修改spark-defaults.conf配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.historyServer.address=hadoop101:18080</span><br><span class="line">spark.history.ui.port=18080</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>k8s以及Mesos模式</p>
<ul>
<li><p>Mesos介绍：Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核，在Twitter得到广泛使用，管理着Twitter超过300000台服务器上的应用部署。但国内依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但原理其实都差不多<br><img src="Mesos%E6%A1%86%E6%9E%B6%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Mesos框架架构图"></p>
</li>
<li><p>k8s模式：容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes(k8s)，Spark也在最近的版本中支持了k8s部署模式。具体介绍网址如下：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p>
</li>
</ul>
</li>
<li><p>部署模式对比</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>机器数</th>
<th>需启动的进程</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>Local</td>
<td>1</td>
<td>无</td>
<td>Spark</td>
</tr>
<tr>
<td>Standalone</td>
<td>3</td>
<td>Master及Worker</td>
<td>单独部署</td>
</tr>
<tr>
<td>Yarn</td>
<td>1</td>
<td>Yarn以及HDFS</td>
<td>混合部署</td>
</tr>
</tbody></table>
</li>
<li><p>端口号总结</p>
<ul>
<li>Spark查看当前Spark-shell运行任务情况端口号：4040(计算)</li>
<li>Spark Master内部通信服务端口号：7077</li>
<li>Standalone模式下，Spark Master Web端口号：8080(资源)</li>
<li>Spark历史服务器端口号：18080</li>
<li>Hadoop YARN任务运行情况查看端口号：8088</li>
</ul>
</li>
</ul>
<h2 id="Spark核心编程"><a href="#Spark核心编程" class="headerlink" title="Spark核心编程"></a>Spark核心编程</h2><ul>
<li><p>基本介绍：Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li>RDD : 弹性分布式数据集</li>
<li>累加器：分布式共享<strong>只写</strong>变量</li>
<li>广播变量：分布式共享<strong>只读</strong>变量</li>
</ul>
</li>
<li><p>RDD</p>
<ul>
<li><p>基本介绍：RDD(Resilient Distributed Dataset)弹性分布式数据集，是Spark中最基本的数据处理模型。Scala代码中是一个抽象类，它代表一个弹性的、不可变、可分区并且其中元素可并行计算的集合</p>
</li>
<li><p>重点：</p>
<ul>
<li>弹性：<ul>
<li>存储的弹性：内存与磁盘的自动切换</li>
<li>容错的弹性：数据丢失可以自动恢复</li>
<li>计算的弹性：计算出错重试机制</li>
<li>分片的弹性：可根据需要重新分片</li>
</ul>
</li>
<li>分布式：数据存储在大数据集群(hadoop的HDFS集群)不同节点上</li>
<li>数据集：RDD封装了计算逻辑，并不保存数据</li>
<li>数据抽象：RDD是一个抽象类，需要子类具体实现</li>
<li>不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</li>
<li>可分区、并行计算</li>
</ul>
</li>
<li><p>基础编程</p>
<ul>
<li><p>RDD创建</p>
<ul>
<li><p>从集合(内存)中创建RDD：两个方法parallelize()和makeRDD()，其中后者只是包装了前者</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="comment">// 从内存中创建RDD</span></span><br><span class="line"><span class="comment">// 1、parallelize:并行</span></span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.parallelize(list)</span><br><span class="line">println(rdd.collect().mkString(<span class="string">","</span>))</span><br><span class="line"><span class="comment">// makeRDD底层代码就是调用了parallelize,只是为了方便理解</span></span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(list)</span><br><span class="line">println(rdd1.collect().mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>
</li>
<li><p>从外部存储(文件)创建RDD：包括本地文件系统、所有Hadoop支持的数据集(HDFS、HBase等)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="comment">// 从磁盘File中创建RDD</span></span><br><span class="line"><span class="comment">// path：读取文件(目录)的路径</span></span><br><span class="line"><span class="comment">// 相对路径,如果是IDEA,那么是从项目根开始查找</span></span><br><span class="line"><span class="comment">// path路径根据环境的不同自动发生改变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark读取文件时,默认采用Hadoop读取文件的规则——一行一行读取</span></span><br><span class="line"><span class="comment">// 指向文件目录,目录的文本文件都会被读取</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取目录</span></span><br><span class="line"><span class="comment">// val fileRDD: RDD[String] = sparkContext.textFile(path = "input")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取指定文件</span></span><br><span class="line"><span class="comment">// val fileRDD: RDD[String] = sparkContext.textFile(path = "input/w.txt")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取通配符文件</span></span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(path = <span class="string">"input/word*.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 文件路径还可以指向第三方存储系统：HDFS</span></span><br><span class="line"><span class="comment">// val fileRDD: RDD[String] = sparkContext.textFile(path = "hdfs://input/word*.txt")</span></span><br><span class="line">println(fileRDD.collect().mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>
</li>
<li><p>从其他RDD创建：通过一个RDD运算完后，再产生新的RDD</p>
</li>
<li><p>直接创建RDD(new)：使用new的方式直接构造RDD，一般由Spark框架自身使用</p>
</li>
</ul>
</li>
<li><p>RDD并行度与分区：</p>
<ul>
<li><p>基本介绍：默认情况下，Spark可以将一个作业切分成多个任务(Task)后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。该数量可以在构建RDD时指定。这里的并行执行的任务数量并不是指的切分任务的数量，不要混淆了</p>
</li>
<li><p>案例：</p>
<ul>
<li><p>内存分区案例1：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从内存中创建RDD</span></span><br><span class="line"><span class="comment">// makeRDD</span></span><br><span class="line"><span class="comment">// 参数1：seq: Seq[T] 数据源</span></span><br><span class="line"><span class="comment">// 参数2：numSlices: Int = defaultParallelism(默认并行度——分区的数量)</span></span><br><span class="line"><span class="comment">// 简单总结：RDD中分区的数量就是并行度,设定并行度就是在设定分区数量</span></span><br><span class="line"><span class="comment">// scheduler.conf.getInt("spark.default.parallelism", totalCores)</span></span><br><span class="line"><span class="comment">// 并行度默认会从Spark配置信息中获取spark.default.parallelism的值</span></span><br><span class="line"><span class="comment">// 如果获取不到指定参数,会采用默认值totalCores——机器的总核数</span></span><br><span class="line"><span class="comment">// 机器总核数= 当前环境中可用核数</span></span><br><span class="line"><span class="comment">// local -&gt; 单核(单线程) -&gt; 1</span></span><br><span class="line"><span class="comment">// local[4] -&gt; 4核(4个线程) -&gt; 4</span></span><br><span class="line"><span class="comment">// local[*] -&gt; 当前最大核数 -&gt; 8</span></span><br><span class="line"><span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment">// println(rdd.collect().mkString(","))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将RDD的处理后的数据保存到分区文件中</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>
</li>
<li><p>内存分区案例2：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 12,34</span></span><br><span class="line"><span class="comment">// 内存中的集合按照平均分的方式进行分区处理</span></span><br><span class="line"><span class="comment">// val rdd = sparkContext.makeRDD(List(1, 2, 3, 4), 2)</span></span><br><span class="line"><span class="comment">// rdd.saveAsTextFile("output1")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1234</span></span><br><span class="line"><span class="comment">// 1,2,34</span></span><br><span class="line"><span class="comment">// 12345</span></span><br><span class="line"><span class="comment">// 1,23,45</span></span><br><span class="line"><span class="comment">// saveAsTextFile方法如果文件已存在,会发生错误</span></span><br><span class="line"><span class="comment">// 内存中数据的分区基本上就是平均分,如果不能整除,会采用一个基本的算法实现分配</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>,<span class="number">5</span>), <span class="number">3</span>)</span><br><span class="line">rdd1.saveAsTextFile(<span class="string">"output2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1,2,3,4</span></span><br><span class="line"><span class="comment">// val rdd2 = sparkContext.makeRDD(List(1, 2, 3, 4), 4)</span></span><br><span class="line"><span class="comment">// rdd2.saveAsTextFile("output3")</span></span><br><span class="line"></span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure>
</li>
<li><p>文件分区案例1：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// textFile</span></span><br><span class="line"><span class="comment">// 参数1 path：读取文件的路径</span></span><br><span class="line"><span class="comment">// 参数2 minPartitions：最小分区数量</span></span><br><span class="line"><span class="comment">// minPartitions默认值为math.min(defaultParallelism, 2)</span></span><br><span class="line"><span class="comment">// 其中defaultParallelism是totalCores</span></span><br><span class="line"><span class="comment">//    val fileRDD1 = sparkContext.textFile("input/w.txt")</span></span><br><span class="line"><span class="comment">//    fileRDD1.saveAsTextFile("output")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    val fileRDD2 = sparkContext.textFile("input/w.txt", 1)</span></span><br><span class="line"><span class="comment">//    fileRDD2.saveAsTextFile("output2")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1、Spark读取文件采用的是Hadoop的读取规则</span></span><br><span class="line"><span class="comment">// 文件切片规则：以字节方式来切片</span></span><br><span class="line"><span class="comment">// 数据读取规则：以行为单位来读取</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2、问题：</span></span><br><span class="line"><span class="comment">//     文件到底切成几片(分区的数量)</span></span><br><span class="line"><span class="comment">//     文件字节数,预计切片数量(2)</span></span><br><span class="line"><span class="comment">// 所谓的最小分区数,取决于总的字节数是否能整除分区数并且剩余的字节小于一定比率(10%,hadoop方式)</span></span><br><span class="line"><span class="comment">// 实际产生的分区数量可能大于最小分区数</span></span><br><span class="line"><span class="keyword">val</span> fileRDD1 = sparkContext.textFile(<span class="string">"input/w.txt"</span>, <span class="number">2</span>)</span><br><span class="line">fileRDD1.saveAsTextFile(<span class="string">"output3"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区的数据如何存储?</span></span><br><span class="line"><span class="comment">// 分区数据是以行为单位读取的,不是以字节</span></span><br><span class="line"><span class="comment">// 数据是以行的方式读取,但是会考虑偏移量(数据的offset)的设置</span></span><br><span class="line"><span class="comment">// 1@@ =&gt; 012</span></span><br><span class="line"><span class="comment">// 2@@ =&gt; 345</span></span><br><span class="line"><span class="comment">// 3@@ =&gt; 678</span></span><br><span class="line"><span class="comment">// 4   =&gt; 9</span></span><br><span class="line"><span class="comment">// 10 byte / 4 = 2 .... 2 =&gt; 5</span></span><br><span class="line"><span class="comment">// 以行为单位...</span></span><br><span class="line"><span class="comment">// 以下左右都是闭区间(取得到)</span></span><br><span class="line"><span class="comment">// 0 =&gt; (0, 2) =&gt; 1</span></span><br><span class="line"><span class="comment">// 1 =&gt; (2, 4) =&gt; 2</span></span><br><span class="line"><span class="comment">// 2 =&gt; (4, 6) =&gt; 3</span></span><br><span class="line"><span class="comment">// 3 =&gt; (6, 8) =&gt;</span></span><br><span class="line"><span class="comment">// 4 =&gt; (8,10) =&gt; 4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    val fileRDD3 = sparkContext.textFile("input/w.txt", 4)</span></span><br><span class="line"><span class="comment">//    fileRDD3.saveAsTextFile("output3")</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    val fileRDD4 = sparkContext.textFile("input/w.txt", 3)</span></span><br><span class="line"><span class="comment">//    fileRDD4.saveAsTextFile("output4")</span></span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>文件分区案例2：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6 / 2 = 3</span></span><br><span class="line"><span class="comment">// (0 , 0 + 3)</span></span><br><span class="line"><span class="comment">// (3 , 3 + 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1@@ =&gt; 012</span></span><br><span class="line"><span class="comment">// 234 =&gt; 345</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// hadoop分区是以文件为单位进行划分的</span></span><br><span class="line"><span class="comment">// 读取数据不能跨越文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 10 / 3 = 3 ... 1 =&gt; 4</span></span><br><span class="line"><span class="comment">// (0,3) (3,6)</span></span><br><span class="line"><span class="keyword">val</span> fileRDD1 = sparkContext.textFile(<span class="string">"input"</span>, <span class="number">3</span>)</span><br><span class="line">fileRDD1.saveAsTextFile(<span class="string">"output"</span>)</span><br><span class="line"></span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>分区原理</p>
<ul>
<li><p>读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">  (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">    <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">    <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">    (start, end)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> InputSplit[] getSplits(JobConf job, <span class="keyword">int</span> numSplits)</span><br><span class="line">  <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">  <span class="keyword">for</span> (FileStatus file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">    <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Not a file: "</span>+ file.getPath());</span><br><span class="line">    &#125;</span><br><span class="line">    totalSize += file.getLen();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">  <span class="keyword">long</span> minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">    FileInputFormat.SPLIT_MINSIZE, <span class="number">1</span>), minSplitSize);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (FileStatus file: files) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">      <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">      <span class="keyword">long</span> splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">computeSplitSize</span><span class="params">(<span class="keyword">long</span> goalSize, <span class="keyword">long</span> minSize, <span class="keyword">long</span> blockSize)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>RDD转换算子：RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型</p>
<ul>
<li><p>Value类型</p>
<ul>
<li><p>map</p>
<ul>
<li><p>函数签名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>函数说明：将处理的数据逐条进行映射转换(可以是类型的转换,也可以是值的转换)</p>
</li>
<li><p>案例1：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Spark - RDD - 算子(方法)</span></span><br><span class="line"><span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 旧RDD -&gt; 转换算子 -&gt; 新RDD</span></span><br><span class="line"><span class="comment">// 转换算子能将旧的RDD通过方法转换为新的RDD,但是不会触发作业的执行</span></span><br><span class="line"><span class="comment">// 分区问题</span></span><br><span class="line"><span class="comment">// RDD中有分区列表</span></span><br><span class="line"><span class="comment">// 默认分区数量不变,数据会转换后输出</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.map(_ * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line"><span class="comment">// collect方法不会转换RDD,会触发作业的执行</span></span><br><span class="line"><span class="comment">// 所以将collect这样的方法称之为行动(action)算子</span></span><br><span class="line"><span class="comment">//    println(rdd1.collect.mkString(","))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    rdd1.saveAsTextFile("output")</span></span><br><span class="line">println(rdd1.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>案例2：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.map(x =&gt; &#123;</span><br><span class="line">  println(<span class="string">s"Map 1st : <span class="subst">$x</span>"</span>)</span><br><span class="line">  x</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.map(x =&gt; &#123;</span><br><span class="line">  println(<span class="string">s"Map 2nd : <span class="subst">$x</span>"</span>)</span><br><span class="line">  x</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// (1, 2)           1(1)           1(2) 2(1) 2(2)</span></span><br><span class="line"><span class="comment">// (3, 4) 3(1) 3(2)      4(1) 4(2)</span></span><br><span class="line"><span class="comment">// 分区内数据按照顺序依次执行,每一条数据的所有逻辑全部执行完毕后才会执行下一条数据</span></span><br><span class="line"><span class="comment">// 分区间数据执行没有顺序,而且无需等待</span></span><br><span class="line">println(rdd2.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>小功能：从服务器日志数据apache.log中获取用户请求URL资源路径</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从服务器日志数据apache.log中获取用户请求URL资源路径</span></span><br><span class="line"><span class="keyword">val</span> fileRDD = sparkContext.textFile(<span class="string">"input/apache.log"</span>)</span><br><span class="line"><span class="keyword">val</span> urlRDD = fileRDD.map(</span><br><span class="line">  line =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> datas = line.split(<span class="string">" "</span>)</span><br><span class="line">    datas(<span class="number">6</span>)</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">urlRDD.collect.foreach(println)</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>mapPartitions</p>
<ul>
<li><p>函数签名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">  f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">  preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据)</p>
</li>
<li><p>案例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// mapPartitions</span></span><br><span class="line"><span class="comment">// 以分区为单位进行计算,和map算子很相似</span></span><br><span class="line"><span class="comment">// 区别就在于map算子是一个一个执行,mapPartitions是一个分区一个分区执行</span></span><br><span class="line"><span class="comment">// 类似于批处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// map方法是全量数据操作,不能丢失数据</span></span><br><span class="line"><span class="comment">// mapPartitions一次性获取分区的所有数据,那么可以执行迭代器集合的所有操作(filter、max、sum)</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"><span class="comment">//    val rdd = dataRDD.mapPartitions(iter =&gt; &#123;</span></span><br><span class="line"><span class="comment">//      iter.map(_ * 2)</span></span><br><span class="line"><span class="comment">//    &#125;)</span></span><br><span class="line"><span class="comment">//    println(rdd.collect.mkString(","))</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitions(iter =&gt; &#123;</span><br><span class="line">  iter.filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">&#125;)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>小功能：获取每个数据分区的最大值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取每个数据分区的最大值</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitions(iter =&gt; <span class="type">List</span>(iter.max).iterator)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>mapPartitionsWithIndex</p>
<ul>
<li><p>函数签名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">  f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">  preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>函数说明：将待处理的数据以分区为单位发送到计算节点进行处理(可以进行任意的处理,哪怕是过滤数据)，在处理时同时可以获取当前分区索引</p>
</li>
<li><p>案例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取每个分区最大值以及分区号</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>), <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitionsWithIndex(</span><br><span class="line">  (index, iter) =&gt; &#123;</span><br><span class="line">    <span class="type">List</span>((index, iter.max)).iterator</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>小功能：获取第二个数据分区的数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取第二个数据分区的数据</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取的分区索引是从0开始的</span></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.mapPartitionsWithIndex(</span><br><span class="line">  (index, iter) =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (index == <span class="number">1</span>) iter</span><br><span class="line">    <span class="keyword">else</span> <span class="type">Nil</span>.iterator</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>flatMap</p>
<ul>
<li><p>函数签名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>函数说明：将处理的数据进行扁平化后再进行映射处理，也称之为扁平映射</p>
</li>
<li><p>案例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"spark-sobxiong"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(</span><br><span class="line">  <span class="type">List</span>(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.flatMap(list =&gt; list)</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
<li><p>小功能：将List(List(1, 2), 3, List(4, 5))进行扁平化操作</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"rddMemory"</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(</span><br><span class="line">  <span class="type">List</span>(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="number">3</span>, <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = dataRDD.flatMap &#123;</span><br><span class="line">  <span class="keyword">case</span> list: <span class="type">List</span>[_] =&gt; list</span><br><span class="line">  <span class="keyword">case</span> d =&gt; <span class="type">List</span>(d)</span><br><span class="line">&#125;</span><br><span class="line">println(rdd.collect.mkString(<span class="string">","</span>))</span><br><span class="line">sparkContext.stop</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>核心属性</p>
</li>
<li><p>执行原理</p>
</li>
</ul>
</li>
</ul>

          
            <div class='article_footer'>
              
                
  
    
    



  

  
    
    



  

  
    
    

<section class="widget copyright  desktop mobile">
  <div class='content'>
    
      <blockquote>
        
          
            <p>博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</p>

          
        
          
            <p>本文永久链接是：<a href=https://sobxiong.github.io/2020/09/04/BigData/Spark/>https://sobxiong.github.io/2020/09/04/BigData/Spark/</a></p>
          
        
      </blockquote>
    
  </div>
</section>

  

  
    
    

<section class="widget qrcode  desktop mobile">
  

  <div class='content article-entry'>
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
  </div>
</section>

  


              
            </div>
          
        </div>
        
          


  <section class='meta' id="footer-meta">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2020-09-16T23:11:18+08:00">
  <a class='notlink'>
    <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：2020年9月16日</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>大数据</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/Spark/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>Spark</p></a></div>


        
      
        
          

        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=https://sobxiong.github.io/2020/09/04/BigData/Spark/&title=Spark - SOBXiong的博客&summary=内容
Spark概述
Spark快速上手
Spark运行环境
Spark核心编程
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://sobxiong.github.io/2020/09/04/BigData/Spark/&title=Spark - SOBXiong的博客&summary=内容
Spark概述
Spark快速上手
Spark运行环境
Spark核心编程
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=https://sobxiong.github.io/2020/09/04/BigData/Spark/&title=Spark - SOBXiong的博客&summary=内容
Spark概述
Spark快速上手
Spark运行环境
Spark核心编程
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
          <div class="prev-next">
            
              <a class='prev' href='/2020/09/06/Spring/SpringCloud/SpringCloud%E7%AC%AC%E4%B8%80%E5%AD%A3/'>
                <p class='title'><i class="fas fa-chevron-left" aria-hidden="true"></i>SpringCloud第一季</p>
                <p class='content'>内容
微服务概述



微服务概述
微服务是什么：微服务的核心就是将传统的一站式应用根据业务拆分成一个一个的服务，彻底地去耦合。每一个微服务提供单个业务功能的服务，一个服务做一件事。从技术角度看...</p>
              </a>
            
            
              <a class='next' href='/2020/07/24/language/Scala/Scala/'>
                <p class='title'>Scala<i class="fas fa-chevron-right" aria-hidden="true"></i></p>
                <p class='content'>内容
Scala概述
变量
运算符
程序流程控制
函数式编程基础
面向对象编程-基础
面向对象编程-中级



Scala概述
学习Scala的原因

Spark是新一代内存级大数据计算框架，是...</p>
              </a>
            
          </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  <article class="post white-box reveal comments shadow">
    <section class="article typo">
      <p ct><i class='fas fa-comments'></i> 评论</p>
      
      
      
      
      
      
        <section id="comments">
          <div id="valine_container" class="valine_thread">
            <i class="fas fa-cog fa-spin fa-fw fa-2x"></i>
          </div>
        </section>
      
      
    </section>
  </article>


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->



  <script>
    window.subData = {
      title: 'Spark',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
  

  
    
    



  <section class="widget toc-wrapper shadow desktop mobile" id="toc-div" >
    
  <header>
    
      <i class="fas fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#内容"><span class="toc-text">内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark概述"><span class="toc-text">Spark概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark快速上手"><span class="toc-text">Spark快速上手</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark运行环境"><span class="toc-text">Spark运行环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark核心编程"><span class="toc-text">Spark核心编程</span></a></li></ol>
    </div>
  </section>


  


</aside>


  
  <footer class="clearfix">
    <br><br>
    
      
        <br>
        <div class="social-wrapper">
          
            
              <a href="mailto:1942991710@qq.com"
                class="social fas fa-envelope flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
            
              <a href="https://github.com/SOBXiong"
                class="social fab fa-github flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
        </div>
      
    
      
        <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
      
    
      
        <div class='copyright'>
        <p><a href="http://xiongjc.top" target="_blank" rel="noopener">Copyright © 2017-2020 SOBXiong</a></p>

        </div>
      
    
  </footer>

<script>setLoadingBarProgress(80);</script>


      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4/dist/jquery.min.js"></script>


  <script>
    
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/" || "/";
    if (!ROOT.endsWith('/')) ROOT += '/';
  </script>





  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/instant_page.js" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>


  <script src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.6/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      ScrollReveal().reveal('.l_main .reveal', {
        distance: '8px',
        duration: '800',
        interval: '100',
        scale: '1'
      });
    });
  </script>


  
<script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>

  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script defer src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>



  
  
  
    
<script src="https://cdn.jsdelivr.net/npm/jquery-backstretch@2.1.18/jquery.backstretch.min.js"></script>

    <script type="text/javascript">
      $(function(){
        var imgs=["https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg"];
        if ('true' == 'true') {
          function shuffle(arr){
            /*From countercurrent-time*/
            var n = arr.length;
            while(n--) {
              var index = Math.floor(Math.random() * n);
              var temp = arr[index];
              arr[index] = arr[n];
              arr[n] = temp;
            }
          }
          shuffle(imgs);
        }
        if ('.cover') {
          $('.cover').backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        } else {
          $.backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        }
      });
    </script>
  



  
    
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js"></script>

  
    
<script src="https://cdn.jsdelivr.net/npm/meting@2.0/dist/Meting.min.js"></script>

  









  
    
<script src="https://cdn.jsdelivr.net/npm/valine@1.4/dist/Valine.min.js"></script>

  
  <script>
  var GUEST_INFO = ['nick','mail','link'];
  var meta = 'nick,mail,link'.split(',').filter(function(item){
    return GUEST_INFO.indexOf(item) > -1
  });
  var REQUIRED_FIELDS = ['nick','mail','link'];
  var requiredFields = 'nick,mail'.split(',').filter(function(item){
    return REQUIRED_FIELDS.indexOf(item) > -1
  });
  var valine = new Valine();
  function emoji(path, idx, ext) {
      return path + "/" + path + "-" + idx + "." + ext;
  }
  var emojiMaps = {};
  for (var i = 1; i <= 54; i++) {
    emojiMaps['tieba-' + i] = emoji('tieba', i, 'png');
  }
  for (var i = 1; i <= 101; i++) {
    emojiMaps['qq-' + i] = emoji('qq', i, 'gif');
  }
  for (var i = 1; i <= 116; i++) {
    emojiMaps['aru-' + i] = emoji('aru', i, 'gif');
  }
  for (var i = 1; i <= 125; i++) {
    emojiMaps['twemoji-' + i] = emoji('twemoji', i, 'png');
  }
  for (var i = 1; i <= 4; i++) {
    emojiMaps['weibo-' + i] = emoji('weibo', i, 'png');
  }
  valine.init({
    el: '#valine_container',
    meta: meta,
    
    appId: "dogUA2FSKGTo029M1SEwGROT-MdYXbMMI",
    appKey: "u0NdtQ8nvHoMdJPSYqm1LRxE",
    placeholder: "快来评论吧~",
    pageSize:'10',
    avatar:'robohash',
    lang:'zh-cn',
    visitor: 'true',
    highlight: 'true',
    mathJax: 'false',
    enableQQ: 'true',
    requiredFields: requiredFields,
    emojiCDN: 'https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/emoji/valine/',
    emojiMaps: emojiMaps
  })
  </script>





  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2.6.5/js/app.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2.6.5/js/search.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/comment_typing.js"></script>






<!-- 复制 -->

  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="fas fa-copy"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-check-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-check-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-times-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-times-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>




<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  function pjax_fancybox() {
    $(".article-entry").find("img").not('.inline').not('a img').each(function () { //渲染 fancybox
      var element = document.createElement("a"); // a 标签
      $(element).attr("pjax-fancybox", "");  // 过滤 pjax
      $(element).attr("href", $(this).attr("src"));
      if ($(this).attr("data-original")) {
        $(element).attr("href", $(this).attr("data-original"));
      }
      $(element).attr("data-fancybox", "images");
      var caption = "";   // 描述信息
      if ($(this).attr('alt')) {  // 标准 markdown 描述信息
        $(element).attr('data-caption', $(this).attr('alt'));
        caption = $(this).attr('alt');
      }
      var div = document.createElement("div");
      $(div).addClass("fancybox");
      $(this).wrap(div); // 最外层套 div ，其实主要作用还是 class 样式
      var span = document.createElement("span");
      $(span).addClass("image-caption");
      $(span).text(caption); // 加描述
      $(this).after(span);  // 再套一层描述
      $(this).wrap(element);  // 最后套 a 标签
    })
    $(".article-entry").find("img").fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
      closeClick: true,
      helpers: {
        overlay: {closeClick: true}
      },
      buttons: [
        "zoom",
        "close"
      ]
    });
  };
  $(function () {
    pjax_fancybox();
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
