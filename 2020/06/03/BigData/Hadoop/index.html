<!DOCTYPE html>
<html>
<head hexo-theme='https://volantis.js.org/#2.6.6'>
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
    
  
  <!-- 渲染优化 -->
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- 页面元数据 -->
  
    <title>Hadoop - SOBXiong的博客</title>
  
    <meta name="keywords" content="BigData">
  
  
    <meta name="description" content="内容
概论
Hadoop介绍
环境搭建
Hadoop运行模式
Hadoop编译源码
HDFS概述
HDFS的Shell操作
HDFS客户端操作
HDFS的数据流
NameNode和SecondaryNameNode
DataNode
HDFS2.X新特性
MapReduce概述
Hadoop序列化
MapRedu...">
  

  <!-- feed -->
  

  <!-- import meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13/css/all.min.css">
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">

  

  
  <link rel="shortcut icon" type='image/x-icon' href="https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/favicon.png">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css">
  

  

  <!-- import link -->
  

  
  
    
<link rel="stylesheet" href="/css/style.css">

  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
</head>

<body>
  
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>
<header class="l_header shadow blur">
  <div class='container'>
  <div class='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h'>
        <li><a class="s-comment fas fa-comments fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
          <li><a class="s-toc fas fa-list fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
          
            <i class='https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/favicon.png'></i>
          
          
            SOBXiong
          
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h'>
          
          
          
            
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
				</ul>
			</div>

      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fas fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>

			<ul class='switcher nav-list-h'>
				
					<li><a class="s-search fas fa-search fa-fw" target="_self" href='javascript:void(0)'></a></li>
				
				<li>
          <a class="s-menu fas fa-bars fa-fw" target="_self" href='javascript:void(0)'></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
          </ul>
        </li>
			</ul>
		</div>
	</div>
  </div>
</header>

<script>setLoadingBarProgress(40);</script>



  <div class="l_body nocover">
    <div class='body-wrapper'>
      

<div class='l_main'>
  

  
    <article id="post" class="post white-box reveal shadow article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
      
      
      <div class="meta" id="header-meta">
        
          
  <h1 class="title">
    <a href="/2020/06/03/BigData/Hadoop/">
      Hadoop
    </a>
  </h1>


        
        <div class='new-meta-box'>
          
            
          
            
              
<div class='new-meta-item author'>
  <a href="http://xiongjc.top" target="_blank" rel="nofollow noopener">
    <img src="https://cdn.jsdelivr.net/gh/SOBXiong/imageBed/hdImg_f757fa7e0fd65077ce52d251fc7a01d815860762755.jpg">
    <p>SOBXiong</p>
  </a>
</div>

            
          
            
              
  
  <div class='new-meta-item category'>
    <a href='/categories/%E7%BC%96%E7%A8%8B/' rel="nofollow">
      <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>
      <p>编程</p>
    </a>
  </div>


            
          
            
              <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：2020年6月3日</p>
  </a>
</div>

            
          
            
              

            
          
        </div>
        
          <hr>
        
      </div>
    
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          
          <h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul>
<li><a href="#概论">概论</a></li>
<li><a href="#Hadoop介绍">Hadoop介绍</a></li>
<li><a href="#环境搭建">环境搭建</a></li>
<li><a href="#Hadoop运行模式">Hadoop运行模式</a></li>
<li><a href="#Hadoop编译源码">Hadoop编译源码</a></li>
<li><a href="#HDFS概述">HDFS概述</a></li>
<li><a href="#HDFS的Shell操作">HDFS的Shell操作</a></li>
<li><a href="#HDFS客户端操作">HDFS客户端操作</a></li>
<li><a href="#HDFS的数据流">HDFS的数据流</a></li>
<li><a href="#NameNode和SecondaryNameNode">NameNode和SecondaryNameNode</a></li>
<li><a href="#DataNode">DataNode</a></li>
<li><a href="#HDFS2.X新特性">HDFS2.X新特性</a></li>
<li><a href="#MapReduce概述">MapReduce概述</a></li>
<li><a href="#Hadoop序列化">Hadoop序列化</a></li>
<li><a href="#MapReduce框架原理">MapReduce框架原理</a></li>
<li><a href="#Hadoop数据压缩">Hadoop数据压缩</a></li>
<li><a href="#Yarn资源调度器">Yarn资源调度器</a></li>
<li><a href="#Hadoop企业优化">Hadoop企业优化</a></li>
<li><a href="#MapReduce扩展案例">MapReduce扩展案例</a></li>
<li><a href="#常见错误及解决方案">常见错误及解决方案</a></li>
</ul>
<a id="more"></a>

<h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><ul>
<li><p>概念：大数据指<strong>无法在一定时间范围</strong>内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的<strong>海量、高增长率和多样化的信息资产</strong>。需要解决的问题：海量数据的<strong>存储</strong>和海量数据的<strong>分析计算</strong>问题。</p>
</li>
<li><p>大数据特点(4V)：</p>
<ul>
<li>Volume(大量)</li>
<li>Velocity(高速)</li>
<li>Variety(多样)：<strong>结构化/非结构化数据</strong>，结构化数据以数据库/文本为主，非结构化数据包括网络日志、音频、视频、图片和地理位置信息等。</li>
<li>Value(低价值密度)：价值密度的高度与数据总量的大小成反比，如何<strong>快速对有价值数据“提纯”称为目前大数据背景下待解决的难题</strong>。</li>
</ul>
</li>
<li><p>大数据应用场景：物流仓储、零售、旅游、商品广告推荐、保险、金融、房产、人工智能</p>
</li>
<li><p>大数据部门业务流程：<br>产品人员提需求(统计总用户数、日活跃用户数、回流用户数等) =&gt; 数据部门搭建数据平台、分析数据指标 =&gt; 数据可视化(报表展示、邮件发送、大屏幕展示等)</p>
</li>
<li><p>大数据部门组织结构：<br><img src="%E9%83%A8%E9%97%A8%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84.png" alt="部门组织结构"></p>
</li>
</ul>
<h2 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h2><ul>
<li><p>Hadoop是什么：</p>
<ul>
<li>是一个由Apache基金会开发的<strong>分布式系统基础架构</strong>。</li>
<li>主要解决海量数据的<strong>存储</strong>和海量数据的<strong>分析计算</strong>问题。</li>
<li>广义上来说，Hadoop通常指更广泛的概念——Hadoop生态圈。</li>
</ul>
</li>
<li><p>Hadoop发展历史</p>
<ul>
<li><p>Lucene框架是<strong>Doug Cutting</strong>开创的开源软件，用Java编写，实现与Goole类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询和索引引擎。</p>
</li>
<li><p>2001年年底Lucene称为Apache基金会的一个子项目。</p>
</li>
<li><p>对于海量数据的场景，Lucene面对与Google同样的困难，<strong>存储数据困难，检索速度慢</strong>。</p>
</li>
<li><p>学习和模仿Google解决这些问题的办法：微型版Nutch。</p>
</li>
<li><p>Google是Hadoop的思想之源(其在大数据方面的三篇论文)<br><strong>GFS -&gt; HDFS Map-Reduce -&gt; MR BigTable -&gt; HBase</strong></p>
</li>
<li><p>2003年~04年，Google公开部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用<strong>2年业余时间</strong>实现了DFS和MapReduce机制，使Nutch性能飙升。</p>
</li>
<li><p>2005年Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。</p>
</li>
<li><p>2006年3月份，Map-Reduce和Nutch Distributed File System(NDFS)分别被纳入到Hadoop项目中，Hadoop就正式诞生，标志着大数据十代来临。</p>
</li>
<li><p>Hadoop名字来源于Doug Cutting儿子的玩具大象<br><img src="logo.jpg" alt="logo"></p>
</li>
</ul>
</li>
<li><p>Hadoop三大发行版本</p>
<ul>
<li>Apache：最原始(基础)的版本，对于入门学习最好</li>
<li>Cloudera：在大型互联网企业中用的较多，产品主要为CDH，Cloudera Manager，Cloudera Support：<ul>
<li>CDH是Cloudera的Hadoop发行版，完全开源，比Apache版本在兼容性、安全性、稳定性上有所增强。</li>
<li>Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。</li>
<li>Cloudera Support即是对Hadoop的技术支持。</li>
</ul>
</li>
<li>Hortonworks：文档较好<ul>
<li>Hortonworks的主打产品是Hortonworks Data Platform(HDP)，也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari——一款开源的安装和管理系统。</li>
<li>HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。</li>
</ul>
</li>
</ul>
</li>
<li><p>Hadoop的优势(4高)</p>
<ul>
<li>高可靠性：Hadoop底层维护多个数据副本，即使某个计算元素或存储出现故障，也不会导致数据的丢失。</li>
<li>高扩展性：在集群间分配任务数据，可方便地扩展数以千计的节点。</li>
<li>高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</li>
<li>高容错性：能够自动将失败的任务重新分配。</li>
</ul>
</li>
<li><p>Hadoop组成</p>
<ul>
<li>1.x：Common(辅助工具)、HDFS(数据存储)、MapReduce(计算+资源调度)</li>
<li>2.x：Common(辅助工具)、HDFS(数据存储)、<strong>Yarn(资源调度)</strong>、<strong>MapReduce(计算)</strong></li>
</ul>
</li>
<li><p>HDFS架构概述：</p>
<ul>
<li>HDFS全名——Hadoop Distributed File System</li>
<li>组成：<ul>
<li>NameNode(nn)：存储文件的元数据，如文件名、文件目录结构、文件属性(生成时间、副本数、文件权限)以及每个文件的块列表和块所在的DataNode等——类似书的目录(索引)</li>
<li>DataNode(dn)：在本地文件系统存储文件块数据以及块数据的校验和——具体的书章节内容</li>
<li>Secondary NameNode(2nn)：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照——辅助NameNode工作</li>
</ul>
</li>
</ul>
</li>
<li><p>Yarn架构概述<br><img src="Yarn%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Yarn架构图"></p>
</li>
<li><p>MapReduce架构概述</p>
<ul>
<li>将计算分为两个阶段：Map和Reduce</li>
<li>Map阶段并行处理输入数据</li>
<li>Reduce阶段对Map结果进行汇总</li>
</ul>
</li>
<li><p>大数据技术生态体系<br><img src="%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E7%94%9F%E6%80%81%E4%BD%93%E7%B3%BB.png" alt="大数据技术生态体系"></p>
</li>
</ul>
<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><ul>
<li><p>配置Java环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 修改环境变量</span><br><span class="line">sudo vim &#x2F;etc&#x2F;profile</span><br><span class="line">##JAVA_HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_251</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line">&#x2F;&#x2F; 让环境变量修改生效</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">&#x2F;&#x2F; 查看Java版本</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置Hadoop环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 修改环境变量</span><br><span class="line">sudo vim &#x2F;etc&#x2F;profile</span><br><span class="line">##HADOOP_HOME</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-3.1.3</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin</span><br><span class="line">&#x2F;&#x2F; 让环境变量修改生效</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">&#x2F;&#x2F; 查看Hadoop版本</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hadoop目录说明</p>
<ul>
<li>bin目录：存放对Hadoop相关服务(HDFS,YARN)进行操作的脚本</li>
<li>etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</li>
<li>lib目录：存放Hadoop的本地库(对数据进行压缩解压缩功能)</li>
<li>sbin目录：存放启动或停止Hadoop相关服务的脚本</li>
<li>share目录：存放Hadoop的依赖jar包、文档、和官方案例</li>
</ul>
</li>
</ul>
<h2 id="Hadoop运行模式"><a href="#Hadoop运行模式" class="headerlink" title="Hadoop运行模式"></a>Hadoop运行模式</h2><ul>
<li><p>本地模式</p>
<ul>
<li>官方WordCount案例(统计单词数目)：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 创建wcinput文件夹</span><br><span class="line">mkdir wcinput</span><br><span class="line">&#x2F;&#x2F; 创建wc.input文件</span><br><span class="line">cd wcinput</span><br><span class="line">touch wc.input</span><br><span class="line">&#x2F;&#x2F; 编辑wc.input随意输入字符</span><br><span class="line">vim wc.input</span><br><span class="line">&#x2F;&#x2F; 回到Hadoop目录执行程序</span><br><span class="line">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput&#x2F; wcouput</span><br><span class="line">&#x2F;&#x2F; 查看结果</span><br><span class="line">cat wcoutput&#x2F;part-r-00000</span><br></pre></td></tr></table></figure>
</li>
<li><p>伪分布式模式</p>
<ul>
<li><p>配置集群</p>
<ul>
<li>设置hadoop-env.sh：在文件中设置JAVA_HOME为本地JDK地址</li>
<li>设置core-site.xml：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>设置hdfs-site.xml：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动集群</p>
<ul>
<li>格式化NameNode：bin/hdfs namenode -format</li>
<li><strong>启动NameNode和DataNode：sbin/start-dfs.sh(关闭stop)</strong></li>
</ul>
</li>
<li><p>查看集群</p>
<ul>
<li>查看是否启动成功：jps(JDK中的命令,不是Linux命令,类似ps)</li>
<li>web端查看HDFS文件系统：<a href="http://192.168.232.100:9870" target="_blank" rel="noopener">http://192.168.232.100:9870</a>(需要CentOS主机上设置关闭防火墙,在3.x版本端口号默认为9870)</li>
<li>查看产生的log日志：cd /hadoop/logs</li>
<li>注意：不能一直格式化NameNode，格式化NameNode会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。所以，格式NameNode时，一定要先删除data数据和log日志，然后再格式化NameNode。(最好关闭NameNode和DataNode)</li>
</ul>
</li>
<li><p>操作集群(所有命令类似于在Linux Terminal的命令行操作,需要加上固定前缀bin/hdfs dfs -)</p>
<ul>
<li>在HDFS文件系统上创建一个input文件夹：bin/hdfs dfs -mkdir -p /user/sobxiong/input</li>
<li>将测试文件内容上传到文件系统上：bin/hdfs dfs -put wcinput/wc.input /user/sobxiong/input/</li>
<li>查看上传的文件是否正确：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -ls /user/sobxiong/input/</span><br><span class="line">bin/hdfs dfs -cat /user/sobxiong/input/wc.input</span><br></pre></td></tr></table></figure>

<ul>
<li>运行MapReduce程序：hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/sobxiong/input/ /user/sobxiong/output</li>
<li>查看输出结果：bin/hdfs dfs -cat /user/sobxiong/output/*</li>
<li>也可以在浏览器的文件系统中查看  </li>
<li>将测试文件内容下载到本地：bin/hdfs dfs -get /user/sobxiong/output/part-r-00000 ./wcoutput/</li>
<li>删除输出结果：bin/hdfs dfs -rm -r /user/sobxiong/output</li>
</ul>
</li>
<li><p>启动Yarn并运行MapReduce程序</p>
<ul>
<li><p>配置集群</p>
<ul>
<li>配置yarn-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251</li>
<li>配置yarn-site.xml</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置Yarn应用的classPath --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 不配置出现：错误: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>命令行下输入hadoop classpath的一长串环境<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 不需要设置yarn.resourcemanager.hostname，可以直接通过ip+端口号的方式访问 --&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>配置mapred-env.sh(配置JAVA_HOME)： export JAVA_HOME=/opt/module/jdk1.8.0_251</li>
<li>配置mapred-site.xml：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在YARN上 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 默认是local，本地文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动集群</p>
<ul>
<li>启动前必须保证NameNode和DataNode已启动</li>
<li><strong>启动ResourceManager和NodeManager：sbin/start-yarn.sh(关闭stop)</strong></li>
</ul>
</li>
<li><p>集群操作</p>
<ul>
<li>yarn浏览器页面查看：8088端口</li>
<li>删除文件系统上的output文件：bin/hdfs dfs -rm -r /user/sobxiong/output</li>
<li>执行MapReduce程序：同上hadoop操作</li>
<li>查看结果：同上cat操作，也可以在浏览器端查看</li>
</ul>
</li>
</ul>
</li>
<li><p>配置历史服务器</p>
<ul>
<li><p>配置mapred-site.xml：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>172.16.85.130:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>172.16.85.130:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动历史服务器：bin/mapred –daemon start historyserver(stop关闭)</p>
</li>
<li><p>查看历史服务器是否启动：jps</p>
</li>
<li><p>查看JobHistory：<a href="http://172.16.85.130:19888/jobhistory" target="_blank" rel="noopener">http://172.16.85.130:19888/jobhistory</a></p>
</li>
</ul>
</li>
<li><p>配置日志的聚集：</p>
<ul>
<li>概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上</li>
<li>好处：可以方便的查看到程序运行详情，方便开发调试</li>
<li>注意：开启日志聚集功能，需要重新启动NodeManager、ResourceManager和HistoryManager</li>
<li>配置yarn-site.xml：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>配置文件说明</p>
<ul>
<li><p>默认配置文件：</p>
<ul>
<li>core-defalut.xml - hadoop-common-3.1.3.jar/core-default.xml</li>
<li>hdfs-default.xml - hadoop-hdfs-3.1.3.jar/hdfs-default.xml</li>
<li>yarn-default.xml - hadoop-yarn-common-3.1.3.jar/yarn-default.xml</li>
<li>mapred-default.xml - hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</li>
</ul>
</li>
<li><p>自定义配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置(优先级高)</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>完全分布式运行模式</p>
<ul>
<li><p>虚拟机准备(3台，完全复制)</p>
</li>
<li><p>编写集群分发脚本xsync</p>
<ul>
<li><p>scp(secure copy)安全拷贝</p>
<ul>
<li>定义：scp可以实现服务器与服务器之间的数据拷贝</li>
<li>基本语法：<table>
<thead>
<tr>
<th>命令</th>
<th>参数</th>
<th>要拷贝的文件路径/名称</th>
<th>目的用户@主机:目的路径/名称</th>
</tr>
</thead>
<tbody><tr>
<td>scp</td>
<td>-r(递归)</td>
<td>$pdir/$fname</td>
<td>$user@$host:$pdir/$fname</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>rsync远程同步工具</p>
<ul>
<li>作用：主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点</li>
<li>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去</li>
<li>基本语法：<table>
<thead>
<tr>
<th>命令</th>
<th>参数</th>
<th>要拷贝的文件路径/名称</th>
<th>目的用户@主机:目的路径/名称</th>
</tr>
</thead>
<tbody><tr>
<td>rsync</td>
<td>-r(递归)v(显示复制过程)l(拷贝符号连接)</td>
<td>$pdir/$fname</td>
<td>$user@$host:$pdir/$fname</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>xsync集群分发脚本</p>
<ul>
<li><p>需求：循环复制文件到所有节点的相同目录下</p>
</li>
<li><p>需求分析：</p>
<ul>
<li>rsync命令原始：rsync -rvl /opt/module root@hadoop2:/opt/</li>
<li>期望脚本：xsync 需同步的文件名</li>
<li>说明：在home/sobxiong/bin这个目录下存放的脚本，sobxiong用户在系统任何地方都可以直接执行</li>
</ul>
</li>
<li><p>脚本实现</p>
<ul>
<li>在/home/sobxiong目录下创建bin目录，并在bin目录下创建xsync文件</li>
<li>在xsync中键入如下代码：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=2; host&lt;4; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<ul>
<li>修改脚本xsync具有执行权限：chmod 777 xsync</li>
<li>调用脚本形式：xsync 文件名</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>集群配置</p>
<ul>
<li><p>集群部署规划：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>hadoop1</th>
<th>hadoop2</th>
<th>hadoop3</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode、DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode、DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>ResourceManager、NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
</li>
<li><p>配置集群</p>
<ul>
<li>核心配置文件core-site.xml：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>HDFS配置文件hdfs-site.xml</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 副本数目 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop3:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>YARN配置文件yarn-site.xml</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>MapReduce配置文件mapred-site.xml</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MR运行在Yarn上 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在集群上分发配置好的hadoop配置文件：xsync /opt/module/hadoop-3.1.3/etc</p>
</li>
<li><p>查看文件分发情况  </p>
</li>
</ul>
</li>
<li><p>集群单点启动</p>
<ul>
<li>集群第一次启动，需要格式化NameNode：hadoop namenode -format(把log和data文件删除)</li>
<li>在hadoop1上启动NameNode：hadoop-daemon.sh start namenode</li>
<li>在hadoop1、2、3上分别启动DataNode：hadoop-daemon.sh start datanode</li>
</ul>
</li>
<li><p>SSH免密登陆配置</p>
<ul>
<li>配置ssh<ul>
<li>基本语法：ssh ip</li>
</ul>
</li>
<li>无密钥配置<ul>
<li>免密登录原理：</li>
<li>生成公钥和私钥：ssh-keygen -t rsa(生成文件id_rsa-私钥,id_rsa.pub-公钥)</li>
<li>将公钥拷贝到要免密登录的目标机器上：ssh-copy-id hadoop2(只是当前用户,root还需要另外配置)</li>
</ul>
</li>
<li>.ssh文件下(~/.ssh)的文件功能<ul>
<li>known_hosts：记录ssh访问过的计算机的公钥</li>
<li>id_rsa：生成的私钥</li>
<li>id_rsa.pub：生成的公钥</li>
<li>authorized_keys：存放授权过的无密登录服务器公钥</li>
</ul>
</li>
</ul>
</li>
<li><p>群起集群</p>
<ul>
<li><p>配置workers：vim etc/hadoop/workers；加入hadoop1、hadoop2、hadoop3(不能有空行和空格)；同步所有节点配置文件 - xsync etc/hadoop/workers</p>
</li>
<li><p>启动集群</p>
<ul>
<li>集群第一次启动，需要格式化NameNode(格式化前关闭启动的所有namenode和datanode进程,然后再删除data和log数据)：bin/hdfs namenode -format</li>
<li>启动HDFS：sbin/start-dfs.sh(在hadoop1上启动,这样hadoo1、2、3均会启动对应的进程)</li>
<li>启动YARN：sbin/start-yarn.sh(在hadoop2上启动,在ResourceManager所在机器hadoop2上启动Yarn)</li>
<li>查看NameNode：hadoop1:9870</li>
</ul>
</li>
<li><p>集群基本测试</p>
<ul>
<li>上传文件到集群：bin/hdfs dfs -put xx xx</li>
<li>查看上传文件存储位置<ul>
<li>查看HDFS文件存储路径：/opt/module/hadoop-3.1.3/data/tmp/dfs/data/current/BP-1002151198-172.16.85.130-1591848799222/current/finalized/subdir0/subdir0</li>
<li>查看HDFS在磁盘存储文件的内容：cat blk_xxx(文本文件)</li>
<li>拼接大文件：cat blk_xxx &gt;&gt; temp，最后temp就是初始的文件</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>集群启动/停止方式总结</p>
<ul>
<li>各个服务组件逐一启动/停止<ul>
<li>分别启动/停止HDFS组件：hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode</li>
<li>启动/停止YARN：yarn-daemon.sh start/stop resourcemanager/nodemanager</li>
</ul>
</li>
<li>各个模块分开启动/停止(配置ssh是前提)常用<ul>
<li>整体启动/停止HDFS：start-dfs.sh/stop-dfs.sh</li>
<li>整体启动/停止YARN：start-yarn.sh/stop-yarn.sh</li>
</ul>
</li>
</ul>
</li>
<li><p>集群时间同步</p>
<ul>
<li><p>crontab定时任务：</p>
<ul>
<li>基本语法：crontab[选项]</li>
<li>选项说明<ul>
<li>-e：编辑crontab定时任务</li>
<li>-l：查询crontab任务</li>
<li>-r：删除当前用户所有的crontab任务</li>
</ul>
</li>
<li>参数说明：***** [任务]<ul>
<li>*的含义：<ul>
<li>第一个：一小时当中的第几分钟(0~59)</li>
<li>第二个：一天当中的第几个小时(0~23)</li>
<li>第三个：一个月当中的第几天(1~31)</li>
<li>第四个：一年当中的第几月(1~12)</li>
<li>第五个：一周当中的星期几(0~7,0和7均代表星期日)</li>
</ul>
</li>
<li>特殊符号：<ul>
<li><em>：代表任何时间。比如第一个“</em>”代表一小时中每分钟都执行一次</li>
<li>,：代表不连续的时间。如“0 8,12,16 * * *”命令，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</li>
<li>-：代表连续的时间范围。比如“0 5 * * 1-6”命令，代表在周一到周六的凌晨5点0分执行命令</li>
<li><em>/n：代表每隔多久执行一次。比如“</em>/10 * * * *”命令，代表每隔10分钟就执行一遍命令</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>ntp方式进行同步</p>
<ul>
<li><p>具体思路：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。<br><img src="%E9%9B%86%E7%BE%A4%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5.png" alt="集群时间同步"></p>
</li>
<li><p>具体实操</p>
<ul>
<li><p>时间服务器配置：</p>
<ul>
<li>检查ntp是否安装：rpm -qa&#124;grep ntp，有ntp、fontpackages-filesystem以及ntpdate</li>
<li>修改ntp配置文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 授权1172.16.85.0-172.16.85.255网段上的所有机器可以从这台机器上查询和同步时间</span></span><br><span class="line">restrict 172.16.85.0 mask 172.16.85.130.0 nomodify notrap</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改集群在局域网中,不使用其他互联网上的时间</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="meta">#</span><span class="bash">server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步</span></span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>

<ul>
<li>修改/etc/sysconfig/ntpd文件：SYNC_HWCLOCK=yes(让硬件时间与系统时间一起同步)</li>
<li>重新启动ntpd服务：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service ntpd status</span><br><span class="line">service ntpd start</span><br></pre></td></tr></table></figure>

<ul>
<li>设置ntpd服务开机自启动：chkconfig ntpd on</li>
</ul>
</li>
<li><p>其他机器配置(root用户)：</p>
<ul>
<li>配置10分钟与时间服务器同步一次：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">crontab -e</span><br><span class="line">*/10 * * * * /usr/sbin/ntpdate hadoop1</span><br></pre></td></tr></table></figure>

<ul>
<li>修改任意机器时间：date -s “2020-11-11 11:11:11”</li>
<li>十分钟后查看机器是否与时间服务器同步：date</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Hadoop编译源码"><a href="#Hadoop编译源码" class="headerlink" title="Hadoop编译源码"></a>Hadoop编译源码</h2><ul>
<li><p>前期准备</p>
<ul>
<li><p>jar包准备(hadoop源码、JDK8、Maven、Ant、Protobuf)：Protobuf在Google的github中的Release页面(3.1.3Hadoop对应2.5.0版本)</p>
</li>
<li><p>jar包安装</p>
<ul>
<li>安装JDK</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u251-linux-x64.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> JAVA_HOME(/etc/profile)</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_251</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure>

<ul>
<li>安装Maven</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> MAVEN_HOME(/etc/profile)</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.6.3</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">mvn -version</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改maven仓库镜像</span></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>安装Ant</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-ant-1.10.8-bin.tar.gz -C /opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ANT_HOME(/etc/profile)</span></span><br><span class="line">export ANT_HOME=/opt/module/apache-ant-1.10.8</span><br><span class="line">export PATH=$PATH:$ANT_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">ant -version</span><br></pre></td></tr></table></figure>

<ul>
<li><p>安装glibc-headers和g++：yum install glibc-headers、yum install gcc-c++</p>
</li>
<li><p>安装make和cmake：yum install make</p>
</li>
<li><p>安装cmake(要装3.x版本,低版本编译不通过)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cmake-3.17.3.tar.gz -C /opt/module</span><br><span class="line">cd /opt/module/cmake-3.17.3</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> CMAKE_HOME(/etc/profile)</span></span><br><span class="line">export CMAKE_HOME=/opt/module/cmake-3.17.3</span><br><span class="line">export PATH=$PATH:$CMAKE_HOME/bin</span><br><span class="line">source /etc/profile</span><br><span class="line">cmake --version</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装protobuf：</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/</span><br><span class="line">cd /opt/module/protobuf-2.5.0/</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make check</span><br><span class="line">make install</span><br><span class="line">ldconfig</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> LD_LIBRARY_PATH(/etc/profile)</span></span><br><span class="line">export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</span><br><span class="line">export PATH=$PATH:$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line">protoc --version</span><br></pre></td></tr></table></figure>

<ul>
<li>安装openssl库：yum install openssl-devel</li>
<li>安装ncurses-devel库：yum install ncurses-devel</li>
</ul>
</li>
<li><p>编译源码</p>
<ul>
<li>解压源码到/opt目录</li>
<li>进入hadoop源码主目录</li>
<li>通过maven执行编译命令：mvn package -Pdist,native -DskipTests -Dtar</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h2><ul>
<li><p>HDFS产出背景及定义</p>
<ul>
<li>产生背景：随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种</li>
<li>定义：HDFS(Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色</li>
<li>使用背景：<strong>适合一次写入，多次读出的场景，且不支持文件的修改</strong>。适合用来做数据分析，并不适合用来做网盘应用</li>
</ul>
</li>
<li><p>HDFS优缺点</p>
<ul>
<li>优点：<ul>
<li>高容错性<ul>
<li>数据自动保存多个副本。它通过增加副本的形式，提高容错性</li>
<li>某一个副本丢失以后，它可以自动恢复(通过将副本复制到另一个可用的节点)</li>
</ul>
</li>
<li>适合处理大数据<ul>
<li>数据规模：能够处理数据规模达到GB、TB、甚至PB级别的数据</li>
<li>文件规模：能够处理百万规模以上的文件数量，数量相当之大</li>
</ul>
</li>
<li>可构建在廉价机器上，通过多副本机制，提高可靠性</li>
</ul>
</li>
<li>缺点：<ul>
<li><strong>不适合低延时数据访问</strong>，比如毫秒级的存储数据，是做不到的</li>
<li><strong>无法高效的对大量小文件进行存储</strong>：<ul>
<li>存储大量小文件的话，它会占用NameNode大量的内存来存储文件目录和块信息。这样是不可取的，因为NameNode的内存总是有限的</li>
<li>小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标</li>
</ul>
</li>
<li>不支持并发写入、文件随机修改：<ul>
<li>一个文件只能有一个写，不允许多个线程同时写</li>
<li><strong>仅支持数据appen(追加)</strong>，不支持文件的随机修改</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>HDFS组成架构<br><img src="HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84.png" alt="HDFS组成架构"></p>
<ul>
<li><p>NameNode(nn)：Master，一个主管、管理者</p>
<ul>
<li>管理HDFS的名称空间</li>
<li>配置副本策略</li>
<li>管理数据块(Block)映射信息</li>
<li>处理客户端读写请求</li>
</ul>
</li>
<li><p>DataNode：Slave。NameNode下达命令，DataNode执行实际的操作</p>
<ul>
<li>存储实际的数据块</li>
<li>执行数据块的读/写操作</li>
</ul>
</li>
<li><p>Client：客户端</p>
<ul>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据</li>
<li>Client提供一些命令来管理HDFS，比如NameNode格式化</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li>
</ul>
</li>
<li><p>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务</p>
<ul>
<li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复NameNode</li>
</ul>
</li>
</ul>
</li>
<li><p>HDFS文件块大小<br>HDFS中的文件在物理上是分块存储(Block)，块的大小可以通过配置参数(dfs.blocksize)来规定，默认大小在Hadoop2.x版本中是128M，老版本中是64M<br><img src="%E6%96%87%E4%BB%B6%E5%9D%97%E5%A4%A7%E5%B0%8F%E5%A4%A7%E8%87%B4%E8%AE%A1%E7%AE%97.png" alt="文件块大小大致计算"><br>为什么文件块的大小不能设置太小，也不能设置太大？</p>
<ul>
<li>HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置</li>
<li>如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢</li>
<li>总结：HDFS块的大小设置主要取决于磁盘传输速率</li>
</ul>
</li>
</ul>
<h2 id="HDFS的Shell操作"><a href="#HDFS的Shell操作" class="headerlink" title="HDFS的Shell操作"></a>HDFS的Shell操作</h2><ul>
<li><p>基本语法<br>bin/hadoop fs 具体命令 OR bin/hdfs dfs 具体命令<br>其中dfs是fs的实现类</p>
</li>
<li><p>命令大全：bin/hadoop fs</p>
</li>
<li><p>使用命令：</p>
<ul>
<li>-help：输出命令的帮助(hadoop fs -help rm)</li>
<li>-ls：显示目录信息(hadoop fs -ls /)</li>
<li>-mkdir：在HDFS上创建目录[-p递归](hdoop fs -mkdir -p /sobxiong/test)</li>
<li>-moveFromLocal：从本地剪切粘贴到HDFS[前路径为本地,后路径为HDFS](hadoop fs -moveFromLocal ./test.txt /sobxiong/test/)</li>
<li>-appendToFile：追加一个文件到已经存在的文件末尾[前路径为本地,后路径为HDFS](hadoop fs -appendToFile ./test.txt /sobxiong/test/test.txt)</li>
<li>-cat：显示文件内容(hadoop fs -cat /sobxiong/test/test.txt)</li>
<li>-chgrp、-chmod、-chown：修改文件所属的权限，同Linux文件系统中的用法</li>
<li>-copyFromLocal：从本地文件系统拷贝文件到HDFS中，同-moveFromLocal</li>
<li>-copyToLocal：从HDFS拷贝文件到本地[前路径为HDFS,后路径为本地](hadoop fs -copyToLocal /sobxiong/test/test.txt ./)</li>
<li>-cp：把文件从HDFS的一个路径拷贝到HDFS的另一个路径</li>
<li>-mv：把文件从HDFS的一个路径移动到HDFS的另一个路径</li>
<li>-get：等同于copyToLocal(用法同copyToLocal)，从HDFS下载文件到本地</li>
<li>-getmerge：合并下载多个文件(hadoop fs -getmerge /sobxiong/test/* ./all.txt)</li>
<li>-put：等同于copyFromLocal(用法同copyFromLocal)</li>
<li>-tail：显示一个文件的末尾(hadoop fs -tail /sobxiong/test/test.txt)</li>
<li>-rm：删除文件或文件夹[-r递归删除目录]</li>
<li>-rmdir：删除空目录</li>
<li>-du：统计文件夹的大小信息[-h显示单位,-s总和](hadoop fs -du -h -s /)</li>
<li>-setrep：设置HDFS中文件的副本数目[这里设置的副本数只是记录在NameNode的元数据中,是否真的会有这么多副本还得看DataNode的数量.因为目前只有3台设备,最多也就3个副本,只有节点数的增加到10台时,副本数才能达到10;只要加入一台设备,就会把副本复制到设备上,直到加到10台](hadoop fs -setrep 10 /sobxiong/test/test.txt)</li>
</ul>
</li>
</ul>
<h2 id="HDFS客户端操作"><a href="#HDFS客户端操作" class="headerlink" title="HDFS客户端操作"></a>HDFS客户端操作</h2><ul>
<li><p>客户端环境准备</p>
<ul>
<li>将Hadoop安装到mac上，并设置环境变量</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> HADOOP_HOME(~/.bash_profile)</span></span><br><span class="line">export HADOOP_HOME="/Users/sobxiong/module/hadoop-3.1.3"</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br><span class="line">hadoop version</span><br></pre></td></tr></table></figure>

<ul>
<li>创建Maven工程测试：idea创建quickstart项目</li>
<li>导入依赖：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-slf4j-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>创建测试类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSClient</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, InterruptedException </span>&#123;</span><br><span class="line">      Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">      <span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop1:9000");</span></span><br><span class="line">      <span class="comment">// 1、获取hdfs客户端对象</span></span><br><span class="line">      FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 2、在hdfs上创建路径</span></span><br><span class="line">      fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/sobxiong2/test"</span>));</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 3、关闭资源</span></span><br><span class="line">      fileSystem.close();</span><br><span class="line"></span><br><span class="line">      System.out.println(<span class="string">"finish"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>HDFS的API操作</p>
<ul>
<li>文件上传</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 参数优先级：</span></span><br><span class="line"><span class="comment">  *  1、客户端代码中设置的值</span></span><br><span class="line"><span class="comment">  *  2、ClassPath(resources)下的用户自定义配置文件(hdfs-site.xml)</span></span><br><span class="line"><span class="comment">  *  3、服务器的默认配置</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="comment">// 1、文件上传</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"2"</span>);</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行上传API</span></span><br><span class="line">    fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Documents/文件块大小大致计算.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将hdfs-site.xml拷贝至项目的根目录resources资源文件夹下</span></span><br><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=<span class="string">"text/xsl"</span> href=<span class="string">"configuration.xsl"</span>?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>文件下载</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2、文件下载</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行下载操作</span></span><br><span class="line">    <span class="comment">// fileSystem.copyToLocalFile(new Path("/sobxiong/test2.png"), new Path("/Users/sobxiong/Documents/test.png"));</span></span><br><span class="line">    <span class="comment">// 本地模式,true,不会产生crc文件</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Documents/test1.png"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>文件删除</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3、文件删除</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、文件删除(第二个参数,是否递归删除,文件夹时有效)</span></span><br><span class="line">    fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/sobxiong/test2.png"</span>), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>文件更名</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 4、文件更名</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、执行更名操作</span></span><br><span class="line">    fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/sobxiong/test1.png"</span>), <span class="keyword">new</span> Path(<span class="string">"/sobxiong/1tset.png"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>文件详情查看</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 5、文件详情查看</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、查看文件详情</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (listFiles.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查看文件名称、权限、长度</span></span><br><span class="line">        System.out.println(<span class="string">"name: "</span> + fileStatus.getPath().getName());</span><br><span class="line">        System.out.println(<span class="string">"permission: "</span> + fileStatus.getPermission());</span><br><span class="line">        System.out.println(<span class="string">"length: "</span> + fileStatus.getLen());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 查看块信息</span></span><br><span class="line">        BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line">        <span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">                System.out.println(<span class="string">"host = "</span> + host);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"----------------"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>判断是文件还是文件夹</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 6、判断是文件还是文件夹</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、判断操作</span></span><br><span class="line">    FileStatus[] fileStatuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"file = "</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"dir = "</span> + fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、关闭资源</span></span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>HDFS的I/O流操作</p>
<ul>
<li>HDFS文件上传</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 把本地文件上传到HDFS根目录</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">upload</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FileInputStream fileInputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/课件.rar"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FSDataOutputStream fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/test.rar"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fileInputStream, fsDataOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fsDataOutputStream);</span><br><span class="line">    IOUtils.closeStream(fileInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>HDFS文件下载</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从HDFS下载文件到本地磁盘</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">download</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/test.rar"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/test1.rar"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>定位文件获取</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载第一块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-3.1.3.tar.gz"</span>));</span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part1"</span>));</span><br><span class="line">    <span class="comment">// 4、流的对拷(只拷贝第一个块128MB)</span></span><br><span class="line">    <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++) &#123;</span><br><span class="line">        fsDataInputStream.read(buf);</span><br><span class="line">        fileOutputStream.write(buf);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下载第二块</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 1、获取fs对象</span></span><br><span class="line">    FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop1:9000"</span>), configuration, <span class="string">"sobxiong"</span>);</span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line">    FSDataInputStream fsDataInputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-3.1.3.tar.gz"</span>));</span><br><span class="line">    <span class="comment">// 3、设置指定读取的起点</span></span><br><span class="line">    fsDataInputStream.seek(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128</span>);</span><br><span class="line">    <span class="comment">// 4、获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"/Users/sobxiong/Downloads/hadoop-3.1.3.tar.gz.part2"</span>));</span><br><span class="line">    <span class="comment">// 5、流的对拷(拷贝剩下的两个Block块)</span></span><br><span class="line">    IOUtils.copyBytes(fsDataInputStream, fileOutputStream, configuration);</span><br><span class="line">    <span class="comment">// 6、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataInputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下载完第一块和剩余的部分后,可通过cat hadoop-3.1.3.tar.gz.part2 &gt;&gt; hadoop-3.1.3.tar.gz.part1将剩余部分追加到第一块上,修改文件名(删去.part1),就得到完整的文件</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="HDFS的数据流"><a href="#HDFS的数据流" class="headerlink" title="HDFS的数据流"></a>HDFS的数据流</h2><ul>
<li><p>HDFS写数据流程</p>
<ul>
<li><p>剖析文件写入：<br><img src="HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS写数据流程"></p>
<ul>
<li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在</li>
<li>NameNode返回是否可以上传</li>
<li>客户端请求第一个Block上传到哪几个DataNode服务器上</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3</li>
<li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</li>
<li>dn1、dn2、dn3逐级应答客户端</li>
<li>客户端开始往dn1上传第一个Block(先从磁盘读取数据放到一个本地内存缓存)，以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器(此后重复执行3-7步)</li>
</ul>
</li>
<li><p>网络拓扑-节点距离计算<br>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。节点距离：两个节点到达最近的共同祖先的距离总和<br><img src="%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91-%E8%8A%82%E7%82%B9%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97.png" alt="网络拓扑-节点距离计算"></p>
</li>
<li><p>机架感知(2.7.2版本副本节点选择,性能和安全的综合考量)</p>
<ul>
<li>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个</li>
<li>第二个副本和第一个副本位于相同机架，随机节点</li>
<li>第三个副本位于不同机架，随机节点</li>
</ul>
</li>
</ul>
</li>
<li><p>HDFS读数据流程<br><img src="HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS读数据流程"></p>
<ul>
<li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址</li>
<li>挑选一台DataNode(就近原则，然后随机)服务器，请求读取数据</li>
<li>DataNode开始传输数据给客户端(从磁盘里面读取数据输入流，以Packet为单位来做校验)</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件</li>
</ul>
</li>
</ul>
<h2 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h2><ul>
<li><p>NN和2NN工作机制<br>思考：NameNode中的元数据是存储在哪里的？<br>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage<br>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据<br>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并<br><img src="NameNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="NameNode工作机制"></p>
<ul>
<li><p>第一阶段：NameNode启动</p>
<ul>
<li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存</li>
<li>客户端对元数据进行增删改的请求</li>
<li>NameNode记录操作日志，更新滚动日志(先记日志,类似数据库)</li>
<li>NameNode在内存中对数据进行增删改</li>
</ul>
</li>
<li><p>第二阶段：Secondary NameNode工作</p>
<ul>
<li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果</li>
<li>Secondary NameNode请求执行CheckPoint</li>
<li>NameNode滚动正在写的Edits日志</li>
<li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</li>
<li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并</li>
<li>生成新的镜像文件fsimage.chkpoint</li>
<li>拷贝fsimage.chkpoint到NameNode</li>
<li>NameNode将fsimage.chkpoint重新命名成fsimage</li>
</ul>
</li>
<li><p>补充：<br>Fsimage：NameNode内存中元数据序列化后形成的文件。<br>Edits：记录客户端更新元数据信息的每一步操作(可通过Edits运算出元数据)。<br>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中(查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息)，如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。<br>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并(所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage)。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。<br>SecondaryNameNode首先会询问NameNode是否需要CheckPoint(触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了)。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中</p>
</li>
</ul>
</li>
<li><p>Fsimage和Edits解析</p>
<ul>
<li>概念<ul>
<li>NameNode被格式化之后，将在/data/tmp/dfs/name/current目录中产生如下文件<br>fsimage_0000000000000000000<br>fsimage_0000000000000000000.md5<br>seen_txid<br>VERSION</li>
<li>Fsimage文件：HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息</li>
<li>Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中</li>
<li>seen_txid文件保存的是一个数字，就是最后一个edits_的数字</li>
<li><strong>每次NameNode启动的时候</strong>都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并</li>
</ul>
</li>
<li>查看Fsimage文件：hdfs oiv -p 文件类型(XML) -i 镜像文件 -o 转换后文件输出路径<br>例：hdfs oiv -p XML -i fsimage_0000000000000000025 o fsimage.xml<br>Fsimage中没有记录块所对应的DataNode，为什么？<br>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报</li>
<li>查看Edits文件：hdfs oev -p 文件类型(XML) -i 编辑日志 -o 转换后文件输出路径<br>例：hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o edits.xml<br>NameNode如何确定下次开机启动的时候合并那些Edits？<br>通过seen_txid查看</li>
</ul>
</li>
<li><p>CheckPoint时间设置</p>
<ul>
<li>通常情况下，SecondaryNameNode每隔一小时执行一次</li>
<li>一分钟检查一次操作次数</li>
<li>当操作次数达到1百万时，SecondaryNameNode执行一次</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-default.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>NameNode故障处理</p>
<ul>
<li><p>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</p>
<ul>
<li>kiil -9 NameNode进程编号(用jps查看NameNode的进程编号)</li>
<li>删除NameNode存储的数据(data/tmp/dfs/name)：rm -rf /data/tmp/dfs/name/*</li>
<li>拷贝SecondaryNameNode(hadoop2)中数据到原NameNode(hadoop1)存储数据目录：scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary/* ./name/</li>
<li>重新启动NameNode(hadoop1)：sbin/hadoop-daemon.sh start namenode</li>
</ul>
</li>
<li><p>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中</p>
<ul>
<li>修改hdfs-site.xml(加入下述内容)：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>kill -9 NameNode进程</li>
<li>删除NameNode存储的数据(同方法一)</li>
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r sobxiong@hadoop2:/opt/module/hadoop-3.1.3/data/tmp/dfs/namesecondary /data/tmp/dfs/</span><br><span class="line">cd /data/tmp/dfs/namesecondary</span><br><span class="line">rm -rf in_use.lock</span><br></pre></td></tr></table></figure>

<ul>
<li>导入检查点数据(等待一会ctrl+c结束掉)</li>
<li>启动NameNode：sbin/hadoop-daemon.sh start namenode</li>
</ul>
</li>
</ul>
</li>
<li><p>集群安全模式</p>
<ul>
<li><p>概述</p>
<ul>
<li>NameNode启动<br>NameNode启动时，首先将镜像文件(Fsimage)载入内存，并执行编辑日志(Edits)中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。<strong>这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的</strong></li>
<li>DataNode启动</li>
</ul>
<p><strong>系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。</strong>在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统</p>
<ul>
<li>安全模式退出判断<br>如果满足“<strong>最小副本条件</strong>”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别(默认值：dfs.replication.min=1)。<strong>在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式</strong></li>
</ul>
</li>
<li><p>基本语法<br>集群处于安全模式，不能执行重要操作(写操作)。集群启动完成后，自动退出安全模式</p>
<ul>
<li>查看安全模式状态：bin/hdfs dfsadmin -safemode get</li>
<li>进入安全模式状态：bin/hdfs dfsadmin -safemode enter</li>
<li>离开安全模式状态：bin/hdfs dfsadmin -safemode leave</li>
<li><strong>等待安全模式状态：bin/hdfs dfsadmin -safemode wait</strong></li>
</ul>
</li>
<li><p>案例<br>模拟等待安全模式</p>
<ul>
<li>查看当前模式：bin/hdfs dfsadmin -safemode get</li>
<li>先进入安全模式：bin/hdfs dfsadmin -safemode enter</li>
<li>创建并执行下面的脚本</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">touch safemode.sh</span><br><span class="line">vim safemode.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> safemode.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /</span><br><span class="line"></span><br><span class="line">chmod 777 safemode.sh</span><br><span class="line">./safemode.sh</span><br></pre></td></tr></table></figure>

<ul>
<li>再打开一个窗口，执行：hdfs dfsadmin -safemode leave</li>
<li>安全模式退出，HDFS集群上已经有上传的数据了</li>
</ul>
</li>
</ul>
</li>
<li><p>NameNode多目录配置</p>
<ul>
<li><p>NameNode的本地目录可以配置成多个，但每个目录存放内容相同(相当于备份)，增加了可靠性</p>
</li>
<li><p>具体配置如下</p>
<ul>
<li>在hdfs-site.xml文件中增加如下内容</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>停止集群，删除data和logs中所有数据</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop2：sbin/stop-yarn.sh</span><br><span class="line">hadoop2：rm -rf data/ logs/</span><br><span class="line">hadoop1：sbin/stop-dfs.sh</span><br><span class="line">hadoop2：rm -rf data/ logs/s</span><br><span class="line">hadoop3：rm -rf data/ logs/s</span><br></pre></td></tr></table></figure>

<ul>
<li>格式化集群并启动</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop1：bin/hdfs namenode -format</span><br><span class="line">hadoop1：sbin/start-dfs.sh</span><br><span class="line">hadoop2：sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<ul>
<li>查看结果：dfs目录下出现两个目录name1和name2</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><ul>
<li><p>DataNode工作机制<br><img src="DataNode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="DataNode工作机制"></p>
<ul>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳</li>
<li>DataNode启动后向NameNode注册，通过后，周期性(1小时)的向NameNode上报所有的块信息</li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用</li>
<li>集群运行中可以安全加入和退出一些机器</li>
</ul>
</li>
<li><p>数据完整性<br>DataNode节点保证数据完整性的方法：</p>
<ul>
<li>当DataNode读取Block的时候，它会计算CheckSum(类似crc校验位)</li>
<li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏</li>
<li>Client读取其他DataNode上的Block</li>
<li>DataNode在其文件创建后周期验证CheckSum</li>
</ul>
</li>
<li><p>掉线时限参数设置<br><img src="DataNode%E6%8E%89%E7%BA%BF%E6%97%B6%E9%99%90%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE.png" alt="DataNode掉线时限参数设置"><br>hdfs-default.xml：</p>
</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- heartbeat.recheck.interval单位为毫秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- dfs.heartbeat.interval单位为秒 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p><del>服役新数据节点(hadoop4未服役)</del></p>
<ul>
<li><p>需求：随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点</p>
</li>
<li><p>环境准备</p>
<ul>
<li>利用hadoop3主机再克隆一台hadoop4主机</li>
<li>修改hadoop4主机IP地址和主机名称</li>
<li>在hadoop1主机上将/etc/hosts下添加hadoop4的ip地址映射条目，并分发到hadoop2-4</li>
<li><strong>hadoop4主机删除原来HDFS文件系统留存的文件(data和log目录)——不然会发生3和4轮换出现的问题,因为3和4有着一样的data和log</strong></li>
<li>reboot重启加载配置</li>
</ul>
</li>
<li><p>服役新节点具体步骤</p>
<ul>
<li>hadoop1-3按之前步骤已启动</li>
<li>在hadoop4主机上单独启动：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br><span class="line">yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure>

<ul>
<li>刷新NameNode和ResourceManager：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<ul>
<li>刷新<a href="http://hadoop1:9870" target="_blank" rel="noopener">http://hadoop1:9870</a>web页面，等待</li>
<li>在hadoop4上上传文件</li>
<li>如果数据不均衡，可以使用命令实现集群的在平衡：sbin/start-balancer.sh</li>
</ul>
</li>
<li><p>结束后在workers文件中加入hadoop4，之后直接start-dfs.sh和start-yarn.sh即可启动</p>
</li>
</ul>
</li>
<li><p><del>退役旧数据节点(hadoop4未退役)</del></p>
<ul>
<li><p><del>添加白名单(hadoop4未退役)</del><br>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出</p>
<ul>
<li>在NameNode的hadoop-3.1.3/etc/hadoop目录下创建dfs.hosts文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">touch dfs.hosts</span><br><span class="line">vim dfs.hosts</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dfs.hosts(不添加hadoop4,不允许有空行和空格)</span></span><br><span class="line">hadoop1</span><br><span class="line">hadoop2</span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure>

<ul>
<li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>配置文件分发：xsync hdfs-site.xml;xsync dfs.hosts</li>
<li>刷新NameNode和ResourceManager：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<ul>
<li>在web页面刷新等待</li>
</ul>
</li>
<li><p><del>黑名单退役(hadoop4未退役)</del><br>在黑名单上面的主机都会被强制退出。<strong>注意：不允许白名单和黑名单中同时出现同一个主机名称</strong></p>
<ul>
<li>在hadoop-3.1.3/etc/hadoop下创建dfs.hosts.exclude文件，并加入要退役节点</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">touch dfs.hosts.exclude</span><br><span class="line">vim dfs.hosts.exclude</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> dfs.hosts.exclude</span></span><br><span class="line">hadoop4</span><br></pre></td></tr></table></figure>

<ul>
<li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>刷新NameNode和ResourceManager：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<ul>
<li>刷新web页面等待</li>
</ul>
</li>
</ul>
</li>
<li><p>DataNode多目录配置</p>
<ul>
<li>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</li>
<li>需要在hdfs-site.xml上修改配置</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>关闭当前运行的hadoop节点</li>
<li>删除各节点的data和log目录</li>
<li>格式化dataNode</li>
<li>重启部署hadoop节点</li>
</ul>
</li>
</ul>
<h2 id="HDFS2-X新特性"><a href="#HDFS2-X新特性" class="headerlink" title="HDFS2.X新特性"></a>HDFS2.X新特性</h2><ul>
<li><p>集群间数据拷贝</p>
<ul>
<li>scp实现两个远程主机之间的文件复制</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从当前主机向目的主机 推 push</span></span><br><span class="line">scp -r hello.txt root@hadoop2:/user/sobxiong/hello.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从目的主机向当前主机 拉 pull</span></span><br><span class="line">scp -r root@hadoop2:/user/sobxiong/hello.txt hello.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过本主机中转实现两个远程主机的文件复制</span></span><br><span class="line">scp -r root@hadoop2:/user/sobxiong/hello.txt root@hadoop3:/user/sobxiong</span><br></pre></td></tr></table></figure>

<ul>
<li>采用distcp命令实现<strong>两个Hadoop集群之间</strong>的递归数据复制</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop distcp hdfs://haoop1:9000/user/sobxiong/hello.txt hdfs://hadoop2:9000/user/sobxiong/hello.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>小文件存档</p>
<ul>
<li><p>HDFS存储小文件弊端<br>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为<strong>大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关</strong>。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB</p>
</li>
<li><p>解决存储小文件办法之一<br>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存<br><img src="%E5%B0%8F%E6%96%87%E4%BB%B6%E5%BD%92%E6%A1%A3.png" alt="小文件归档"></p>
</li>
<li><p>实际操作</p>
<ul>
<li>启动YARN进程：start-yarn.sh(hadoop2)</li>
<li>归档文件(归档后的路径不得实现存在)<br>把/sobxiong目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/sobxiongOutput路径下：hadoop archive -archiveName input.har -p /sobxiong /sobxiongOutput</li>
<li>查看归档：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 普通查看文件命令</span></span><br><span class="line">hadoop fs -ls R /sobxiongOutput/input.har</span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup          0 2020-06-24 16:06 /sobxiongOutput/input.har/_SUCCESS</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup        305 2020-06-24 16:06 /sobxiongOutput/input.har/_index</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup         23 2020-06-24 16:06 /sobxiongOutput/input.har/_masterindex</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup     317029 2020-06-24 16:06 /sobxiongOutput/input.har/part-0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 采用har格式查看文件(可以像以往一样操作内部文件,也需要har格式)</span></span><br><span class="line">hadoop fs -ls R har:///sobxiongOutput/input.har</span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup      84942 2020-06-21 11:38 har:///sobxiongOutput/input.har/1tset.png</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup      84942 2020-06-21 11:33 har:///sobxiongOutput/input.har/test.png</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -rw-r--r--   3 sobxiong supergroup     147145 2020-06-23 13:52 har:///sobxiongOutput/input.har/test6.txt</span></span><br></pre></td></tr></table></figure>

<ul>
<li>解归档文件：hadoop fs -cp har:///sobxiongOutput/input.har/* /</li>
</ul>
</li>
</ul>
</li>
<li><p>回收站<br>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用</p>
<ul>
<li>开启回收站功能参数说明：<ul>
<li>默认值fs.trash.interval = 0，0表示禁用回收站；<strong>其他值表示设置文件的存活时间(分钟为单位)</strong></li>
<li>默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。该值设置和fs.trash.interval的参数值相同(要求fs.trash.checkpoint.interval &lt;= fs.trash.interval)</li>
</ul>
</li>
<li>回收站工作机制<br><img src="%E5%9B%9E%E6%94%B6%E7%AB%99%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="回收站工作机制"></li>
<li>启动回收站：修改core-site.xml，配置垃圾回收时间为1分钟</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>查看回收站：回收站在集群中的路径：/user/sobxiong/.Trash/</li>
<li>修改访问回收站用户名称：进入垃圾回收站用户名称，默认是dr.who，修改为sobxiong(同样是core-site.xml)</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>恢复回收站数据：hadoop fs -mv /user/sobxiong/.Trash/Current/user/sobxiong/input /</li>
<li>清空回收站：hadoop fs -expunge</li>
</ul>
</li>
<li><p>快照管理</p>
<ul>
<li><p>命令介绍<br><img src="%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86.png" alt="快照管理"></p>
</li>
<li><p>实际操作</p>
<ul>
<li>开启/禁用指定目录的快照功能：hdfs dfsadmin -allowSnapshot(-disallowSnapshot) /user/sobxiong/input</li>
<li>对目录创建快照</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -createSnapshot /user/sobxiong/input</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 快照和源文件使用相同数据</span></span><br><span class="line">hdfs dfs -ls R /user/sobxiong/input/.snapshot/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定名称创建快照</span></span><br><span class="line">hdfs dfs -createSnapshot /user/sobxiong/input test</span><br></pre></td></tr></table></figure>

<ul>
<li>重命名快照：hdfs dfs -renameSnapshot /user/sobxiong/input test test01</li>
<li>列出当前用户所有可快照目录：hdfs lsSnapshottableDir</li>
<li><strong>比较两个快照目录的不同之处(可以是源文件和快照,使用’.’,此时”.snapshot/name”用于指定具体快照)</strong>：hdfs snapshotDiff /user/sobxiong/input . .snapshot/test01</li>
<li>恢复快照：hdfs dfs -cp /user/sobxiong/input/.snapshot/s20200624-134303.027 /</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h2><ul>
<li><p>MapReduce定义</p>
<ul>
<li>MapReduce是一个<strong>分布式运算程序的编程框架</strong>，是用户开发“基于Hadoop的数据分析应用”的核心框架</li>
<li>MapReduce核心功能是<strong>将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序</strong>，并发运行在一个Hadoop集群上</li>
</ul>
</li>
<li><p>MapReduce优缺点</p>
<ul>
<li>优点：<ul>
<li>MapReduce易于编程<br>它<strong>简单地实现一些接口，就可以完成一个分布式程序</strong>，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行</li>
<li>良好的扩展性(hadoop)<br>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力</li>
<li>高容错性<br>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。<strong>比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</strong>，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的</li>
<li>适合PB级以上海量数据的离线处理：可以实现上千台服务器集群并发工作，提供数据处理能力</li>
</ul>
</li>
<li>缺点：<ul>
<li>不擅长实时计算：MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果</li>
<li>擅长流式计算<br>流式计算的输入数据是动态的，<strong>而MapReduce的输入数据集是静态的，不能动态变化</strong>。这是因为MapReduce自身的设计特点决定了数据源必须是静态的</li>
<li>不擅长DAG(有向图)计算<br>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，<strong>每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>核心思想<br><img src="MapRecude%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3.png" alt="MapRecude核心编程思想"></p>
</li>
<li><p>MapReduce进程<br>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<ul>
<li>MrAppMaster：负责整个程序的过程调度及状态协调</li>
<li>MapTask：负责Map阶段的整个数据处理流程</li>
<li>ReuceTask：负责Reduce阶段的整个数据处理流程</li>
</ul>
</li>
<li><p>常用数据序列化类型</p>
<table>
<thead>
<tr>
<th>Java类型</th>
<th>Hadoop Writable类型</th>
</tr>
</thead>
<tbody><tr>
<td>boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
</li>
<li><p>MapReduce编程规范<br>用户编写的程序分成三个部分：Mapper、Reducer和Driver</p>
<ul>
<li>Mapper<ul>
<li>用户自定义的Mapper要继承自己的父类</li>
<li>Mapper的输入数据是KV对的形式(KV的类型可自定义)</li>
<li>Mapper中的业务逻辑写在map()方法中</li>
<li>Mapper的输出数据是KV对的形式(KV的类型可自定义)</li>
<li><strong>map()方法(MapTask进程)对每一个&lt;K,V&gt;调用一次</strong></li>
</ul>
</li>
<li>Reducer<ul>
<li>用户自定义的Reducer要继承自己的父类</li>
<li>Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</li>
<li>Reducer的业务逻辑写在reduce()方法中</li>
<li><strong>ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</strong></li>
</ul>
</li>
<li>Driver：相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象</li>
</ul>
</li>
<li><p>WordCount案例实操</p>
<ul>
<li><p>需求：在给定的文本文件中统计输出每一个单词出现的总次数</p>
</li>
<li><p>需求分析<br><img src="WordCount%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90.png" alt="WordCount需求分析"></p>
</li>
<li><p>环境准备</p>
<ul>
<li>创建maven空项目</li>
<li>改pom</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.13.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>在resources资源文件夹下新建log4j.properties文件</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>编写MapReduce程序</p>
<ul>
<li>编写mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* map阶段</span></span><br><span class="line"><span class="comment">* KEYIN：输入数据的key类型(默认写LongWritable:偏移量)</span></span><br><span class="line"><span class="comment">* VALUEIN：输入数据的value类型</span></span><br><span class="line"><span class="comment">* KEYOUT：输出数据的key类型</span></span><br><span class="line"><span class="comment">* VALUEOUT：输出数据的value类型</span></span><br><span class="line"><span class="comment">* &lt;sobxiong,1&gt;输出数据</span></span><br><span class="line"><span class="comment">* 输出作为reduce阶段的输入</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// sobxiong sobxiong</span></span><br><span class="line">        <span class="comment">// 1、获取一行</span></span><br><span class="line">        String lineStr = value.toString();</span><br><span class="line">        <span class="comment">// 2、切割单词</span></span><br><span class="line">        String[] words = lineStr.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 3、循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="comment">// &lt;sobxiong,1&gt;</span></span><br><span class="line">            k.set(word);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* reduce阶段</span></span><br><span class="line"><span class="comment">* KEYIN,VALUEIN：map阶段输出的kv</span></span><br><span class="line"><span class="comment">* KEYOUT,VALUEOUT：reduce输出的kv</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 1、累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2、写出 &lt;sobxiong,2&gt;</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key, v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写Driver驱动类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、获取Job对象</span></span><br><span class="line">      Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">      Job job = Job.getInstance(conf);</span><br><span class="line">      <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">      job.setJarByClass(WordCountDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">      job.setMapperClass(WordCountMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setReducerClass(WordCountReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">      job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">      job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">      <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">      FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">      FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">      <span class="comment">// 7、提交job</span></span><br><span class="line">      <span class="comment">// job.submit();</span></span><br><span class="line">      <span class="comment">// true：打印一些信息</span></span><br><span class="line">      <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">      <span class="comment">// 额外</span></span><br><span class="line">      System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>本地测试：启动旁边的下箭头Edit Configuration，新增Application，选择Main Class并在Program arguments中加入两个参数中间用空格隔开，前者是input文件所在文件夹，后者是输出文件夹，要求不能存在(不然会出错)。例如：/Users/sobxiong/Downloads/input /Users/sobxiong/Downloads/ouputTest</p>
</li>
<li><p>在集群上测试</p>
<ul>
<li>用maven打jar包，添加打包插件依赖</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.xiong.hadoop.WordCountDriver<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>将程序打成jar包，maven install即可</li>
<li>将获取的两个jar包(一个不带依赖,另一带)，将不带依赖的jar上传到hadoop1主机上</li>
<li>启动Hadoop集群</li>
<li>执行WordCount程序：hadoop jar WordCount.jar com.xiong.hadoop.WordCountDriver /sobxiong /outputTest(第四个参数为启动的主类名,第五个为输入文件所在文件夹,第六个为输出文件夹——不能事先存在)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Hadoop序列化"><a href="#Hadoop序列化" class="headerlink" title="Hadoop序列化"></a>Hadoop序列化</h2><ul>
<li><p>序列化概述</p>
<ul>
<li>什么是序列化：<em>序列化</em>就是把<strong>内存中的对象，转换成字节序列</strong>(或其他数据传输协议)以便于存储到磁盘(持久化)和网络传输；<em>反序列化</em>就是<strong>将收到字节序列(或其他数据传输协议)或者是磁盘的持久化数据，转换成内存中的对象</strong></li>
<li>为什么要序列化：一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。然而<strong>序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机</strong></li>
<li>为什么不用Java的序列化：Java的序列化是一个重量级序列化框架(Serializable)，一个对象被序列化后，会附带很多额外的信息(各种校验信息，Header，继承体系等)，不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制(Writable)</li>
<li>Hadoop序列化特点：<ul>
<li>紧凑：高效使用存储空间</li>
<li>快速：读写数据的额外开销小</li>
<li>可扩展：随着通信协议的升级而可升级</li>
<li>互操作：支持多语言的交互</li>
</ul>
</li>
</ul>
</li>
<li><p>自定义bean对象实现序列化接口(Writable)<br>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。<br>具体实现bean对象序列化步骤如下7步：</p>
<ul>
<li>实现Writable接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li>
<li>重写序列化方法</li>
<li>重写反序列化方法(<strong>注意反序列化的顺序和序列化的顺序完全一致</strong>)</li>
<li>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用</li>
<li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框架中的Shuffle过程要求对key必须能排序</li>
</ul>
</li>
<li><p>序列化案例实操</p>
<ul>
<li><p>需求：统计每一个手机号耗费的总上行流量、下行流量、总流量</p>
</li>
<li><p>案例分析<br><img src="%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="序列化案例分析"></p>
</li>
<li><p>编写MapReduce程序</p>
<ul>
<li>编写流量统计的Bean对象</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">测试数据</span></span><br><span class="line"><span class="comment">1;13736230513;192.196.100.1;www.atguigu.com;2481;24681;200</span></span><br><span class="line"><span class="comment">2;13846544121;192.196.100.2;264;0;200</span></span><br><span class="line"><span class="comment">3;13956435636;192.196.100.3;132;1512;200</span></span><br><span class="line"><span class="comment">4;13966251146;192.168.100.1;240;0;404</span></span><br><span class="line"><span class="comment">5;18271575951;192.168.100.2;www.atguigu.com;1527;2106;200</span></span><br><span class="line"><span class="comment">6;84188413;192.168.100.3;www.atguigu.com;4116;1432;200</span></span><br><span class="line"><span class="comment">7;13590439668;192.168.100.4;1116;954;200</span></span><br><span class="line"><span class="comment">8;15910133277;192.168.100.5;wwww.haol23.com;3156;2936;200</span></span><br><span class="line"><span class="comment">9;13729199489;192.168.100.6;240;0;200</span></span><br><span class="line"><span class="comment">10;13630577991;192.168.100.7;www.shouhu.com;6960;690;200</span></span><br><span class="line"><span class="comment">11;15043685818;192.168.100.8;www.baidu.com;3659;3538;200</span></span><br><span class="line"><span class="comment">12;15959002129;192.168.100.9;www.atguigu.com;1938;180;500</span></span><br><span class="line"><span class="comment">13;13560439638;192.168.100.10;918;4938;200</span></span><br><span class="line"><span class="comment">14;13470253144;192.168.100.11;180;180;200</span></span><br><span class="line"><span class="comment">15;13682846555;192.168.100.12;wwww.qq.com;1938;2910;200</span></span><br><span class="line"><span class="comment">16;13992314666;192.168.100.13;www.gaga.com;3008;3720;200</span></span><br><span class="line"><span class="comment">17;13509468723;192.168.100.14;www.qinghua.com;7335;110349;404</span></span><br><span class="line"><span class="comment">18;18390173782;192.168.100.15;www.sogou.com;9531;2412;200</span></span><br><span class="line"><span class="comment">19;13975057813;192.168.100.16;www.baidu.com;11058;48243;200</span></span><br><span class="line"><span class="comment">20;13768778790;192.168.100.17;120;120;200</span></span><br><span class="line"><span class="comment">21;13568436656;192.168.100.18;www.alibaba.com;2481;24681;200</span></span><br><span class="line"><span class="comment">22;13568436656;192.168.100.19;1116;954;200</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 上行流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">  <span class="comment">// 下行流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">  <span class="comment">// 总流量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line">  <span class="comment">// 空参构造,为了后续反射</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 序列化方法</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      dataOutput.writeLong(upFlow);</span><br><span class="line">      dataOutput.writeLong(downFlow);</span><br><span class="line">      dataOutput.writeLong(sumFlow);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 反序列化方法</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      <span class="comment">// 必须要求和序列化方法顺序一致</span></span><br><span class="line">      upFlow = dataInput.readLong();</span><br><span class="line">      downFlow = dataInput.readLong();</span><br><span class="line">      sumFlow = dataInput.readLong();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">      <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">      <span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、获取一行</span></span><br><span class="line">      String lineStr = value.toString();</span><br><span class="line">      <span class="comment">// 2、切割\t</span></span><br><span class="line">      String[] fields = lineStr.split(<span class="string">"\t"</span>);</span><br><span class="line">      <span class="comment">// 3、封装对象</span></span><br><span class="line">      k.set(fields[<span class="number">1</span>]);</span><br><span class="line">      <span class="keyword">long</span> upFlow = Long.parseLong(fields[fields.length - <span class="number">3</span>]);</span><br><span class="line">      <span class="keyword">long</span> downFlow = Long.parseLong(fields[fields.length - <span class="number">2</span>]);</span><br><span class="line">      flowBean.set(upFlow, downFlow);</span><br><span class="line">      <span class="comment">// 4、写出</span></span><br><span class="line">      context.write(k, flowBean);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FlowBean v = <span class="keyword">new</span> FlowBean();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、累加求和</span></span><br><span class="line">      <span class="keyword">long</span> sumUpFlow = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">long</span> sumDownFlow = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">          sumUpFlow += value.getUpFlow();</span><br><span class="line">          sumDownFlow += value.getDownFlow();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      v.set(sumUpFlow, sumDownFlow);</span><br><span class="line">      context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写Driver驱动类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取job对象</span></span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf);</span><br><span class="line">    <span class="comment">// 2、设计jar路径</span></span><br><span class="line">    job.setJarByClass(FlowDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">    job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">    job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">    job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// 7、提交job</span></span><br><span class="line">    <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h2><ul>
<li><p>InputFormat数据输入</p>
<ul>
<li><p>切片与MapTask并行度决定机制</p>
<ul>
<li>问题引出：MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度<br>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</li>
<li>MapTask并行度决定机制</li>
</ul>
<p><strong>数据块</strong>：Block是HDFS物理上把数据分成一块一块<br><strong>数据切片</strong>：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储<br><img src="%E6%95%B0%E6%8D%AE%E5%88%87%E7%89%87%E4%B8%8EMapTask%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6.png" alt="数据切片与MapTask并行度决定机制"></p>
</li>
<li><p>Job提交流程源码和切片源码详解</p>
<ul>
<li>Job提交流程源码详解</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion();</span><br><span class="line">submit();</span><br><span class="line">  <span class="comment">// 1、建立连接</span></span><br><span class="line">  connect();</span><br><span class="line">    <span class="comment">// 1)创建提交Job的代理</span></span><br><span class="line">    <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">      <span class="comment">// 判断是本地yarn还是远程</span></span><br><span class="line">      initialize(jobTrackAddr, conf);</span><br><span class="line">  <span class="comment">// 2、提交job</span></span><br><span class="line">  submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">  <span class="comment">// 1)创建给集群提交数据的Stag路径</span></span><br><span class="line">  Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">  <span class="comment">// 2)获取jobid,并创建Job路径</span></span><br><span class="line">  JobID jobId = submitClient.getNewJobID();</span><br><span class="line">  <span class="comment">// 3)拷贝jar包到集群</span></span><br><span class="line">  copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">  rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line">  <span class="comment">// 4)计算切片,生成切片规划文件</span></span><br><span class="line">  writeSplits(job, submitJobDir);</span><br><span class="line">    maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">    input.getSplits(job);</span><br><span class="line">  <span class="comment">// 5)向Stag路径写XML配置文件</span></span><br><span class="line">  writeConf(conf, submitJobFile);</span><br><span class="line">  conf.writeXml(out);</span><br><span class="line">  <span class="comment">// 6)提交Job,返回提交状态</span></span><br><span class="line">  status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>

<p><img src="Job%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.png" alt="Job提交流程源码分析"></p>
<ul>
<li>FileInputFormat切片源码解析(input.getSplits(job))<ul>
<li>程序先找到你数据存储的目录</li>
<li>开始遍历处理(规划切片)目录下的每一个文件</li>
<li>遍历第一个文件ss.txt<ul>
<li>获取文件大小fs.sizeOf(ss.txt)</li>
<li>计算切片大小：computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M(YARN集群默认128M——2.x,62M-1.x;本地运行默认32M)</li>
<li><strong>默认情况下，切片大小=blocksize</strong></li>
<li>开始切，形成第1个切片：ss.txt—0:128M、第2个切片ss.txt—128:256M、第3个切片ss.txt—256M:300M(<strong>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片</strong>)</li>
<li>将切片信息写到一个切片规划文件中</li>
<li>整个切片的核心过程在getSplit()方法中完成</li>
<li><strong>InputSplit只记录了切片的元数据信息</strong>，比如起始位置、长度以及所在的节点列表等</li>
</ul>
</li>
<li><strong>提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>FileInputFormat切片机制</p>
<ul>
<li><p>切片机制</p>
<ul>
<li>简单地按照文件的内容长度进行切片</li>
<li>切片大小，默认等于Block大小</li>
<li><strong>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</strong></li>
</ul>
</li>
<li><p>案例分析</p>
<ul>
<li>输入数据有两个文件：file1.txt - 320M;file2.txt - 10M</li>
<li>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</li>
</ul>
<table>
<thead>
<tr>
<th>文件</th>
<th>切片区间</th>
</tr>
</thead>
<tbody><tr>
<td>file1.txt.split1</td>
<td>0~128</td>
</tr>
<tr>
<td>file1.txt.split2</td>
<td>129~256</td>
</tr>
<tr>
<td>file1.txt.split3</td>
<td>257~320</td>
</tr>
<tr>
<td>file2.txt.split1</td>
<td>0~10</td>
</tr>
</tbody></table>
</li>
<li><p>切片大小参数配置</p>
<ul>
<li>源码中计算切片大小的公式：Math.max(minSize, Math.min(maxSize, blockSize));<br>mapreduce.input.fileinputformat.split.minsize=<strong>1</strong> 默认值为1<br>mapreduce.input.fileinputformat.split.maxsize=<strong>Long.MAXValue</strong> 默认值Long.MAXValue</li>
</ul>
<p><strong>默认情况下，切片大小=blocksize</strong></p>
<ul>
<li>切片大小设置<br>maxsize(切片最大值)：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值<br>minsize(切片最小值)：参数调的比blockSize大，则可以让切片变得比blockSize还大</li>
<li>获取切片信息API</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取切片的文件名称</span></span><br><span class="line">String name = inputSplit.getPath().getName();</span><br><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line">FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>CombineTextInputFormat切片机制<br>框架默认的TextInputFormat切片机制是对任务按文件规划切片，<strong>不管文件多小，都会是一个单独的切片</strong>，都会交给一个MapTask，<strong>这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下</strong></p>
<ul>
<li>应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理</li>
<li>虚拟存储切片最大值设置：CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);(注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值)</li>
<li>切片机制切片机制：生成切片过程包括<strong>虚拟存储过程和切片过程二部分</strong><br><img src="CombineTextInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6.png" alt="CombineTextInputFormat切片机制"><ul>
<li>虚拟存储过程<br>将输入目录下所有文件的大小依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件<strong>均分</strong>成2个虚拟存储块(防止出现太小切片)<br>例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成(2.01M和2.01M)两个文件</li>
<li>切片过程<ul>
<li>判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片</li>
<li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片</li>
<li><strong>测试举例</strong>：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M、(2.55M、2.55M)、3.4M、(3.4M、3.4M)。最终会形成3个切片，大小分别为：(1.7M + 2.55M)，(2.55M + 3.4M)，(3.4M + 3.4M)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>CombineTextInputFormat案例实操</p>
<ul>
<li><p>需求：将输入的大量小文件合并成一个切片统一处理</p>
<ul>
<li>输入数据：准备4个小文件</li>
<li>期望：期望一个切片处理4个文件</li>
</ul>
</li>
<li><p>实现过程</p>
<ul>
<li>不做任何处理，运行之前的WordCount案例程序，观察切片个数为4——(number of splits:4)</li>
<li>在WordcountDriver中增加如下代码(设置切片最大值为4m)，运行程序，观察运行的切片个数为3</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>在WordcountDriver中增加如下代码(设置切片最大值为20m)，运行程序，观察运行的切片个数为1(代码同上,值改为20971520)</li>
</ul>
</li>
</ul>
</li>
<li><p>FileInputFormat实现类<br>思考：在运行MapReduce程序时，<em>输入的文件格式包括：基于行的日志文件、二进制格式文件、数据库表等</em>。那么，针对不同的数据类型，MapReduce是如何读取这些数据的呢?<br>FileInputFormat常见的接口实现类包括：<strong>TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat</strong>等</p>
<ul>
<li>TextInputFormat<br>TextInputFormat是默认的FileInputFormat实现类。<strong>按行读取每条记录，键是存储该行在整个文件中的起始字节偏移量——LongWritable类型；值是这行的内容，不包括任何行终止符(换行符和回车符)——Text类型</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例：</span><br><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对：</span><br><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>

<ul>
<li>KeyValueTextInputFormat<br>每一行均为一条记录，被分隔符分割为&lt;key,value&gt;对。<strong>可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, “\t”)来设定分隔符——默认分隔符是tab(\t)</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例(其中——&gt;表示一个水平方向的制表符)：</span><br><span class="line">line1 ——&gt;Rich learning form</span><br><span class="line">line2 ——&gt;Intelligent learning engine</span><br><span class="line">line3 ——&gt;Learning more convenient</span><br><span class="line">line4 ——&gt;From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 每条记录表示为以下键&#x2F;值对(键是每行排在制表符之前的Text序列)：</span><br><span class="line">(line1,Rich learning form)</span><br><span class="line">(line2,Intelligent learning engine)</span><br><span class="line">(line3,Learning more convenient)</span><br><span class="line">(line4,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>

<ul>
<li>NLineInputFormat<br>如果使用NlineInputFormat，代表每个map进程处理的<strong>InputSplit不再按Block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数 / N = 切片数，如果不整除，切片数 = 商 + 1</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 示例：</span><br><span class="line">Rich learning form</span><br><span class="line">Intelligent learning engine</span><br><span class="line">Learning more convenient</span><br><span class="line">From the real demand for more close to the enterprise</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 如果N是2,则每个输入分片包含两行。开启2个MapTask(键和值与TextInputFormat生成的一样)：</span><br><span class="line">(0,Rich learning form)</span><br><span class="line">(19,Intelligent learning engine)</span><br><span class="line">&#x2F;&#x2F; 另一个mapper则收到后两行：</span><br><span class="line">(47,Learning more convenient)</span><br><span class="line">(72,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure>
</li>
<li><p>KeyValueTextInputFormat使用案例</p>
<ul>
<li><p>需求：统计输入文件中每一行的第一个单词相同的行数</p>
</li>
<li><p>案例分析<br><img src="KeyValueTextInputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="KeyValueTextInputFormat案例分析"></p>
</li>
<li><p>代码实现</p>
<ul>
<li>Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable intWritable = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、封装对象</span></span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      context.write(key, intWritable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable intWritable = <span class="keyword">new</span> IntWritable();</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 1、累加求和</span></span><br><span class="line">      <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">          sum += value.get();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 2、写出</span></span><br><span class="line">      intWritable.set(sum);</span><br><span class="line">      context.write(key, intWritable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">" "</span>);</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(KVTextDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(KVTextMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(KVTextReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setInputFormatClass(KeyValueTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>NLineInputFormat使用案例</p>
<ul>
<li><p>需求：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中</p>
</li>
<li><p>案例分析<br><img src="NLineInputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="NLineInputFormat案例分析"></p>
</li>
<li><p>代码实现</p>
<ul>
<li>Mapper和Reducer类参照WordCount</li>
<li>Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 设置切片InputSplit中划分三条记录</span></span><br><span class="line">  NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">  <span class="comment">// 使用NLineInputFormat处理记录数</span></span><br><span class="line">  job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(NLineDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(NLineMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(NLineReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>观察控制台打印的number of splits</p>
</li>
</ul>
</li>
<li><p>自定义InputFormat<br>在企业开发中，Hadoop框架自带的InputFormat类型不能满足所有应用场景，需要自定义InputFormat来解决实际问题<br>自定义InputFormat步骤如下：</p>
<ul>
<li>自定义一个类继承FileInputFormat</li>
<li>改写RecordReader，实现一次读取一个完整文件封装为KV</li>
<li>在输出时使用SequenceFileOutPutFormat输出合并文件</li>
</ul>
</li>
<li><p>自定义InputFormat案例实操</p>
<ul>
<li><p>需求：将多个小文件合并成一个SequenceFile文件(SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式)，SequenceFile里面存储着多个文件，存储的形式为key——文件路径 + 名称，value——文件内容</p>
</li>
<li><p>案例分析<br><img src="%E8%87%AA%E5%AE%9A%E4%B9%89InputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="自定义InputFormat案例分析"></p>
</li>
<li><p>代码实现</p>
<ul>
<li>自定义InputFormat</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      WholeRecordReader recordReader = <span class="keyword">new</span> WholeRecordReader();</span><br><span class="line">      recordReader.initialize(split, context);</span><br><span class="line">      <span class="keyword">return</span> recordReader;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>自定义RecordReader类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FileSplit split;</span><br><span class="line">  <span class="keyword">private</span> Configuration configuration;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> BytesWritable v = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">  <span class="keyword">boolean</span> isProgress = <span class="keyword">true</span>;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 初始化</span></span><br><span class="line">      <span class="keyword">this</span>.split = (FileSplit) split;</span><br><span class="line">      configuration = context.getConfiguration();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 核心业务逻辑</span></span><br><span class="line">      <span class="comment">// 每个文件创建一次reader</span></span><br><span class="line">      <span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line">          <span class="comment">// 1、获取fs对象</span></span><br><span class="line">          Path path = split.getPath();</span><br><span class="line">          FileSystem fileSystem = path.getFileSystem(configuration);</span><br><span class="line">          <span class="comment">// 2、获取输入流</span></span><br><span class="line">          FSDataInputStream fis = fileSystem.open(path);</span><br><span class="line">          <span class="comment">// 3、拷贝</span></span><br><span class="line">          <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) split.getLength()];</span><br><span class="line">          IOUtils.readFully(fis, buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line">          <span class="comment">// 4、封装v</span></span><br><span class="line">          v.set(buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line">          <span class="comment">// 5、封装key</span></span><br><span class="line">          k.set(path.toString());</span><br><span class="line">          <span class="comment">// 6、关闭资源</span></span><br><span class="line">          IOUtils.closeStream(fis);</span><br><span class="line">          isProgress = <span class="keyword">false</span>;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> k; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> v; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      context.write(key, value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">      <span class="comment">// 循环写出</span></span><br><span class="line">      <span class="keyword">for</span> (BytesWritable value : values) &#123;</span><br><span class="line">          context.write(key, value);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  job.setInputFormatClass(WholeFileInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputFormatClass(SequenceFileOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 2、设计jar路径</span></span><br><span class="line">  job.setJarByClass(SequenceFileDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">  job.setMapperClass(SequenceFileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(SequenceFileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MapReduce工作流程</p>
<ul>
<li>流程示意图<br><img src="MapReduce%E8%AF%A6%E7%BB%86%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B(1).png" alt="MapReduce详细工作流程(1)"><br><img src="MapReduce%E8%AF%A6%E7%BB%86%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B(2).png" alt="MapReduce详细工作流程(2)"></li>
<li>流程详解<br>上面的流程是整个MapReduce的全部工作流程，Shuffle过程是从第7步开始到第16步结束，具体Shuffle过程详解，如下：<ul>
<li>MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</li>
<li>从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</li>
<li>多个溢出文件会被合并成大的溢出文件</li>
<li>在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li>
<li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li>
<li>ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并(归并排序)</li>
<li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程——从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法</li>
</ul>
</li>
<li>注意<br>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。<br>缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M</li>
</ul>
</li>
<li><p>Shuffle</p>
<ul>
<li><p>Shuffle机制：Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle<br><img src="Shuffle%E6%9C%BA%E5%88%B6.png" alt="Shuffle机制"></p>
</li>
<li><p>Partition分区</p>
<ul>
<li>问题引出：要求将统计结果按照条件输出到不同文件中(分区)。比如：将统计结果按照手机归属地不同省份输出到不同文件中(分区)</li>
<li>默认Partitionr分区：默认分区是根据key的hashCode对ReduceTasks个数取模得到的(如果分区数大于1)。用户没法控制哪个key存储到哪个分区</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">K2</span>, <span class="title">V2</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(JobConf job)</span> </span>&#123;&#125;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K2 key, V2 value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>自定义Partitioner步骤</p>
<ul>
<li>自定义类继承Partitioner，重写getPartition()方法</li>
<li>在Job驱动中，设置自定义Partitioner：job.setPartitionerClass(CustomPartitioner.class);</li>
<li>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask：job.setNumReduceTasks(5);</li>
</ul>
</li>
<li><p><strong>分区总结</strong></p>
<ul>
<li>如果ReduceTask的数量 &gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx</li>
<li>如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，会抛出异常</li>
<li>如果ReduceTask的数量 = 1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000</li>
<li>分区号必须从零开始，逐一累加</li>
</ul>
</li>
<li><p>案例分析：假设自定义分区数为5，则</p>
<ul>
<li>job.setNumReduceTasks(1)：会正常运行，只不过会产生一个输出文件</li>
<li>job.setNumReduceTasks(2)：会报错</li>
<li>job.setNumReduceTasks(6)：大于5，程序会正常运行，会产生空文件part-r-00005</li>
</ul>
</li>
</ul>
</li>
<li><p>Partition分区案例实操</p>
<ul>
<li><p>需求：将统计结果按照手机归属地不同省份输出到不同文件中(分区)</p>
</li>
<li><p>案例分析<br><img src="Partition%E5%88%86%E5%8C%BA%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Partition分区案例分析"></p>
</li>
<li><p>实操</p>
<ul>
<li>在FlowBean案例基础上，增加一个分区类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// key是手机号,value是流量信息</span></span><br><span class="line">    <span class="comment">// 获取手机号前三位</span></span><br><span class="line">    String prePhoneNum = text.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="string">"136"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"138"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">2</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"139"</span>.equals(prePhoneNum)) &#123;</span><br><span class="line">        partition = <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> partition;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>在Driver驱动主函数中添加自定义数据分区设置和ReduceTask设置</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定自定义数据分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 指定ReduceTask的数目</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">6</span>);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>WritableComparable排序</p>
<ul>
<li><p>排序概述：<br>排序是MapReduce框架中最重要的操作之一。MapTask和ReduceTask均会对数据<strong>按照key进行排序</strong>，该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是<strong>按照字典顺序</strong>排序，且实现该排序的方法是<strong>快速排序</strong>。<br>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。<br>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</p>
</li>
<li><p>排序的分类</p>
<ul>
<li>部分排序：MapReduce根据输入记录的键对数据集排序。保证<strong>输出的每个文件内部有序</strong></li>
<li>全排序：<strong>最终输出结果只有一个文件，且文件内部有序</strong>。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构</li>
<li>辅助排序(GroupingComparator分组)：在Reduce端对key进行分组。应用——在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序</li>
<li>二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序</li>
</ul>
</li>
<li><p>自定义排序WritableComparable</p>
<ul>
<li><p>原理分析：bean对象做为key传输，需要<strong>实现WritableComparable接口重写compareTo方法</strong>，就可以实现排序</p>
</li>
<li><p>案例实操(全排序)</p>
<ul>
<li><p>需求：对FlowBean案例产生的结果再次对总流量进行排序</p>
</li>
<li><p>案例分析<br><img src="WritableComparable%E6%8E%92%E5%BA%8F%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="WritableComparable排序案例分析"></p>
</li>
<li><p>代码实现</p>
<ul>
<li>FlowBean对象在之前案例基础上实现WritableComparable接口，实现compareTo()方法</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 按总流量倒序</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; bean.sumFlow ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、封装对象</span></span><br><span class="line">    v.set(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">long</span> upFlow = Long.parseLong(fields[fields.length - <span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">long</span> downFlow = Long.parseLong(fields[fields.length - <span class="number">2</span>]);</span><br><span class="line">    flowBean.set(upFlow, downFlow);</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(flowBean, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">        context.write(value, key);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 2、设计jar路径</span></span><br><span class="line">  job.setJarByClass(FlowCountSortDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联mapper和reducer</span></span><br><span class="line">  job.setMapperClass(FlowCountSortMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(FlowCountSortReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置mapper输出的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Combiner合并</p>
<ul>
<li><p>Combiner介绍<br><img src="Combiner%E4%BB%8B%E7%BB%8D.png" alt="Combiner介绍"></p>
</li>
<li><p>自定义Combiner实现步骤</p>
<ul>
<li>自定义一个Combiner继承Reducer，重写reduce方法</li>
<li>在Driver驱动类中设置：job.setCombinerClass(xxCombiner.class);</li>
</ul>
</li>
<li><p>自定义Combiner实操</p>
<ul>
<li><p>需求：统计过程中对每一个MapTask的输出进行局部汇总，以减小网络传输量即采用Combiner功能</p>
</li>
<li><p>案例分析<br><img src="Combiner%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Combiner案例分析"></p>
</li>
<li><p>代码实现</p>
<ul>
<li>方案一</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 新建WordCountCombiner类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、累加求和</span></span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">        sum += value.get();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、写出</span></span><br><span class="line">    v.set(sum);</span><br><span class="line">    context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在WordCountDriver驱动类中指定Combiner</span></span><br><span class="line">job.setCombinerClass(WordcountCombiner<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>方案二：将WordCountReducer作为Combiner</li>
</ul>
</li>
<li><p>结果查看：控制台打印中的Map-Reduce Framework中的Combine记录</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>GroupingComparator分组(辅助排序)</p>
<ul>
<li><p>简要介绍：对Reduce阶段的数据根据某一个或几个字段进行分组</p>
</li>
<li><p>分组排序步骤</p>
<ul>
<li>自定义类继承WritableComparator</li>
<li>重写compare()方法</li>
<li>创建一个构造将比较对象的类传给父类</li>
</ul>
</li>
<li><p>GroupingComparator分组实操</p>
<ul>
<li><p>需求：求出每一个订单中最贵的商品</p>
</li>
<li><p>案例分析<br><img src="GroupingComparator%E5%88%86%E7%BB%84%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="GroupingComparator分组案例分析"></p>
<ul>
<li>利用“订单id和成交金额price”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序。Reduce将从Map中获取排序好的数据</li>
<li>在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品</li>
</ul>
</li>
<li><p>代码实现</p>
<ul>
<li>编写订单信息OrderBean类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 忽略了空参/全参构造器、getter/setter和toString方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> orderId;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">double</span> price;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean bean)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result;</span><br><span class="line">    <span class="keyword">if</span> (orderId &gt; bean.orderId) &#123;</span><br><span class="line">        result = <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (orderId &lt; bean.orderId) &#123;</span><br><span class="line">        result = -<span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        result = price &gt; bean.price ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeInt(orderId);</span><br><span class="line">    out.writeDouble(price);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    orderId = in.readInt();</span><br><span class="line">    price = in.readDouble();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写OrderGroupingComparator类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(OrderBean<span class="class">.<span class="keyword">class</span>, <span class="title">true</span>)</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 要求只要id相同,就认为是相同的key</span></span><br><span class="line">    OrderBean aBean = (OrderBean) a;</span><br><span class="line">    OrderBean bBean = (OrderBean) b;</span><br><span class="line">    <span class="keyword">if</span> (aBean.getOrderId() == bBean.getOrderId()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> aBean.getOrderId() &gt; bBean.getOrderId() ? <span class="number">1</span> : -<span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写OrderMapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> OrderBean orderBean = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、封装对象</span></span><br><span class="line">    orderBean.setOrderId(Integer.parseInt(fields[<span class="number">0</span>]));</span><br><span class="line">    orderBean.setPrice(Double.parseDouble(fields[<span class="number">2</span>]));</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(orderBean, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写OrderReducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 输出第一个</span></span><br><span class="line">    context.write(key, NullWritable.get());</span><br><span class="line">    <span class="comment">// 循环几次输出前几</span></span><br><span class="line">    <span class="comment">//for (NullWritable value : values) &#123;</span></span><br><span class="line">    <span class="comment">//    context.write(key, NullWritable.get());</span></span><br><span class="line">    <span class="comment">//&#125;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>编写OrderDriver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取Job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置jar存储位置</span></span><br><span class="line">  job.setJarByClass(OrderDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联Map和Reduce类</span></span><br><span class="line">  job.setMapperClass(OrderMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(OrderReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置map阶段输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置最终数据输出的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(OrderBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setGroupingComparatorClass(OrderGroupingComparator<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、提交job</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  <span class="comment">// 额外</span></span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MapTask工作机制<br><img src="MapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="MapTask工作机制"></p>
<ul>
<li>Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value</li>
<li>Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value</li>
<li>Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区(调用Partitioner)，并写入一个环形内存缓冲区中</li>
<li>Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作<ul>
<li>溢写步骤1：利用快速排序算法对缓存区内的数据进行排序。排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序</li>
<li>溢写步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out(N表示当前溢写次数)中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作</li>
<li>溢写步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中</li>
</ul>
</li>
<li>Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件<br>当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。<br>在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor(默认10)个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。<br>让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销</li>
</ul>
</li>
<li><p>ReduceTask</p>
<ul>
<li><p>ReduceTask工作机制<br><img src="ReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="ReduceTask工作机制"></p>
<ul>
<li>Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中</li>
<li>Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多</li>
<li>Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可</li>
<li>Reduce阶段：reduce()函数将计算结果写到HDFS上</li>
</ul>
</li>
<li><p>设置ReduceTask并行度(个数)<br>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置(默认值为1)：job.setNumReduceTasks(4);</p>
</li>
<li><p>一个测试ReduceTask数目的实验</p>
<ul>
<li>实验环境：1个Master节点，16个Slave节点；CPU：8GHZ，内存: 2G</li>
<li>实验结论(数据量为1G)</li>
</ul>
<table>
<thead>
<tr>
<th>ReduceTask数目</th>
<th>总时间</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>892</td>
</tr>
<tr>
<td>5</td>
<td>146</td>
</tr>
<tr>
<td>10</td>
<td>110</td>
</tr>
<tr>
<td>15</td>
<td>92</td>
</tr>
<tr>
<td>16</td>
<td>88</td>
</tr>
<tr>
<td>20</td>
<td>100</td>
</tr>
<tr>
<td>25</td>
<td>128</td>
</tr>
<tr>
<td>30</td>
<td>101</td>
</tr>
<tr>
<td>45</td>
<td>145</td>
</tr>
<tr>
<td>60</td>
<td>104</td>
</tr>
</tbody></table>
</li>
<li><p>注意事项</p>
<ul>
<li>ReduceTask = 0，表示没有Reduce阶段，输出文件个数和Map个数一致</li>
<li>ReduceTask默认值就是1，所以输出文件个数为一个</li>
<li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜(一些节点很忙,其余空闲)</li>
<li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask</li>
<li>具体多少个ReduceTask，需要根据集群性能而定</li>
<li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程？答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行</li>
</ul>
</li>
</ul>
</li>
<li><p>OutputFormat数据输出</p>
<ul>
<li><p>OutputFormat接口实现类<br>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了OutputFormat接口。以下是几种常见的OutputFormat实现类</p>
<ul>
<li><p>文本输出TextOutputFormat<br>默认的输出格式是TextOutputFormat，<strong>它把每条记录写为文本行</strong>。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串</p>
</li>
<li><p>SequenceFileOutputFormat<br>将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的<strong>格式紧凑，很容易被压缩</strong></p>
</li>
<li><p>自定义OutputFormat：根据用户需求，自定义实现输出</p>
<ul>
<li><p>使用场景<br>为了实现<strong>控制最终文件的输出路径和输出格式</strong>，可以自定义OutputFormat<br>例如：要在一个MapReduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义OutputFormat来实现</p>
</li>
<li><p>自定义OutputFormat步骤</p>
<ul>
<li>自定义一个类继承FileOutputFormat</li>
<li>改写RecordWriter，具体改写输出数据的方法write()</li>
</ul>
</li>
<li><p>自定义OutputFormat案例实操</p>
<ul>
<li><p>需求：过滤输入的log日志，指定包含某特定字段的记录输出到一个文件，其他则输出到另一个文件</p>
</li>
<li><p>案例分析<br><img src="%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="自定义OutputFormat案例分析"></p>
</li>
<li><p>代码编写</p>
<ul>
<li>Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// http://www.baidu.com</span></span><br><span class="line">    context.write(value, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    String line = key.toString();</span><br><span class="line">    line += <span class="string">"\r\n"</span>;</span><br><span class="line">    k.set(line);</span><br><span class="line">    <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">        context.write(k, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>自定义OutputFormat、RecordWriter类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> FRecordWriter(job);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> FSDataOutputStream fosSobxiong;</span><br><span class="line">  <span class="keyword">private</span> FSDataOutputStream fosOther;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">FRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 1、获取文件系统</span></span><br><span class="line">      FileSystem fs = FileSystem.get(job.getConfiguration());</span><br><span class="line">      <span class="comment">// 2、创建输出到sobxiong.log的输出流</span></span><br><span class="line">      fosSobxiong = fs.create(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Downloads/sobxiong.log"</span>));</span><br><span class="line">      <span class="comment">// 3、创建输出到other.log的输出流</span></span><br><span class="line">      fosOther = fs.create(<span class="keyword">new</span> Path(<span class="string">"/Users/sobxiong/Downloads/other.log"</span>));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 判断key中是否有sobxiong,如果有写出到sobxiong,否则输出到other</span></span><br><span class="line">    <span class="keyword">if</span> (key.toString().contains(<span class="string">"sobxiong"</span>)) &#123;</span><br><span class="line">        fosSobxiong.write(key.toString().getBytes());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fosOther.write(key.toString().getBytes());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    IOUtils.closeStream(fosOther);</span><br><span class="line">    IOUtils.closeStream(fosSobxiong);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">  job.setJarByClass(FilterDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapperClass(FilterMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(FilterReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">  job.setOutputFormatClass(FilterOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 虽然我们自定义了outputFormat,但是因为我们的outputFormat继承自fileOutputFormat</span></span><br><span class="line">  <span class="comment">// 而fileOutputFormat要输出一个_SUCCESS文件,所以,在这还得指定一个输出目录</span></span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Join多种应用</p>
<ul>
<li><p>Reduce Join</p>
<ul>
<li><p>工作原理<br>Map端的主要工作：为来自不同表或文件的key/value对，<strong>打标签以区别不同来源的记录</strong>。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出<br>Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些<strong>来源于不同文件的记录(在Map阶段已经打标志)分开</strong>，最后进行合并</p>
</li>
<li><p>案例实操</p>
<ul>
<li><p>需求：将商品信息表中数据根据商品pid合并到订单数据表中</p>
</li>
<li><p>案例分析：通过将关联条件作为Map输出的key，将两表满足Join条件的数据并携带数据所来源的文件信息，发往同一个ReduceTask，在Reduce中进行数据的串联<br><img src="Reduce%E7%AB%AF%E8%A1%A8%E5%90%88%E5%B9%B6.png" alt="Reduce端表合并"></p>
</li>
<li><p>代码编写</p>
<ul>
<li>合并后的Bean类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 略去空参/全参构造器、getter/setter方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 订单id</span></span><br><span class="line">  <span class="keyword">private</span> String id;</span><br><span class="line">  <span class="comment">// 产品id</span></span><br><span class="line">  <span class="keyword">private</span> String pid;</span><br><span class="line">  <span class="comment">// 数量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> amount;</span><br><span class="line">  <span class="comment">// 产品名称</span></span><br><span class="line">  <span class="keyword">private</span> String pName;</span><br><span class="line">  <span class="comment">// 标记: 产品/订单</span></span><br><span class="line">  <span class="keyword">private</span> String flag;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> id + <span class="string">'\t'</span> + amount + <span class="string">'\t'</span> + pName;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeUTF(id);</span><br><span class="line">    out.writeUTF(pid);</span><br><span class="line">    out.writeInt(amount);</span><br><span class="line">    out.writeUTF(pName);</span><br><span class="line">    out.writeUTF(flag);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    id = in.readUTF();</span><br><span class="line">    pid = in.readUTF();</span><br><span class="line">    amount = in.readInt();</span><br><span class="line">    pName = in.readUTF();</span><br><span class="line">    flag = in.readUTF();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String fileName;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> TableBean tableBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Text key = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取文件的名称</span></span><br><span class="line">    FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">    fileName = inputSplit.getPath().getName();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// id pid amount</span></span><br><span class="line">  <span class="comment">// 1001 01 1</span></span><br><span class="line">  <span class="comment">// pid pname</span></span><br><span class="line">  <span class="comment">// 01 小米</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="keyword">if</span> (fileName.startsWith(<span class="string">"order"</span>)) &#123; <span class="comment">// 订单表</span></span><br><span class="line">        <span class="comment">// 封装kv</span></span><br><span class="line">        tableBean.setId(fields[<span class="number">0</span>]);</span><br><span class="line">        tableBean.setPid(fields[<span class="number">1</span>]);</span><br><span class="line">        tableBean.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">        <span class="comment">// 属性不能为空,不然会序列化会出错</span></span><br><span class="line">        tableBean.setpName(<span class="string">""</span>);</span><br><span class="line">        tableBean.setFlag(<span class="string">"order"</span>);</span><br><span class="line">        <span class="keyword">this</span>.key.set(fields[<span class="number">1</span>]);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// 产品表</span></span><br><span class="line">        <span class="comment">// 封装kv</span></span><br><span class="line">        tableBean.setId(<span class="string">""</span>);</span><br><span class="line">        tableBean.setPid(fields[<span class="number">0</span>]);</span><br><span class="line">        tableBean.setAmount(<span class="number">0</span>);</span><br><span class="line">        tableBean.setpName(fields[<span class="number">1</span>]);</span><br><span class="line">        tableBean.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">        <span class="keyword">this</span>.key.set(fields[<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 写出</span></span><br><span class="line">    context.write(<span class="keyword">this</span>.key, tableBean);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Reducer类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 存储所有订单集合</span></span><br><span class="line">    List&lt;TableBean&gt; beans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="comment">// 存储产品信息</span></span><br><span class="line">    TableBean pdBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">    <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"order"</span>.equals(value.getFlag())) &#123;</span><br><span class="line">            TableBean tmpBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// value是引用,tmpBean是实实在在的对象</span></span><br><span class="line">                BeanUtils.copyProperties(tmpBean, value);</span><br><span class="line">                beans.add(tmpBean);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                BeanUtils.copyProperties(pdBean, value);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 拼接表,设置商品名称</span></span><br><span class="line">    <span class="keyword">for</span> (TableBean bean : beans) &#123;</span><br><span class="line">        bean.setpName(pdBean.getpName());</span><br><span class="line">        context.write(bean, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取配置信息,创建job对象实例</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定本程序的jar包所在的本地路径</span></span><br><span class="line">  job.setJarByClass(TableDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定本业务job要使用的Mapper/Reducer业务类</span></span><br><span class="line">  job.setMapperClass(TableMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TableReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定Mapper输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(TableBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始文件所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>测试结果：</li>
</ul>
<table>
<thead>
<tr>
<th>pid</th>
<th>pname</th>
<th>amount</th>
</tr>
</thead>
<tbody><tr>
<td>1001</td>
<td>小米</td>
<td>1</td>
</tr>
<tr>
<td>1001</td>
<td>小米</td>
<td>1</td>
</tr>
<tr>
<td>1002</td>
<td>华为</td>
<td>2</td>
</tr>
<tr>
<td>1002</td>
<td>华为</td>
<td>2</td>
</tr>
<tr>
<td>1003</td>
<td>格力</td>
<td>3</td>
</tr>
<tr>
<td>1003</td>
<td>格力</td>
<td>3</td>
</tr>
</tbody></table>
<ul>
<li>总结<ul>
<li>缺点：这种方式中，合并的操作是在Reduce阶段完成，Reduce端的处理压力太大，Map节点的运算负载则很低，资源利用率不高，且在Reduce阶段极易产生数据倾斜</li>
<li>解决方案：Map端实现数据合并</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Map Join</p>
<ul>
<li><p>使用场景：适用于一张表十分小、一张表很大的场景</p>
</li>
<li><p>优点<br>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？<br>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜</p>
</li>
<li><p>具体方法：采用DistributedCache</p>
<ul>
<li>在Mapper的setup阶段，将文件读取到缓存集合中</li>
<li>在驱动函数中加载缓存(缓存普通文件到Task运行节点)：job.addCacheFile(new URI(“file:///Users/sobxiong/Downloads/testInput3/pd.txt”))</li>
</ul>
</li>
<li><p>案例实操</p>
<ul>
<li><p>需求同Reduce Join</p>
</li>
<li><p>案例分析<br><img src="Map%E7%AB%AF%E8%A1%A8%E5%90%88%E5%B9%B6.png" alt="Map端表合并"></p>
</li>
<li><p>代码编写</p>
<ul>
<li>Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job信息</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、设置加载jar包路径</span></span><br><span class="line">  job.setJarByClass(DistributedCacheDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联map</span></span><br><span class="line">  job.setMapperClass(DistributedCacheMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 没有reduce阶段,map阶段输出即为最终输出</span></span><br><span class="line">  <span class="comment">// 4、设置最终输出数据类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、设置输入输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 6、加载缓存数据</span></span><br><span class="line">  job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///Users/sobxiong/Downloads/testInput3/pd.txt"</span>));</span><br><span class="line">  <span class="comment">// 7、Map端Join的逻辑不需要Reduce阶段，设置reduceTask数量为0</span></span><br><span class="line">  job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 8、提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributedCacheMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">5</span>);</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 缓存小表</span></span><br><span class="line">    String cachePath = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">    BufferedReader bufferedReader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(cachePath), StandardCharsets.UTF_8));</span><br><span class="line">    String lineStr;</span><br><span class="line">    <span class="keyword">while</span> (StringUtils.isNotEmpty(lineStr = bufferedReader.readLine())) &#123;</span><br><span class="line">        <span class="comment">// 1、切割</span></span><br><span class="line">        <span class="comment">// pid pname</span></span><br><span class="line">        <span class="comment">// 01 小米</span></span><br><span class="line">        String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">        <span class="comment">// 2、封装到集合去</span></span><br><span class="line">        pdMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(bufferedReader);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// id pid amount</span></span><br><span class="line">    <span class="comment">// 1001 01 1</span></span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">";"</span>);</span><br><span class="line">    <span class="comment">// 3、获取pid</span></span><br><span class="line">    String pid = fields[<span class="number">1</span>];</span><br><span class="line">    <span class="comment">// 4、取出pname</span></span><br><span class="line">    String pName = pdMap.get(pid);</span><br><span class="line">    <span class="comment">// 5、拼接</span></span><br><span class="line">    lineStr = lineStr.replace(<span class="string">';'</span>, <span class="string">'\t'</span>) + <span class="string">'\t'</span> + pName;</span><br><span class="line">    k.set(lineStr);</span><br><span class="line">    <span class="comment">// 6、写出</span></span><br><span class="line">    context.write(k, NullWritable.get());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>计数器应用<br>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量</p>
<ul>
<li><p>计数器API</p>
<ul>
<li>采用枚举的方式统计计数</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> MyCounter&#123;MALFORORMED,NORMAL&#125;</span><br><span class="line"><span class="comment">//对枚举定义的自定义计数器加1</span></span><br><span class="line">context.getCounter(MyCounter.MALFORORMED).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>采用计数器组、计数器名称的方式统计</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 组名和计数器名称随便起,但最好有意义</span></span><br><span class="line">context.getCounter(<span class="string">"counterGroup"</span>, <span class="string">"counter"</span>).increment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>计数结果在程序运行后的控制台上查看</li>
</ul>
</li>
</ul>
</li>
<li><p>数据清洗(ETL)<br>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序</p>
<ul>
<li><p>案例实操(简单解析版——运用计数器)——复杂版(字段多,过滤的需求多,思路与下面无差)</p>
<ul>
<li><p>需求：去除日志中字段长度小于等于11的日志</p>
</li>
<li><p>代码编写</p>
<ul>
<li>Mapper类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    <span class="comment">// 2、解析数据</span></span><br><span class="line">    <span class="keyword">boolean</span> isDirty = parseLog(line, context);</span><br><span class="line">    <span class="keyword">if</span> (!isDirty) &#123;</span><br><span class="line">        <span class="comment">// 3、解析通过,写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line">    String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line">    <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"clean"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"dirty"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Driver类</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job信息</span></span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  <span class="comment">// 2、加载jar包</span></span><br><span class="line">  job.setJarByClass(LogDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、关联map</span></span><br><span class="line">  job.setMapperClass(LogMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、设置最终输出类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 设置reduceTask个数为0</span></span><br><span class="line">  job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">  <span class="comment">// 5、设置输入和输出路径</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 6、提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MapReduce开发总结<br>编写MapReduce程序时，需要考虑如下方面</p>
<ul>
<li><p>输入数据接口：InputFormat</p>
<ul>
<li>默认使用的实现类是：TextInputFormat</li>
<li>TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回</li>
<li>KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key/value。默认分隔符是tab(\t)</li>
<li>NlineInputFormat按照指定的行数N来划分切片</li>
<li>CombineTextInputFormat可以把多个小文件合并成一个切片处理，提高处理效率</li>
<li>自定义InputFormat</li>
</ul>
</li>
<li><p>逻辑处理接口：Mapper<br>用户根据业务需求实现其中三个方法：map()、setup()、cleanup()</p>
</li>
<li><p>Partitioner分区</p>
<ul>
<li>默认实现HashPartitioner，逻辑是根据key的哈希值和numReduces来返回一个分区号(分区数大于1时)</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key.hashCode() &amp; Integer.MAXVALUE % numReduces</span><br></pre></td></tr></table></figure>

<ul>
<li>如果业务上有特别的需求，可以自定义分区</li>
</ul>
</li>
<li><p>Comparable排序</p>
<ul>
<li>当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，重写其中的compareTo()方法</li>
<li>部分排序：对最终输出的每一个文件进行内部排序</li>
<li>全排序：对所有数据进行排序，通常只有一个Reduce</li>
<li>二次排序：排序的条件有两个</li>
</ul>
</li>
<li><p>Combiner合并<br>Combiner合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果</p>
</li>
<li><p>Reduce端分组：GroupingComparator<br>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同(全部字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序</p>
</li>
<li><p>逻辑处理接口：Reduce<br>用户根据业务需求实现其中三个方法：reduce()、setup()、cleanup()</p>
</li>
<li><p>输出数据接口：OutputFormat</p>
<ul>
<li>默认实现类是TextOutputFormat，功能逻辑是：将每一个kv对向目标文本文件输出一行</li>
<li>将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩</li>
<li>自定义OutputFormat</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Hadoop数据压缩"><a href="#Hadoop数据压缩" class="headerlink" title="Hadoop数据压缩"></a>Hadoop数据压缩</h2><ul>
<li><p>概述<br>压缩技术能够有效减少底层存储系统(HDFS)读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要<br>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，<strong>数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩</strong>。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价</p>
<ul>
<li>压缩策略和原则<br>压缩是提高Hadoop运行效率的一种优化策略<br>通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度</li>
</ul>
<p><strong>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能</strong><br>压缩基本原则</p>
<ul>
<li>运算密集型的job，少用压缩</li>
<li>IO密集型的job，多用压缩</li>
</ul>
</li>
<li><p>MR支持的压缩编码</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>是否hadoop自带</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后,原来程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是,直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样,不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是,直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样,不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是,直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样,不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否,需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引,还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否,需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样,不需要修改</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>Gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
<tr>
<td>Snappy</td>
<td>8.3GB</td>
<td>较大</td>
<td>最快</td>
<td>最快</td>
</tr>
</tbody></table>
<ul>
<li><p>压缩方式选择</p>
<ul>
<li>Gzip压缩<ul>
<li>优点<br>压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便</li>
<li>缺点：不支持Split(切片)</li>
<li>应用场景</li>
</ul>
<strong>当每个文件压缩之后在130M以内的(1个块大小内)，都可以考虑用Gzip压缩格式</strong>。例如说一天或者一个小时的日志压缩成一个Gzip文件</li>
<li>Bzip2压缩<ul>
<li>优点：支持Split(切片)；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便</li>
<li>缺点：压缩/解压速度慢</li>
<li>应用场景<br>适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split(切片)，而且兼容之前的应用程序的情况</li>
</ul>
</li>
<li>Lzo压缩<ul>
<li>优点：压缩/解压速度也比较快，合理的压缩率；支持Split(切片)，是Hadoop中最流行的压缩格式之一；可以在Linux系统下安装lzop命令，使用方便</li>
<li>缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理(为了支持Split需要建索引，还需要指定InputFormat为Lzo格式)</li>
<li>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显</li>
</ul>
</li>
<li>Snappy压缩<ul>
<li>优点：高速压缩速度和合理的压缩率</li>
<li>缺点：不支持Split(切片)；压缩率比Gzip要低；Hadoop本身不支持，需要安装</li>
<li>应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入</li>
</ul>
</li>
</ul>
</li>
<li><p>压缩位置选择<br>压缩可以在MapReduce作用的任意阶段启用<br><img src="MapReduce%E5%8E%8B%E7%BC%A9%E4%BD%8D%E7%BD%AE.png" alt="MapReduce压缩位置"></p>
</li>
<li><p>压缩参数配置<br>要在Hadoop中启用压缩，可配置如下参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs(在core-site.xml中配置)</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress(在mapred-site.xml中配置)</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec(在mapred-site.xml中配置)</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress(在mapred-site.xml中配置)</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec(在mapred-site.xml中配置)</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type(在mapred-site.xml中配置)</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
</li>
<li><p>压缩实操</p>
<ul>
<li>数据流的压缩和解压缩<br>CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据：<ul>
<li>要想对正在被写入一个输出流的数据进行<strong>压缩</strong>，我们可以使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流</li>
<li>相反，要想对从输入流读取而来的数据进行<strong>解压缩</strong>，则调用createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream，从而从底层的流读取未压缩的数据<br>测试如下的压缩方式：<table>
<thead>
<tr>
<th>压缩格式</th>
<th>编解码类</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  compress(<span class="string">"/Users/sobxiong/Downloads/test.txt"</span>, <span class="string">"org.apache.hadoop.io.compress.BZip2Codec"</span>);</span><br><span class="line">  decompress(<span class="string">"/Users/sobxiong/Downloads/test.txt.bz2"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 解压缩</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、压缩方式检查</span></span><br><span class="line">  CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line">  CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(filePath));</span><br><span class="line">  <span class="keyword">if</span> (codec == <span class="keyword">null</span>) &#123;</span><br><span class="line">      System.out.println(<span class="string">"Can't process!"</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 2、获取输入流</span></span><br><span class="line">  FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filePath));</span><br><span class="line">  CompressionInputStream cis = codec.createInputStream(fis);</span><br><span class="line">  <span class="comment">// 3、获取输出流</span></span><br><span class="line">  FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filePath + <span class="string">".decode"</span>));</span><br><span class="line">  <span class="comment">// 4、流的对拷</span></span><br><span class="line">  IOUtils.copyBytes(cis, fos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">10</span>, <span class="keyword">false</span>);</span><br><span class="line">  <span class="comment">// 5、关闭资源</span></span><br><span class="line">  IOUtils.closeStream(fos);</span><br><span class="line">  IOUtils.closeStream(cis);</span><br><span class="line">  IOUtils.closeStream(fis);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 压缩</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String filePath, String compressTypeClassName)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取输入流</span></span><br><span class="line">  FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filePath));</span><br><span class="line">  <span class="comment">// 2、获取输出流</span></span><br><span class="line">  Class&lt;?&gt; classCodec = Class.forName(compressTypeClassName);</span><br><span class="line">  CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(classCodec, <span class="keyword">new</span> Configuration());</span><br><span class="line">  FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filePath + codec.getDefaultExtension()));</span><br><span class="line">  CompressionOutputStream cos = codec.createOutputStream(fos);</span><br><span class="line">  <span class="comment">// 3、流的对拷</span></span><br><span class="line">  IOUtils.copyBytes(fis, cos, <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">10</span>, <span class="keyword">false</span>);</span><br><span class="line">  <span class="comment">// 4、关闭资源</span></span><br><span class="line">  IOUtils.closeStream(cos);</span><br><span class="line">  IOUtils.closeStream(fos);</span><br><span class="line">  IOUtils.closeStream(fis);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Map输出端采用压缩(以万能的WordCount案例为例)<br>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可<br>具体实现(只修改Driver部分代码,Mapper和Reducer不变)：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1、获取Job对象</span></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec<span class="class">.<span class="keyword">class</span>, <span class="title">CompressionCodec</span>.<span class="title">class</span>)</span>;</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 之后代码保持原样</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Reduce输出端采用压缩(以万能的WordCount案例为例)<br>具体实现(只修改Driver部分代码,Mapper和Reducer不变)：</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 之前代码保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, GzipCodec<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"><span class="comment">// 6、设置程序运行的输入和输出路径</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 之后代码保持不变</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="Yarn资源调度器"><a href="#Yarn资源调度器" class="headerlink" title="Yarn资源调度器"></a>Yarn资源调度器</h2><p>Yarn是一个资源调度平台，<strong>负责为运算程序提供服务器运算资源</strong>，相当于一个<strong>分布式的操作系统平台</strong>，而MapReduce等运算程序则相当于<strong>运行于操作系统之上的应用程序</strong></p>
<ul>
<li><p>Yarn基本架构<br>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成<br><img src="Yarn%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png" alt="Yarn基本架构"></p>
</li>
<li><p>Yarn工作机制</p>
<ul>
<li>Yarn工作机制图解<br><img src="Yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt="Yarn工作机制"></li>
<li>工作机制详解<ul>
<li>MR程序提交到客户端所在的节点</li>
<li>YarnRunner向ResourceManager申请一个Application</li>
<li>RM将该应用程序的资源路径返回给YarnRunner</li>
<li>该程序将运行所需资源提交到HDFS上</li>
<li>程序资源提交完毕后，申请运行MrAppMaster</li>
<li>RM将用户的请求初始化成一个Task</li>
<li>其中一个NodeManager领取到Task任务</li>
<li>该NodeManager创建容器Container，并产生MrAppmaster</li>
<li>Container从HDFS上拷贝资源到本地</li>
<li>MrAppmaster向RM申请运行MapTask的资源</li>
<li>RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器</li>
<li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序</li>
<li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器运行ReduceTask</li>
<li>ReduceTask向MapTask获取相应分区的数据</li>
<li>程序运行完毕后，MR会向RM申请注销自己</li>
</ul>
</li>
</ul>
</li>
<li><p>作业提交全过程</p>
<ul>
<li>作业提交过程之Yarn图解<br><img src="%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BYarn.png" alt="作业提交过程之Yarn"></li>
<li>作业提交过程之MapReduce图解<br><img src="%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BMapReduce.png" alt="作业提交过程之MapReduce"></li>
<li>作业提交过程详解<ul>
<li>作业提交<ul>
<li>Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业</li>
<li>Client向RM申请一个作业id</li>
<li>RM给Client返回该job资源的提交路径和作业id</li>
<li>Client提交jar包、切片信息和配置文件到指定的资源提交路径</li>
<li>Client提交完资源后，向RM申请运行MrAppMaster</li>
</ul>
</li>
<li>作业初始化<ul>
<li>当RM收到Client的请求后，将该job添加到容量调度器中</li>
<li>某一个空闲的NM领取到该Job</li>
<li>该NM创建Container，并产生MrAppmaster</li>
<li>下载Client提交的资源到本地</li>
</ul>
</li>
<li>任务分配<ul>
<li>MrAppMaster向RM申请运行多个MapTask任务资源</li>
<li>RM将需运行的MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器</li>
</ul>
</li>
<li>任务运行<ul>
<li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask将对数据分区排序</li>
<li>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask</li>
<li>ReduceTask向MapTask获取相应分区的数据</li>
<li>程序运行完毕后，MR会向RM申请注销自己</li>
</ul>
</li>
<li>进度和状态更新<br>YARN中的任务将其进度和状态(包括counter)返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新，展示给用户</li>
<li>作业完成<br>除了向应用管理器请求作业进度外，客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后，应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查</li>
</ul>
</li>
</ul>
</li>
<li><p>资源调度器<br>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。</p>
<ul>
<li>先进先出调度器(FIFO)<br><img src="FIFO%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="FIFO调度器"></li>
<li>容量调度器(Capacity Scheduler)<br><img src="%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="容量调度器"></li>
<li>公平调度器(Fair Scheduler)<br><img src="%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8.png" alt="公平调度器"></li>
</ul>
<p>Hadoop3.1.3默认的资源调度器是Capacity Scheduler。具体设置详见yarn-default.xml文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>任务的推测执行</p>
<ul>
<li><p>作业完成时间取决于最慢的任务完成时间<br>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。<br>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p>
</li>
<li><p>推测执行机制<br>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果</p>
</li>
<li><p>执行推测任务的前提条件</p>
<ul>
<li>每个Task只能有一个备份任务</li>
<li>当前Job已完成的Task必须不小于0.05(5%)</li>
<li>开启推测执行参数设置。mapred-site.xml文件中默认是打开的</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>不能启用推测执行机制情况<ul>
<li>任务间存在严重的负载倾斜</li>
<li>特殊任务，比如任务向数据库中写数据</li>
</ul>
</li>
<li>算法原理<br><img src="%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86.png" alt="推测执行算法原理"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Hadoop企业优化"><a href="#Hadoop企业优化" class="headerlink" title="Hadoop企业优化"></a>Hadoop企业优化</h2><ul>
<li><p>MapReduce跑的慢的原因<br>MapReduce程序效率的瓶颈在于两点：</p>
<ul>
<li>计算机性能：CPU、内存、磁盘健康、网络</li>
<li>I/O操作优化<ul>
<li>I/O操作优化</li>
<li>Map和Reduce数设置不合理</li>
<li>Map运行时间太长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量的不可分块的超大文件</li>
<li>Spill次数过多</li>
<li>Merge次数过多等</li>
</ul>
</li>
</ul>
</li>
<li><p>MapReduce优化方法<br>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数</p>
<ul>
<li><p>数据输入</p>
<ul>
<li>合并小文件：在执行MR任务前将小文件进行合并，大量的小文件会产生大量的Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢</li>
<li>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景</li>
</ul>
</li>
<li><p>Map阶段</p>
<ul>
<li><strong>减少溢写(Spill)次数</strong>：通过调整io.sort.mb及sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO</li>
<li><strong>减少合并(Merge)次数</strong>：通过调整io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间</li>
<li>在Map之后，<strong>不影响业务逻辑前提下，先进行Combine处理</strong>，减少I/O</li>
</ul>
</li>
<li><p>Reduce阶段</p>
<ul>
<li><strong>合理设置Map和Reduce数</strong>：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误</li>
<li><strong>设置Map、Reduce共存</strong>：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</li>
<li><strong>规避使用Reduce</strong>：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗</li>
<li><strong>合理设置Reduce端的Buffer</strong>：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：<strong>mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整</strong></li>
</ul>
</li>
<li><p>I/O传输</p>
<ul>
<li><strong>采用数据压缩的方式</strong>，减少网络IO的的时间。安装Snappy和LZO压缩编码器</li>
<li><strong>使用SequenceFile二进制文件</strong></li>
</ul>
</li>
<li><p>数据倾斜问题</p>
<ul>
<li>数据倾斜现象<ul>
<li>数据频率倾斜：某一个区域的数据量要远远大于其他区域</li>
<li>数据大小倾斜：部分记录的大小远远大于平均值</li>
</ul>
</li>
<li>减少数据倾斜的方法<ul>
<li><strong>抽样和范围分区</strong>：可以通过对原始数据进行抽样得到的结果集来预设分区边界值</li>
<li><strong>自定义分区</strong>：基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例</li>
<li><strong>Combine</strong>：使用Combine可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据</li>
<li><strong>采用Map Join，尽量避免Reduce Join</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>常用的调优参数</p>
<ul>
<li><p>资源相关参数</p>
<ul>
<li>以下参数是在用户自己的MR应用程序中配置就可以生效(mapred-default.xml)</li>
</ul>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.memory.mb</td>
<td>一个MapTask可使用的资源上限(单位:MB)，默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>一个ReduceTask可使用的资源上限(单位:MB)，默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死</td>
</tr>
<tr>
<td>mapreduce.map.cpu.vcores</td>
<td>每个MapTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.cpu.vcores</td>
<td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.parallelcopies</td>
<td>每个Reduce去Map中取数据的并行数。默认值是5</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.merge.percent</td>
<td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.input.buffer.percent</td>
<td>Buffer大小占Reduce可用内存的比例。默认值0.7</td>
</tr>
<tr>
<td>mapreduce.reduce.input.buffer.percent</td>
<td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td>
</tr>
</tbody></table>
<ul>
<li>应该在YARN启动之前就配置在服务器的配置文件中才能生效(yarn-default.xml)</li>
</ul>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>给应用程序Container分配的最小内存，默认值：1024</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>给应用程序Container分配的最大内存，默认值：8192</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>每个Container申请的最小CPU核数，默认值：1</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>每个Container申请的最大CPU核数，默认值：32</td>
</tr>
<tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>给Containers分配的最大物理内存，默认值：8192</td>
</tr>
</tbody></table>
<ul>
<li>Shuffle性能优化的关键参数，应在YARN启动之前就配置好(mapred-default.xml)</li>
</ul>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.task.io.sort.mb</td>
<td>Shuffle的环形缓冲区大小，默认100m</td>
</tr>
<tr>
<td>mapreduce.map.sort.spill.percent</td>
<td>环形缓冲区溢出的阈值，默认80%</td>
</tr>
</tbody></table>
</li>
<li><p>容错相关参数(MapReduce性能优化)</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4</td>
</tr>
<tr>
<td>mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间(单位毫秒)，默认是600000。如果你的程序对每条输入数据的处理时间过长(比如会访问数据库，通过网络拉取数据等)，建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>HDFS小文件优化方法</p>
<ul>
<li>HDFS小文件弊端<br>HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件。<strong>一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢</strong></li>
<li>HDFS小文件解决方案<ul>
<li>在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS</li>
<li>在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并</li>
<li>在MapReduce处理时，可采用CombineTextInputFormat提高效率</li>
</ul>
</li>
<li>具体方案<ul>
<li>Hadoop Archive<br>是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了NameNode的内存使用</li>
<li>Sequence File<br>Sequence File由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件</li>
<li>CombineFileInputFormat<br>CombineFileInputFormat是一种新的InputFormat，用于将多个文件合并成一个单独的Split，另外，它会考虑数据的存储位置</li>
<li>开启JVM重用<br>对于大量小文件Job，可以开启JVM重用会减少45%运行时间。JVM重用原理：一个Map运行在一个JVM上，开启重用的话，该Map在JVM上运行完毕后，JVM继续运行其他Map。<br>具体设置：mapreduce.job.jvm.numtasks值在10-20之间</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="MapReduce扩展案例"><a href="#MapReduce扩展案例" class="headerlink" title="MapReduce扩展案例"></a>MapReduce扩展案例</h2><ul>
<li><p>倒排索引案例(多job串联)</p>
<ul>
<li><p>需求：有大量的文本(文档、网页)，需要建立搜索索引</p>
</li>
<li><p>案例分析<br><img src="%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95(%E5%A4%9Ajob%E4%B8%B2%E8%81%94)%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="倒排索引(多job串联)案例分析"></p>
</li>
<li><p>第一次处理</p>
<ul>
<li>OneIndexMapper</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneIndexMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String fileName;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">    fileName = fileSplit.getPath().getName();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">" "</span>);</span><br><span class="line">    <span class="comment">// 3、写出</span></span><br><span class="line">    <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line">      k.set(field + <span class="string">"--"</span> + fileName);</span><br><span class="line">      context.write(k, v);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>OneIndexReducer</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneIndexReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 1、累加求和</span></span><br><span class="line">    <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">      sum += value.get();</span><br><span class="line">    &#125;</span><br><span class="line">    v.set(sum);</span><br><span class="line">    <span class="comment">// 2、写出</span></span><br><span class="line">    context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>OneIndexDriver</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(conf);</span><br><span class="line">  job.setJarByClass(OneIndexDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapperClass(OneIndexMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(OneIndexReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>第二次处理</p>
<ul>
<li>TwoIndexMapper</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoIndexMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// haha--a.txt 2</span></span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String lineStr = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = lineStr.split(<span class="string">"--"</span>);</span><br><span class="line">    <span class="comment">// 3、封装</span></span><br><span class="line">    k.set(fields[<span class="number">0</span>]);</span><br><span class="line">    v.set(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="comment">// 4、写出</span></span><br><span class="line">    context.write(k, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>TwoIndexReducer</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoIndexReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">      sb.append(value.toString().replace(<span class="string">"\t"</span>, <span class="string">"--&gt;"</span>))</span><br><span class="line">              .append(<span class="string">'\t'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    v.set(sb.toString());</span><br><span class="line">    context.write(key, v);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>TwoIndexDriver</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">  Configuration config = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(config);</span><br><span class="line">  job.setJarByClass(TwoIndexDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapperClass(TwoIndexMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TwoIndexReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>先运行OneIndexDriver，将得到的输入作为TwoIndexDriver的输入，在运行TwoIndexDriver得到最终结果</p>
</li>
</ul>
</li>
<li><p>TopN案例</p>
<ul>
<li><p>需求：输出流量使用量在前10的用户信息</p>
</li>
<li><p>案例分析<br><img src="Top10%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90.png" alt="Top10案例分析"></p>
</li>
<li><p>代码实现</p>
<ul>
<li>TopNMapper</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="comment">// 定义一个TreeMap作为存储数据的容器(天然按key排序)</span></span><br><span class="line">  <span class="keyword">private</span> TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> TreeMap&lt;FlowBean, Text&gt;();</span><br><span class="line">  <span class="keyword">private</span> FlowBean kBean;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    kBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    Text v = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="comment">// 1、获取一行</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">    <span class="comment">// 3、封装数据</span></span><br><span class="line">    String phoneNum = fields[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">long</span> sumFlow = Long.parseLong(fields[<span class="number">3</span>]);</span><br><span class="line">    kBean.setDownFlow(downFlow);</span><br><span class="line">    kBean.setUpFlow(upFlow);</span><br><span class="line">    kBean.setSumFlow(sumFlow);</span><br><span class="line">    v.set(phoneNum);</span><br><span class="line">    <span class="comment">// 4、向TreeMap中添加数据</span></span><br><span class="line">    flowMap.put(kBean, v);</span><br><span class="line">    <span class="comment">// 5、限制TreeMap的数据量,超过10条就删除掉流量最小的一条数据</span></span><br><span class="line">    <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">      flowMap.remove(flowMap.lastKey());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 6、遍历treeMap集合,输出数据</span></span><br><span class="line">    Iterator&lt;FlowBean&gt; bean = flowMap.keySet().iterator();</span><br><span class="line">    <span class="keyword">while</span> (bean.hasNext()) &#123;</span><br><span class="line">      FlowBean k = bean.next();</span><br><span class="line">      context.write(k, flowMap.get(k));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>TopNReducer</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">// 定义一个TreeMap作为存储数据的容器（天然按key排序）</span></span><br><span class="line">  TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> TreeMap&lt;FlowBean, Text&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">      FlowBean bean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">      bean.set(key.getDownFlow(), key.getUpFlow());</span><br><span class="line">      <span class="comment">// 1、向treeMap集合中添加数据</span></span><br><span class="line">      flowMap.put(bean, <span class="keyword">new</span> Text(value));</span><br><span class="line">      <span class="comment">// 2、限制TreeMap数据量,超过10条就删除掉流量最小的一条数据</span></span><br><span class="line">      <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">10</span>) &#123;</span><br><span class="line">        flowMap.remove(flowMap.lastKey());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 3、遍历集合,输出数据</span></span><br><span class="line">    Iterator&lt;FlowBean&gt; it = flowMap.keySet().iterator();</span><br><span class="line">    <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">      FlowBean v = it.next();</span><br><span class="line">      context.write(<span class="keyword">new</span> Text(flowMap.get(v)), v);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>TopNDriver</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取配置信息,或者job对象实例</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定本程序的jar包所在的本地路径</span></span><br><span class="line">  job.setJarByClass(TopNDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">  job.setMapperClass(TopNMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TopNReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定mapper输出数据的kv类型</span></span><br><span class="line">  job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据的kv类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始文件所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7、将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>找博客共同好友案例</p>
<ul>
<li>需求<br>以下是博客的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友(数据中的好友关系是单向的)。<br>求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？</li>
<li>示例数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:F,A,D,I</span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J</span><br></pre></td></tr></table></figure>

<ul>
<li>案例分析</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 第一次先求出A、B、C...等是谁的好友</span><br><span class="line">A I,K,C,B,G,F,H,O,D</span><br><span class="line">B A,F,J,E</span><br><span class="line">C A,E,B,H,F,G,K</span><br><span class="line">D G,C,K,A,L,F,E,H</span><br><span class="line">E G,M,L,H,A,F,B,D</span><br><span class="line">F L,M,D,C,G,A</span><br><span class="line">G M</span><br><span class="line">H O</span><br><span class="line">I O,C</span><br><span class="line">J O</span><br><span class="line">K B</span><br><span class="line">L D,E</span><br><span class="line">M E,F</span><br><span class="line">O A,H,I,J,F</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 第二次找出共同好友</span><br><span class="line">A-B E C</span><br><span class="line">A-C D F</span><br><span class="line">A-D E F</span><br><span class="line">A-E D B C</span><br><span class="line">A-F O B C D E</span><br><span class="line">A-G F E C D</span><br><span class="line">A-H E C D O</span><br><span class="line">A-I O</span><br><span class="line">A-J O B</span><br><span class="line">A-K D C</span><br><span class="line">A-L F E D</span><br><span class="line">A-M E F</span><br><span class="line">B-C A</span><br><span class="line">B-D A E</span><br><span class="line">B-E C</span><br><span class="line">B-F E A C</span><br><span class="line">B-G C E A</span><br><span class="line">B-H A E C</span><br><span class="line">B-I A</span><br><span class="line">B-K C A</span><br><span class="line">B-L E</span><br><span class="line">B-M E</span><br><span class="line">B-O A</span><br><span class="line">C-D A F</span><br><span class="line">C-E D</span><br><span class="line">C-F D A</span><br><span class="line">C-G D F A</span><br><span class="line">C-H D A</span><br><span class="line">C-I A</span><br><span class="line">C-K A D</span><br><span class="line">C-L D F</span><br><span class="line">C-M F</span><br><span class="line">C-O I A</span><br><span class="line">D-E L</span><br><span class="line">D-F A E</span><br><span class="line">D-G E A F</span><br><span class="line">D-H A E</span><br><span class="line">D-I A</span><br><span class="line">D-K A</span><br><span class="line">D-L E F</span><br><span class="line">D-M F E</span><br><span class="line">D-O A</span><br><span class="line">E-F D M C B</span><br><span class="line">E-G C D</span><br><span class="line">E-H C D</span><br><span class="line">E-J B</span><br><span class="line">E-K C D</span><br><span class="line">E-L D</span><br><span class="line">F-G D C A E</span><br><span class="line">F-H A D O E C</span><br><span class="line">F-I O A</span><br><span class="line">F-J B O</span><br><span class="line">F-K D C A</span><br><span class="line">F-L E D</span><br><span class="line">F-M E</span><br><span class="line">F-O A</span><br><span class="line">G-H D C E A</span><br><span class="line">G-I A</span><br><span class="line">G-K D A C</span><br><span class="line">G-L D F E</span><br><span class="line">G-M E F</span><br><span class="line">G-O A</span><br><span class="line">H-I O A</span><br><span class="line">H-J O</span><br><span class="line">H-K A C D</span><br><span class="line">H-L D E</span><br><span class="line">H-M E</span><br><span class="line">H-O A</span><br><span class="line">I-J O</span><br><span class="line">I-K A</span><br><span class="line">I-O A</span><br><span class="line">K-L D</span><br><span class="line">K-O A</span><br><span class="line">L-M E F</span><br></pre></td></tr></table></figure>

<ul>
<li><p>代码实现</p>
<ul>
<li>第一次Mapper</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneShareFriendsMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 1、获取一行 A:B,C,D,F,E,O</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    <span class="comment">// 2、切割</span></span><br><span class="line">    String[] fields = line.split(<span class="string">":"</span>);</span><br><span class="line">    <span class="comment">// 3、获取person和好友</span></span><br><span class="line">    String person = fields[<span class="number">0</span>];</span><br><span class="line">    String[] friends = fields[<span class="number">1</span>].split(<span class="string">","</span>);</span><br><span class="line">    <span class="comment">// 4、写出去</span></span><br><span class="line">    <span class="keyword">for</span>(String friend: friends)&#123;</span><br><span class="line">      <span class="comment">// 输出 &lt;好友，人&gt;</span></span><br><span class="line">      context.write(<span class="keyword">new</span> Text(friend), <span class="keyword">new</span> Text(person));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第一次Reducer</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneShareFriendsReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">    <span class="comment">// 1、拼接</span></span><br><span class="line">    <span class="keyword">for</span>(Text person: values)&#123;</span><br><span class="line">      sb.append(person).append(<span class="string">","</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 取出多余的','</span></span><br><span class="line">    sb.deleteCharAt(sb.length() - <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 2、写出</span></span><br><span class="line">    context.write(key, <span class="keyword">new</span> Text(sb.toString()));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第一次Driver</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定jar包运行的路径</span></span><br><span class="line">  job.setJarByClass(OneShareFriendsDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定map/reduce使用的类</span></span><br><span class="line">  job.setMapperClass(OneShareFriendsMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(OneShareFriendsReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定map输出的数据类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7 提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第二次Mapper</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoShareFriendsMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// A I,K,C,B,G,F,H,O,D</span></span><br><span class="line">    <span class="comment">// 友 人,人,人</span></span><br><span class="line">    String line = value.toString();</span><br><span class="line">    String[] friend_persons = line.split(<span class="string">"\t"</span>);</span><br><span class="line">    String friend = friend_persons[<span class="number">0</span>];</span><br><span class="line">    String[] persons = friend_persons[<span class="number">1</span>].split(<span class="string">","</span>);</span><br><span class="line">    Arrays.sort(persons);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; persons.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; persons.length; j++) &#123;</span><br><span class="line">        <span class="comment">// 发出 &lt;人-人,好友&gt;,这样，相同的“人-人”对的所有好友就会到同1个reduce中去</span></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(persons[i] + <span class="string">"-"</span> + persons[j]), <span class="keyword">new</span> Text(friend));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第二次Reducer</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoShareFriendsReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">    <span class="keyword">for</span> (Text friend : values) &#123;</span><br><span class="line">      sb.append(friend).append(<span class="string">" "</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    context.write(key, <span class="keyword">new</span> Text(sb.toString()));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第二次Driver</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="comment">// 1、获取job对象</span></span><br><span class="line">  Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">  Job job = Job.getInstance(configuration);</span><br><span class="line">  <span class="comment">// 2、指定jar包运行的路径</span></span><br><span class="line">  job.setJarByClass(TwoShareFriendsDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 3、指定map/reduce使用的类</span></span><br><span class="line">  job.setMapperClass(TwoShareFriendsMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setReducerClass(TwoShareFriendsReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 4、指定map输出的数据类型</span></span><br><span class="line">  job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 5、指定最终输出的数据类型</span></span><br><span class="line">  job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">  <span class="comment">// 6、指定job的输入原始所在目录</span></span><br><span class="line">  FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">  FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">  <span class="comment">// 7 提交</span></span><br><span class="line">  <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">  System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="常见错误及解决方案"><a href="#常见错误及解决方案" class="headerlink" title="常见错误及解决方案"></a>常见错误及解决方案</h2><ul>
<li>导包错误，尤其是Text和CombineTextInputFormat</li>
<li>Mapper中第一个输入的参数必须是LongWritable或者NullWritable，不可以是IntWritable。报的错误是类型转换异常</li>
<li>java.lang.Exception: java.io.IOException: Illegal partition for 13926435656(4)；说明Partition和ReduceTask个数没对上，调整ReduceTask个数</li>
<li>如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行</li>
<li>报类型转换异常：通常都是在驱动函数中设置Map输出和最终输出时编写错误；Map输出的key如果没有排序，也会报类型转换异常</li>
<li>集群中运行wc.jar时出现了无法获得输入文件。原因：WordCount案例的输入文件不能放用HDFS集群的根目录</li>
<li>自定义Outputformat时，注意在RecordWirter中的close方法必须关闭流资源。否则输出的文件内容中数据为空</li>
</ul>

          
            <div class='article_footer'>
              
                
  
    
    



  

  
    
    



  

  
    
    

<section class="widget copyright  desktop mobile">
  <div class='content'>
    
      <blockquote>
        
          
            <p>博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</p>

          
        
          
            <p>本文永久链接是：<a href=https://sobxiong.github.io/2020/06/03/BigData/Hadoop/>https://sobxiong.github.io/2020/06/03/BigData/Hadoop/</a></p>
          
        
      </blockquote>
    
  </div>
</section>

  

  
    
    

<section class="widget qrcode  desktop mobile">
  

  <div class='content article-entry'>
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
  </div>
</section>

  


              
            </div>
          
        </div>
        
          


  <section class='meta' id="footer-meta">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2020-09-20T22:30:07+08:00">
  <a class='notlink'>
    <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：2020年9月20日</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/BigData/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>BigData</p></a></div>


        
      
        
          

        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=https://sobxiong.github.io/2020/06/03/BigData/Hadoop/&title=Hadoop - SOBXiong的博客&summary=内容
概论
Hadoop介绍
环境搭建
Hadoop运行模式
Hadoop编译源码
HDFS概述
HDFS的Shell操作
HDFS客户端操作
HDFS的数据流
NameNode和SecondaryNameNode
DataNode
HDFS2.X新特性
MapReduce概述
Hadoop序列化
MapReduce框架原理
Hadoop数据压缩
Yarn资源调度器
Hadoop企业优化
MapReduce扩展案例
常见错误及解决方案
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://sobxiong.github.io/2020/06/03/BigData/Hadoop/&title=Hadoop - SOBXiong的博客&summary=内容
概论
Hadoop介绍
环境搭建
Hadoop运行模式
Hadoop编译源码
HDFS概述
HDFS的Shell操作
HDFS客户端操作
HDFS的数据流
NameNode和SecondaryNameNode
DataNode
HDFS2.X新特性
MapReduce概述
Hadoop序列化
MapReduce框架原理
Hadoop数据压缩
Yarn资源调度器
Hadoop企业优化
MapReduce扩展案例
常见错误及解决方案
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=https://sobxiong.github.io/2020/06/03/BigData/Hadoop/&title=Hadoop - SOBXiong的博客&summary=内容
概论
Hadoop介绍
环境搭建
Hadoop运行模式
Hadoop编译源码
HDFS概述
HDFS的Shell操作
HDFS客户端操作
HDFS的数据流
NameNode和SecondaryNameNode
DataNode
HDFS2.X新特性
MapReduce概述
Hadoop序列化
MapReduce框架原理
Hadoop数据压缩
Yarn资源调度器
Hadoop企业优化
MapReduce扩展案例
常见错误及解决方案
"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
          <div class="prev-next">
            
              <a class='prev' href='/2020/06/19/Spring/Spring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/'>
                <p class='title'><i class="fas fa-chevron-left" aria-hidden="true"></i>Spring注解驱动开发</p>
                <p class='content'>内容
容器
扩展原理
Web



容器
@Configuration：类注解(配置类=配置文件,注解告诉Spring这是一个配置类)
@Bean：方法注解，在类方法中给出返回Bean的方法，并...</p>
              </a>
            
            
          </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  <article class="post white-box reveal comments shadow">
    <section class="article typo">
      <p ct><i class='fas fa-comments'></i> 评论</p>
      
      
      
      
      
      
        <section id="comments">
          <div id="valine_container" class="valine_thread">
            <i class="fas fa-cog fa-spin fa-fw fa-2x"></i>
          </div>
        </section>
      
      
    </section>
  </article>


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->



  <script>
    window.subData = {
      title: 'Hadoop',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
  

  
    
    



  <section class="widget toc-wrapper shadow desktop mobile" id="toc-div" >
    
  <header>
    
      <i class="fas fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#内容"><span class="toc-text">内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#概论"><span class="toc-text">概论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop介绍"><span class="toc-text">Hadoop介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#环境搭建"><span class="toc-text">环境搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop运行模式"><span class="toc-text">Hadoop运行模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop编译源码"><span class="toc-text">Hadoop编译源码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS概述"><span class="toc-text">HDFS概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS的Shell操作"><span class="toc-text">HDFS的Shell操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS客户端操作"><span class="toc-text">HDFS客户端操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS的数据流"><span class="toc-text">HDFS的数据流</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NameNode和SecondaryNameNode"><span class="toc-text">NameNode和SecondaryNameNode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataNode"><span class="toc-text">DataNode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS2-X新特性"><span class="toc-text">HDFS2.X新特性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce概述"><span class="toc-text">MapReduce概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop序列化"><span class="toc-text">Hadoop序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce框架原理"><span class="toc-text">MapReduce框架原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop数据压缩"><span class="toc-text">Hadoop数据压缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Yarn资源调度器"><span class="toc-text">Yarn资源调度器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop企业优化"><span class="toc-text">Hadoop企业优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce扩展案例"><span class="toc-text">MapReduce扩展案例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#常见错误及解决方案"><span class="toc-text">常见错误及解决方案</span></a></li></ol>
    </div>
  </section>


  


</aside>


  
  <footer class="clearfix">
    <br><br>
    
      
        <br>
        <div class="social-wrapper">
          
            
              <a href="mailto:1942991710@qq.com"
                class="social fas fa-envelope flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
            
              <a href="https://github.com/SOBXiong"
                class="social fab fa-github flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
        </div>
      
    
      
        <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
      
    
      
        <div class='copyright'>
        <p><a href="http://xiongjc.top" target="_blank" rel="noopener">Copyright © 2017-2020 SOBXiong</a></p>

        </div>
      
    
  </footer>

<script>setLoadingBarProgress(80);</script>


      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4/dist/jquery.min.js"></script>


  <script>
    
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/" || "/";
    if (!ROOT.endsWith('/')) ROOT += '/';
  </script>





  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/instant_page.js" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>


  <script src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.6/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      ScrollReveal().reveal('.l_main .reveal', {
        distance: '8px',
        duration: '800',
        interval: '100',
        scale: '1'
      });
    });
  </script>


  
<script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>

  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script defer src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>



  
  
  
    
<script src="https://cdn.jsdelivr.net/npm/jquery-backstretch@2.1.18/jquery.backstretch.min.js"></script>

    <script type="text/javascript">
      $(function(){
        var imgs=["https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg"];
        if ('true' == 'true') {
          function shuffle(arr){
            /*From countercurrent-time*/
            var n = arr.length;
            while(n--) {
              var index = Math.floor(Math.random() * n);
              var temp = arr[index];
              arr[index] = arr[n];
              arr[n] = temp;
            }
          }
          shuffle(imgs);
        }
        if ('.cover') {
          $('.cover').backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        } else {
          $.backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        }
      });
    </script>
  



  
    
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js"></script>

  
    
<script src="https://cdn.jsdelivr.net/npm/meting@2.0/dist/Meting.min.js"></script>

  









  
    
<script src="https://cdn.jsdelivr.net/npm/valine@1.4/dist/Valine.min.js"></script>

  
  <script>
  var GUEST_INFO = ['nick','mail','link'];
  var meta = 'nick,mail,link'.split(',').filter(function(item){
    return GUEST_INFO.indexOf(item) > -1
  });
  var REQUIRED_FIELDS = ['nick','mail','link'];
  var requiredFields = 'nick,mail'.split(',').filter(function(item){
    return REQUIRED_FIELDS.indexOf(item) > -1
  });
  var valine = new Valine();
  function emoji(path, idx, ext) {
      return path + "/" + path + "-" + idx + "." + ext;
  }
  var emojiMaps = {};
  for (var i = 1; i <= 54; i++) {
    emojiMaps['tieba-' + i] = emoji('tieba', i, 'png');
  }
  for (var i = 1; i <= 101; i++) {
    emojiMaps['qq-' + i] = emoji('qq', i, 'gif');
  }
  for (var i = 1; i <= 116; i++) {
    emojiMaps['aru-' + i] = emoji('aru', i, 'gif');
  }
  for (var i = 1; i <= 125; i++) {
    emojiMaps['twemoji-' + i] = emoji('twemoji', i, 'png');
  }
  for (var i = 1; i <= 4; i++) {
    emojiMaps['weibo-' + i] = emoji('weibo', i, 'png');
  }
  valine.init({
    el: '#valine_container',
    meta: meta,
    
    appId: "dogUA2FSKGTo029M1SEwGROT-MdYXbMMI",
    appKey: "u0NdtQ8nvHoMdJPSYqm1LRxE",
    placeholder: "快来评论吧~",
    pageSize:'10',
    avatar:'robohash',
    lang:'zh-cn',
    visitor: 'true',
    highlight: 'true',
    mathJax: 'false',
    enableQQ: 'true',
    requiredFields: requiredFields,
    emojiCDN: 'https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/emoji/valine/',
    emojiMaps: emojiMaps
  })
  </script>





  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2.6.5/js/app.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2.6.5/js/search.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/comment_typing.js"></script>






<!-- 复制 -->

  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="fas fa-copy"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-check-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-check-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-times-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-times-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>




<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  function pjax_fancybox() {
    $(".article-entry").find("img").not('.inline').not('a img').each(function () { //渲染 fancybox
      var element = document.createElement("a"); // a 标签
      $(element).attr("pjax-fancybox", "");  // 过滤 pjax
      $(element).attr("href", $(this).attr("src"));
      if ($(this).attr("data-original")) {
        $(element).attr("href", $(this).attr("data-original"));
      }
      $(element).attr("data-fancybox", "images");
      var caption = "";   // 描述信息
      if ($(this).attr('alt')) {  // 标准 markdown 描述信息
        $(element).attr('data-caption', $(this).attr('alt'));
        caption = $(this).attr('alt');
      }
      var div = document.createElement("div");
      $(div).addClass("fancybox");
      $(this).wrap(div); // 最外层套 div ，其实主要作用还是 class 样式
      var span = document.createElement("span");
      $(span).addClass("image-caption");
      $(span).text(caption); // 加描述
      $(this).after(span);  // 再套一层描述
      $(this).wrap(element);  // 最后套 a 标签
    })
    $(".article-entry").find("img").fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
      closeClick: true,
      helpers: {
        overlay: {closeClick: true}
      },
      buttons: [
        "zoom",
        "close"
      ]
    });
  };
  $(function () {
    pjax_fancybox();
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
